D:\Anaconda\Anaconda\envs\EEG\python.exe D:/学习/毕设/project/train.py
-------------subject: 1-------------
==========valence==========
******fold 0******
[327, 281]
training...
setp: 0, Loss: 0.6926221251487732
setp: 100, Loss: 0.6785602569580078
setp: 200, Loss: 0.6599019169807434
setp: 300, Loss: 0.5297528505325317
setp: 400, Loss: 0.4403536915779114
setp: 500, Loss: 0.35185468196868896
setp: 600, Loss: 0.3341605067253113
setp: 700, Loss: 0.34152114391326904
setp: 800, Loss: 0.3209279179573059
setp: 900, Loss: 0.32002151012420654
setp: 1000, Loss: 0.3214541971683502
setp: 1100, Loss: 0.3190414011478424
setp: 1200, Loss: 0.3185015916824341
setp: 1300, Loss: 0.3192537724971771
setp: 1400, Loss: 0.3170017600059509
setp: 1500, Loss: 0.3182261884212494
setp: 1600, Loss: 0.31748899817466736
setp: 1700, Loss: 0.44571998715400696
setp: 1800, Loss: 0.34723883867263794
setp: 1900, Loss: 0.3264281451702118
setp: 2000, Loss: 0.3225264251232147
setp: 2100, Loss: 0.3221206068992615
setp: 2200, Loss: 0.32137414813041687
setp: 2300, Loss: 0.3213385045528412
setp: 2400, Loss: 0.3200991749763489
setp: 2500, Loss: 0.3208053410053253
setp: 2600, Loss: 0.31985628604888916
setp: 2700, Loss: 0.3202904760837555
setp: 2800, Loss: 0.32071805000305176
setp: 2900, Loss: 0.32148417830467224
setp: 3000, Loss: 0.32313355803489685
setp: 3100, Loss: 0.35126689076423645
setp: 3200, Loss: 0.34479013085365295
setp: 3300, Loss: 0.3223303258419037
setp: 3400, Loss: 0.3228614926338196
setp: 3500, Loss: 0.32158321142196655
setp: 3600, Loss: 0.32214003801345825
setp: 3700, Loss: 0.32283449172973633
setp: 3800, Loss: 0.3223057985305786
setp: 3900, Loss: 0.32152602076530457
setp: 4000, Loss: 0.3215729892253876
setp: 4100, Loss: 0.3215969204902649
setp: 4200, Loss: 0.3221970498561859
setp: 4300, Loss: 0.32129108905792236
setp: 4400, Loss: 0.3213168680667877
setp: 4500, Loss: 0.3208373785018921
setp: 4600, Loss: 0.32130101323127747
setp: 4700, Loss: 0.373423308134079
setp: 4800, Loss: 0.3466109037399292
setp: 4900, Loss: 0.32641467452049255
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.881578947368421
precision: 0.8292682926829268
recall: 0.9444444444444444
F_score: 0.883116883116883
******fold 1******
[318, 290]
training...
setp: 0, Loss: 0.6928985714912415
setp: 100, Loss: 0.6678466796875
setp: 200, Loss: 0.48903244733810425
setp: 300, Loss: 0.3859602212905884
setp: 400, Loss: 0.3282429873943329
setp: 500, Loss: 0.33608657121658325
setp: 600, Loss: 0.3328994810581207
setp: 700, Loss: 0.31966373324394226
setp: 800, Loss: 0.3183138370513916
setp: 900, Loss: 0.3177415132522583
setp: 1000, Loss: 0.31819483637809753
setp: 1100, Loss: 0.31928151845932007
setp: 1200, Loss: 0.31766098737716675
setp: 1300, Loss: 0.3175325393676758
setp: 1400, Loss: 0.31738924980163574
setp: 1500, Loss: 0.31612628698349
setp: 1600, Loss: 0.3197932541370392
setp: 1700, Loss: 0.3180208206176758
setp: 1800, Loss: 0.31822189688682556
setp: 1900, Loss: 0.31638848781585693
setp: 2000, Loss: 0.31838124990463257
setp: 2100, Loss: 0.31756556034088135
setp: 2200, Loss: 0.3160801827907562
setp: 2300, Loss: 0.6936081647872925
setp: 2400, Loss: 0.554935097694397
setp: 2500, Loss: 0.3498792052268982
setp: 2600, Loss: 0.3933641016483307
setp: 2700, Loss: 0.34607550501823425
setp: 2800, Loss: 0.34082648158073425
setp: 2900, Loss: 0.32249951362609863
setp: 3000, Loss: 0.3244827687740326
setp: 3100, Loss: 0.32150664925575256
setp: 3200, Loss: 0.351115345954895
setp: 3300, Loss: 0.32144424319267273
setp: 3400, Loss: 0.31934890151023865
setp: 3500, Loss: 0.32176297903060913
setp: 3600, Loss: 0.32070299983024597
setp: 3700, Loss: 0.3218994736671448
setp: 3800, Loss: 0.31901195645332336
setp: 3900, Loss: 0.32122206687927246
setp: 4000, Loss: 0.32006505131721497
setp: 4100, Loss: 0.7227271199226379
setp: 4200, Loss: 0.44995686411857605
setp: 4300, Loss: 0.3779244124889374
setp: 4400, Loss: 0.3221396505832672
setp: 4500, Loss: 0.3236865997314453
setp: 4600, Loss: 0.31927329301834106
setp: 4700, Loss: 0.3196875751018524
setp: 4800, Loss: 0.31870296597480774
setp: 4900, Loss: 0.31955617666244507
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9968652037617555
recall: 1.0
F_score: 0.9984301412872841
validating...
acc: 0.9605263157894737
precision: 0.9746835443037974
recall: 0.9506172839506173
F_score: 0.9625
******fold 2******
[323, 285]
training...
setp: 0, Loss: 0.6927502155303955
setp: 100, Loss: 0.6737253069877625
setp: 200, Loss: 0.5691264867782593
setp: 300, Loss: 0.5007442831993103
setp: 400, Loss: 0.35350891947746277
setp: 500, Loss: 0.34144070744514465
setp: 600, Loss: 0.36541035771369934
setp: 700, Loss: 0.3311542272567749
setp: 800, Loss: 0.3230571448802948
setp: 900, Loss: 0.3219330608844757
setp: 1000, Loss: 0.32021528482437134
setp: 1100, Loss: 0.32521411776542664
setp: 1200, Loss: 0.3206557631492615
setp: 1300, Loss: 0.32139039039611816
setp: 1400, Loss: 0.3191229999065399
setp: 1500, Loss: 0.31824299693107605
setp: 1600, Loss: 0.3184220790863037
setp: 1700, Loss: 0.3785282373428345
setp: 1800, Loss: 0.3286812901496887
setp: 1900, Loss: 0.31975311040878296
setp: 2000, Loss: 0.31944090127944946
setp: 2100, Loss: 0.31812456250190735
setp: 2200, Loss: 0.3193560838699341
setp: 2300, Loss: 0.31852129101753235
setp: 2400, Loss: 0.3184751272201538
setp: 2500, Loss: 0.31897255778312683
setp: 2600, Loss: 0.31852099299430847
setp: 2700, Loss: 0.31840983033180237
setp: 2800, Loss: 0.33658933639526367
setp: 2900, Loss: 0.3180396556854248
setp: 3000, Loss: 0.3199205994606018
setp: 3100, Loss: 0.3187839686870575
setp: 3200, Loss: 0.3193095922470093
setp: 3300, Loss: 0.3183111250400543
setp: 3400, Loss: 0.3180209994316101
setp: 3500, Loss: 0.31849172711372375
setp: 3600, Loss: 0.3191710412502289
setp: 3700, Loss: 0.319354385137558
setp: 3800, Loss: 0.31872332096099854
setp: 3900, Loss: 0.3201499879360199
setp: 4000, Loss: 0.31726306676864624
setp: 4100, Loss: 0.31838205456733704
setp: 4200, Loss: 0.3181225657463074
setp: 4300, Loss: 0.3182242512702942
setp: 4400, Loss: 0.318841814994812
setp: 4500, Loss: 0.31820639967918396
setp: 4600, Loss: 0.3184567987918854
setp: 4700, Loss: 0.3191859722137451
setp: 4800, Loss: 0.31846609711647034
setp: 4900, Loss: 0.3233635127544403
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.99079754601227
recall: 1.0
F_score: 0.9953775038520801
validating...
acc: 0.9407894736842105
precision: 0.9135802469135802
recall: 0.9736842105263158
F_score: 0.9426751592356688
******fold 3******
[320, 288]
training...
setp: 0, Loss: 0.6955102682113647
setp: 100, Loss: 0.6950311064720154
setp: 200, Loss: 0.6295669078826904
setp: 300, Loss: 0.531138002872467
setp: 400, Loss: 0.3685852289199829
setp: 500, Loss: 0.35243791341781616
setp: 600, Loss: 0.3333566188812256
setp: 700, Loss: 0.32426610589027405
setp: 800, Loss: 0.33378058671951294
setp: 900, Loss: 0.32310789823532104
setp: 1000, Loss: 0.3196101784706116
setp: 1100, Loss: 0.3256552219390869
setp: 1200, Loss: 0.3265642523765564
setp: 1300, Loss: 0.32097938656806946
setp: 1400, Loss: 0.35848015546798706
setp: 1500, Loss: 0.3286716043949127
setp: 1600, Loss: 0.32083436846733093
setp: 1700, Loss: 0.32406723499298096
setp: 1800, Loss: 0.3191555440425873
setp: 1900, Loss: 0.31902942061424255
setp: 2000, Loss: 0.31875836849212646
setp: 2100, Loss: 0.3184090852737427
setp: 2200, Loss: 0.31892186403274536
setp: 2300, Loss: 0.31755542755126953
setp: 2400, Loss: 0.3192358613014221
setp: 2500, Loss: 0.31977295875549316
setp: 2600, Loss: 0.3193225860595703
setp: 2700, Loss: 0.31873953342437744
setp: 2800, Loss: 0.31921717524528503
setp: 2900, Loss: 0.31808027625083923
setp: 3000, Loss: 0.4362994432449341
setp: 3100, Loss: 0.3185637295246124
setp: 3200, Loss: 0.31791579723358154
setp: 3300, Loss: 0.3184684216976166
setp: 3400, Loss: 0.3185372054576874
setp: 3500, Loss: 0.31847360730171204
setp: 3600, Loss: 0.31898024678230286
setp: 3700, Loss: 0.3187800645828247
setp: 3800, Loss: 0.31828227639198303
setp: 3900, Loss: 0.3182830214500427
setp: 4000, Loss: 0.3177745044231415
setp: 4100, Loss: 0.31817543506622314
setp: 4200, Loss: 0.35218945145606995
setp: 4300, Loss: 0.3327621519565582
setp: 4400, Loss: 0.32013529539108276
setp: 4500, Loss: 0.31813502311706543
setp: 4600, Loss: 0.3433523178100586
setp: 4700, Loss: 0.3191780149936676
setp: 4800, Loss: 0.3202633261680603
setp: 4900, Loss: 0.3183271884918213
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9078947368421053
precision: 0.8823529411764706
recall: 0.9493670886075949
F_score: 0.9146341463414634
******fold 4******
[308, 300]
training...
setp: 0, Loss: 0.6882306337356567
setp: 100, Loss: 0.6395663022994995
setp: 200, Loss: 0.4411962330341339
setp: 300, Loss: 0.3468306064605713
setp: 400, Loss: 0.33027711510658264
setp: 500, Loss: 0.35909712314605713
setp: 600, Loss: 0.33768585324287415
setp: 700, Loss: 0.32052090764045715
setp: 800, Loss: 0.32014161348342896
setp: 900, Loss: 0.3212285339832306
setp: 1000, Loss: 0.3214203119277954
setp: 1100, Loss: 0.3202018439769745
setp: 1200, Loss: 0.31831637024879456
setp: 1300, Loss: 0.3199630677700043
setp: 1400, Loss: 0.3182520270347595
setp: 1500, Loss: 0.31788548827171326
setp: 1600, Loss: 0.31873294711112976
setp: 1700, Loss: 0.3177942931652069
setp: 1800, Loss: 0.34848564863204956
setp: 1900, Loss: 0.31793901324272156
setp: 2000, Loss: 0.31691017746925354
setp: 2100, Loss: 0.3742798864841461
setp: 2200, Loss: 0.37193840742111206
setp: 2300, Loss: 0.31941312551498413
setp: 2400, Loss: 0.31878429651260376
setp: 2500, Loss: 0.3181963860988617
setp: 2600, Loss: 0.31773093342781067
setp: 2700, Loss: 0.3175209164619446
setp: 2800, Loss: 0.3182092308998108
setp: 2900, Loss: 0.31897345185279846
setp: 3000, Loss: 0.31850796937942505
setp: 3100, Loss: 0.3183951675891876
setp: 3200, Loss: 0.3187398612499237
setp: 3300, Loss: 0.317471444606781
setp: 3400, Loss: 0.3172686994075775
setp: 3500, Loss: 0.33729860186576843
setp: 3600, Loss: 0.33123570680618286
setp: 3700, Loss: 0.3202833831310272
setp: 3800, Loss: 0.31844934821128845
setp: 3900, Loss: 0.31759604811668396
setp: 4000, Loss: 0.3183792233467102
setp: 4100, Loss: 0.3170849084854126
setp: 4200, Loss: 0.3171810507774353
setp: 4300, Loss: 0.31802406907081604
setp: 4400, Loss: 0.31751173734664917
setp: 4500, Loss: 0.32043230533599854
setp: 4600, Loss: 0.3177022635936737
setp: 4700, Loss: 0.3261682391166687
setp: 4800, Loss: 0.3178115487098694
setp: 4900, Loss: 0.31725090742111206
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9671052631578947
precision: 0.967391304347826
recall: 0.978021978021978
F_score: 0.9726775956284153
model saved.
avg_acc: 0.9315789473684211, avg_f_score: 0.9351207568644861
==========arousal==========
******fold 0******
[258, 350]
training...
setp: 0, Loss: 0.7054228186607361
setp: 100, Loss: 0.6995473504066467
setp: 200, Loss: 0.5892481207847595
setp: 300, Loss: 0.4202750325202942
setp: 400, Loss: 0.3533499240875244
setp: 500, Loss: 0.3410149812698364
setp: 600, Loss: 0.34351587295532227
setp: 700, Loss: 0.319654256105423
setp: 800, Loss: 0.3193466365337372
setp: 900, Loss: 0.3175738453865051
setp: 1000, Loss: 0.32007572054862976
setp: 1100, Loss: 0.3187345564365387
setp: 1200, Loss: 0.3189552426338196
setp: 1300, Loss: 0.3170752227306366
setp: 1400, Loss: 0.3179393708705902
setp: 1500, Loss: 0.3456288278102875
setp: 1600, Loss: 0.31899309158325195
setp: 1700, Loss: 0.3176100254058838
setp: 1800, Loss: 0.3171772360801697
setp: 1900, Loss: 0.31794843077659607
setp: 2000, Loss: 0.3175615668296814
setp: 2100, Loss: 0.31855782866477966
setp: 2200, Loss: 0.31734153628349304
setp: 2300, Loss: 0.3170413672924042
setp: 2400, Loss: 0.3184836804866791
setp: 2500, Loss: 0.31740662455558777
setp: 2600, Loss: 0.31734415888786316
setp: 2700, Loss: 0.6232572793960571
setp: 2800, Loss: 0.4334019422531128
setp: 2900, Loss: 0.4066241979598999
setp: 3000, Loss: 0.34298843145370483
setp: 3100, Loss: 0.35840943455696106
setp: 3200, Loss: 0.3573659658432007
setp: 3300, Loss: 0.34077316522598267
setp: 3400, Loss: 0.35053861141204834
setp: 3500, Loss: 0.36439651250839233
setp: 3600, Loss: 0.3202161192893982
setp: 3700, Loss: 0.3209637403488159
setp: 3800, Loss: 0.3533632457256317
setp: 3900, Loss: 0.38997122645378113
setp: 4000, Loss: 0.32201436161994934
setp: 4100, Loss: 0.3205244243144989
setp: 4200, Loss: 0.3190940320491791
setp: 4300, Loss: 0.319943904876709
setp: 4400, Loss: 0.31890439987182617
setp: 4500, Loss: 0.318965345621109
setp: 4600, Loss: 0.3197307288646698
setp: 4700, Loss: 0.3177044093608856
setp: 4800, Loss: 0.31965601444244385
setp: 4900, Loss: 0.3234964907169342
training successfully ended.
validating...
acc: 0.9917763157894737
precision: 0.9884169884169884
recall: 0.9922480620155039
F_score: 0.9903288201160542
validating...
acc: 0.9276315789473685
precision: 0.8431372549019608
recall: 0.9347826086956522
F_score: 0.8865979381443299
******fold 1******
[238, 370]
training...
setp: 0, Loss: 0.6736111640930176
setp: 100, Loss: 0.6836009621620178
setp: 200, Loss: 0.4227909743785858
setp: 300, Loss: 0.3571338653564453
setp: 400, Loss: 0.34981173276901245
setp: 500, Loss: 0.3303622007369995
setp: 600, Loss: 0.3271488547325134
setp: 700, Loss: 0.3235217034816742
setp: 800, Loss: 0.32305896282196045
setp: 900, Loss: 0.3203336000442505
setp: 1000, Loss: 0.32213684916496277
setp: 1100, Loss: 0.36887189745903015
setp: 1200, Loss: 0.32001498341560364
setp: 1300, Loss: 0.3181079030036926
setp: 1400, Loss: 0.31908008456230164
setp: 1500, Loss: 0.31883567571640015
setp: 1600, Loss: 0.3175845444202423
setp: 1700, Loss: 0.3180091083049774
setp: 1800, Loss: 0.3189135789871216
setp: 1900, Loss: 0.4984332025051117
setp: 2000, Loss: 0.37237000465393066
setp: 2100, Loss: 0.31888827681541443
setp: 2200, Loss: 0.31801995635032654
setp: 2300, Loss: 0.31862249970436096
setp: 2400, Loss: 0.3182304799556732
setp: 2500, Loss: 0.3170979619026184
setp: 2600, Loss: 0.3186948895454407
setp: 2700, Loss: 0.3188491463661194
setp: 2800, Loss: 0.31723594665527344
setp: 2900, Loss: 0.646984875202179
setp: 3000, Loss: 0.4073314666748047
setp: 3100, Loss: 0.3779411017894745
setp: 3200, Loss: 0.3335120677947998
setp: 3300, Loss: 0.3319729268550873
setp: 3400, Loss: 0.3285295069217682
setp: 3500, Loss: 0.33116409182548523
setp: 3600, Loss: 0.32709378004074097
setp: 3700, Loss: 0.32752272486686707
setp: 3800, Loss: 0.32631880044937134
setp: 3900, Loss: 0.32567113637924194
setp: 4000, Loss: 0.32359281182289124
setp: 4100, Loss: 0.34763652086257935
setp: 4200, Loss: 0.33214518427848816
setp: 4300, Loss: 0.32600265741348267
setp: 4400, Loss: 0.32425355911254883
setp: 4500, Loss: 0.32492193579673767
setp: 4600, Loss: 0.3261551558971405
setp: 4700, Loss: 0.3249712586402893
setp: 4800, Loss: 0.3255157768726349
setp: 4900, Loss: 0.32562288641929626
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.868421052631579
precision: 0.859375
recall: 0.8333333333333334
F_score: 0.8461538461538461
******fold 2******
[240, 368]
training...
setp: 0, Loss: 0.6853078603744507
setp: 100, Loss: 0.5551588535308838
setp: 200, Loss: 0.567793071269989
setp: 300, Loss: 0.39526858925819397
setp: 400, Loss: 0.37451034784317017
setp: 500, Loss: 0.3828171193599701
setp: 600, Loss: 0.49584484100341797
setp: 700, Loss: 0.38793620467185974
setp: 800, Loss: 0.4157649278640747
setp: 900, Loss: 0.35061150789260864
setp: 1000, Loss: 0.37236857414245605
setp: 1100, Loss: 0.3424196243286133
setp: 1200, Loss: 0.32215964794158936
setp: 1300, Loss: 0.31799089908599854
setp: 1400, Loss: 0.3191993236541748
setp: 1500, Loss: 0.323383629322052
setp: 1600, Loss: 0.3454776108264923
setp: 1700, Loss: 0.31647881865501404
setp: 1800, Loss: 0.3167910873889923
setp: 1900, Loss: 0.3172449469566345
setp: 2000, Loss: 0.3168170750141144
setp: 2100, Loss: 0.31814244389533997
setp: 2200, Loss: 0.31613075733184814
setp: 2300, Loss: 0.3180999755859375
setp: 2400, Loss: 0.3163520395755768
setp: 2500, Loss: 0.3178364634513855
setp: 2600, Loss: 0.3172886073589325
setp: 2700, Loss: 0.3170520067214966
setp: 2800, Loss: 0.3170902729034424
setp: 2900, Loss: 0.31643542647361755
setp: 3000, Loss: 0.3183326721191406
setp: 3100, Loss: 0.318372905254364
setp: 3200, Loss: 0.4055843651294708
setp: 3300, Loss: 0.32242777943611145
setp: 3400, Loss: 0.3164566457271576
setp: 3500, Loss: 0.3204095959663391
setp: 3600, Loss: 0.31650567054748535
setp: 3700, Loss: 0.31673663854599
setp: 3800, Loss: 0.31738555431365967
setp: 3900, Loss: 0.3162199854850769
setp: 4000, Loss: 0.31786033511161804
setp: 4100, Loss: 0.3164127469062805
setp: 4200, Loss: 0.3174489438533783
setp: 4300, Loss: 0.3165203630924225
setp: 4400, Loss: 0.317534476518631
setp: 4500, Loss: 0.31694889068603516
setp: 4600, Loss: 0.3178165853023529
setp: 4700, Loss: 0.31636425852775574
setp: 4800, Loss: 0.3176209330558777
setp: 4900, Loss: 0.68616783618927
training successfully ended.
validating...
acc: 0.9144736842105263
precision: 0.9122807017543859
recall: 0.8666666666666667
F_score: 0.8888888888888888
validating...
acc: 0.9276315789473685
precision: 0.9491525423728814
recall: 0.875
F_score: 0.9105691056910569
******fold 3******
[242, 366]
training...
setp: 0, Loss: 0.6852958798408508
setp: 100, Loss: 0.630686342716217
setp: 200, Loss: 0.38677817583084106
setp: 300, Loss: 0.34641528129577637
setp: 400, Loss: 0.33174628019332886
setp: 500, Loss: 0.3245944380760193
setp: 600, Loss: 0.3329049050807953
setp: 700, Loss: 0.3202511966228485
setp: 800, Loss: 0.3193366825580597
setp: 900, Loss: 0.3198305368423462
setp: 1000, Loss: 0.3202578127384186
setp: 1100, Loss: 0.31950849294662476
setp: 1200, Loss: 0.31853291392326355
setp: 1300, Loss: 0.3174602687358856
setp: 1400, Loss: 0.31930190324783325
setp: 1500, Loss: 0.6636244058609009
setp: 1600, Loss: 0.3473628759384155
setp: 1700, Loss: 0.3286706507205963
setp: 1800, Loss: 0.35466116666793823
setp: 1900, Loss: 0.3217676877975464
setp: 2000, Loss: 0.32208821177482605
setp: 2100, Loss: 0.3210127353668213
setp: 2200, Loss: 0.32306116819381714
setp: 2300, Loss: 0.3229745626449585
setp: 2400, Loss: 0.32325324416160583
setp: 2500, Loss: 0.32300862669944763
setp: 2600, Loss: 0.32096296548843384
setp: 2700, Loss: 0.32133740186691284
setp: 2800, Loss: 0.3222742974758148
setp: 2900, Loss: 0.322385311126709
setp: 3000, Loss: 0.32266002893447876
setp: 3100, Loss: 0.4109635651111603
setp: 3200, Loss: 0.32438117265701294
setp: 3300, Loss: 0.32196223735809326
setp: 3400, Loss: 0.32196173071861267
setp: 3500, Loss: 0.3235016465187073
setp: 3600, Loss: 0.3222488462924957
setp: 3700, Loss: 0.3228508532047272
setp: 3800, Loss: 0.32238471508026123
setp: 3900, Loss: 0.32125425338745117
setp: 4000, Loss: 0.32032814621925354
setp: 4100, Loss: 0.32218462228775024
setp: 4200, Loss: 0.3224432170391083
setp: 4300, Loss: 0.35743218660354614
setp: 4400, Loss: 0.32380691170692444
setp: 4500, Loss: 0.320621520280838
setp: 4600, Loss: 0.3203861713409424
setp: 4700, Loss: 0.32104119658470154
setp: 4800, Loss: 0.3231687843799591
setp: 4900, Loss: 0.32135581970214844
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9473684210526315
precision: 0.9655172413793104
recall: 0.9032258064516129
F_score: 0.9333333333333333
******fold 4******
[238, 370]
training...
setp: 0, Loss: 0.6897318363189697
setp: 100, Loss: 0.6774246096611023
setp: 200, Loss: 0.7027825713157654
setp: 300, Loss: 0.6638411283493042
setp: 400, Loss: 0.7073372602462769
setp: 500, Loss: 0.6768594980239868
setp: 600, Loss: 0.6755810976028442
setp: 700, Loss: 0.627096951007843
setp: 800, Loss: 0.6922242045402527
setp: 900, Loss: 0.6913039088249207
setp: 1000, Loss: 0.626103937625885
setp: 1100, Loss: 0.5083202719688416
setp: 1200, Loss: 0.425070196390152
setp: 1300, Loss: 0.3339870572090149
setp: 1400, Loss: 0.3219473361968994
setp: 1500, Loss: 0.32796093821525574
setp: 1600, Loss: 0.352626234292984
setp: 1700, Loss: 0.3520289361476898
setp: 1800, Loss: 0.32205474376678467
setp: 1900, Loss: 0.3497295081615448
setp: 2000, Loss: 0.32116493582725525
setp: 2100, Loss: 0.3194951117038727
setp: 2200, Loss: 0.3253677785396576
setp: 2300, Loss: 0.32439619302749634
setp: 2400, Loss: 0.3189881443977356
setp: 2500, Loss: 0.3190617263317108
setp: 2600, Loss: 0.31922751665115356
setp: 2700, Loss: 0.328705370426178
setp: 2800, Loss: 0.3223840892314911
setp: 2900, Loss: 0.3183963894844055
setp: 3000, Loss: 0.3177962303161621
setp: 3100, Loss: 0.3193570077419281
setp: 3200, Loss: 0.31841719150543213
setp: 3300, Loss: 0.318183034658432
setp: 3400, Loss: 0.320308655500412
setp: 3500, Loss: 0.3198080360889435
setp: 3600, Loss: 0.31943994760513306
setp: 3700, Loss: 0.32035303115844727
setp: 3800, Loss: 0.32845667004585266
setp: 3900, Loss: 0.31941893696784973
setp: 4000, Loss: 0.3177112638950348
setp: 4100, Loss: 0.31972965598106384
setp: 4200, Loss: 0.31954053044319153
setp: 4300, Loss: 0.31842947006225586
setp: 4400, Loss: 0.31916192173957825
setp: 4500, Loss: 0.31868505477905273
setp: 4600, Loss: 0.31862226128578186
setp: 4700, Loss: 0.31951409578323364
setp: 4800, Loss: 0.3232133686542511
setp: 4900, Loss: 0.3179360330104828
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9875518672199171
recall: 1.0
F_score: 0.9937369519832986
validating...
acc: 0.9078947368421053
precision: 0.8513513513513513
recall: 0.9545454545454546
F_score: 0.9
model saved.
avg_acc: 0.9157894736842106, avg_f_score: 0.8953308446645133
-------------subject: 2-------------
==========valence==========
******fold 0******
[240, 368]
training...
setp: 0, Loss: 0.6922416687011719
setp: 100, Loss: 0.690068781375885
setp: 200, Loss: 0.5888785719871521
setp: 300, Loss: 0.5945584774017334
setp: 400, Loss: 0.45939332246780396
setp: 500, Loss: 0.4436028301715851
setp: 600, Loss: 0.4601205885410309
setp: 700, Loss: 0.37564894556999207
setp: 800, Loss: 0.3984404504299164
setp: 900, Loss: 0.44626063108444214
setp: 1000, Loss: 0.40965521335601807
setp: 1100, Loss: 0.3398604691028595
setp: 1200, Loss: 0.40936994552612305
setp: 1300, Loss: 0.3774985671043396
setp: 1400, Loss: 0.4124983847141266
setp: 1500, Loss: 0.3840726315975189
setp: 1600, Loss: 0.3725211024284363
setp: 1700, Loss: 0.3657014071941376
setp: 1800, Loss: 0.38414865732192993
setp: 1900, Loss: 0.350575715303421
setp: 2000, Loss: 0.3504011332988739
setp: 2100, Loss: 0.34996336698532104
setp: 2200, Loss: 0.34783801436424255
setp: 2300, Loss: 0.34896785020828247
setp: 2400, Loss: 0.4182968735694885
setp: 2500, Loss: 0.3831755518913269
setp: 2600, Loss: 0.31732168793678284
setp: 2700, Loss: 0.35486525297164917
setp: 2800, Loss: 0.3503511846065521
setp: 2900, Loss: 0.3822382092475891
setp: 3000, Loss: 0.3182085454463959
setp: 3100, Loss: 0.35025399923324585
setp: 3200, Loss: 0.3168559670448303
setp: 3300, Loss: 0.3973563611507416
setp: 3400, Loss: 0.444692999124527
setp: 3500, Loss: 0.3613063395023346
setp: 3600, Loss: 0.3506913185119629
setp: 3700, Loss: 0.35642677545547485
setp: 3800, Loss: 0.3477103114128113
setp: 3900, Loss: 0.34922364354133606
setp: 4000, Loss: 0.3174726068973541
setp: 4100, Loss: 0.3473561704158783
setp: 4200, Loss: 0.34782925248146057
setp: 4300, Loss: 0.34681084752082825
setp: 4400, Loss: 0.556710958480835
setp: 4500, Loss: 0.32219189405441284
setp: 4600, Loss: 0.3507271111011505
setp: 4700, Loss: 0.35123664140701294
setp: 4800, Loss: 0.3803097903728485
setp: 4900, Loss: 0.3180844187736511
training successfully ended.
validating...
acc: 0.975328947368421
precision: 1.0
recall: 0.9375
F_score: 0.967741935483871
validating...
acc: 0.9144736842105263
precision: 0.9
recall: 0.8
F_score: 0.8470588235294118
******fold 1******
[219, 389]
training...
setp: 0, Loss: 0.6257721781730652
setp: 100, Loss: 0.627569854259491
setp: 200, Loss: 0.5855767726898193
setp: 300, Loss: 0.5594418048858643
setp: 400, Loss: 0.44220855832099915
setp: 500, Loss: 0.4065244197845459
setp: 600, Loss: 0.502140462398529
setp: 700, Loss: 0.423893004655838
setp: 800, Loss: 0.4221232831478119
setp: 900, Loss: 0.41777101159095764
setp: 1000, Loss: 0.4062768816947937
setp: 1100, Loss: 0.38181254267692566
setp: 1200, Loss: 0.39759206771850586
setp: 1300, Loss: 0.38518226146698
setp: 1400, Loss: 0.3572319746017456
setp: 1500, Loss: 0.3554195463657379
setp: 1600, Loss: 0.3823927044868469
setp: 1700, Loss: 0.3872578740119934
setp: 1800, Loss: 0.349872887134552
setp: 1900, Loss: 0.3878163695335388
setp: 2000, Loss: 0.35190537571907043
setp: 2100, Loss: 0.4066160023212433
setp: 2200, Loss: 0.383808970451355
setp: 2300, Loss: 0.38750818371772766
setp: 2400, Loss: 0.3490259051322937
setp: 2500, Loss: 0.37965095043182373
setp: 2600, Loss: 0.3834569454193115
setp: 2700, Loss: 0.3805806338787079
setp: 2800, Loss: 0.38099750876426697
setp: 2900, Loss: 0.383270800113678
setp: 3000, Loss: 0.35375842452049255
setp: 3100, Loss: 0.3819217383861542
setp: 3200, Loss: 0.3788408041000366
setp: 3300, Loss: 0.34806638956069946
setp: 3400, Loss: 0.3490009307861328
setp: 3500, Loss: 0.3806266188621521
setp: 3600, Loss: 0.3516823351383209
setp: 3700, Loss: 0.35104167461395264
setp: 3800, Loss: 0.38275420665740967
setp: 3900, Loss: 0.3484487533569336
setp: 4000, Loss: 0.36915451288223267
setp: 4100, Loss: 0.40490785241127014
setp: 4200, Loss: 0.38338932394981384
setp: 4300, Loss: 0.3477707803249359
setp: 4400, Loss: 0.37996262311935425
setp: 4500, Loss: 0.3804318904876709
setp: 4600, Loss: 0.3798633813858032
setp: 4700, Loss: 0.38009586930274963
setp: 4800, Loss: 0.3790300786495209
setp: 4900, Loss: 0.34962576627731323
training successfully ended.
validating...
acc: 0.8453947368421053
precision: 0.7177700348432056
recall: 0.9406392694063926
F_score: 0.8142292490118577
validating...
acc: 0.8157894736842105
precision: 0.7375
recall: 0.8939393939393939
F_score: 0.8082191780821918
******fold 2******
[227, 381]
training...
setp: 0, Loss: 0.6620303392410278
setp: 100, Loss: 0.644216775894165
setp: 200, Loss: 0.6622639298439026
setp: 300, Loss: 0.5635223388671875
setp: 400, Loss: 0.6301854848861694
setp: 500, Loss: 0.5246846675872803
setp: 600, Loss: 0.49632707238197327
setp: 700, Loss: 0.44747546315193176
setp: 800, Loss: 0.4162481725215912
setp: 900, Loss: 0.42480912804603577
setp: 1000, Loss: 0.41300639510154724
setp: 1100, Loss: 0.38261789083480835
setp: 1200, Loss: 0.33934900164604187
setp: 1300, Loss: 0.367045521736145
setp: 1400, Loss: 0.4160110056400299
setp: 1500, Loss: 0.3611242473125458
setp: 1600, Loss: 0.3543873131275177
setp: 1700, Loss: 0.339726984500885
setp: 1800, Loss: 0.3494624197483063
setp: 1900, Loss: 0.31836849451065063
setp: 2000, Loss: 0.3536807596683502
setp: 2100, Loss: 0.3507738709449768
setp: 2200, Loss: 0.3533703684806824
setp: 2300, Loss: 0.3589191436767578
setp: 2400, Loss: 0.3481074273586273
setp: 2500, Loss: 0.35158514976501465
setp: 2600, Loss: 0.3523157835006714
setp: 2700, Loss: 0.352822482585907
setp: 2800, Loss: 0.35258686542510986
setp: 2900, Loss: 0.3480222821235657
setp: 3000, Loss: 0.3508264720439911
setp: 3100, Loss: 0.3203478157520294
setp: 3200, Loss: 0.31644901633262634
setp: 3300, Loss: 0.37859123945236206
setp: 3400, Loss: 0.34952905774116516
setp: 3500, Loss: 0.35035157203674316
setp: 3600, Loss: 0.31794649362564087
setp: 3700, Loss: 0.34878847002983093
setp: 3800, Loss: 0.31807154417037964
setp: 3900, Loss: 0.596498966217041
setp: 4000, Loss: 0.4890824854373932
setp: 4100, Loss: 0.37972599267959595
setp: 4200, Loss: 0.38396748900413513
setp: 4300, Loss: 0.38040849566459656
setp: 4400, Loss: 0.35144320130348206
setp: 4500, Loss: 0.357128381729126
setp: 4600, Loss: 0.3497161865234375
setp: 4700, Loss: 0.39914003014564514
setp: 4800, Loss: 0.35051411390304565
setp: 4900, Loss: 0.3482833504676819
training successfully ended.
validating...
acc: 0.9703947368421053
precision: 1.0
recall: 0.920704845814978
F_score: 0.9587155963302753
validating...
acc: 0.9078947368421053
precision: 0.9583333333333334
recall: 0.7931034482758621
F_score: 0.8679245283018867
******fold 3******
[237, 371]
training...
setp: 0, Loss: 0.7178872227668762
setp: 100, Loss: 0.6321487426757812
setp: 200, Loss: 0.6231072545051575
setp: 300, Loss: 0.5942216515541077
setp: 400, Loss: 0.5785267353057861
setp: 500, Loss: 0.4391174614429474
setp: 600, Loss: 0.46773120760917664
setp: 700, Loss: 0.47605428099632263
setp: 800, Loss: 0.3685533106327057
setp: 900, Loss: 0.4640602469444275
setp: 1000, Loss: 0.39420801401138306
setp: 1100, Loss: 0.41633158922195435
setp: 1200, Loss: 0.3928196430206299
setp: 1300, Loss: 0.416565865278244
setp: 1400, Loss: 0.41304871439933777
setp: 1500, Loss: 0.39718398451805115
setp: 1600, Loss: 0.4035612642765045
setp: 1700, Loss: 0.3788561224937439
setp: 1800, Loss: 0.41270580887794495
setp: 1900, Loss: 0.38533204793930054
setp: 2000, Loss: 0.4272247552871704
setp: 2100, Loss: 0.397126704454422
setp: 2200, Loss: 0.38090020418167114
setp: 2300, Loss: 0.3611376881599426
setp: 2400, Loss: 0.35123157501220703
setp: 2500, Loss: 0.35118311643600464
setp: 2600, Loss: 0.35267630219459534
setp: 2700, Loss: 0.42492544651031494
setp: 2800, Loss: 0.3618433475494385
setp: 2900, Loss: 0.37056764960289
setp: 3000, Loss: 0.3496871888637543
setp: 3100, Loss: 0.35153695940971375
setp: 3200, Loss: 0.3502335250377655
setp: 3300, Loss: 0.38030368089675903
setp: 3400, Loss: 0.3605029284954071
setp: 3500, Loss: 0.3561316132545471
setp: 3600, Loss: 0.316885381937027
setp: 3700, Loss: 0.3835482597351074
setp: 3800, Loss: 0.36362868547439575
setp: 3900, Loss: 0.35073554515838623
setp: 4000, Loss: 0.3655449151992798
setp: 4100, Loss: 0.34863343834877014
setp: 4200, Loss: 0.3222675621509552
setp: 4300, Loss: 0.3600294291973114
setp: 4400, Loss: 0.31868332624435425
setp: 4500, Loss: 0.35263791680336
setp: 4600, Loss: 0.31687986850738525
setp: 4700, Loss: 0.3227764666080475
setp: 4800, Loss: 0.34262338280677795
setp: 4900, Loss: 0.3539896607398987
training successfully ended.
validating...
acc: 0.9539473684210527
precision: 1.0
recall: 0.8818565400843882
F_score: 0.937219730941704
validating...
acc: 0.8881578947368421
precision: 0.9428571428571428
recall: 0.6875
F_score: 0.7951807228915663
******fold 4******
[217, 391]
training...
setp: 0, Loss: 0.7364928126335144
setp: 100, Loss: 0.6211498975753784
setp: 200, Loss: 0.6147212386131287
setp: 300, Loss: 0.5782349705696106
setp: 400, Loss: 0.6337680220603943
setp: 500, Loss: 0.6001588702201843
setp: 600, Loss: 0.5182948112487793
setp: 700, Loss: 0.4431867003440857
setp: 800, Loss: 0.38856378197669983
setp: 900, Loss: 0.41882073879241943
setp: 1000, Loss: 0.39458978176116943
setp: 1100, Loss: 0.41398099064826965
setp: 1200, Loss: 0.41492336988449097
setp: 1300, Loss: 0.387051522731781
setp: 1400, Loss: 0.4727272391319275
setp: 1500, Loss: 0.3868097960948944
setp: 1600, Loss: 0.3418121039867401
setp: 1700, Loss: 0.36483505368232727
setp: 1800, Loss: 0.3794820308685303
setp: 1900, Loss: 0.3825206160545349
setp: 2000, Loss: 0.3518050014972687
setp: 2100, Loss: 0.35076138377189636
setp: 2200, Loss: 0.3187251389026642
setp: 2300, Loss: 0.39105215668678284
setp: 2400, Loss: 0.4110575020313263
setp: 2500, Loss: 0.34826141595840454
setp: 2600, Loss: 0.40112465620040894
setp: 2700, Loss: 0.3192245662212372
setp: 2800, Loss: 0.3619266152381897
setp: 2900, Loss: 0.351176381111145
setp: 3000, Loss: 0.4094529151916504
setp: 3100, Loss: 0.413116991519928
setp: 3200, Loss: 0.37907862663269043
setp: 3300, Loss: 0.4130101501941681
setp: 3400, Loss: 0.3826455771923065
setp: 3500, Loss: 0.3270816206932068
setp: 3600, Loss: 0.3520723283290863
setp: 3700, Loss: 0.3788782060146332
setp: 3800, Loss: 0.37862277030944824
setp: 3900, Loss: 0.31964999437332153
setp: 4000, Loss: 0.3615017831325531
setp: 4100, Loss: 0.3289923667907715
setp: 4200, Loss: 0.424042284488678
setp: 4300, Loss: 0.4098624289035797
setp: 4400, Loss: 0.34840330481529236
setp: 4500, Loss: 0.3788391053676605
setp: 4600, Loss: 0.3182334303855896
setp: 4700, Loss: 0.3491152822971344
setp: 4800, Loss: 0.337084025144577
setp: 4900, Loss: 0.3555631935596466
training successfully ended.
validating...
acc: 0.9671052631578947
precision: 0.966824644549763
recall: 0.9400921658986175
F_score: 0.9532710280373832
validating...
acc: 0.8947368421052632
precision: 0.8611111111111112
recall: 0.9117647058823529
F_score: 0.8857142857142858
model saved.
avg_acc: 0.8842105263157893, avg_f_score: 0.8408195077038686
==========arousal==========
******fold 0******
[253, 355]
training...
setp: 0, Loss: 0.708590030670166
setp: 100, Loss: 0.6994175314903259
setp: 200, Loss: 0.6692206263542175
setp: 300, Loss: 0.5838298797607422
setp: 400, Loss: 0.5292971134185791
setp: 500, Loss: 0.5068874955177307
setp: 600, Loss: 0.4405016303062439
setp: 700, Loss: 0.3621879518032074
setp: 800, Loss: 0.36407721042633057
setp: 900, Loss: 0.3937615156173706
setp: 1000, Loss: 0.39467400312423706
setp: 1100, Loss: 0.3921996057033539
setp: 1200, Loss: 0.3342403173446655
setp: 1300, Loss: 0.35725775361061096
setp: 1400, Loss: 0.32876715064048767
setp: 1500, Loss: 0.36118611693382263
setp: 1600, Loss: 0.348044753074646
setp: 1700, Loss: 0.32221508026123047
setp: 1800, Loss: 0.32142117619514465
setp: 1900, Loss: 0.3190406560897827
setp: 2000, Loss: 0.3200206458568573
setp: 2100, Loss: 0.31867051124572754
setp: 2200, Loss: 0.3195897936820984
setp: 2300, Loss: 0.37398648262023926
setp: 2400, Loss: 0.31913816928863525
setp: 2500, Loss: 0.3296848237514496
setp: 2600, Loss: 0.3212742805480957
setp: 2700, Loss: 0.32110562920570374
setp: 2800, Loss: 0.350729376077652
setp: 2900, Loss: 0.3194757103919983
setp: 3000, Loss: 0.31879645586013794
setp: 3100, Loss: 0.31880658864974976
setp: 3200, Loss: 0.31951990723609924
setp: 3300, Loss: 0.3196983337402344
setp: 3400, Loss: 0.3219105005264282
setp: 3500, Loss: 0.3204399049282074
setp: 3600, Loss: 0.320854127407074
setp: 3700, Loss: 0.32078540325164795
setp: 3800, Loss: 0.37055906653404236
setp: 3900, Loss: 0.324756383895874
setp: 4000, Loss: 0.3196966052055359
setp: 4100, Loss: 0.32715508341789246
setp: 4200, Loss: 0.32171860337257385
setp: 4300, Loss: 0.31850874423980713
setp: 4400, Loss: 0.319151371717453
setp: 4500, Loss: 0.31988152861595154
setp: 4600, Loss: 0.32012736797332764
setp: 4700, Loss: 0.3500668704509735
setp: 4800, Loss: 0.3207128643989563
setp: 4900, Loss: 0.320354163646698
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9960474308300395
F_score: 0.998019801980198
validating...
acc: 0.8947368421052632
precision: 0.8571428571428571
recall: 0.8235294117647058
F_score: 0.84
******fold 1******
[240, 368]
training...
setp: 0, Loss: 0.6671103835105896
setp: 100, Loss: 0.6487255096435547
setp: 200, Loss: 0.6310098767280579
setp: 300, Loss: 0.532116174697876
setp: 400, Loss: 0.4608357548713684
setp: 500, Loss: 0.36657822132110596
setp: 600, Loss: 0.38664737343788147
setp: 700, Loss: 0.35689741373062134
setp: 800, Loss: 0.36284154653549194
setp: 900, Loss: 0.3483317792415619
setp: 1000, Loss: 0.35663262009620667
setp: 1100, Loss: 0.33276525139808655
setp: 1200, Loss: 0.32792896032333374
setp: 1300, Loss: 0.32180479168891907
setp: 1400, Loss: 0.36114266514778137
setp: 1500, Loss: 0.3272501826286316
setp: 1600, Loss: 0.3307788670063019
setp: 1700, Loss: 0.32566913962364197
setp: 1800, Loss: 0.318147748708725
setp: 1900, Loss: 0.3244945704936981
setp: 2000, Loss: 0.3323894441127777
setp: 2100, Loss: 0.3247453272342682
setp: 2200, Loss: 0.3211935758590698
setp: 2300, Loss: 0.3359792232513428
setp: 2400, Loss: 0.3198077082633972
setp: 2500, Loss: 0.32703766226768494
setp: 2600, Loss: 0.31911909580230713
setp: 2700, Loss: 0.34855732321739197
setp: 2800, Loss: 0.32126766443252563
setp: 2900, Loss: 0.32078608870506287
setp: 3000, Loss: 0.31969156861305237
setp: 3100, Loss: 0.31918755173683167
setp: 3200, Loss: 0.3193238377571106
setp: 3300, Loss: 0.31986555457115173
setp: 3400, Loss: 0.3201865255832672
setp: 3500, Loss: 0.31875288486480713
setp: 3600, Loss: 0.3213440775871277
setp: 3700, Loss: 0.31910738348960876
setp: 3800, Loss: 0.3200041949748993
setp: 3900, Loss: 0.3202885687351227
setp: 4000, Loss: 0.32032081484794617
setp: 4100, Loss: 0.4013122618198395
setp: 4200, Loss: 0.3705001473426819
setp: 4300, Loss: 0.3197195529937744
setp: 4400, Loss: 0.32156190276145935
setp: 4500, Loss: 0.3195752501487732
setp: 4600, Loss: 0.32758092880249023
setp: 4700, Loss: 0.31871166825294495
setp: 4800, Loss: 0.3212163746356964
setp: 4900, Loss: 0.3211266100406647
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9013157894736842
precision: 0.855072463768116
recall: 0.921875
F_score: 0.887218045112782
******fold 2******
[245, 363]
training...
setp: 0, Loss: 0.6835557818412781
setp: 100, Loss: 0.6483091115951538
setp: 200, Loss: 0.5513427257537842
setp: 300, Loss: 0.5090370774269104
setp: 400, Loss: 0.4349798858165741
setp: 500, Loss: 0.41820043325424194
setp: 600, Loss: 0.3381673991680145
setp: 700, Loss: 0.33022329211235046
setp: 800, Loss: 0.3299484848976135
setp: 900, Loss: 0.4187757968902588
setp: 1000, Loss: 0.35400786995887756
setp: 1100, Loss: 0.3239811360836029
setp: 1200, Loss: 0.32093214988708496
setp: 1300, Loss: 0.33412909507751465
setp: 1400, Loss: 0.3260292112827301
setp: 1500, Loss: 0.32673555612564087
setp: 1600, Loss: 0.3208463490009308
setp: 1700, Loss: 0.32120800018310547
setp: 1800, Loss: 0.3205748200416565
setp: 1900, Loss: 0.31985586881637573
setp: 2000, Loss: 0.3204887807369232
setp: 2100, Loss: 0.3198069632053375
setp: 2200, Loss: 0.3194291889667511
setp: 2300, Loss: 0.32023948431015015
setp: 2400, Loss: 0.318327933549881
setp: 2500, Loss: 0.3191153109073639
setp: 2600, Loss: 0.31898680329322815
setp: 2700, Loss: 0.4929344058036804
setp: 2800, Loss: 0.35268595814704895
setp: 2900, Loss: 0.3398989140987396
setp: 3000, Loss: 0.32074931263923645
setp: 3100, Loss: 0.31779783964157104
setp: 3200, Loss: 0.31983229517936707
setp: 3300, Loss: 0.3437761068344116
setp: 3400, Loss: 0.3202357590198517
setp: 3500, Loss: 0.3192461133003235
setp: 3600, Loss: 0.32017508149147034
setp: 3700, Loss: 0.32092392444610596
setp: 3800, Loss: 0.31930387020111084
setp: 3900, Loss: 0.3199426531791687
setp: 4000, Loss: 0.3193454146385193
setp: 4100, Loss: 0.31949350237846375
setp: 4200, Loss: 0.31999024748802185
setp: 4300, Loss: 0.3179956078529358
setp: 4400, Loss: 0.31945860385894775
setp: 4500, Loss: 0.3190438151359558
setp: 4600, Loss: 0.33330973982810974
setp: 4700, Loss: 0.3611275553703308
setp: 4800, Loss: 0.3278745412826538
setp: 4900, Loss: 0.3409412205219269
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9919028340080972
recall: 1.0
F_score: 0.9959349593495934
validating...
acc: 0.9539473684210527
precision: 0.9333333333333333
recall: 0.9491525423728814
F_score: 0.9411764705882353
******fold 3******
[240, 368]
training...
setp: 0, Loss: 0.7290261387825012
setp: 100, Loss: 0.6814665794372559
setp: 200, Loss: 0.663020133972168
setp: 300, Loss: 0.5196800231933594
setp: 400, Loss: 0.40296560525894165
setp: 500, Loss: 0.38452768325805664
setp: 600, Loss: 0.42143315076828003
setp: 700, Loss: 0.3835221827030182
setp: 800, Loss: 0.36780691146850586
setp: 900, Loss: 0.36604878306388855
setp: 1000, Loss: 0.3560939133167267
setp: 1100, Loss: 0.34350791573524475
setp: 1200, Loss: 0.3305228054523468
setp: 1300, Loss: 0.3995547592639923
setp: 1400, Loss: 0.3629821240901947
setp: 1500, Loss: 0.33777204155921936
setp: 1600, Loss: 0.3416688144207001
setp: 1700, Loss: 0.31961894035339355
setp: 1800, Loss: 0.3192228674888611
setp: 1900, Loss: 0.3500310182571411
setp: 2000, Loss: 0.320056289434433
setp: 2100, Loss: 0.3235761225223541
setp: 2200, Loss: 0.31979408860206604
setp: 2300, Loss: 0.3197368383407593
setp: 2400, Loss: 0.3197919726371765
setp: 2500, Loss: 0.3195524513721466
setp: 2600, Loss: 0.3492271602153778
setp: 2700, Loss: 0.32005825638771057
setp: 2800, Loss: 0.5742989778518677
setp: 2900, Loss: 0.4401530623435974
setp: 3000, Loss: 0.41511473059654236
setp: 3100, Loss: 0.3416401445865631
setp: 3200, Loss: 0.33837470412254333
setp: 3300, Loss: 0.3352105915546417
setp: 3400, Loss: 0.3494034707546234
setp: 3500, Loss: 0.3349679112434387
setp: 3600, Loss: 0.32439959049224854
setp: 3700, Loss: 0.3304080665111542
setp: 3800, Loss: 0.32886072993278503
setp: 3900, Loss: 0.32219961285591125
setp: 4000, Loss: 0.32725369930267334
setp: 4100, Loss: 0.3277053236961365
setp: 4200, Loss: 0.3300405740737915
setp: 4300, Loss: 0.3306252360343933
setp: 4400, Loss: 0.3224278688430786
setp: 4500, Loss: 0.3212340474128723
setp: 4600, Loss: 0.32252010703086853
setp: 4700, Loss: 0.3209380507469177
setp: 4800, Loss: 0.32243940234184265
setp: 4900, Loss: 0.3219583034515381
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8552631578947368
precision: 0.828125
recall: 0.828125
F_score: 0.828125
******fold 4******
[238, 370]
training...
setp: 0, Loss: 0.6752694845199585
setp: 100, Loss: 0.6328573226928711
setp: 200, Loss: 0.5311042666435242
setp: 300, Loss: 0.45801421999931335
setp: 400, Loss: 0.45664164423942566
setp: 500, Loss: 0.4399178922176361
setp: 600, Loss: 0.3598107695579529
setp: 700, Loss: 0.3458232879638672
setp: 800, Loss: 0.3540097177028656
setp: 900, Loss: 0.4060944616794586
setp: 1000, Loss: 0.33618852496147156
setp: 1100, Loss: 0.33143967390060425
setp: 1200, Loss: 0.32604771852493286
setp: 1300, Loss: 0.32821956276893616
setp: 1400, Loss: 0.3298487663269043
setp: 1500, Loss: 0.3572002947330475
setp: 1600, Loss: 0.32398921251296997
setp: 1700, Loss: 0.32127392292022705
setp: 1800, Loss: 0.3245736062526703
setp: 1900, Loss: 0.35002845525741577
setp: 2000, Loss: 0.3260694146156311
setp: 2100, Loss: 0.3255554735660553
setp: 2200, Loss: 0.3215600252151489
setp: 2300, Loss: 0.3292749226093292
setp: 2400, Loss: 0.33150362968444824
setp: 2500, Loss: 0.318637877702713
setp: 2600, Loss: 0.31992998719215393
setp: 2700, Loss: 0.3200736343860626
setp: 2800, Loss: 0.32102909684181213
setp: 2900, Loss: 0.32105356454849243
setp: 3000, Loss: 0.32067325711250305
setp: 3100, Loss: 0.3206784129142761
setp: 3200, Loss: 0.32166948914527893
setp: 3300, Loss: 0.32156458497047424
setp: 3400, Loss: 0.3233312964439392
setp: 3500, Loss: 0.40873849391937256
setp: 3600, Loss: 0.38056400418281555
setp: 3700, Loss: 0.3558140993118286
setp: 3800, Loss: 0.324323445558548
setp: 3900, Loss: 0.3229397237300873
setp: 4000, Loss: 0.3702048361301422
setp: 4100, Loss: 0.38757702708244324
setp: 4200, Loss: 0.31968897581100464
setp: 4300, Loss: 0.3181718587875366
setp: 4400, Loss: 0.31798332929611206
setp: 4500, Loss: 0.3199675977230072
setp: 4600, Loss: 0.31927311420440674
setp: 4700, Loss: 0.3198639452457428
setp: 4800, Loss: 0.32006338238716125
setp: 4900, Loss: 0.3196620047092438
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9013157894736842
precision: 0.9322033898305084
recall: 0.8333333333333334
F_score: 0.8800000000000001
model saved.
avg_acc: 0.9013157894736843, avg_f_score: 0.8753039031402035
-------------subject: 3-------------
==========valence==========
******fold 0******
[273, 335]
training...
setp: 0, Loss: 0.6940183043479919
setp: 100, Loss: 0.6760560274124146
setp: 200, Loss: 0.6077060699462891
setp: 300, Loss: 0.5781570076942444
setp: 400, Loss: 0.4513639211654663
setp: 500, Loss: 0.48082444071769714
setp: 600, Loss: 0.4466622471809387
setp: 700, Loss: 0.37285712361335754
setp: 800, Loss: 0.32408666610717773
setp: 900, Loss: 0.37667587399482727
setp: 1000, Loss: 0.3190796971321106
setp: 1100, Loss: 0.3514305353164673
setp: 1200, Loss: 0.35138556361198425
setp: 1300, Loss: 0.3539440929889679
setp: 1400, Loss: 0.3221144676208496
setp: 1500, Loss: 0.35337013006210327
setp: 1600, Loss: 0.3478318452835083
setp: 1700, Loss: 0.3635755777359009
setp: 1800, Loss: 0.3518300950527191
setp: 1900, Loss: 0.3528868556022644
setp: 2000, Loss: 0.41809743642807007
setp: 2100, Loss: 0.3546782433986664
setp: 2200, Loss: 0.3602116107940674
setp: 2300, Loss: 0.31877052783966064
setp: 2400, Loss: 0.355324387550354
setp: 2500, Loss: 0.3555319905281067
setp: 2600, Loss: 0.3579743504524231
setp: 2700, Loss: 0.31769296526908875
setp: 2800, Loss: 0.353964239358902
setp: 2900, Loss: 0.3433211147785187
setp: 3000, Loss: 0.34817731380462646
setp: 3100, Loss: 0.3470730781555176
setp: 3200, Loss: 0.35075172781944275
setp: 3300, Loss: 0.33004051446914673
setp: 3400, Loss: 0.35829782485961914
setp: 3500, Loss: 0.32034003734588623
setp: 3600, Loss: 0.32467740774154663
setp: 3700, Loss: 0.34363457560539246
setp: 3800, Loss: 0.38807418942451477
setp: 3900, Loss: 0.3215442895889282
setp: 4000, Loss: 0.3179762661457062
setp: 4100, Loss: 0.33445999026298523
setp: 4200, Loss: 0.3168284595012665
setp: 4300, Loss: 0.31845396757125854
setp: 4400, Loss: 0.3196937143802643
setp: 4500, Loss: 0.3335631489753723
setp: 4600, Loss: 0.32438018918037415
setp: 4700, Loss: 0.3198372423648834
setp: 4800, Loss: 0.3170732259750366
setp: 4900, Loss: 0.3177783489227295
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9963235294117647
recall: 0.9926739926739927
F_score: 0.9944954128440368
validating...
acc: 0.9407894736842105
precision: 0.9545454545454546
recall: 0.9130434782608695
F_score: 0.9333333333333332
******fold 1******
[273, 335]
training...
setp: 0, Loss: 0.761026918888092
setp: 100, Loss: 0.6643933653831482
setp: 200, Loss: 0.5817314386367798
setp: 300, Loss: 0.4455777406692505
setp: 400, Loss: 0.4526013731956482
setp: 500, Loss: 0.34967514872550964
setp: 600, Loss: 0.3888700604438782
setp: 700, Loss: 0.3597109615802765
setp: 800, Loss: 0.3803532123565674
setp: 900, Loss: 0.32231420278549194
setp: 1000, Loss: 0.3514331877231598
setp: 1100, Loss: 0.3873143196105957
setp: 1200, Loss: 0.34938305616378784
setp: 1300, Loss: 0.3242352604866028
setp: 1400, Loss: 0.4391108453273773
setp: 1500, Loss: 0.3528197407722473
setp: 1600, Loss: 0.31906166672706604
setp: 1700, Loss: 0.34909844398498535
setp: 1800, Loss: 0.33506321907043457
setp: 1900, Loss: 0.35124319791793823
setp: 2000, Loss: 0.3593308627605438
setp: 2100, Loss: 0.3516395688056946
setp: 2200, Loss: 0.34743326902389526
setp: 2300, Loss: 0.3478650450706482
setp: 2400, Loss: 0.3210597336292267
setp: 2500, Loss: 0.3500831425189972
setp: 2600, Loss: 0.3653712868690491
setp: 2700, Loss: 0.3267398178577423
setp: 2800, Loss: 0.31870636343955994
setp: 2900, Loss: 0.31928059458732605
setp: 3000, Loss: 0.31849244236946106
setp: 3100, Loss: 0.31663885712623596
setp: 3200, Loss: 0.3181181848049164
setp: 3300, Loss: 0.31875374913215637
setp: 3400, Loss: 0.3201465308666229
setp: 3500, Loss: 0.3173087537288666
setp: 3600, Loss: 0.31704720854759216
setp: 3700, Loss: 0.3725072145462036
setp: 3800, Loss: 0.3463979959487915
setp: 3900, Loss: 0.3496721684932709
setp: 4000, Loss: 0.3388281464576721
setp: 4100, Loss: 0.31761860847473145
setp: 4200, Loss: 0.3329228460788727
setp: 4300, Loss: 0.3161967396736145
setp: 4400, Loss: 0.31974926590919495
setp: 4500, Loss: 0.31803539395332336
setp: 4600, Loss: 0.31750163435935974
setp: 4700, Loss: 0.3165636956691742
setp: 4800, Loss: 0.3172396719455719
setp: 4900, Loss: 0.35267966985702515
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9539473684210527
precision: 0.9696969696969697
recall: 0.927536231884058
F_score: 0.9481481481481481
******fold 2******
[285, 323]
training...
setp: 0, Loss: 0.6984755992889404
setp: 100, Loss: 0.6076462268829346
setp: 200, Loss: 0.6337206959724426
setp: 300, Loss: 0.501844048500061
setp: 400, Loss: 0.4841997027397156
setp: 500, Loss: 0.4661436378955841
setp: 600, Loss: 0.4470236897468567
setp: 700, Loss: 0.38465172052383423
setp: 800, Loss: 0.41933247447013855
setp: 900, Loss: 0.3556460738182068
setp: 1000, Loss: 0.37336426973342896
setp: 1100, Loss: 0.3315691649913788
setp: 1200, Loss: 0.33615005016326904
setp: 1300, Loss: 0.3530230224132538
setp: 1400, Loss: 0.3178863525390625
setp: 1500, Loss: 0.3483278751373291
setp: 1600, Loss: 0.32739970088005066
setp: 1700, Loss: 0.3278154730796814
setp: 1800, Loss: 0.32443496584892273
setp: 1900, Loss: 0.3189237713813782
setp: 2000, Loss: 0.31741002202033997
setp: 2100, Loss: 0.31841060519218445
setp: 2200, Loss: 0.3166212737560272
setp: 2300, Loss: 0.3186676502227783
setp: 2400, Loss: 0.31695207953453064
setp: 2500, Loss: 0.31854984164237976
setp: 2600, Loss: 0.3177660405635834
setp: 2700, Loss: 0.3199712038040161
setp: 2800, Loss: 0.3189947009086609
setp: 2900, Loss: 0.3183470070362091
setp: 3000, Loss: 0.3169086277484894
setp: 3100, Loss: 0.48780956864356995
setp: 3200, Loss: 0.323212206363678
setp: 3300, Loss: 0.3183959722518921
setp: 3400, Loss: 0.317636102437973
setp: 3500, Loss: 0.3165922462940216
setp: 3600, Loss: 0.31782156229019165
setp: 3700, Loss: 0.3166414797306061
setp: 3800, Loss: 0.31686562299728394
setp: 3900, Loss: 0.3165895938873291
setp: 4000, Loss: 0.3175632655620575
setp: 4100, Loss: 0.5581565499305725
setp: 4200, Loss: 0.4663025438785553
setp: 4300, Loss: 0.3522988259792328
setp: 4400, Loss: 0.31852248311042786
setp: 4500, Loss: 0.31665661931037903
setp: 4600, Loss: 0.3184266984462738
setp: 4700, Loss: 0.3181910514831543
setp: 4800, Loss: 0.3175531029701233
setp: 4900, Loss: 0.3171157240867615
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9137931034482759
recall: 0.9298245614035088
F_score: 0.9217391304347825
******fold 3******
[272, 336]
training...
setp: 0, Loss: 0.6769961714744568
setp: 100, Loss: 0.6003896594047546
setp: 200, Loss: 0.6235111355781555
setp: 300, Loss: 0.42686140537261963
setp: 400, Loss: 0.4659084379673004
setp: 500, Loss: 0.4566323459148407
setp: 600, Loss: 0.44517844915390015
setp: 700, Loss: 0.32524290680885315
setp: 800, Loss: 0.4173913300037384
setp: 900, Loss: 0.38135480880737305
setp: 1000, Loss: 0.4144148826599121
setp: 1100, Loss: 0.3568771779537201
setp: 1200, Loss: 0.35163646936416626
setp: 1300, Loss: 0.4265930950641632
setp: 1400, Loss: 0.38597455620765686
setp: 1500, Loss: 0.38408729434013367
setp: 1600, Loss: 0.34942826628685
setp: 1700, Loss: 0.3749516010284424
setp: 1800, Loss: 0.3543113172054291
setp: 1900, Loss: 0.31994113326072693
setp: 2000, Loss: 0.3800482153892517
setp: 2100, Loss: 0.35398098826408386
setp: 2200, Loss: 0.42054229974746704
setp: 2300, Loss: 0.34895122051239014
setp: 2400, Loss: 0.35823723673820496
setp: 2500, Loss: 0.35204994678497314
setp: 2600, Loss: 0.33062776923179626
setp: 2700, Loss: 0.34903281927108765
setp: 2800, Loss: 0.33474797010421753
setp: 2900, Loss: 0.3471483886241913
setp: 3000, Loss: 0.31648215651512146
setp: 3100, Loss: 0.349032461643219
setp: 3200, Loss: 0.34778109192848206
setp: 3300, Loss: 0.43019115924835205
setp: 3400, Loss: 0.31878840923309326
setp: 3500, Loss: 0.32197293639183044
setp: 3600, Loss: 0.31809133291244507
setp: 3700, Loss: 0.325423002243042
setp: 3800, Loss: 0.3435128927230835
setp: 3900, Loss: 0.3528231978416443
setp: 4000, Loss: 0.3961974084377289
setp: 4100, Loss: 0.321446031332016
setp: 4200, Loss: 0.3497515022754669
setp: 4300, Loss: 0.33006978034973145
setp: 4400, Loss: 0.3257942795753479
setp: 4500, Loss: 0.318158358335495
setp: 4600, Loss: 0.32857006788253784
setp: 4700, Loss: 0.3360292315483093
setp: 4800, Loss: 0.3199802041053772
setp: 4900, Loss: 0.324885755777359
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.9926470588235294
F_score: 0.996309963099631
validating...
acc: 0.9539473684210527
precision: 0.9701492537313433
recall: 0.9285714285714286
F_score: 0.948905109489051
******fold 4******
[265, 343]
training...
setp: 0, Loss: 0.7001610398292542
setp: 100, Loss: 0.5902318954467773
setp: 200, Loss: 0.5218716263771057
setp: 300, Loss: 0.37908774614334106
setp: 400, Loss: 0.40825155377388
setp: 500, Loss: 0.44317004084587097
setp: 600, Loss: 0.3827006220817566
setp: 700, Loss: 0.37785080075263977
setp: 800, Loss: 0.3575836420059204
setp: 900, Loss: 0.3244849741458893
setp: 1000, Loss: 0.34929952025413513
setp: 1100, Loss: 0.35120466351509094
setp: 1200, Loss: 0.316114217042923
setp: 1300, Loss: 0.34902286529541016
setp: 1400, Loss: 0.3801021873950958
setp: 1500, Loss: 0.3552868366241455
setp: 1600, Loss: 0.35383886098861694
setp: 1700, Loss: 0.3658860921859741
setp: 1800, Loss: 0.35542625188827515
setp: 1900, Loss: 0.35855790972709656
setp: 2000, Loss: 0.35244324803352356
setp: 2100, Loss: 0.35532087087631226
setp: 2200, Loss: 0.31715846061706543
setp: 2300, Loss: 0.35750100016593933
setp: 2400, Loss: 0.33392590284347534
setp: 2500, Loss: 0.34037625789642334
setp: 2600, Loss: 0.35216882824897766
setp: 2700, Loss: 0.35572341084480286
setp: 2800, Loss: 0.3211607038974762
setp: 2900, Loss: 0.34925031661987305
setp: 3000, Loss: 0.3478148281574249
setp: 3100, Loss: 0.31614115834236145
setp: 3200, Loss: 0.35135263204574585
setp: 3300, Loss: 0.3509242832660675
setp: 3400, Loss: 0.3616963028907776
setp: 3500, Loss: 0.35928988456726074
setp: 3600, Loss: 0.3179416060447693
setp: 3700, Loss: 0.37718284130096436
setp: 3800, Loss: 0.3281657099723816
setp: 3900, Loss: 0.3253578841686249
setp: 4000, Loss: 0.316700279712677
setp: 4100, Loss: 0.3851882517337799
setp: 4200, Loss: 0.323762983083725
setp: 4300, Loss: 0.32278650999069214
setp: 4400, Loss: 0.3169659674167633
setp: 4500, Loss: 0.32153064012527466
setp: 4600, Loss: 0.32032227516174316
setp: 4700, Loss: 0.31785985827445984
setp: 4800, Loss: 0.31692856550216675
setp: 4900, Loss: 0.32531192898750305
training successfully ended.
validating...
acc: 0.9917763157894737
precision: 1.0
recall: 0.9811320754716981
F_score: 0.9904761904761905
validating...
acc: 0.9671052631578947
precision: 1.0
recall: 0.935064935064935
F_score: 0.9664429530201343
model saved.
avg_acc: 0.9513157894736842, avg_f_score: 0.9437137348850898
==========arousal==========
******fold 0******
[482, 126]
training...
setp: 0, Loss: 0.6943020820617676
setp: 100, Loss: 0.5471053719520569
setp: 200, Loss: 0.4704297184944153
setp: 300, Loss: 0.3942893147468567
setp: 400, Loss: 0.43697017431259155
setp: 500, Loss: 0.4213048219680786
setp: 600, Loss: 0.36649736762046814
setp: 700, Loss: 0.32770803570747375
setp: 800, Loss: 0.33543097972869873
setp: 900, Loss: 0.3279581367969513
setp: 1000, Loss: 0.31744566559791565
setp: 1100, Loss: 0.3181571960449219
setp: 1200, Loss: 0.316854864358902
setp: 1300, Loss: 0.31981831789016724
setp: 1400, Loss: 0.3204367458820343
setp: 1500, Loss: 0.32010650634765625
setp: 1600, Loss: 0.3166073262691498
setp: 1700, Loss: 0.3176875710487366
setp: 1800, Loss: 0.31858015060424805
setp: 1900, Loss: 0.3174264132976532
setp: 2000, Loss: 0.3179282248020172
setp: 2100, Loss: 0.3191068470478058
setp: 2200, Loss: 0.3162580728530884
setp: 2300, Loss: 0.3167436718940735
setp: 2400, Loss: 0.32487952709198
setp: 2500, Loss: 0.31604093313217163
setp: 2600, Loss: 0.316636323928833
setp: 2700, Loss: 0.3159501850605011
setp: 2800, Loss: 0.3189798593521118
setp: 2900, Loss: 0.3170467019081116
setp: 3000, Loss: 0.3165111243724823
setp: 3100, Loss: 0.31604787707328796
setp: 3200, Loss: 0.44502004981040955
setp: 3300, Loss: 0.3479345142841339
setp: 3400, Loss: 0.3199480175971985
setp: 3500, Loss: 0.3196624219417572
setp: 3600, Loss: 0.3243579864501953
setp: 3700, Loss: 0.3201129138469696
setp: 3800, Loss: 0.31791386008262634
setp: 3900, Loss: 0.31833285093307495
setp: 4000, Loss: 0.3180699050426483
setp: 4100, Loss: 0.3174651861190796
setp: 4200, Loss: 0.34164074063301086
setp: 4300, Loss: 0.3354717195034027
setp: 4400, Loss: 0.3158939778804779
setp: 4500, Loss: 0.3180347681045532
setp: 4600, Loss: 0.31793585419654846
setp: 4700, Loss: 0.31720083951950073
setp: 4800, Loss: 0.31943273544311523
setp: 4900, Loss: 0.3178575932979584
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9868421052631579
precision: 0.984375
recall: 1.0
F_score: 0.9921259842519685
******fold 1******
[491, 117]
training...
setp: 0, Loss: 0.6944108009338379
setp: 100, Loss: 0.4985859990119934
setp: 200, Loss: 0.5128575563430786
setp: 300, Loss: 0.4151950478553772
setp: 400, Loss: 0.39450541138648987
setp: 500, Loss: 0.4442094564437866
setp: 600, Loss: 0.4238347113132477
setp: 700, Loss: 0.5119207501411438
setp: 800, Loss: 0.3534044325351715
setp: 900, Loss: 0.37494659423828125
setp: 1000, Loss: 0.3170503079891205
setp: 1100, Loss: 0.31691861152648926
setp: 1200, Loss: 0.34668663144111633
setp: 1300, Loss: 0.3165127635002136
setp: 1400, Loss: 0.31763672828674316
setp: 1500, Loss: 0.31731611490249634
setp: 1600, Loss: 0.3476736545562744
setp: 1700, Loss: 0.3481244444847107
setp: 1800, Loss: 0.34705880284309387
setp: 1900, Loss: 0.3486982583999634
setp: 2000, Loss: 0.3158735930919647
setp: 2100, Loss: 0.3170744776725769
setp: 2200, Loss: 0.3187527358531952
setp: 2300, Loss: 0.4794211983680725
setp: 2400, Loss: 0.3268289864063263
setp: 2500, Loss: 0.3161880075931549
setp: 2600, Loss: 0.31642016768455505
setp: 2700, Loss: 0.31719160079956055
setp: 2800, Loss: 0.3208335041999817
setp: 2900, Loss: 0.41352853178977966
setp: 3000, Loss: 0.41127443313598633
setp: 3100, Loss: 0.34094002842903137
setp: 3200, Loss: 0.34683722257614136
setp: 3300, Loss: 0.34655821323394775
setp: 3400, Loss: 0.34681445360183716
setp: 3500, Loss: 0.34698259830474854
setp: 3600, Loss: 0.3497307598590851
setp: 3700, Loss: 0.37775713205337524
setp: 3800, Loss: 0.3179547190666199
setp: 3900, Loss: 0.31589576601982117
setp: 4000, Loss: 0.31761813163757324
setp: 4100, Loss: 0.3162212371826172
setp: 4200, Loss: 0.3160427510738373
setp: 4300, Loss: 0.343533456325531
setp: 4400, Loss: 0.31701844930648804
setp: 4500, Loss: 0.316019743680954
setp: 4600, Loss: 0.31584984064102173
setp: 4700, Loss: 0.34767553210258484
setp: 4800, Loss: 0.3468441963195801
setp: 4900, Loss: 0.3472629189491272
training successfully ended.
validating...
acc: 0.9867617107942973
precision: 0.9760956175298805
recall: 0.9979633401221996
F_score: 0.9869083585095668
validating...
acc: 0.9276315789473685
precision: 0.9344262295081968
recall: 0.9743589743589743
F_score: 0.9539748953974896
******fold 2******
[484, 124]
training...
setp: 0, Loss: 0.7069253921508789
setp: 100, Loss: 0.613215446472168
setp: 200, Loss: 0.44710013270378113
setp: 300, Loss: 0.4125170111656189
setp: 400, Loss: 0.4209824502468109
setp: 500, Loss: 0.40318357944488525
setp: 600, Loss: 0.33244287967681885
setp: 700, Loss: 0.44374412298202515
setp: 800, Loss: 0.3512246012687683
setp: 900, Loss: 0.34683337807655334
setp: 1000, Loss: 0.31913647055625916
setp: 1100, Loss: 0.3409665822982788
setp: 1200, Loss: 0.34856539964675903
setp: 1300, Loss: 0.3176436126232147
setp: 1400, Loss: 0.35019874572753906
setp: 1500, Loss: 0.3181367516517639
setp: 1600, Loss: 0.3167590796947479
setp: 1700, Loss: 0.3184376657009125
setp: 1800, Loss: 0.3170919716358185
setp: 1900, Loss: 0.317546546459198
setp: 2000, Loss: 0.34515658020973206
setp: 2100, Loss: 0.31829389929771423
setp: 2200, Loss: 0.3164607286453247
setp: 2300, Loss: 0.3166833817958832
setp: 2400, Loss: 0.3183058798313141
setp: 2500, Loss: 0.3167652189731598
setp: 2600, Loss: 0.3177300989627838
setp: 2700, Loss: 0.31628409028053284
setp: 2800, Loss: 0.3165511190891266
setp: 2900, Loss: 0.31651589274406433
setp: 3000, Loss: 0.332590788602829
setp: 3100, Loss: 0.3186379671096802
setp: 3200, Loss: 0.3178374171257019
setp: 3300, Loss: 0.31624555587768555
setp: 3400, Loss: 0.31583425402641296
setp: 3500, Loss: 0.31707170605659485
setp: 3600, Loss: 0.317306250333786
setp: 3700, Loss: 0.31573954224586487
setp: 3800, Loss: 0.3218691945075989
setp: 3900, Loss: 0.3157430589199066
setp: 4000, Loss: 0.31655898690223694
setp: 4100, Loss: 0.31512266397476196
setp: 4200, Loss: 0.31983324885368347
setp: 4300, Loss: 0.3193458020687103
setp: 4400, Loss: 0.3158336281776428
setp: 4500, Loss: 0.3162562847137451
setp: 4600, Loss: 0.31601208448410034
setp: 4700, Loss: 0.3165171146392822
setp: 4800, Loss: 0.3161507248878479
setp: 4900, Loss: 0.3159838020801544
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9802631578947368
precision: 0.984
recall: 0.9919354838709677
F_score: 0.9879518072289156
******fold 3******
[482, 126]
training...
setp: 0, Loss: 0.6931498050689697
setp: 100, Loss: 0.5653239488601685
setp: 200, Loss: 0.4073588252067566
setp: 300, Loss: 0.3582627773284912
setp: 400, Loss: 0.40073496103286743
setp: 500, Loss: 0.35125306248664856
setp: 600, Loss: 0.33995628356933594
setp: 700, Loss: 0.3289744555950165
setp: 800, Loss: 0.323441743850708
setp: 900, Loss: 0.31846120953559875
setp: 1000, Loss: 0.3164193034172058
setp: 1100, Loss: 0.3190326392650604
setp: 1200, Loss: 0.3169059455394745
setp: 1300, Loss: 0.31866174936294556
setp: 1400, Loss: 0.4363488554954529
setp: 1500, Loss: 0.31644728779792786
setp: 1600, Loss: 0.31651920080184937
setp: 1700, Loss: 0.31761112809181213
setp: 1800, Loss: 0.3179134726524353
setp: 1900, Loss: 0.3225502073764801
setp: 2000, Loss: 0.31825488805770874
setp: 2100, Loss: 0.32008689641952515
setp: 2200, Loss: 0.3159855008125305
setp: 2300, Loss: 0.3160317838191986
setp: 2400, Loss: 0.3195977807044983
setp: 2500, Loss: 0.3173742890357971
setp: 2600, Loss: 0.3170974552631378
setp: 2700, Loss: 0.31608137488365173
setp: 2800, Loss: 0.3174192011356354
setp: 2900, Loss: 0.3175800144672394
setp: 3000, Loss: 0.31941431760787964
setp: 3100, Loss: 0.3158330023288727
setp: 3200, Loss: 0.3178369998931885
setp: 3300, Loss: 0.3159763813018799
setp: 3400, Loss: 0.317030131816864
setp: 3500, Loss: 0.3390690088272095
setp: 3600, Loss: 0.3482150435447693
setp: 3700, Loss: 0.31691253185272217
setp: 3800, Loss: 0.3183015286922455
setp: 3900, Loss: 0.3226962983608246
setp: 4000, Loss: 0.31775495409965515
setp: 4100, Loss: 0.3156009614467621
setp: 4200, Loss: 0.31745707988739014
setp: 4300, Loss: 0.31568512320518494
setp: 4400, Loss: 0.31707486510276794
setp: 4500, Loss: 0.3175352215766907
setp: 4600, Loss: 0.31560802459716797
setp: 4700, Loss: 0.320267915725708
setp: 4800, Loss: 0.3170454204082489
setp: 4900, Loss: 0.3187737464904785
training successfully ended.
validating...
acc: 0.9865145228215768
precision: 1.0
recall: 0.9730290456431535
F_score: 0.9863301787592008
validating...
acc: 0.9407894736842105
precision: 1.0
recall: 0.9285714285714286
F_score: 0.962962962962963
******fold 4******
[493, 115]
training...
setp: 0, Loss: 0.6929450631141663
setp: 100, Loss: 0.49068930745124817
setp: 200, Loss: 0.4371868371963501
setp: 300, Loss: 0.41957777738571167
setp: 400, Loss: 0.35768139362335205
setp: 500, Loss: 0.36665377020835876
setp: 600, Loss: 0.3502300977706909
setp: 700, Loss: 0.3187304735183716
setp: 800, Loss: 0.3186955451965332
setp: 900, Loss: 0.3167875409126282
setp: 1000, Loss: 0.31663233041763306
setp: 1100, Loss: 0.31616702675819397
setp: 1200, Loss: 0.31735724210739136
setp: 1300, Loss: 0.328013151884079
setp: 1400, Loss: 0.3176381587982178
setp: 1500, Loss: 0.3172687888145447
setp: 1600, Loss: 0.3169691264629364
setp: 1700, Loss: 0.31810280680656433
setp: 1800, Loss: 0.31659719347953796
setp: 1900, Loss: 0.4016338586807251
setp: 2000, Loss: 0.31656625866889954
setp: 2100, Loss: 0.31680363416671753
setp: 2200, Loss: 0.31593090295791626
setp: 2300, Loss: 0.34746697545051575
setp: 2400, Loss: 0.3670526146888733
setp: 2500, Loss: 0.3154454529285431
setp: 2600, Loss: 0.3162685036659241
setp: 2700, Loss: 0.3166778087615967
setp: 2800, Loss: 0.3160001039505005
setp: 2900, Loss: 0.3287915885448456
setp: 3000, Loss: 0.31718170642852783
setp: 3100, Loss: 0.3163195252418518
setp: 3200, Loss: 0.3161866366863251
setp: 3300, Loss: 0.3165743052959442
setp: 3400, Loss: 0.36623507738113403
setp: 3500, Loss: 0.3472723960876465
setp: 3600, Loss: 0.317412406206131
setp: 3700, Loss: 0.3163504898548126
setp: 3800, Loss: 0.3164503276348114
setp: 3900, Loss: 0.31706833839416504
setp: 4000, Loss: 0.3163052499294281
setp: 4100, Loss: 0.31515803933143616
setp: 4200, Loss: 0.31530046463012695
setp: 4300, Loss: 0.31623491644859314
setp: 4400, Loss: 0.31654858589172363
setp: 4500, Loss: 0.32660967111587524
setp: 4600, Loss: 0.3157886266708374
setp: 4700, Loss: 0.3161194920539856
setp: 4800, Loss: 0.3163989186286926
setp: 4900, Loss: 0.3168841600418091
training successfully ended.
validating...
acc: 0.9959432048681541
precision: 0.9919517102615694
recall: 1.0
F_score: 0.9959595959595959
validating...
acc: 0.9868421052631579
precision: 0.991304347826087
recall: 0.991304347826087
F_score: 0.991304347826087
model saved.
avg_acc: 0.9644736842105264, avg_f_score: 0.9776639995334847
-------------subject: 4-------------
==========valence==========
******fold 0******
[375, 233]
training...
setp: 0, Loss: 0.6523122191429138
setp: 100, Loss: 0.6189966201782227
setp: 200, Loss: 0.564852237701416
setp: 300, Loss: 0.4961392879486084
setp: 400, Loss: 0.5116220116615295
setp: 500, Loss: 0.4302402436733246
setp: 600, Loss: 0.3870311379432678
setp: 700, Loss: 0.36482372879981995
setp: 800, Loss: 0.43270450830459595
setp: 900, Loss: 0.36294811964035034
setp: 1000, Loss: 0.3239244520664215
setp: 1100, Loss: 0.3363380432128906
setp: 1200, Loss: 0.32491734623908997
setp: 1300, Loss: 0.3570541441440582
setp: 1400, Loss: 0.3298693299293518
setp: 1500, Loss: 0.32836318016052246
setp: 1600, Loss: 0.323984295129776
setp: 1700, Loss: 0.3231702446937561
setp: 1800, Loss: 0.3210752308368683
setp: 1900, Loss: 0.4571751058101654
setp: 2000, Loss: 0.3415013253688812
setp: 2100, Loss: 0.3205586373806
setp: 2200, Loss: 0.3525300621986389
setp: 2300, Loss: 0.3247717320919037
setp: 2400, Loss: 0.31806719303131104
setp: 2500, Loss: 0.355678528547287
setp: 2600, Loss: 0.3511500358581543
setp: 2700, Loss: 0.32019558548927307
setp: 2800, Loss: 0.3214486837387085
setp: 2900, Loss: 0.32109335064888
setp: 3000, Loss: 0.31776776909828186
setp: 3100, Loss: 0.31909656524658203
setp: 3200, Loss: 0.32038694620132446
setp: 3300, Loss: 0.3261740207672119
setp: 3400, Loss: 0.3624158203601837
setp: 3500, Loss: 0.3489049971103668
setp: 3600, Loss: 0.32611003518104553
setp: 3700, Loss: 0.32263463735580444
setp: 3800, Loss: 0.36389708518981934
setp: 3900, Loss: 0.3216032087802887
setp: 4000, Loss: 0.3285798728466034
setp: 4100, Loss: 0.357574999332428
setp: 4200, Loss: 0.3209664225578308
setp: 4300, Loss: 0.31863394379615784
setp: 4400, Loss: 0.32368049025535583
setp: 4500, Loss: 0.3213714361190796
setp: 4600, Loss: 0.35568270087242126
setp: 4700, Loss: 0.3173524737358093
setp: 4800, Loss: 0.3326151967048645
setp: 4900, Loss: 0.32255253195762634
training successfully ended.
validating...
acc: 0.9736842105263158
precision: 0.959079283887468
recall: 1.0
F_score: 0.97911227154047
validating...
acc: 0.8947368421052632
precision: 0.8350515463917526
recall: 1.0
F_score: 0.9101123595505618
******fold 1******
[356, 252]
training...
setp: 0, Loss: 0.6754444241523743
setp: 100, Loss: 0.6386826634407043
setp: 200, Loss: 0.5405564904212952
setp: 300, Loss: 0.5129282474517822
setp: 400, Loss: 0.49916625022888184
setp: 500, Loss: 0.4205416142940521
setp: 600, Loss: 0.40271660685539246
setp: 700, Loss: 0.3990500271320343
setp: 800, Loss: 0.3648056983947754
setp: 900, Loss: 0.3402281403541565
setp: 1000, Loss: 0.3940541446208954
setp: 1100, Loss: 0.3832595944404602
setp: 1200, Loss: 0.3266056478023529
setp: 1300, Loss: 0.40417173504829407
setp: 1400, Loss: 0.3525088131427765
setp: 1500, Loss: 0.33132535219192505
setp: 1600, Loss: 0.3212885856628418
setp: 1700, Loss: 0.32768383622169495
setp: 1800, Loss: 0.32124093174934387
setp: 1900, Loss: 0.36029934883117676
setp: 2000, Loss: 0.3510349988937378
setp: 2100, Loss: 0.32514819502830505
setp: 2200, Loss: 0.3212999701499939
setp: 2300, Loss: 0.3219357430934906
setp: 2400, Loss: 0.32076728343963623
setp: 2500, Loss: 0.3196261525154114
setp: 2600, Loss: 0.32197192311286926
setp: 2700, Loss: 0.3214869499206543
setp: 2800, Loss: 0.3493959307670593
setp: 2900, Loss: 0.34374985098838806
setp: 3000, Loss: 0.32021138072013855
setp: 3100, Loss: 0.32329750061035156
setp: 3200, Loss: 0.31942033767700195
setp: 3300, Loss: 0.3228011429309845
setp: 3400, Loss: 0.3200899064540863
setp: 3500, Loss: 0.321178674697876
setp: 3600, Loss: 0.32270368933677673
setp: 3700, Loss: 0.31922245025634766
setp: 3800, Loss: 0.32244786620140076
setp: 3900, Loss: 0.34052419662475586
setp: 4000, Loss: 0.32286232709884644
setp: 4100, Loss: 0.3441197872161865
setp: 4200, Loss: 0.3309375047683716
setp: 4300, Loss: 0.32076138257980347
setp: 4400, Loss: 0.3188670873641968
setp: 4500, Loss: 0.3193305432796478
setp: 4600, Loss: 0.31967654824256897
setp: 4700, Loss: 0.320028156042099
setp: 4800, Loss: 0.3206197917461395
setp: 4900, Loss: 0.3190026581287384
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.868421052631579
precision: 0.9
recall: 0.9
F_score: 0.9
******fold 2******
[365, 243]
training...
setp: 0, Loss: 0.7299889922142029
setp: 100, Loss: 0.6578737497329712
setp: 200, Loss: 0.5452849268913269
setp: 300, Loss: 0.5495331883430481
setp: 400, Loss: 0.5325650572776794
setp: 500, Loss: 0.3896234631538391
setp: 600, Loss: 0.38925328850746155
setp: 700, Loss: 0.3438832461833954
setp: 800, Loss: 0.34091490507125854
setp: 900, Loss: 0.3441927433013916
setp: 1000, Loss: 0.36928775906562805
setp: 1100, Loss: 0.3349563777446747
setp: 1200, Loss: 0.3406655192375183
setp: 1300, Loss: 0.354167103767395
setp: 1400, Loss: 0.35343706607818604
setp: 1500, Loss: 0.3541480004787445
setp: 1600, Loss: 0.32160162925720215
setp: 1700, Loss: 0.3266371786594391
setp: 1800, Loss: 0.3228895366191864
setp: 1900, Loss: 0.3310685157775879
setp: 2000, Loss: 0.3304881453514099
setp: 2100, Loss: 0.3255250155925751
setp: 2200, Loss: 0.3225061595439911
setp: 2300, Loss: 0.3702394962310791
setp: 2400, Loss: 0.3233804404735565
setp: 2500, Loss: 0.3223282992839813
setp: 2600, Loss: 0.3267064690589905
setp: 2700, Loss: 0.32091212272644043
setp: 2800, Loss: 0.3505428731441498
setp: 2900, Loss: 0.3212113082408905
setp: 3000, Loss: 0.3339178264141083
setp: 3100, Loss: 0.3196471333503723
setp: 3200, Loss: 0.32546791434288025
setp: 3300, Loss: 0.35273241996765137
setp: 3400, Loss: 0.3556634485721588
setp: 3500, Loss: 0.32043924927711487
setp: 3600, Loss: 0.3662523031234741
setp: 3700, Loss: 0.3463701009750366
setp: 3800, Loss: 0.3349657356739044
setp: 3900, Loss: 0.3234274089336395
setp: 4000, Loss: 0.32502424716949463
setp: 4100, Loss: 0.32506993412971497
setp: 4200, Loss: 0.43671849370002747
setp: 4300, Loss: 0.319568008184433
setp: 4400, Loss: 0.32662394642829895
setp: 4500, Loss: 0.3188101053237915
setp: 4600, Loss: 0.3212490379810333
setp: 4700, Loss: 0.3207032382488251
setp: 4800, Loss: 0.32493114471435547
setp: 4900, Loss: 0.3253127336502075
training successfully ended.
validating...
acc: 0.9901315789473685
precision: 0.9838274932614556
recall: 1.0
F_score: 0.9918478260869567
validating...
acc: 0.8881578947368421
precision: 0.8557692307692307
recall: 0.978021978021978
F_score: 0.9128205128205128
******fold 3******
[374, 234]
training...
setp: 0, Loss: 0.6998609900474548
setp: 100, Loss: 0.6426578164100647
setp: 200, Loss: 0.5512523055076599
setp: 300, Loss: 0.5300157070159912
setp: 400, Loss: 0.5424104332923889
setp: 500, Loss: 0.44670766592025757
setp: 600, Loss: 0.5270028114318848
setp: 700, Loss: 0.43351271748542786
setp: 800, Loss: 0.44247570633888245
setp: 900, Loss: 0.5531615614891052
setp: 1000, Loss: 0.40032678842544556
setp: 1100, Loss: 0.3926962614059448
setp: 1200, Loss: 0.36397120356559753
setp: 1300, Loss: 0.3590630888938904
setp: 1400, Loss: 0.403725802898407
setp: 1500, Loss: 0.41242486238479614
setp: 1600, Loss: 0.39115822315216064
setp: 1700, Loss: 0.36501479148864746
setp: 1800, Loss: 0.32209885120391846
setp: 1900, Loss: 0.34219008684158325
setp: 2000, Loss: 0.32593968510627747
setp: 2100, Loss: 0.3239217698574066
setp: 2200, Loss: 0.31971120834350586
setp: 2300, Loss: 0.3231867849826813
setp: 2400, Loss: 0.369463175535202
setp: 2500, Loss: 0.32751956582069397
setp: 2600, Loss: 0.3785367012023926
setp: 2700, Loss: 0.33046606183052063
setp: 2800, Loss: 0.3212081789970398
setp: 2900, Loss: 0.32085883617401123
setp: 3000, Loss: 0.3200479745864868
setp: 3100, Loss: 0.31985220313072205
setp: 3200, Loss: 0.32008224725723267
setp: 3300, Loss: 0.3205225467681885
setp: 3400, Loss: 0.32189977169036865
setp: 3500, Loss: 0.32081636786460876
setp: 3600, Loss: 0.31983622908592224
setp: 3700, Loss: 0.3196258842945099
setp: 3800, Loss: 0.5498170256614685
setp: 3900, Loss: 0.44261443614959717
setp: 4000, Loss: 0.33274105191230774
setp: 4100, Loss: 0.3198249340057373
setp: 4200, Loss: 0.3198919892311096
setp: 4300, Loss: 0.3185473084449768
setp: 4400, Loss: 0.3203561007976532
setp: 4500, Loss: 0.31818464398384094
setp: 4600, Loss: 0.31883254647254944
setp: 4700, Loss: 0.3225961923599243
setp: 4800, Loss: 0.36776402592658997
setp: 4900, Loss: 0.3268967568874359
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.9176470588235294
recall: 0.9512195121951219
F_score: 0.9341317365269461
******fold 4******
[354, 254]
training...
setp: 0, Loss: 0.6641865372657776
setp: 100, Loss: 0.6605499386787415
setp: 200, Loss: 0.5578585863113403
setp: 300, Loss: 0.5441795587539673
setp: 400, Loss: 0.5488673448562622
setp: 500, Loss: 0.4780561625957489
setp: 600, Loss: 0.42202481627464294
setp: 700, Loss: 0.3468964099884033
setp: 800, Loss: 0.4307892620563507
setp: 900, Loss: 0.3759346902370453
setp: 1000, Loss: 0.36025410890579224
setp: 1100, Loss: 0.4377705454826355
setp: 1200, Loss: 0.3447790741920471
setp: 1300, Loss: 0.3616679906845093
setp: 1400, Loss: 0.35470643639564514
setp: 1500, Loss: 0.34229776263237
setp: 1600, Loss: 0.33175092935562134
setp: 1700, Loss: 0.3337744176387787
setp: 1800, Loss: 0.32869499921798706
setp: 1900, Loss: 0.32463058829307556
setp: 2000, Loss: 0.3783823549747467
setp: 2100, Loss: 0.36229103803634644
setp: 2200, Loss: 0.3325634300708771
setp: 2300, Loss: 0.342712938785553
setp: 2400, Loss: 0.3231830298900604
setp: 2500, Loss: 0.3183961808681488
setp: 2600, Loss: 0.31951913237571716
setp: 2700, Loss: 0.31888920068740845
setp: 2800, Loss: 0.32132261991500854
setp: 2900, Loss: 0.3209261894226074
setp: 3000, Loss: 0.31986764073371887
setp: 3100, Loss: 0.3194902241230011
setp: 3200, Loss: 0.3189595639705658
setp: 3300, Loss: 0.3212316036224365
setp: 3400, Loss: 0.34035083651542664
setp: 3500, Loss: 0.32006242871284485
setp: 3600, Loss: 0.32148438692092896
setp: 3700, Loss: 0.32031697034835815
setp: 3800, Loss: 0.3327014744281769
setp: 3900, Loss: 0.32264745235443115
setp: 4000, Loss: 0.32301995158195496
setp: 4100, Loss: 0.3215928375720978
setp: 4200, Loss: 0.32238662242889404
setp: 4300, Loss: 0.31970077753067017
setp: 4400, Loss: 0.3185080587863922
setp: 4500, Loss: 0.3188736140727997
setp: 4600, Loss: 0.31878626346588135
setp: 4700, Loss: 0.32012566924095154
setp: 4800, Loss: 0.3865276575088501
setp: 4900, Loss: 0.33882635831832886
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9971830985915493
recall: 1.0
F_score: 0.998589562764457
validating...
acc: 0.9210526315789473
precision: 0.9245283018867925
recall: 0.9607843137254902
F_score: 0.9423076923076923
model saved.
avg_acc: 0.9, avg_f_score: 0.9198744602411427
==========arousal==========
******fold 0******
[369, 239]
training...
setp: 0, Loss: 0.689768373966217
setp: 100, Loss: 0.5797586441040039
setp: 200, Loss: 0.5652457475662231
setp: 300, Loss: 0.40241432189941406
setp: 400, Loss: 0.47870323061943054
setp: 500, Loss: 0.41824355721473694
setp: 600, Loss: 0.37501996755599976
setp: 700, Loss: 0.3541228175163269
setp: 800, Loss: 0.39803293347358704
setp: 900, Loss: 0.40071791410446167
setp: 1000, Loss: 0.3941543400287628
setp: 1100, Loss: 0.40407827496528625
setp: 1200, Loss: 0.39018142223358154
setp: 1300, Loss: 0.38250309228897095
setp: 1400, Loss: 0.38259556889533997
setp: 1500, Loss: 0.38567212224006653
setp: 1600, Loss: 0.4008597433567047
setp: 1700, Loss: 0.34075474739074707
setp: 1800, Loss: 0.383161336183548
setp: 1900, Loss: 0.41838979721069336
setp: 2000, Loss: 0.3521847128868103
setp: 2100, Loss: 0.33183208107948303
setp: 2200, Loss: 0.32248732447624207
setp: 2300, Loss: 0.3568207025527954
setp: 2400, Loss: 0.3524974286556244
setp: 2500, Loss: 0.35250890254974365
setp: 2600, Loss: 0.32147476077079773
setp: 2700, Loss: 0.34834712743759155
setp: 2800, Loss: 0.39155876636505127
setp: 2900, Loss: 0.32179200649261475
setp: 3000, Loss: 0.3264828622341156
setp: 3100, Loss: 0.3251400589942932
setp: 3200, Loss: 0.3247530460357666
setp: 3300, Loss: 0.3198210895061493
setp: 3400, Loss: 0.32543838024139404
setp: 3500, Loss: 0.323709636926651
setp: 3600, Loss: 0.32506680488586426
setp: 3700, Loss: 0.35363447666168213
setp: 3800, Loss: 0.32417744398117065
setp: 3900, Loss: 0.31744635105133057
setp: 4000, Loss: 0.3210967183113098
setp: 4100, Loss: 0.3193662464618683
setp: 4200, Loss: 0.31967559456825256
setp: 4300, Loss: 0.3190805912017822
setp: 4400, Loss: 0.3188910484313965
setp: 4500, Loss: 0.32427117228507996
setp: 4600, Loss: 0.3284268379211426
setp: 4700, Loss: 0.32142946124076843
setp: 4800, Loss: 0.3206236660480499
setp: 4900, Loss: 0.3177185356616974
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9736842105263158
precision: 0.9770114942528736
recall: 0.9770114942528736
F_score: 0.9770114942528736
******fold 1******
[358, 250]
training...
setp: 0, Loss: 0.7912827134132385
setp: 100, Loss: 0.6591281890869141
setp: 200, Loss: 0.6049910187721252
setp: 300, Loss: 0.5521661639213562
setp: 400, Loss: 0.5597407221794128
setp: 500, Loss: 0.4288288950920105
setp: 600, Loss: 0.37991446256637573
setp: 700, Loss: 0.39436081051826477
setp: 800, Loss: 0.37482935190200806
setp: 900, Loss: 0.372785359621048
setp: 1000, Loss: 0.3218959867954254
setp: 1100, Loss: 0.38072192668914795
setp: 1200, Loss: 0.3738776445388794
setp: 1300, Loss: 0.34299248456954956
setp: 1400, Loss: 0.3396259546279907
setp: 1500, Loss: 0.4001123607158661
setp: 1600, Loss: 0.37706753611564636
setp: 1700, Loss: 0.354982852935791
setp: 1800, Loss: 0.348256379365921
setp: 1900, Loss: 0.3589831292629242
setp: 2000, Loss: 0.3521122932434082
setp: 2100, Loss: 0.3310711085796356
setp: 2200, Loss: 0.3522595763206482
setp: 2300, Loss: 0.381661593914032
setp: 2400, Loss: 0.3623652458190918
setp: 2500, Loss: 0.32163819670677185
setp: 2600, Loss: 0.34928274154663086
setp: 2700, Loss: 0.3496600091457367
setp: 2800, Loss: 0.35015562176704407
setp: 2900, Loss: 0.32972776889801025
setp: 3000, Loss: 0.40845686197280884
setp: 3100, Loss: 0.319791704416275
setp: 3200, Loss: 0.32568955421447754
setp: 3300, Loss: 0.31848257780075073
setp: 3400, Loss: 0.35487017035484314
setp: 3500, Loss: 0.31918156147003174
setp: 3600, Loss: 0.3754197657108307
setp: 3700, Loss: 0.3199448585510254
setp: 3800, Loss: 0.3471183776855469
setp: 3900, Loss: 0.33749350905418396
setp: 4000, Loss: 0.3181189298629761
setp: 4100, Loss: 0.35683736205101013
setp: 4200, Loss: 0.3248879611492157
setp: 4300, Loss: 0.32114678621292114
setp: 4400, Loss: 0.3177390694618225
setp: 4500, Loss: 0.3201454281806946
setp: 4600, Loss: 0.3240598440170288
setp: 4700, Loss: 0.3234555423259735
setp: 4800, Loss: 0.3176056444644928
setp: 4900, Loss: 0.34781113266944885
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9944444444444445
recall: 1.0
F_score: 0.9972144846796658
validating...
acc: 0.9144736842105263
precision: 0.9381443298969072
recall: 0.9285714285714286
F_score: 0.9333333333333333
******fold 2******
[368, 240]
training...
setp: 0, Loss: 0.7027413249015808
setp: 100, Loss: 0.6358532905578613
setp: 200, Loss: 0.6224462389945984
setp: 300, Loss: 0.55387282371521
setp: 400, Loss: 0.5340396165847778
setp: 500, Loss: 0.4289895296096802
setp: 600, Loss: 0.4203225374221802
setp: 700, Loss: 0.3701493740081787
setp: 800, Loss: 0.39037951827049255
setp: 900, Loss: 0.32530397176742554
setp: 1000, Loss: 0.35744717717170715
setp: 1100, Loss: 0.35353830456733704
setp: 1200, Loss: 0.3610891103744507
setp: 1300, Loss: 0.3573983311653137
setp: 1400, Loss: 0.3508569598197937
setp: 1500, Loss: 0.38287460803985596
setp: 1600, Loss: 0.3250109851360321
setp: 1700, Loss: 0.3548349440097809
setp: 1800, Loss: 0.3501971364021301
setp: 1900, Loss: 0.3572448790073395
setp: 2000, Loss: 0.32121261954307556
setp: 2100, Loss: 0.35255756974220276
setp: 2200, Loss: 0.34997478127479553
setp: 2300, Loss: 0.3541049659252167
setp: 2400, Loss: 0.39502182602882385
setp: 2500, Loss: 0.3931583762168884
setp: 2600, Loss: 0.34899836778640747
setp: 2700, Loss: 0.3515835404396057
setp: 2800, Loss: 0.3195899724960327
setp: 2900, Loss: 0.3518511652946472
setp: 3000, Loss: 0.3499048352241516
setp: 3100, Loss: 0.3540645241737366
setp: 3200, Loss: 0.351567804813385
setp: 3300, Loss: 0.355749249458313
setp: 3400, Loss: 0.368831992149353
setp: 3500, Loss: 0.3207298517227173
setp: 3600, Loss: 0.355726957321167
setp: 3700, Loss: 0.34983405470848083
setp: 3800, Loss: 0.35333457589149475
setp: 3900, Loss: 0.32064110040664673
setp: 4000, Loss: 0.35142210125923157
setp: 4100, Loss: 0.3626638352870941
setp: 4200, Loss: 0.3554528057575226
setp: 4300, Loss: 0.3329317569732666
setp: 4400, Loss: 0.3241651654243469
setp: 4500, Loss: 0.3181423544883728
setp: 4600, Loss: 0.3198036551475525
setp: 4700, Loss: 0.3185308873653412
setp: 4800, Loss: 0.32051917910575867
setp: 4900, Loss: 0.3185441792011261
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.9318181818181818
recall: 0.9318181818181818
F_score: 0.9318181818181818
******fold 3******
[374, 234]
training...
setp: 0, Loss: 0.7038591504096985
setp: 100, Loss: 0.6593335866928101
setp: 200, Loss: 0.516845703125
setp: 300, Loss: 0.5187303423881531
setp: 400, Loss: 0.45653727650642395
setp: 500, Loss: 0.3901418447494507
setp: 600, Loss: 0.44223207235336304
setp: 700, Loss: 0.3778204321861267
setp: 800, Loss: 0.39614683389663696
setp: 900, Loss: 0.3842783570289612
setp: 1000, Loss: 0.36781829595565796
setp: 1100, Loss: 0.38892248272895813
setp: 1200, Loss: 0.3565422296524048
setp: 1300, Loss: 0.3523053824901581
setp: 1400, Loss: 0.3856026232242584
setp: 1500, Loss: 0.4490019977092743
setp: 1600, Loss: 0.38741835951805115
setp: 1700, Loss: 0.41526302695274353
setp: 1800, Loss: 0.34474074840545654
setp: 1900, Loss: 0.34173962473869324
setp: 2000, Loss: 0.36100730299949646
setp: 2100, Loss: 0.38700971007347107
setp: 2200, Loss: 0.350495308637619
setp: 2300, Loss: 0.3860415816307068
setp: 2400, Loss: 0.35097500681877136
setp: 2500, Loss: 0.3586007356643677
setp: 2600, Loss: 0.3490149676799774
setp: 2700, Loss: 0.31965020298957825
setp: 2800, Loss: 0.3509313464164734
setp: 2900, Loss: 0.34840431809425354
setp: 3000, Loss: 0.3495732843875885
setp: 3100, Loss: 0.35333794355392456
setp: 3200, Loss: 0.3825460970401764
setp: 3300, Loss: 0.36488455533981323
setp: 3400, Loss: 0.35134443640708923
setp: 3500, Loss: 0.3491441309452057
setp: 3600, Loss: 0.3499719202518463
setp: 3700, Loss: 0.32357850670814514
setp: 3800, Loss: 0.3209601938724518
setp: 3900, Loss: 0.3500635623931885
setp: 4000, Loss: 0.35178062319755554
setp: 4100, Loss: 0.34953466057777405
setp: 4200, Loss: 0.37201130390167236
setp: 4300, Loss: 0.362297922372818
setp: 4400, Loss: 0.3478887379169464
setp: 4500, Loss: 0.34653720259666443
setp: 4600, Loss: 0.31807368993759155
setp: 4700, Loss: 0.34876424074172974
setp: 4800, Loss: 0.34820422530174255
setp: 4900, Loss: 0.3634805679321289
training successfully ended.
validating...
acc: 0.9703947368421053
precision: 0.9944444444444445
recall: 0.9572192513368984
F_score: 0.9754768392370572
validating...
acc: 0.9013157894736842
precision: 0.8681318681318682
recall: 0.9634146341463414
F_score: 0.9132947976878613
******fold 4******
[355, 253]
training...
setp: 0, Loss: 0.6746730804443359
setp: 100, Loss: 0.6061438918113708
setp: 200, Loss: 0.6145092844963074
setp: 300, Loss: 0.5202741026878357
setp: 400, Loss: 0.5925758481025696
setp: 500, Loss: 0.45189520716667175
setp: 600, Loss: 0.55225670337677
setp: 700, Loss: 0.47268638014793396
setp: 800, Loss: 0.469722718000412
setp: 900, Loss: 0.46962645649909973
setp: 1000, Loss: 0.4787811040878296
setp: 1100, Loss: 0.4447198510169983
setp: 1200, Loss: 0.3901292383670807
setp: 1300, Loss: 0.5209441184997559
setp: 1400, Loss: 0.43381133675575256
setp: 1500, Loss: 0.4319438338279724
setp: 1600, Loss: 0.373378723859787
setp: 1700, Loss: 0.4085303246974945
setp: 1800, Loss: 0.36535871028900146
setp: 1900, Loss: 0.3469417989253998
setp: 2000, Loss: 0.33048215508461
setp: 2100, Loss: 0.36725783348083496
setp: 2200, Loss: 0.31906065344810486
setp: 2300, Loss: 0.3315426707267761
setp: 2400, Loss: 0.3346695303916931
setp: 2500, Loss: 0.32467564940452576
setp: 2600, Loss: 0.32992902398109436
setp: 2700, Loss: 0.3246316611766815
setp: 2800, Loss: 0.31703081727027893
setp: 2900, Loss: 0.34418490529060364
setp: 3000, Loss: 0.34215518832206726
setp: 3100, Loss: 0.35097166895866394
setp: 3200, Loss: 0.32174691557884216
setp: 3300, Loss: 0.31902363896369934
setp: 3400, Loss: 0.32087311148643494
setp: 3500, Loss: 0.31731146574020386
setp: 3600, Loss: 0.31920963525772095
setp: 3700, Loss: 0.33094021677970886
setp: 3800, Loss: 0.3293662667274475
setp: 3900, Loss: 0.3243862986564636
setp: 4000, Loss: 0.3596768081188202
setp: 4100, Loss: 0.31835299730300903
setp: 4200, Loss: 0.3228715658187866
setp: 4300, Loss: 0.31742626428604126
setp: 4400, Loss: 0.32213810086250305
setp: 4500, Loss: 0.3175698220729828
setp: 4600, Loss: 0.3178594410419464
setp: 4700, Loss: 0.3179350197315216
setp: 4800, Loss: 0.3446741998195648
setp: 4900, Loss: 0.3218376636505127
training successfully ended.
validating...
acc: 0.9884868421052632
precision: 0.9806629834254144
recall: 1.0
F_score: 0.9902370990237099
validating...
acc: 0.9210526315789473
precision: 0.9405940594059405
recall: 0.9405940594059405
F_score: 0.9405940594059405
model saved.
avg_acc: 0.9263157894736842, avg_f_score: 0.9392103732996381
-------------subject: 5-------------
==========valence==========
******fold 0******
[249, 359]
training...
setp: 0, Loss: 0.7026563286781311
setp: 100, Loss: 0.6666026711463928
setp: 200, Loss: 0.6682009100914001
setp: 300, Loss: 0.5429157018661499
setp: 400, Loss: 0.5079440474510193
setp: 500, Loss: 0.4918724596500397
setp: 600, Loss: 0.44349077343940735
setp: 700, Loss: 0.4149107336997986
setp: 800, Loss: 0.36533284187316895
setp: 900, Loss: 0.3914855718612671
setp: 1000, Loss: 0.3863476812839508
setp: 1100, Loss: 0.3864610195159912
setp: 1200, Loss: 0.3878333270549774
setp: 1300, Loss: 0.35117027163505554
setp: 1400, Loss: 0.38732317090034485
setp: 1500, Loss: 0.34228894114494324
setp: 1600, Loss: 0.32181352376937866
setp: 1700, Loss: 0.31711435317993164
setp: 1800, Loss: 0.31947267055511475
setp: 1900, Loss: 0.31734856963157654
setp: 2000, Loss: 0.32144397497177124
setp: 2100, Loss: 0.3292030096054077
setp: 2200, Loss: 0.3186425566673279
setp: 2300, Loss: 0.31708940863609314
setp: 2400, Loss: 0.31757843494415283
setp: 2500, Loss: 0.3178081214427948
setp: 2600, Loss: 0.3171272277832031
setp: 2700, Loss: 0.3177453279495239
setp: 2800, Loss: 0.5793518424034119
setp: 2900, Loss: 0.36982789635658264
setp: 3000, Loss: 0.3996700942516327
setp: 3100, Loss: 0.3230513036251068
setp: 3200, Loss: 0.33292385935783386
setp: 3300, Loss: 0.35258030891418457
setp: 3400, Loss: 0.32142314314842224
setp: 3500, Loss: 0.3200032711029053
setp: 3600, Loss: 0.31936097145080566
setp: 3700, Loss: 0.3195306360721588
setp: 3800, Loss: 0.3180737793445587
setp: 3900, Loss: 0.317889541387558
setp: 4000, Loss: 0.3219010829925537
setp: 4100, Loss: 0.32606151700019836
setp: 4200, Loss: 0.3182523846626282
setp: 4300, Loss: 0.3173525333404541
setp: 4400, Loss: 0.3174448609352112
setp: 4500, Loss: 0.31772106885910034
setp: 4600, Loss: 0.31732258200645447
setp: 4700, Loss: 0.3176831901073456
setp: 4800, Loss: 0.31962960958480835
setp: 4900, Loss: 0.31998908519744873
training successfully ended.
validating...
acc: 0.9473684210526315
precision: 0.8888888888888888
recall: 0.9959839357429718
F_score: 0.9393939393939393
validating...
acc: 0.9144736842105263
precision: 0.828125
recall: 0.9636363636363636
F_score: 0.8907563025210083
******fold 1******
[239, 369]
training...
setp: 0, Loss: 0.7388015389442444
setp: 100, Loss: 0.6529116630554199
setp: 200, Loss: 0.613689124584198
setp: 300, Loss: 0.505036473274231
setp: 400, Loss: 0.4615575969219208
setp: 500, Loss: 0.4724453389644623
setp: 600, Loss: 0.4872545003890991
setp: 700, Loss: 0.4565751850605011
setp: 800, Loss: 0.4062994718551636
setp: 900, Loss: 0.3968029320240021
setp: 1000, Loss: 0.3379914462566376
setp: 1100, Loss: 0.44440749287605286
setp: 1200, Loss: 0.38458186388015747
setp: 1300, Loss: 0.4072500467300415
setp: 1400, Loss: 0.38376933336257935
setp: 1500, Loss: 0.36511245369911194
setp: 1600, Loss: 0.3344634473323822
setp: 1700, Loss: 0.3277926743030548
setp: 1800, Loss: 0.32442575693130493
setp: 1900, Loss: 0.33563488721847534
setp: 2000, Loss: 0.319985955953598
setp: 2100, Loss: 0.3234650492668152
setp: 2200, Loss: 0.3190552294254303
setp: 2300, Loss: 0.3176116347312927
setp: 2400, Loss: 0.3199695646762848
setp: 2500, Loss: 0.3185688853263855
setp: 2600, Loss: 0.32291755080223083
setp: 2700, Loss: 0.3212266266345978
setp: 2800, Loss: 0.3194747567176819
setp: 2900, Loss: 0.3196708858013153
setp: 3000, Loss: 0.3267897069454193
setp: 3100, Loss: 0.3233545422554016
setp: 3200, Loss: 0.3617020845413208
setp: 3300, Loss: 0.33577874302864075
setp: 3400, Loss: 0.35067567229270935
setp: 3500, Loss: 0.3358019292354584
setp: 3600, Loss: 0.3199668526649475
setp: 3700, Loss: 0.34957271814346313
setp: 3800, Loss: 0.3226916491985321
setp: 3900, Loss: 0.317901611328125
setp: 4000, Loss: 0.3567151427268982
setp: 4100, Loss: 0.3174624443054199
setp: 4200, Loss: 0.31740713119506836
setp: 4300, Loss: 0.31668147444725037
setp: 4400, Loss: 0.31824684143066406
setp: 4500, Loss: 0.3175734579563141
setp: 4600, Loss: 0.31638938188552856
setp: 4700, Loss: 0.31908708810806274
setp: 4800, Loss: 0.3175220191478729
setp: 4900, Loss: 0.321911096572876
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9473684210526315
precision: 0.9830508474576272
recall: 0.8923076923076924
F_score: 0.9354838709677421
******fold 2******
[244, 364]
training...
setp: 0, Loss: 0.6636443138122559
setp: 100, Loss: 0.6216247081756592
setp: 200, Loss: 0.6026857495307922
setp: 300, Loss: 0.5183157324790955
setp: 400, Loss: 0.4375770390033722
setp: 500, Loss: 0.5427075028419495
setp: 600, Loss: 0.580146849155426
setp: 700, Loss: 0.5121139287948608
setp: 800, Loss: 0.4953935146331787
setp: 900, Loss: 0.5327206254005432
setp: 1000, Loss: 0.49571678042411804
setp: 1100, Loss: 0.4174290597438812
setp: 1200, Loss: 0.3863582909107208
setp: 1300, Loss: 0.35709381103515625
setp: 1400, Loss: 0.34707432985305786
setp: 1500, Loss: 0.33111199736595154
setp: 1600, Loss: 0.3755321204662323
setp: 1700, Loss: 0.32431310415267944
setp: 1800, Loss: 0.32647329568862915
setp: 1900, Loss: 0.3827536702156067
setp: 2000, Loss: 0.31904709339141846
setp: 2100, Loss: 0.3512631952762604
setp: 2200, Loss: 0.3222385048866272
setp: 2300, Loss: 0.31604620814323425
setp: 2400, Loss: 0.31972426176071167
setp: 2500, Loss: 0.32140183448791504
setp: 2600, Loss: 0.3206830322742462
setp: 2700, Loss: 0.3494183421134949
setp: 2800, Loss: 0.3508166968822479
setp: 2900, Loss: 0.3208507001399994
setp: 3000, Loss: 0.31935781240463257
setp: 3100, Loss: 0.3164607882499695
setp: 3200, Loss: 0.3510734438896179
setp: 3300, Loss: 0.38491520285606384
setp: 3400, Loss: 0.3653252422809601
setp: 3500, Loss: 0.32938680052757263
setp: 3600, Loss: 0.31698185205459595
setp: 3700, Loss: 0.3332768380641937
setp: 3800, Loss: 0.3193928897380829
setp: 3900, Loss: 0.3204015791416168
setp: 4000, Loss: 0.31901419162750244
setp: 4100, Loss: 0.3190988302230835
setp: 4200, Loss: 0.31673723459243774
setp: 4300, Loss: 0.31826549768447876
setp: 4400, Loss: 0.31939131021499634
setp: 4500, Loss: 0.3191975951194763
setp: 4600, Loss: 0.31840381026268005
setp: 4700, Loss: 0.3491910696029663
setp: 4800, Loss: 0.31972309947013855
setp: 4900, Loss: 0.31833788752555847
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.9918032786885246
F_score: 0.9958847736625513
validating...
acc: 0.9473684210526315
precision: 0.9642857142857143
recall: 0.9
F_score: 0.9310344827586207
******fold 3******
[249, 359]
training...
setp: 0, Loss: 0.7115113735198975
setp: 100, Loss: 0.6743316054344177
setp: 200, Loss: 0.58193039894104
setp: 300, Loss: 0.45521336793899536
setp: 400, Loss: 0.5554393529891968
setp: 500, Loss: 0.5334530472755432
setp: 600, Loss: 0.4607265591621399
setp: 700, Loss: 0.4137505292892456
setp: 800, Loss: 0.44306233525276184
setp: 900, Loss: 0.45826536417007446
setp: 1000, Loss: 0.39827245473861694
setp: 1100, Loss: 0.4329080879688263
setp: 1200, Loss: 0.37166357040405273
setp: 1300, Loss: 0.3610333204269409
setp: 1400, Loss: 0.350800096988678
setp: 1500, Loss: 0.3277190327644348
setp: 1600, Loss: 0.3200470209121704
setp: 1700, Loss: 0.32093673944473267
setp: 1800, Loss: 0.3183923065662384
setp: 1900, Loss: 0.3181377351284027
setp: 2000, Loss: 0.32020220160484314
setp: 2100, Loss: 0.3182811439037323
setp: 2200, Loss: 0.3198070824146271
setp: 2300, Loss: 0.31687280535697937
setp: 2400, Loss: 0.3215967118740082
setp: 2500, Loss: 0.31881439685821533
setp: 2600, Loss: 0.32023856043815613
setp: 2700, Loss: 0.3202536106109619
setp: 2800, Loss: 0.36831703782081604
setp: 2900, Loss: 0.3292434513568878
setp: 3000, Loss: 0.3511826992034912
setp: 3100, Loss: 0.3516336977481842
setp: 3200, Loss: 0.3181454837322235
setp: 3300, Loss: 0.3474024534225464
setp: 3400, Loss: 0.32151907682418823
setp: 3500, Loss: 0.31751078367233276
setp: 3600, Loss: 0.3216400444507599
setp: 3700, Loss: 0.3202391564846039
setp: 3800, Loss: 0.325989305973053
setp: 3900, Loss: 0.317218154668808
setp: 4000, Loss: 0.32007408142089844
setp: 4100, Loss: 0.317179411649704
setp: 4200, Loss: 0.3166145086288452
setp: 4300, Loss: 0.31986814737319946
setp: 4400, Loss: 0.31718432903289795
setp: 4500, Loss: 0.3178773522377014
setp: 4600, Loss: 0.3172590732574463
setp: 4700, Loss: 0.320136696100235
setp: 4800, Loss: 0.31872597336769104
setp: 4900, Loss: 0.3195689916610718
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9959839357429718
F_score: 0.9979879275653923
validating...
acc: 0.9342105263157895
precision: 0.9787234042553191
recall: 0.8363636363636363
F_score: 0.9019607843137255
******fold 4******
[235, 373]
training...
setp: 0, Loss: 0.6756243705749512
setp: 100, Loss: 0.6386597156524658
setp: 200, Loss: 0.6368231177330017
setp: 300, Loss: 0.5422002077102661
setp: 400, Loss: 0.4503435492515564
setp: 500, Loss: 0.4713541865348816
setp: 600, Loss: 0.5468251705169678
setp: 700, Loss: 0.5249165296554565
setp: 800, Loss: 0.46054279804229736
setp: 900, Loss: 0.4974921941757202
setp: 1000, Loss: 0.49898695945739746
setp: 1100, Loss: 0.4754244387149811
setp: 1200, Loss: 0.3817101716995239
setp: 1300, Loss: 0.42328137159347534
setp: 1400, Loss: 0.4746277332305908
setp: 1500, Loss: 0.5458528995513916
setp: 1600, Loss: 0.4519563615322113
setp: 1700, Loss: 0.4148455262184143
setp: 1800, Loss: 0.41337257623672485
setp: 1900, Loss: 0.3876761198043823
setp: 2000, Loss: 0.38726750016212463
setp: 2100, Loss: 0.3578827381134033
setp: 2200, Loss: 0.41042402386665344
setp: 2300, Loss: 0.34790873527526855
setp: 2400, Loss: 0.4126313626766205
setp: 2500, Loss: 0.3894593417644501
setp: 2600, Loss: 0.4634225070476532
setp: 2700, Loss: 0.41124144196510315
setp: 2800, Loss: 0.39121413230895996
setp: 2900, Loss: 0.4156506061553955
setp: 3000, Loss: 0.45142850279808044
setp: 3100, Loss: 0.3843114972114563
setp: 3200, Loss: 0.4575670063495636
setp: 3300, Loss: 0.36125341057777405
setp: 3400, Loss: 0.4663432538509369
setp: 3500, Loss: 0.38361990451812744
setp: 3600, Loss: 0.34973880648612976
setp: 3700, Loss: 0.35397768020629883
setp: 3800, Loss: 0.35804134607315063
setp: 3900, Loss: 0.31909874081611633
setp: 4000, Loss: 0.31959107518196106
setp: 4100, Loss: 0.3213035762310028
setp: 4200, Loss: 0.3172456622123718
setp: 4300, Loss: 0.3169516623020172
setp: 4400, Loss: 0.32401132583618164
setp: 4500, Loss: 0.32046130299568176
setp: 4600, Loss: 0.32227596640586853
setp: 4700, Loss: 0.3466881513595581
setp: 4800, Loss: 0.32735589146614075
setp: 4900, Loss: 0.31896185874938965
training successfully ended.
validating...
acc: 0.944078947368421
precision: 0.8736059479553904
recall: 1.0
F_score: 0.9325396825396826
validating...
acc: 0.875
precision: 0.7976190476190477
recall: 0.9710144927536232
F_score: 0.8758169934640523
model saved.
avg_acc: 0.9236842105263158, avg_f_score: 0.9070104868050297
==========arousal==========
******fold 0******
[324, 284]
training...
setp: 0, Loss: 0.7368230223655701
setp: 100, Loss: 0.6900230050086975
setp: 200, Loss: 0.6255876421928406
setp: 300, Loss: 0.5242955088615417
setp: 400, Loss: 0.42798227071762085
setp: 500, Loss: 0.3849967420101166
setp: 600, Loss: 0.4091949462890625
setp: 700, Loss: 0.4072703719139099
setp: 800, Loss: 0.37644657492637634
setp: 900, Loss: 0.40790417790412903
setp: 1000, Loss: 0.3706901967525482
setp: 1100, Loss: 0.32167568802833557
setp: 1200, Loss: 0.3202984929084778
setp: 1300, Loss: 0.3391776382923126
setp: 1400, Loss: 0.3482852280139923
setp: 1500, Loss: 0.32794153690338135
setp: 1600, Loss: 0.357247918844223
setp: 1700, Loss: 0.32678085565567017
setp: 1800, Loss: 0.31643620133399963
setp: 1900, Loss: 0.3291763663291931
setp: 2000, Loss: 0.322297066450119
setp: 2100, Loss: 0.32958316802978516
setp: 2200, Loss: 0.31849774718284607
setp: 2300, Loss: 0.318600058555603
setp: 2400, Loss: 0.3299188017845154
setp: 2500, Loss: 0.31867870688438416
setp: 2600, Loss: 0.31752100586891174
setp: 2700, Loss: 0.3154717683792114
setp: 2800, Loss: 0.34971651434898376
setp: 2900, Loss: 0.31867286562919617
setp: 3000, Loss: 0.3182135820388794
setp: 3100, Loss: 0.3176620900630951
setp: 3200, Loss: 0.3173157870769501
setp: 3300, Loss: 0.3186213970184326
setp: 3400, Loss: 0.31836915016174316
setp: 3500, Loss: 0.3191438615322113
setp: 3600, Loss: 0.31945091485977173
setp: 3700, Loss: 0.7133024334907532
setp: 3800, Loss: 0.48084592819213867
setp: 3900, Loss: 0.3754402995109558
setp: 4000, Loss: 0.35551267862319946
setp: 4100, Loss: 0.4313194155693054
setp: 4200, Loss: 0.3501286208629608
setp: 4300, Loss: 0.3220455050468445
setp: 4400, Loss: 0.32619592547416687
setp: 4500, Loss: 0.34369853138923645
setp: 4600, Loss: 0.31883901357650757
setp: 4700, Loss: 0.3242955505847931
setp: 4800, Loss: 0.32116633653640747
setp: 4900, Loss: 0.3204464614391327
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9969135802469136
F_score: 0.9984544049459042
validating...
acc: 0.9144736842105263
precision: 0.9078947368421053
recall: 0.92
F_score: 0.913907284768212
******fold 1******
[322, 286]
training...
setp: 0, Loss: 0.7229776382446289
setp: 100, Loss: 0.6890080571174622
setp: 200, Loss: 0.6597049236297607
setp: 300, Loss: 0.510962188243866
setp: 400, Loss: 0.45600348711013794
setp: 500, Loss: 0.44255706667900085
setp: 600, Loss: 0.3773084580898285
setp: 700, Loss: 0.3493700325489044
setp: 800, Loss: 0.33072981238365173
setp: 900, Loss: 0.32802486419677734
setp: 1000, Loss: 0.35855743288993835
setp: 1100, Loss: 0.3360118865966797
setp: 1200, Loss: 0.32317307591438293
setp: 1300, Loss: 0.3246060013771057
setp: 1400, Loss: 0.32170650362968445
setp: 1500, Loss: 0.32141900062561035
setp: 1600, Loss: 0.3229253888130188
setp: 1700, Loss: 0.31859034299850464
setp: 1800, Loss: 0.3192457854747772
setp: 1900, Loss: 0.36142486333847046
setp: 2000, Loss: 0.3222605884075165
setp: 2100, Loss: 0.31814417243003845
setp: 2200, Loss: 0.3183782994747162
setp: 2300, Loss: 0.31994250416755676
setp: 2400, Loss: 0.3191082179546356
setp: 2500, Loss: 0.6882029175758362
setp: 2600, Loss: 0.4389682412147522
setp: 2700, Loss: 0.3506573736667633
setp: 2800, Loss: 0.34117525815963745
setp: 2900, Loss: 0.34096601605415344
setp: 3000, Loss: 0.33225926756858826
setp: 3100, Loss: 0.32834672927856445
setp: 3200, Loss: 0.32908692955970764
setp: 3300, Loss: 0.33177924156188965
setp: 3400, Loss: 0.32560306787490845
setp: 3500, Loss: 0.332621693611145
setp: 3600, Loss: 0.3229651153087616
setp: 3700, Loss: 0.32443100214004517
setp: 3800, Loss: 0.3254866898059845
setp: 3900, Loss: 0.3236379027366638
setp: 4000, Loss: 0.4303998053073883
setp: 4100, Loss: 0.3308499753475189
setp: 4200, Loss: 0.32700830698013306
setp: 4300, Loss: 0.3238895833492279
setp: 4400, Loss: 0.321815550327301
setp: 4500, Loss: 0.32778874039649963
setp: 4600, Loss: 0.32164889574050903
setp: 4700, Loss: 0.3220018744468689
setp: 4800, Loss: 0.32318639755249023
setp: 4900, Loss: 0.3364831209182739
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9473684210526315
recall: 0.935064935064935
F_score: 0.9411764705882352
******fold 2******
[306, 302]
training...
setp: 0, Loss: 0.6913456320762634
setp: 100, Loss: 0.6756964921951294
setp: 200, Loss: 0.6543294787406921
setp: 300, Loss: 0.5557093620300293
setp: 400, Loss: 0.4763442277908325
setp: 500, Loss: 0.4614514410495758
setp: 600, Loss: 0.43792328238487244
setp: 700, Loss: 0.427885502576828
setp: 800, Loss: 0.34019187092781067
setp: 900, Loss: 0.40712791681289673
setp: 1000, Loss: 0.33965426683425903
setp: 1100, Loss: 0.3538423478603363
setp: 1200, Loss: 0.3202364146709442
setp: 1300, Loss: 0.3203299343585968
setp: 1400, Loss: 0.33901622891426086
setp: 1500, Loss: 0.32193613052368164
setp: 1600, Loss: 0.3191835284233093
setp: 1700, Loss: 0.3283816874027252
setp: 1800, Loss: 0.3177240192890167
setp: 1900, Loss: 0.33246368169784546
setp: 2000, Loss: 0.4352818727493286
setp: 2100, Loss: 0.34234189987182617
setp: 2200, Loss: 0.38616955280303955
setp: 2300, Loss: 0.33103233575820923
setp: 2400, Loss: 0.32621315121650696
setp: 2500, Loss: 0.3175368309020996
setp: 2600, Loss: 0.31689104437828064
setp: 2700, Loss: 0.31938666105270386
setp: 2800, Loss: 0.3286266326904297
setp: 2900, Loss: 0.3221898078918457
setp: 3000, Loss: 0.3203648030757904
setp: 3100, Loss: 0.31794488430023193
setp: 3200, Loss: 0.35116153955459595
setp: 3300, Loss: 0.3241223990917206
setp: 3400, Loss: 0.3255113661289215
setp: 3500, Loss: 0.31805551052093506
setp: 3600, Loss: 0.31648412346839905
setp: 3700, Loss: 0.3276279866695404
setp: 3800, Loss: 0.3171207010746002
setp: 3900, Loss: 0.318482369184494
setp: 4000, Loss: 0.3178260624408722
setp: 4100, Loss: 0.3356707692146301
setp: 4200, Loss: 0.33345305919647217
setp: 4300, Loss: 0.345870703458786
setp: 4400, Loss: 0.3202769160270691
setp: 4500, Loss: 0.3255123198032379
setp: 4600, Loss: 0.34122025966644287
setp: 4700, Loss: 0.3423025608062744
setp: 4800, Loss: 0.3454355001449585
setp: 4900, Loss: 0.31832337379455566
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.9456521739130435
recall: 0.9354838709677419
F_score: 0.9405405405405404
******fold 3******
[319, 289]
training...
setp: 0, Loss: 0.6920292973518372
setp: 100, Loss: 0.6940563321113586
setp: 200, Loss: 0.7053921222686768
setp: 300, Loss: 0.6914158463478088
setp: 400, Loss: 0.6915830373764038
setp: 500, Loss: 0.6887073516845703
setp: 600, Loss: 0.694695234298706
setp: 700, Loss: 0.6915026307106018
setp: 800, Loss: 0.6893661022186279
setp: 900, Loss: 0.688478946685791
setp: 1000, Loss: 0.6945670247077942
setp: 1100, Loss: 0.6893985271453857
setp: 1200, Loss: 0.691525936126709
setp: 1300, Loss: 0.688161313533783
setp: 1400, Loss: 0.6975052952766418
setp: 1500, Loss: 0.6916241645812988
setp: 1600, Loss: 0.6914794445037842
setp: 1700, Loss: 0.6946818232536316
setp: 1800, Loss: 0.6913584470748901
setp: 1900, Loss: 0.6896558403968811
setp: 2000, Loss: 0.6939840316772461
setp: 2100, Loss: 0.7061073780059814
setp: 2200, Loss: 0.6914542317390442
setp: 2300, Loss: 0.6916489601135254
setp: 2400, Loss: 0.6887348890304565
setp: 2500, Loss: 0.6947979927062988
setp: 2600, Loss: 0.6915470361709595
setp: 2700, Loss: 0.6895561814308167
setp: 2800, Loss: 0.6884859800338745
setp: 2900, Loss: 0.6946052312850952
setp: 3000, Loss: 0.6895660161972046
setp: 3100, Loss: 0.6915656924247742
setp: 3200, Loss: 0.6881389021873474
setp: 3300, Loss: 0.6974905729293823
setp: 3400, Loss: 0.6916670799255371
setp: 3500, Loss: 0.6915010809898376
setp: 3600, Loss: 0.6947163939476013
setp: 3700, Loss: 0.6913683414459229
setp: 3800, Loss: 0.6897506713867188
setp: 3900, Loss: 0.6939645409584045
setp: 4000, Loss: 0.7062472701072693
setp: 4100, Loss: 0.6914674043655396
setp: 4200, Loss: 0.6916695237159729
setp: 4300, Loss: 0.6887556314468384
setp: 4400, Loss: 0.6948040723800659
setp: 4500, Loss: 0.691559910774231
setp: 4600, Loss: 0.6895959377288818
setp: 4700, Loss: 0.6884955763816833
setp: 4800, Loss: 0.694596529006958
setp: 4900, Loss: 0.6896018981933594
training successfully ended.
validating...
acc: 0.524671052631579
precision: 0.524671052631579
recall: 1.0
F_score: 0.6882416396979504
validating...
acc: 0.5263157894736842
precision: 0.5263157894736842
recall: 1.0
F_score: 0.6896551724137931
******fold 4******
[325, 283]
training...
setp: 0, Loss: 0.7004345059394836
setp: 100, Loss: 0.6896550059318542
setp: 200, Loss: 0.6742308735847473
setp: 300, Loss: 0.5868914723396301
setp: 400, Loss: 0.6186285018920898
setp: 500, Loss: 0.5299233198165894
setp: 600, Loss: 0.5244002342224121
setp: 700, Loss: 0.47121986746788025
setp: 800, Loss: 0.4237836003303528
setp: 900, Loss: 0.42551663517951965
setp: 1000, Loss: 0.4132382273674011
setp: 1100, Loss: 0.41599762439727783
setp: 1200, Loss: 0.40021270513534546
setp: 1300, Loss: 0.4333680272102356
setp: 1400, Loss: 0.3829909861087799
setp: 1500, Loss: 0.38275355100631714
setp: 1600, Loss: 0.39713194966316223
setp: 1700, Loss: 0.362129807472229
setp: 1800, Loss: 0.32189610600471497
setp: 1900, Loss: 0.3194904029369354
setp: 2000, Loss: 0.3310530483722687
setp: 2100, Loss: 0.31931740045547485
setp: 2200, Loss: 0.3191450238227844
setp: 2300, Loss: 0.3178759515285492
setp: 2400, Loss: 0.3202597498893738
setp: 2500, Loss: 0.3169247508049011
setp: 2600, Loss: 0.3862711191177368
setp: 2700, Loss: 0.3303627073764801
setp: 2800, Loss: 0.3356231153011322
setp: 2900, Loss: 0.31898990273475647
setp: 3000, Loss: 0.3179692327976227
setp: 3100, Loss: 0.31703683733940125
setp: 3200, Loss: 0.3492012619972229
setp: 3300, Loss: 0.32001373171806335
setp: 3400, Loss: 0.31784895062446594
setp: 3500, Loss: 0.3180484473705292
setp: 3600, Loss: 0.3196244239807129
setp: 3700, Loss: 0.3174677789211273
setp: 3800, Loss: 0.318665087223053
setp: 3900, Loss: 0.4647512435913086
setp: 4000, Loss: 0.32366248965263367
setp: 4100, Loss: 0.3299828767776489
setp: 4200, Loss: 0.3173777163028717
setp: 4300, Loss: 0.3164040744304657
setp: 4400, Loss: 0.3178199827671051
setp: 4500, Loss: 0.31887468695640564
setp: 4600, Loss: 0.31605198979377747
setp: 4700, Loss: 0.31875479221343994
setp: 4800, Loss: 0.3169393539428711
setp: 4900, Loss: 0.31719791889190674
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9969230769230769
F_score: 0.9984591679506933
validating...
acc: 0.8552631578947368
precision: 0.8939393939393939
recall: 0.7972972972972973
F_score: 0.8428571428571429
model saved.
avg_acc: 0.8328947368421054, avg_f_score: 0.8656273222335846
-------------subject: 6-------------
==========valence==========
******fold 0******
[156, 452]
training...
setp: 0, Loss: 0.6937751770019531
setp: 100, Loss: 0.6634352207183838
setp: 200, Loss: 0.6542695760726929
setp: 300, Loss: 0.4301695227622986
setp: 400, Loss: 0.389634370803833
setp: 500, Loss: 0.40401491522789
setp: 600, Loss: 0.3872288763523102
setp: 700, Loss: 0.3644266426563263
setp: 800, Loss: 0.3416312634944916
setp: 900, Loss: 0.3196006417274475
setp: 1000, Loss: 0.32216551899909973
setp: 1100, Loss: 0.34312573075294495
setp: 1200, Loss: 0.3239261507987976
setp: 1300, Loss: 0.31846287846565247
setp: 1400, Loss: 0.3177943825721741
setp: 1500, Loss: 0.31784728169441223
setp: 1600, Loss: 0.3180198073387146
setp: 1700, Loss: 0.31693559885025024
setp: 1800, Loss: 0.3167218863964081
setp: 1900, Loss: 0.32062599062919617
setp: 2000, Loss: 0.3187052011489868
setp: 2100, Loss: 0.31713977456092834
setp: 2200, Loss: 0.32948771119117737
setp: 2300, Loss: 0.3172192871570587
setp: 2400, Loss: 0.3165438175201416
setp: 2500, Loss: 0.34681448340415955
setp: 2600, Loss: 0.31657421588897705
setp: 2700, Loss: 0.31706473231315613
setp: 2800, Loss: 0.3171539008617401
setp: 2900, Loss: 0.31690722703933716
setp: 3000, Loss: 0.6940115690231323
setp: 3100, Loss: 0.5524058938026428
setp: 3200, Loss: 0.3232141137123108
setp: 3300, Loss: 0.3748374581336975
setp: 3400, Loss: 0.32368001341819763
setp: 3500, Loss: 0.3206822872161865
setp: 3600, Loss: 0.3174994885921478
setp: 3700, Loss: 0.31748831272125244
setp: 3800, Loss: 0.31852686405181885
setp: 3900, Loss: 0.3181358873844147
setp: 4000, Loss: 0.31741273403167725
setp: 4100, Loss: 0.3201267421245575
setp: 4200, Loss: 0.3204907178878784
setp: 4300, Loss: 0.3243427276611328
setp: 4400, Loss: 0.3167404532432556
setp: 4500, Loss: 0.31755512952804565
setp: 4600, Loss: 0.31671807169914246
setp: 4700, Loss: 0.3164764642715454
setp: 4800, Loss: 0.31786003708839417
setp: 4900, Loss: 0.317340612411499
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9736842105263158
precision: 1.0
recall: 0.8823529411764706
F_score: 0.9375
******fold 1******
[150, 458]
training...
setp: 0, Loss: 0.6965154409408569
setp: 100, Loss: 0.6887915134429932
setp: 200, Loss: 0.46839967370033264
setp: 300, Loss: 0.4422506093978882
setp: 400, Loss: 0.3583965003490448
setp: 500, Loss: 0.39768365025520325
setp: 600, Loss: 0.34086576104164124
setp: 700, Loss: 0.38138020038604736
setp: 800, Loss: 0.3580889403820038
setp: 900, Loss: 0.32199349999427795
setp: 1000, Loss: 0.32256564497947693
setp: 1100, Loss: 0.32307931780815125
setp: 1200, Loss: 0.3211071491241455
setp: 1300, Loss: 0.31761229038238525
setp: 1400, Loss: 0.32179635763168335
setp: 1500, Loss: 0.32063889503479004
setp: 1600, Loss: 0.31928208470344543
setp: 1700, Loss: 0.3174684941768646
setp: 1800, Loss: 0.384710431098938
setp: 1900, Loss: 0.3182062804698944
setp: 2000, Loss: 0.3168599605560303
setp: 2100, Loss: 0.3190130591392517
setp: 2200, Loss: 0.3186935484409332
setp: 2300, Loss: 0.3174026310443878
setp: 2400, Loss: 0.32209017872810364
setp: 2500, Loss: 0.4418545365333557
setp: 2600, Loss: 0.3172527849674225
setp: 2700, Loss: 0.31740492582321167
setp: 2800, Loss: 0.3174503743648529
setp: 2900, Loss: 0.3173156678676605
setp: 3000, Loss: 0.31703460216522217
setp: 3100, Loss: 0.3177824914455414
setp: 3200, Loss: 0.3167564570903778
setp: 3300, Loss: 0.318202942609787
setp: 3400, Loss: 0.32410797476768494
setp: 3500, Loss: 0.3166704773902893
setp: 3600, Loss: 0.31650567054748535
setp: 3700, Loss: 0.3173101246356964
setp: 3800, Loss: 0.3169991075992584
setp: 3900, Loss: 0.3174702227115631
setp: 4000, Loss: 0.42467278242111206
setp: 4100, Loss: 0.3556341826915741
setp: 4200, Loss: 0.316305547952652
setp: 4300, Loss: 0.3170965611934662
setp: 4400, Loss: 0.31798556447029114
setp: 4500, Loss: 0.3191327750682831
setp: 4600, Loss: 0.3179427981376648
setp: 4700, Loss: 0.316867858171463
setp: 4800, Loss: 0.3174270689487457
setp: 4900, Loss: 0.31658726930618286
training successfully ended.
validating...
acc: 0.9989082969432315
precision: 0.9978213507625272
recall: 1.0
F_score: 0.9989094874591058
validating...
acc: 0.9671052631578947
precision: 0.9069767441860465
recall: 0.975
F_score: 0.9397590361445783
******fold 2******
[155, 453]
training...
setp: 0, Loss: 0.7000522613525391
setp: 100, Loss: 0.6629853844642639
setp: 200, Loss: 0.6436089873313904
setp: 300, Loss: 0.49594590067863464
setp: 400, Loss: 0.41457894444465637
setp: 500, Loss: 0.36768776178359985
setp: 600, Loss: 0.32988062500953674
setp: 700, Loss: 0.3291403353214264
setp: 800, Loss: 0.32153454422950745
setp: 900, Loss: 0.32022231817245483
setp: 1000, Loss: 0.3207518756389618
setp: 1100, Loss: 0.32156461477279663
setp: 1200, Loss: 0.3189825415611267
setp: 1300, Loss: 0.31889447569847107
setp: 1400, Loss: 0.32121366262435913
setp: 1500, Loss: 0.3186531960964203
setp: 1600, Loss: 0.3191092312335968
setp: 1700, Loss: 0.31778210401535034
setp: 1800, Loss: 0.35957059264183044
setp: 1900, Loss: 0.3310139775276184
setp: 2000, Loss: 0.3239826261997223
setp: 2100, Loss: 0.3240372836589813
setp: 2200, Loss: 0.32380300760269165
setp: 2300, Loss: 0.3236936330795288
setp: 2400, Loss: 0.32237371802330017
setp: 2500, Loss: 0.3237302303314209
setp: 2600, Loss: 0.32197579741477966
setp: 2700, Loss: 0.3221442699432373
setp: 2800, Loss: 0.3219929337501526
setp: 2900, Loss: 0.32172590494155884
setp: 3000, Loss: 0.3219674825668335
setp: 3100, Loss: 0.3212643563747406
setp: 3200, Loss: 0.32138240337371826
setp: 3300, Loss: 0.32160311937332153
setp: 3400, Loss: 0.3227802813053131
setp: 3500, Loss: 0.32693952322006226
setp: 3600, Loss: 0.32442331314086914
setp: 3700, Loss: 0.32138293981552124
setp: 3800, Loss: 0.3206879794597626
setp: 3900, Loss: 0.3216182291507721
setp: 4000, Loss: 0.32289037108421326
setp: 4100, Loss: 0.32092565298080444
setp: 4200, Loss: 0.32184046506881714
setp: 4300, Loss: 0.32231825590133667
setp: 4400, Loss: 0.3224015235900879
setp: 4500, Loss: 0.32199549674987793
setp: 4600, Loss: 0.3216472268104553
setp: 4700, Loss: 0.32166871428489685
setp: 4800, Loss: 0.3211051821708679
setp: 4900, Loss: 0.3200884461402893
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.8285714285714286
recall: 0.8285714285714286
F_score: 0.8285714285714286
******fold 3******
[153, 455]
training...
setp: 0, Loss: 0.7508546113967896
setp: 100, Loss: 0.6891227960586548
setp: 200, Loss: 0.5458267331123352
setp: 300, Loss: 0.3541567325592041
setp: 400, Loss: 0.3417011499404907
setp: 500, Loss: 0.3514837324619293
setp: 600, Loss: 0.32029542326927185
setp: 700, Loss: 0.32262954115867615
setp: 800, Loss: 0.31904155015945435
setp: 900, Loss: 0.31780415773391724
setp: 1000, Loss: 0.3194601237773895
setp: 1100, Loss: 0.31876853108406067
setp: 1200, Loss: 0.3200198709964752
setp: 1300, Loss: 0.3173292279243469
setp: 1400, Loss: 0.6250021457672119
setp: 1500, Loss: 0.43721893429756165
setp: 1600, Loss: 0.3591831922531128
setp: 1700, Loss: 0.3527091443538666
setp: 1800, Loss: 0.33331790566444397
setp: 1900, Loss: 0.3580133616924286
setp: 2000, Loss: 0.32263198494911194
setp: 2100, Loss: 0.3245815634727478
setp: 2200, Loss: 0.3232313096523285
setp: 2300, Loss: 0.3253077268600464
setp: 2400, Loss: 0.3224615752696991
setp: 2500, Loss: 0.32412317395210266
setp: 2600, Loss: 0.32128405570983887
setp: 2700, Loss: 0.3295463025569916
setp: 2800, Loss: 0.33100390434265137
setp: 2900, Loss: 0.3221351206302643
setp: 3000, Loss: 0.3207119107246399
setp: 3100, Loss: 0.32065549492836
setp: 3200, Loss: 0.3212324380874634
setp: 3300, Loss: 0.32008373737335205
setp: 3400, Loss: 0.32225659489631653
setp: 3500, Loss: 0.3205287754535675
setp: 3600, Loss: 0.3216976523399353
setp: 3700, Loss: 0.3197404444217682
setp: 3800, Loss: 0.32773956656455994
setp: 3900, Loss: 0.31968751549720764
setp: 4000, Loss: 0.3188759982585907
setp: 4100, Loss: 0.32073789834976196
setp: 4200, Loss: 0.31909656524658203
setp: 4300, Loss: 0.31771397590637207
setp: 4400, Loss: 0.3192274570465088
setp: 4500, Loss: 0.3356616795063019
setp: 4600, Loss: 0.31876230239868164
setp: 4700, Loss: 0.31810513138771057
setp: 4800, Loss: 0.3193536102771759
setp: 4900, Loss: 0.31732890009880066
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9671052631578947
precision: 0.9210526315789473
recall: 0.9459459459459459
F_score: 0.9333333333333332
******fold 4******
[146, 462]
training...
setp: 0, Loss: 0.7095742225646973
setp: 100, Loss: 0.6307637095451355
setp: 200, Loss: 0.4435884654521942
setp: 300, Loss: 0.44709843397140503
setp: 400, Loss: 0.4054291546344757
setp: 500, Loss: 0.3957365155220032
setp: 600, Loss: 0.3303455114364624
setp: 700, Loss: 0.32749688625335693
setp: 800, Loss: 0.39333122968673706
setp: 900, Loss: 0.32021141052246094
setp: 1000, Loss: 0.31682348251342773
setp: 1100, Loss: 0.31776162981987
setp: 1200, Loss: 0.3181666135787964
setp: 1300, Loss: 0.31792929768562317
setp: 1400, Loss: 0.31998181343078613
setp: 1500, Loss: 0.3178415298461914
setp: 1600, Loss: 0.317097544670105
setp: 1700, Loss: 0.31629782915115356
setp: 1800, Loss: 0.3173111379146576
setp: 1900, Loss: 0.3182964324951172
setp: 2000, Loss: 0.3171898424625397
setp: 2100, Loss: 0.3178904950618744
setp: 2200, Loss: 0.3188062608242035
setp: 2300, Loss: 0.6947460174560547
setp: 2400, Loss: 0.4876171946525574
setp: 2500, Loss: 0.4379764497280121
setp: 2600, Loss: 0.34634605050086975
setp: 2700, Loss: 0.3262137472629547
setp: 2800, Loss: 0.3260810673236847
setp: 2900, Loss: 0.3243641257286072
setp: 3000, Loss: 0.3233693838119507
setp: 3100, Loss: 0.32278066873550415
setp: 3200, Loss: 0.32216283679008484
setp: 3300, Loss: 0.32088249921798706
setp: 3400, Loss: 0.8094196915626526
setp: 3500, Loss: 0.35541191697120667
setp: 3600, Loss: 0.3565521240234375
setp: 3700, Loss: 0.32453545928001404
setp: 3800, Loss: 0.32109934091567993
setp: 3900, Loss: 0.3189818859100342
setp: 4000, Loss: 0.32721030712127686
setp: 4100, Loss: 0.3222992420196533
setp: 4200, Loss: 0.31987714767456055
setp: 4300, Loss: 0.32235223054885864
setp: 4400, Loss: 0.3228919506072998
setp: 4500, Loss: 0.3173811435699463
setp: 4600, Loss: 0.5418627262115479
setp: 4700, Loss: 0.31704896688461304
setp: 4800, Loss: 0.3181225061416626
setp: 4900, Loss: 0.3178079128265381
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9473684210526315
precision: 0.9090909090909091
recall: 0.9090909090909091
F_score: 0.9090909090909091
model saved.
avg_acc: 0.9552631578947368, avg_f_score: 0.9096509414280499
==========arousal==========
******fold 0******
[349, 259]
training...
setp: 0, Loss: 0.6876773238182068
setp: 100, Loss: 0.6785681247711182
setp: 200, Loss: 0.6785928606987
setp: 300, Loss: 0.6168549060821533
setp: 400, Loss: 0.5463008284568787
setp: 500, Loss: 0.4693611264228821
setp: 600, Loss: 0.4384861886501312
setp: 700, Loss: 0.4921279847621918
setp: 800, Loss: 0.36188212037086487
setp: 900, Loss: 0.4870988428592682
setp: 1000, Loss: 0.3445124328136444
setp: 1100, Loss: 0.3589183986186981
setp: 1200, Loss: 0.3817158341407776
setp: 1300, Loss: 0.37941354513168335
setp: 1400, Loss: 0.34865278005599976
setp: 1500, Loss: 0.3537278175354004
setp: 1600, Loss: 0.3573094606399536
setp: 1700, Loss: 0.355936199426651
setp: 1800, Loss: 0.4209294617176056
setp: 1900, Loss: 0.34831905364990234
setp: 2000, Loss: 0.36392807960510254
setp: 2100, Loss: 0.3483918309211731
setp: 2200, Loss: 0.31756356358528137
setp: 2300, Loss: 0.31866201758384705
setp: 2400, Loss: 0.3178911507129669
setp: 2500, Loss: 0.3475516438484192
setp: 2600, Loss: 0.31924697756767273
setp: 2700, Loss: 0.31736722588539124
setp: 2800, Loss: 0.3184957802295685
setp: 2900, Loss: 0.3185417056083679
setp: 3000, Loss: 0.3183276951313019
setp: 3100, Loss: 0.4067443907260895
setp: 3200, Loss: 0.3588627576828003
setp: 3300, Loss: 0.3171079158782959
setp: 3400, Loss: 0.31761983036994934
setp: 3500, Loss: 0.318055659532547
setp: 3600, Loss: 0.318452924489975
setp: 3700, Loss: 0.31884151697158813
setp: 3800, Loss: 0.3171851336956024
setp: 3900, Loss: 0.3172781467437744
setp: 4000, Loss: 0.3488595485687256
setp: 4100, Loss: 0.31759873032569885
setp: 4200, Loss: 0.31828898191452026
setp: 4300, Loss: 0.3559566140174866
setp: 4400, Loss: 0.3499729037284851
setp: 4500, Loss: 0.32724204659461975
setp: 4600, Loss: 0.3162212669849396
setp: 4700, Loss: 0.31711557507514954
setp: 4800, Loss: 0.31772440671920776
setp: 4900, Loss: 0.3176523447036743
training successfully ended.
validating...
acc: 0.993421052631579
precision: 0.994269340974212
recall: 0.994269340974212
F_score: 0.994269340974212
validating...
acc: 0.881578947368421
precision: 0.9375
recall: 0.8522727272727273
F_score: 0.8928571428571429
******fold 1******
[350, 258]
training...
setp: 0, Loss: 0.6947611570358276
setp: 100, Loss: 0.6328373551368713
setp: 200, Loss: 0.6427521109580994
setp: 300, Loss: 0.5176958441734314
setp: 400, Loss: 0.3991655111312866
setp: 500, Loss: 0.3346823453903198
setp: 600, Loss: 0.3724338412284851
setp: 700, Loss: 0.32575950026512146
setp: 800, Loss: 0.3402808904647827
setp: 900, Loss: 0.32106608152389526
setp: 1000, Loss: 0.32192522287368774
setp: 1100, Loss: 0.32209086418151855
setp: 1200, Loss: 0.31951797008514404
setp: 1300, Loss: 0.32280611991882324
setp: 1400, Loss: 0.3189545273780823
setp: 1500, Loss: 0.32500141859054565
setp: 1600, Loss: 0.36532503366470337
setp: 1700, Loss: 0.345952570438385
setp: 1800, Loss: 0.3323734998703003
setp: 1900, Loss: 0.33403128385543823
setp: 2000, Loss: 0.33063602447509766
setp: 2100, Loss: 0.3282983601093292
setp: 2200, Loss: 0.3331592082977295
setp: 2300, Loss: 0.35534805059432983
setp: 2400, Loss: 0.3828812539577484
setp: 2500, Loss: 0.3612463176250458
setp: 2600, Loss: 0.3294123113155365
setp: 2700, Loss: 0.3258853256702423
setp: 2800, Loss: 0.3226061463356018
setp: 2900, Loss: 0.3252536952495575
setp: 3000, Loss: 0.3245496451854706
setp: 3100, Loss: 0.3226994574069977
setp: 3200, Loss: 0.3244732618331909
setp: 3300, Loss: 0.3227892220020294
setp: 3400, Loss: 0.3240252137184143
setp: 3500, Loss: 0.3212135136127472
setp: 3600, Loss: 0.32231876254081726
setp: 3700, Loss: 0.3226981461048126
setp: 3800, Loss: 0.32299551367759705
setp: 3900, Loss: 0.3228195905685425
setp: 4000, Loss: 0.32333704829216003
setp: 4100, Loss: 0.3265194892883301
setp: 4200, Loss: 0.3211171627044678
setp: 4300, Loss: 0.3208634555339813
setp: 4400, Loss: 0.3215557634830475
setp: 4500, Loss: 0.3210501968860626
setp: 4600, Loss: 0.3208945393562317
setp: 4700, Loss: 0.32027801871299744
setp: 4800, Loss: 0.32250910997390747
setp: 4900, Loss: 0.32209476828575134
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.868421052631579
precision: 0.8681318681318682
recall: 0.9080459770114943
F_score: 0.8876404494382023
******fold 2******
[363, 245]
training...
setp: 0, Loss: 0.7325312495231628
setp: 100, Loss: 0.7056918144226074
setp: 200, Loss: 0.6412395238876343
setp: 300, Loss: 0.6207305788993835
setp: 400, Loss: 0.5706375241279602
setp: 500, Loss: 0.46910521388053894
setp: 600, Loss: 0.40905630588531494
setp: 700, Loss: 0.49352091550827026
setp: 800, Loss: 0.361085444688797
setp: 900, Loss: 0.3349671959877014
setp: 1000, Loss: 0.32403749227523804
setp: 1100, Loss: 0.32043004035949707
setp: 1200, Loss: 0.3492940366268158
setp: 1300, Loss: 0.3510240614414215
setp: 1400, Loss: 0.3256111145019531
setp: 1500, Loss: 0.3276658356189728
setp: 1600, Loss: 0.3180905878543854
setp: 1700, Loss: 0.33318400382995605
setp: 1800, Loss: 0.34874266386032104
setp: 1900, Loss: 0.3196982145309448
setp: 2000, Loss: 0.3185422420501709
setp: 2100, Loss: 0.3170928359031677
setp: 2200, Loss: 0.3186458945274353
setp: 2300, Loss: 0.3401467502117157
setp: 2400, Loss: 0.3171180188655853
setp: 2500, Loss: 0.34762078523635864
setp: 2600, Loss: 0.31775879859924316
setp: 2700, Loss: 0.3162485361099243
setp: 2800, Loss: 0.31814044713974
setp: 2900, Loss: 0.3189830183982849
setp: 3000, Loss: 0.3186853528022766
setp: 3100, Loss: 0.34851038455963135
setp: 3200, Loss: 0.32132962346076965
setp: 3300, Loss: 0.33653774857521057
setp: 3400, Loss: 0.31783390045166016
setp: 3500, Loss: 0.31782621145248413
setp: 3600, Loss: 0.3175237476825714
setp: 3700, Loss: 0.3486347198486328
setp: 3800, Loss: 0.316829651594162
setp: 3900, Loss: 0.31854531168937683
setp: 4000, Loss: 0.37557628750801086
setp: 4100, Loss: 0.4139763414859772
setp: 4200, Loss: 0.31896454095840454
setp: 4300, Loss: 0.3194815218448639
setp: 4400, Loss: 0.31758859753608704
setp: 4500, Loss: 0.3170185089111328
setp: 4600, Loss: 0.3165549635887146
setp: 4700, Loss: 0.3174581527709961
setp: 4800, Loss: 0.31902584433555603
setp: 4900, Loss: 0.3191828429698944
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8552631578947368
precision: 0.782608695652174
recall: 0.972972972972973
F_score: 0.8674698795180723
******fold 3******
[335, 273]
training...
setp: 0, Loss: 0.6871579885482788
setp: 100, Loss: 0.5928505659103394
setp: 200, Loss: 0.5880070328712463
setp: 300, Loss: 0.5647504925727844
setp: 400, Loss: 0.49769729375839233
setp: 500, Loss: 0.5216148495674133
setp: 600, Loss: 0.46114540100097656
setp: 700, Loss: 0.445200115442276
setp: 800, Loss: 0.42070576548576355
setp: 900, Loss: 0.3837050795555115
setp: 1000, Loss: 0.32388871908187866
setp: 1100, Loss: 0.32743898034095764
setp: 1200, Loss: 0.322483628988266
setp: 1300, Loss: 0.3803052604198456
setp: 1400, Loss: 0.37320825457572937
setp: 1500, Loss: 0.34412798285484314
setp: 1600, Loss: 0.3471962511539459
setp: 1700, Loss: 0.32477375864982605
setp: 1800, Loss: 0.36600950360298157
setp: 1900, Loss: 0.3156544268131256
setp: 2000, Loss: 0.31859394907951355
setp: 2100, Loss: 0.35127970576286316
setp: 2200, Loss: 0.31962689757347107
setp: 2300, Loss: 0.31653979420661926
setp: 2400, Loss: 0.3224245011806488
setp: 2500, Loss: 0.3202627897262573
setp: 2600, Loss: 0.3209519684314728
setp: 2700, Loss: 0.3165038228034973
setp: 2800, Loss: 0.3193455636501312
setp: 2900, Loss: 0.31705763936042786
setp: 3000, Loss: 0.31836768984794617
setp: 3100, Loss: 0.31678205728530884
setp: 3200, Loss: 0.31886065006256104
setp: 3300, Loss: 0.31763550639152527
setp: 3400, Loss: 0.319815993309021
setp: 3500, Loss: 0.31692349910736084
setp: 3600, Loss: 0.3183218240737915
setp: 3700, Loss: 0.3191382884979248
setp: 3800, Loss: 0.31680864095687866
setp: 3900, Loss: 0.31802207231521606
setp: 4000, Loss: 0.3484129309654236
setp: 4100, Loss: 0.31909480690956116
setp: 4200, Loss: 0.5414128303527832
setp: 4300, Loss: 0.4291711151599884
setp: 4400, Loss: 0.4616765081882477
setp: 4500, Loss: 0.4361989200115204
setp: 4600, Loss: 0.3349253237247467
setp: 4700, Loss: 0.38782763481140137
setp: 4800, Loss: 0.3244868516921997
setp: 4900, Loss: 0.32210803031921387
training successfully ended.
validating...
acc: 0.993421052631579
precision: 0.996996996996997
recall: 0.991044776119403
F_score: 0.9940119760479041
validating...
acc: 0.9078947368421053
precision: 0.9680851063829787
recall: 0.8921568627450981
F_score: 0.9285714285714286
******fold 4******
[351, 257]
training...
setp: 0, Loss: 0.6921717524528503
setp: 100, Loss: 0.6523125171661377
setp: 200, Loss: 0.7067481279373169
setp: 300, Loss: 0.6216384768486023
setp: 400, Loss: 0.5246652364730835
setp: 500, Loss: 0.4747762680053711
setp: 600, Loss: 0.4532252550125122
setp: 700, Loss: 0.5352568626403809
setp: 800, Loss: 0.41178393363952637
setp: 900, Loss: 0.392561137676239
setp: 1000, Loss: 0.4324307143688202
setp: 1100, Loss: 0.3398609459400177
setp: 1200, Loss: 0.35631003975868225
setp: 1300, Loss: 0.32758936285972595
setp: 1400, Loss: 0.3700135350227356
setp: 1500, Loss: 0.3588835299015045
setp: 1600, Loss: 0.3380891680717468
setp: 1700, Loss: 0.3215230107307434
setp: 1800, Loss: 0.32441720366477966
setp: 1900, Loss: 0.3177351951599121
setp: 2000, Loss: 0.31913626194000244
setp: 2100, Loss: 0.34918472170829773
setp: 2200, Loss: 0.3716742694377899
setp: 2300, Loss: 0.3240569233894348
setp: 2400, Loss: 0.31946009397506714
setp: 2500, Loss: 0.3167887330055237
setp: 2600, Loss: 0.3173035681247711
setp: 2700, Loss: 0.3172217011451721
setp: 2800, Loss: 0.319546103477478
setp: 2900, Loss: 0.31902676820755005
setp: 3000, Loss: 0.3196709454059601
setp: 3100, Loss: 0.3172586262226105
setp: 3200, Loss: 0.3189028203487396
setp: 3300, Loss: 0.3193775415420532
setp: 3400, Loss: 0.319827139377594
setp: 3500, Loss: 0.31813085079193115
setp: 3600, Loss: 0.3190978765487671
setp: 3700, Loss: 0.3191429078578949
setp: 3800, Loss: 0.3183816969394684
setp: 3900, Loss: 0.3263244330883026
setp: 4000, Loss: 0.35200321674346924
setp: 4100, Loss: 0.39231809973716736
setp: 4200, Loss: 0.32459521293640137
setp: 4300, Loss: 0.3180508315563202
setp: 4400, Loss: 0.3163689374923706
setp: 4500, Loss: 0.3173234760761261
setp: 4600, Loss: 0.3166273236274719
setp: 4700, Loss: 0.3172413110733032
setp: 4800, Loss: 0.3187207579612732
setp: 4900, Loss: 0.31748923659324646
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9971590909090909
recall: 1.0
F_score: 0.9985775248933144
validating...
acc: 0.8947368421052632
precision: 0.9069767441860465
recall: 0.9069767441860465
F_score: 0.9069767441860465
model saved.
avg_acc: 0.881578947368421, avg_f_score: 0.8967031289141785
-------------subject: 7-------------
==========valence==========
******fold 0******
[178, 430]
training...
setp: 0, Loss: 0.6932607889175415
setp: 100, Loss: 0.6522474884986877
setp: 200, Loss: 0.5257294178009033
setp: 300, Loss: 0.5303127765655518
setp: 400, Loss: 0.5847910642623901
setp: 500, Loss: 0.3635581433773041
setp: 600, Loss: 0.39370593428611755
setp: 700, Loss: 0.49126335978507996
setp: 800, Loss: 0.4007437229156494
setp: 900, Loss: 0.3530023396015167
setp: 1000, Loss: 0.3211347758769989
setp: 1100, Loss: 0.36514532566070557
setp: 1200, Loss: 0.3474009037017822
setp: 1300, Loss: 0.31838682293891907
setp: 1400, Loss: 0.32031169533729553
setp: 1500, Loss: 0.3174046277999878
setp: 1600, Loss: 0.3168294429779053
setp: 1700, Loss: 0.3528583347797394
setp: 1800, Loss: 0.31945663690567017
setp: 1900, Loss: 0.31621718406677246
setp: 2000, Loss: 0.3654550015926361
setp: 2100, Loss: 0.34772440791130066
setp: 2200, Loss: 0.3172623813152313
setp: 2300, Loss: 0.324209064245224
setp: 2400, Loss: 0.3174580931663513
setp: 2500, Loss: 0.4520309865474701
setp: 2600, Loss: 0.32075148820877075
setp: 2700, Loss: 0.3163454234600067
setp: 2800, Loss: 0.318297803401947
setp: 2900, Loss: 0.31788891553878784
setp: 3000, Loss: 0.3178554177284241
setp: 3100, Loss: 0.3173356056213379
setp: 3200, Loss: 0.3155100643634796
setp: 3300, Loss: 0.31610962748527527
setp: 3400, Loss: 0.31746917963027954
setp: 3500, Loss: 0.3162466287612915
setp: 3600, Loss: 0.31630778312683105
setp: 3700, Loss: 0.31581372022628784
setp: 3800, Loss: 0.31653523445129395
setp: 3900, Loss: 0.45766496658325195
setp: 4000, Loss: 0.33227890729904175
setp: 4100, Loss: 0.33542948961257935
setp: 4200, Loss: 0.3166859447956085
setp: 4300, Loss: 0.32105958461761475
setp: 4400, Loss: 0.3503536581993103
setp: 4500, Loss: 0.3161414861679077
setp: 4600, Loss: 0.31567519903182983
setp: 4700, Loss: 0.3167172968387604
setp: 4800, Loss: 0.31628698110580444
setp: 4900, Loss: 0.31681668758392334
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.8653846153846154
recall: 0.9
F_score: 0.8823529411764707
******fold 1******
[179, 429]
training...
setp: 0, Loss: 0.7130760550498962
setp: 100, Loss: 0.7102544903755188
setp: 200, Loss: 0.5735065937042236
setp: 300, Loss: 0.5656532049179077
setp: 400, Loss: 0.4650736153125763
setp: 500, Loss: 0.41664233803749084
setp: 600, Loss: 0.4624347388744354
setp: 700, Loss: 0.5471354722976685
setp: 800, Loss: 0.43557003140449524
setp: 900, Loss: 0.4219784736633301
setp: 1000, Loss: 0.45190802216529846
setp: 1100, Loss: 0.4176463484764099
setp: 1200, Loss: 0.43201756477355957
setp: 1300, Loss: 0.3248864710330963
setp: 1400, Loss: 0.41634196043014526
setp: 1500, Loss: 0.3165520429611206
setp: 1600, Loss: 0.3796372413635254
setp: 1700, Loss: 0.39214026927948
setp: 1800, Loss: 0.41184425354003906
setp: 1900, Loss: 0.3805490732192993
setp: 2000, Loss: 0.34693944454193115
setp: 2100, Loss: 0.4476579427719116
setp: 2200, Loss: 0.35893234610557556
setp: 2300, Loss: 0.3870181739330292
setp: 2400, Loss: 0.3497469425201416
setp: 2500, Loss: 0.3488434851169586
setp: 2600, Loss: 0.37812668085098267
setp: 2700, Loss: 0.3786594867706299
setp: 2800, Loss: 0.34871986508369446
setp: 2900, Loss: 0.37877950072288513
setp: 3000, Loss: 0.39722177386283875
setp: 3100, Loss: 0.349428653717041
setp: 3200, Loss: 0.3471630811691284
setp: 3300, Loss: 0.4130485951900482
setp: 3400, Loss: 0.38216710090637207
setp: 3500, Loss: 0.38095152378082275
setp: 3600, Loss: 0.379367470741272
setp: 3700, Loss: 0.3817425072193146
setp: 3800, Loss: 0.4091476798057556
setp: 3900, Loss: 0.3782193958759308
setp: 4000, Loss: 0.3168206214904785
setp: 4100, Loss: 0.3789864778518677
setp: 4200, Loss: 0.3161502480506897
setp: 4300, Loss: 0.3787773847579956
setp: 4400, Loss: 0.3601258397102356
setp: 4500, Loss: 0.44017505645751953
setp: 4600, Loss: 0.3800777792930603
setp: 4700, Loss: 0.3183440566062927
setp: 4800, Loss: 0.4092317521572113
setp: 4900, Loss: 0.34751713275909424
training successfully ended.
validating...
acc: 0.9568764568764568
precision: 0.9949494949494949
recall: 0.9184149184149184
F_score: 0.955151515151515
validating...
acc: 0.8947368421052632
precision: 0.8837209302325582
recall: 0.7755102040816326
F_score: 0.826086956521739
******fold 2******
[192, 416]
training...
setp: 0, Loss: 0.6819443106651306
setp: 100, Loss: 0.5551340579986572
setp: 200, Loss: 0.6208018660545349
setp: 300, Loss: 0.5942416787147522
setp: 400, Loss: 0.6626014113426208
setp: 500, Loss: 0.5758577585220337
setp: 600, Loss: 0.6349267363548279
setp: 700, Loss: 0.49862316250801086
setp: 800, Loss: 0.46559643745422363
setp: 900, Loss: 0.4766165018081665
setp: 1000, Loss: 0.47600027918815613
setp: 1100, Loss: 0.4503648281097412
setp: 1200, Loss: 0.35500532388687134
setp: 1300, Loss: 0.3522903025150299
setp: 1400, Loss: 0.35723593831062317
setp: 1500, Loss: 0.3927069306373596
setp: 1600, Loss: 0.4118656516075134
setp: 1700, Loss: 0.37977322936058044
setp: 1800, Loss: 0.35355207324028015
setp: 1900, Loss: 0.34993693232536316
setp: 2000, Loss: 0.3467315435409546
setp: 2100, Loss: 0.3556481897830963
setp: 2200, Loss: 0.35186177492141724
setp: 2300, Loss: 0.38006383180618286
setp: 2400, Loss: 0.3834805190563202
setp: 2500, Loss: 0.3648408353328705
setp: 2600, Loss: 0.37992870807647705
setp: 2700, Loss: 0.3798176050186157
setp: 2800, Loss: 0.34839004278182983
setp: 2900, Loss: 0.381636381149292
setp: 3000, Loss: 0.3199460506439209
setp: 3100, Loss: 0.3497520983219147
setp: 3200, Loss: 0.3466261029243469
setp: 3300, Loss: 0.3498486578464508
setp: 3400, Loss: 0.34764477610588074
setp: 3500, Loss: 0.3935607671737671
setp: 3600, Loss: 0.3478933572769165
setp: 3700, Loss: 0.34816527366638184
setp: 3800, Loss: 0.3516576886177063
setp: 3900, Loss: 0.3293193578720093
setp: 4000, Loss: 0.3165929317474365
setp: 4100, Loss: 0.31819847226142883
setp: 4200, Loss: 0.32294824719429016
setp: 4300, Loss: 0.32224923372268677
setp: 4400, Loss: 0.3174283504486084
setp: 4500, Loss: 0.31699034571647644
setp: 4600, Loss: 0.320879727602005
setp: 4700, Loss: 0.3159376084804535
setp: 4800, Loss: 0.31707173585891724
setp: 4900, Loss: 0.3187636137008667
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9078947368421053
precision: 0.8235294117647058
recall: 0.7777777777777778
F_score: 0.7999999999999999
******fold 3******
[180, 428]
training...
setp: 0, Loss: 0.7037954330444336
setp: 100, Loss: 0.6125531792640686
setp: 200, Loss: 0.5011090040206909
setp: 300, Loss: 0.4963044226169586
setp: 400, Loss: 0.5476841926574707
setp: 500, Loss: 0.47603684663772583
setp: 600, Loss: 0.4968169629573822
setp: 700, Loss: 0.4527648985385895
setp: 800, Loss: 0.4502863883972168
setp: 900, Loss: 0.4551589787006378
setp: 1000, Loss: 0.4552473723888397
setp: 1100, Loss: 0.4227265417575836
setp: 1200, Loss: 0.3860476613044739
setp: 1300, Loss: 0.43688100576400757
setp: 1400, Loss: 0.4096662700176239
setp: 1500, Loss: 0.36735737323760986
setp: 1600, Loss: 0.35339096188545227
setp: 1700, Loss: 0.4582073390483856
setp: 1800, Loss: 0.3795679807662964
setp: 1900, Loss: 0.3212279677391052
setp: 2000, Loss: 0.3837931752204895
setp: 2100, Loss: 0.35053300857543945
setp: 2200, Loss: 0.3469298183917999
setp: 2300, Loss: 0.3794228136539459
setp: 2400, Loss: 0.3801898956298828
setp: 2500, Loss: 0.3794485628604889
setp: 2600, Loss: 0.41187402606010437
setp: 2700, Loss: 0.34951362013816833
setp: 2800, Loss: 0.380756676197052
setp: 2900, Loss: 0.34784823656082153
setp: 3000, Loss: 0.4150015711784363
setp: 3100, Loss: 0.32769346237182617
setp: 3200, Loss: 0.3775472342967987
setp: 3300, Loss: 0.3781764805316925
setp: 3400, Loss: 0.34842172265052795
setp: 3500, Loss: 0.3794184923171997
setp: 3600, Loss: 0.34860196709632874
setp: 3700, Loss: 0.37776732444763184
setp: 3800, Loss: 0.3481484651565552
setp: 3900, Loss: 0.37757670879364014
setp: 4000, Loss: 0.3811545670032501
setp: 4100, Loss: 0.3782936632633209
setp: 4200, Loss: 0.4234372079372406
setp: 4300, Loss: 0.34772032499313354
setp: 4400, Loss: 0.4002375900745392
setp: 4500, Loss: 0.4364960491657257
setp: 4600, Loss: 0.32691389322280884
setp: 4700, Loss: 0.37742695212364197
setp: 4800, Loss: 0.3179585635662079
setp: 4900, Loss: 0.34696343541145325
training successfully ended.
validating...
acc: 0.9544392523364486
precision: 1.0
recall: 0.9088785046728972
F_score: 0.9522643818849449
validating...
acc: 0.881578947368421
precision: 0.9411764705882353
recall: 0.6666666666666666
F_score: 0.7804878048780487
******fold 4******
[183, 425]
training...
setp: 0, Loss: 0.7567317485809326
setp: 100, Loss: 0.5949397683143616
setp: 200, Loss: 0.6479997038841248
setp: 300, Loss: 0.5530210733413696
setp: 400, Loss: 0.5294014811515808
setp: 500, Loss: 0.6205604076385498
setp: 600, Loss: 0.5216432809829712
setp: 700, Loss: 0.4593558609485626
setp: 800, Loss: 0.48379477858543396
setp: 900, Loss: 0.48580706119537354
setp: 1000, Loss: 0.426766037940979
setp: 1100, Loss: 0.42317378520965576
setp: 1200, Loss: 0.42331621050834656
setp: 1300, Loss: 0.44562700390815735
setp: 1400, Loss: 0.44265061616897583
setp: 1500, Loss: 0.4135052561759949
setp: 1600, Loss: 0.4146697521209717
setp: 1700, Loss: 0.43111276626586914
setp: 1800, Loss: 0.47129982709884644
setp: 1900, Loss: 0.44106829166412354
setp: 2000, Loss: 0.4086313247680664
setp: 2100, Loss: 0.43997007608413696
setp: 2200, Loss: 0.38019680976867676
setp: 2300, Loss: 0.430755615234375
setp: 2400, Loss: 0.4710024297237396
setp: 2500, Loss: 0.38067561388015747
setp: 2600, Loss: 0.4412896931171417
setp: 2700, Loss: 0.3813287913799286
setp: 2800, Loss: 0.4721221327781677
setp: 2900, Loss: 0.40879571437835693
setp: 3000, Loss: 0.3803918659687042
setp: 3100, Loss: 0.41039544343948364
setp: 3200, Loss: 0.4113919734954834
setp: 3300, Loss: 0.41111037135124207
setp: 3400, Loss: 0.4404503107070923
setp: 3500, Loss: 0.48435255885124207
setp: 3600, Loss: 0.38184642791748047
setp: 3700, Loss: 0.4703634977340698
setp: 3800, Loss: 0.44097673892974854
setp: 3900, Loss: 0.4087177515029907
setp: 4000, Loss: 0.4612409472465515
setp: 4100, Loss: 0.3803917467594147
setp: 4200, Loss: 0.4098527431488037
setp: 4300, Loss: 0.4695597290992737
setp: 4400, Loss: 0.363850861787796
setp: 4500, Loss: 0.41657325625419617
setp: 4600, Loss: 0.3772115707397461
setp: 4700, Loss: 0.3667619824409485
setp: 4800, Loss: 0.3499407172203064
setp: 4900, Loss: 0.35689690709114075
training successfully ended.
validating...
acc: 0.9539473684210527
precision: 1.0
recall: 0.8469945355191257
F_score: 0.9171597633136095
validating...
acc: 0.8618421052631579
precision: 0.9285714285714286
recall: 0.5777777777777777
F_score: 0.7123287671232876
model saved.
avg_acc: 0.8934210526315789, avg_f_score: 0.8002512939399091
==========arousal==========
******fold 0******
[224, 384]
training...
setp: 0, Loss: 0.7000327706336975
setp: 100, Loss: 0.6432546973228455
setp: 200, Loss: 0.6468823552131653
setp: 300, Loss: 0.5376635193824768
setp: 400, Loss: 0.38456085324287415
setp: 500, Loss: 0.353298157453537
setp: 600, Loss: 0.34321996569633484
setp: 700, Loss: 0.32536056637763977
setp: 800, Loss: 0.3259557783603668
setp: 900, Loss: 0.32159629464149475
setp: 1000, Loss: 0.32408976554870605
setp: 1100, Loss: 0.32051897048950195
setp: 1200, Loss: 0.3366902470588684
setp: 1300, Loss: 0.3708350956439972
setp: 1400, Loss: 0.3189624845981598
setp: 1500, Loss: 0.3197416663169861
setp: 1600, Loss: 0.3175560235977173
setp: 1700, Loss: 0.3179979920387268
setp: 1800, Loss: 0.31740322709083557
setp: 1900, Loss: 0.3186585009098053
setp: 2000, Loss: 0.3511716425418854
setp: 2100, Loss: 0.351930171251297
setp: 2200, Loss: 0.34610655903816223
setp: 2300, Loss: 0.31755316257476807
setp: 2400, Loss: 0.32046619057655334
setp: 2500, Loss: 0.3174154460430145
setp: 2600, Loss: 0.3177309036254883
setp: 2700, Loss: 0.31923025846481323
setp: 2800, Loss: 0.31970423460006714
setp: 2900, Loss: 0.33239802718162537
setp: 3000, Loss: 0.31748777627944946
setp: 3100, Loss: 0.31752994656562805
setp: 3200, Loss: 0.3177850842475891
setp: 3300, Loss: 0.3256801962852478
setp: 3400, Loss: 0.3239174783229828
setp: 3500, Loss: 0.31694841384887695
setp: 3600, Loss: 0.31741002202033997
setp: 3700, Loss: 0.3169479966163635
setp: 3800, Loss: 0.3175511062145233
setp: 3900, Loss: 0.32133227586746216
setp: 4000, Loss: 0.32408595085144043
setp: 4100, Loss: 0.31762468814849854
setp: 4200, Loss: 0.3165287971496582
setp: 4300, Loss: 0.31956690549850464
setp: 4400, Loss: 0.3171350359916687
setp: 4500, Loss: 0.3179313838481903
setp: 4600, Loss: 0.3188121020793915
setp: 4700, Loss: 0.32045409083366394
setp: 4800, Loss: 0.330130010843277
setp: 4900, Loss: 0.31708455085754395
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.9807692307692307
recall: 0.8360655737704918
F_score: 0.9026548672566372
******fold 1******
[229, 379]
training...
setp: 0, Loss: 0.6763715744018555
setp: 100, Loss: 0.675208568572998
setp: 200, Loss: 0.6421648859977722
setp: 300, Loss: 0.6524630784988403
setp: 400, Loss: 0.4895198345184326
setp: 500, Loss: 0.6528714299201965
setp: 600, Loss: 0.514819324016571
setp: 700, Loss: 0.6465359926223755
setp: 800, Loss: 0.4909347593784332
setp: 900, Loss: 0.3812774121761322
setp: 1000, Loss: 0.375385046005249
setp: 1100, Loss: 0.37609004974365234
setp: 1200, Loss: 0.35580530762672424
setp: 1300, Loss: 0.33357056975364685
setp: 1400, Loss: 0.31942856311798096
setp: 1500, Loss: 0.3519060015678406
setp: 1600, Loss: 0.3171252906322479
setp: 1700, Loss: 0.3165571987628937
setp: 1800, Loss: 0.3241577446460724
setp: 1900, Loss: 0.31647416949272156
setp: 2000, Loss: 0.31787607073783875
setp: 2100, Loss: 0.3468779921531677
setp: 2200, Loss: 0.38100290298461914
setp: 2300, Loss: 0.31771939992904663
setp: 2400, Loss: 0.3410852253437042
setp: 2500, Loss: 0.31597214937210083
setp: 2600, Loss: 0.31572413444519043
setp: 2700, Loss: 0.32158228754997253
setp: 2800, Loss: 0.32348868250846863
setp: 2900, Loss: 0.3154374361038208
setp: 3000, Loss: 0.3214204013347626
setp: 3100, Loss: 0.4043409824371338
setp: 3200, Loss: 0.31965652108192444
setp: 3300, Loss: 0.3282289505004883
setp: 3400, Loss: 0.32218682765960693
setp: 3500, Loss: 0.31521275639533997
setp: 3600, Loss: 0.3156939446926117
setp: 3700, Loss: 0.315818727016449
setp: 3800, Loss: 0.3156392574310303
setp: 3900, Loss: 0.3166796565055847
setp: 4000, Loss: 0.3158900737762451
setp: 4100, Loss: 0.3177780210971832
setp: 4200, Loss: 0.31645920872688293
setp: 4300, Loss: 0.31863120198249817
setp: 4400, Loss: 0.3165491223335266
setp: 4500, Loss: 0.31565046310424805
setp: 4600, Loss: 0.31705647706985474
setp: 4700, Loss: 0.3169805407524109
setp: 4800, Loss: 0.3160405457019806
setp: 4900, Loss: 0.31568410992622375
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.96
recall: 0.8571428571428571
F_score: 0.9056603773584904
******fold 2******
[228, 380]
training...
setp: 0, Loss: 0.7285094261169434
setp: 100, Loss: 0.6490276455879211
setp: 200, Loss: 0.6179737448692322
setp: 300, Loss: 0.5086383819580078
setp: 400, Loss: 0.3802853524684906
setp: 500, Loss: 0.3671543598175049
setp: 600, Loss: 0.3267292380332947
setp: 700, Loss: 0.36170637607574463
setp: 800, Loss: 0.35106173157691956
setp: 900, Loss: 0.3376666009426117
setp: 1000, Loss: 0.32573747634887695
setp: 1100, Loss: 0.3187370002269745
setp: 1200, Loss: 0.318632036447525
setp: 1300, Loss: 0.3174751102924347
setp: 1400, Loss: 0.31982672214508057
setp: 1500, Loss: 0.31917840242385864
setp: 1600, Loss: 0.3181270956993103
setp: 1700, Loss: 0.31842905282974243
setp: 1800, Loss: 0.3190612196922302
setp: 1900, Loss: 0.31964343786239624
setp: 2000, Loss: 0.319360613822937
setp: 2100, Loss: 0.3190194070339203
setp: 2200, Loss: 0.3503199517726898
setp: 2300, Loss: 0.6539778113365173
setp: 2400, Loss: 0.3735550045967102
setp: 2500, Loss: 0.3607175052165985
setp: 2600, Loss: 0.3465147018432617
setp: 2700, Loss: 0.3843168318271637
setp: 2800, Loss: 0.3267380893230438
setp: 2900, Loss: 0.33052924275398254
setp: 3000, Loss: 0.3248692452907562
setp: 3100, Loss: 0.3249017298221588
setp: 3200, Loss: 0.32237693667411804
setp: 3300, Loss: 0.32703813910484314
setp: 3400, Loss: 0.32575517892837524
setp: 3500, Loss: 0.3229389488697052
setp: 3600, Loss: 0.32320913672447205
setp: 3700, Loss: 0.3228614032268524
setp: 3800, Loss: 0.3547753691673279
setp: 3900, Loss: 0.3248794376850128
setp: 4000, Loss: 0.32313019037246704
setp: 4100, Loss: 0.38582533597946167
setp: 4200, Loss: 0.3267196714878082
setp: 4300, Loss: 0.3229873776435852
setp: 4400, Loss: 0.32589516043663025
setp: 4500, Loss: 0.33347833156585693
setp: 4600, Loss: 0.3228473961353302
setp: 4700, Loss: 0.3225962221622467
setp: 4800, Loss: 0.32372424006462097
setp: 4900, Loss: 0.3234487771987915
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9956140350877193
F_score: 0.9978021978021978
validating...
acc: 0.8618421052631579
precision: 0.8103448275862069
recall: 0.8245614035087719
F_score: 0.8173913043478261
******fold 3******
[227, 381]
training...
setp: 0, Loss: 0.6217830181121826
setp: 100, Loss: 0.6276770830154419
setp: 200, Loss: 0.6360237002372742
setp: 300, Loss: 0.5794913172721863
setp: 400, Loss: 0.7538284659385681
setp: 500, Loss: 0.7024697065353394
setp: 600, Loss: 0.48404744267463684
setp: 700, Loss: 0.5904359221458435
setp: 800, Loss: 0.49395349621772766
setp: 900, Loss: 0.352260947227478
setp: 1000, Loss: 0.38715147972106934
setp: 1100, Loss: 0.39049264788627625
setp: 1200, Loss: 0.38188114762306213
setp: 1300, Loss: 0.38071170449256897
setp: 1400, Loss: 0.34891238808631897
setp: 1500, Loss: 0.404680073261261
setp: 1600, Loss: 0.44209954142570496
setp: 1700, Loss: 0.38051483035087585
setp: 1800, Loss: 0.4305594563484192
setp: 1900, Loss: 0.32708272337913513
setp: 2000, Loss: 0.37855419516563416
setp: 2100, Loss: 0.40512436628341675
setp: 2200, Loss: 0.37853652238845825
setp: 2300, Loss: 0.3832775950431824
setp: 2400, Loss: 0.35097819566726685
setp: 2500, Loss: 0.34691929817199707
setp: 2600, Loss: 0.348211407661438
setp: 2700, Loss: 0.3481166660785675
setp: 2800, Loss: 0.34747374057769775
setp: 2900, Loss: 0.3795332908630371
setp: 3000, Loss: 0.378600537776947
setp: 3100, Loss: 0.3476647138595581
setp: 3200, Loss: 0.37802597880363464
setp: 3300, Loss: 0.31656232476234436
setp: 3400, Loss: 0.37955522537231445
setp: 3500, Loss: 0.3470628559589386
setp: 3600, Loss: 0.37826377153396606
setp: 3700, Loss: 0.37904998660087585
setp: 3800, Loss: 0.3166239261627197
setp: 3900, Loss: 0.3480461537837982
setp: 4000, Loss: 0.3781386911869049
setp: 4100, Loss: 0.37786126136779785
setp: 4200, Loss: 0.3483194410800934
setp: 4300, Loss: 0.34880509972572327
setp: 4400, Loss: 0.4065507650375366
setp: 4500, Loss: 0.39883121848106384
setp: 4600, Loss: 0.5604199767112732
setp: 4700, Loss: 0.3603209853172302
setp: 4800, Loss: 0.4117845296859741
setp: 4900, Loss: 0.3777340054512024
training successfully ended.
validating...
acc: 0.9555921052631579
precision: 0.9901960784313726
recall: 0.8898678414096917
F_score: 0.9373549883990719
validating...
acc: 0.8421052631578947
precision: 0.8541666666666666
recall: 0.7068965517241379
F_score: 0.7735849056603773
******fold 4******
[232, 376]
training...
setp: 0, Loss: 0.6616014242172241
setp: 100, Loss: 0.6928791403770447
setp: 200, Loss: 0.5520004630088806
setp: 300, Loss: 0.4223741590976715
setp: 400, Loss: 0.37061163783073425
setp: 500, Loss: 0.32695111632347107
setp: 600, Loss: 0.35265475511550903
setp: 700, Loss: 0.3888753056526184
setp: 800, Loss: 0.3241109251976013
setp: 900, Loss: 0.3285314738750458
setp: 1000, Loss: 0.32175949215888977
setp: 1100, Loss: 0.3232235312461853
setp: 1200, Loss: 0.3207464814186096
setp: 1300, Loss: 0.3194653391838074
setp: 1400, Loss: 0.31989097595214844
setp: 1500, Loss: 0.3202669024467468
setp: 1600, Loss: 0.318460613489151
setp: 1700, Loss: 0.3190988302230835
setp: 1800, Loss: 0.3187837600708008
setp: 1900, Loss: 0.3418794572353363
setp: 2000, Loss: 0.3226079046726227
setp: 2100, Loss: 0.3244630694389343
setp: 2200, Loss: 0.31967031955718994
setp: 2300, Loss: 0.3193068504333496
setp: 2400, Loss: 0.3185614347457886
setp: 2500, Loss: 0.31793713569641113
setp: 2600, Loss: 0.3793637156486511
setp: 2700, Loss: 0.31949156522750854
setp: 2800, Loss: 0.31938138604164124
setp: 2900, Loss: 0.319233238697052
setp: 3000, Loss: 0.3197750151157379
setp: 3100, Loss: 0.31878194212913513
setp: 3200, Loss: 0.3193160891532898
setp: 3300, Loss: 0.31885287165641785
setp: 3400, Loss: 0.32001373171806335
setp: 3500, Loss: 0.3180316388607025
setp: 3600, Loss: 0.31921616196632385
setp: 3700, Loss: 0.33610591292381287
setp: 3800, Loss: 0.32072630524635315
setp: 3900, Loss: 0.32236137986183167
setp: 4000, Loss: 0.3208247423171997
setp: 4100, Loss: 0.3357568383216858
setp: 4200, Loss: 0.31917285919189453
setp: 4300, Loss: 0.31804677844047546
setp: 4400, Loss: 0.31835874915122986
setp: 4500, Loss: 0.3196025788784027
setp: 4600, Loss: 0.31966373324394226
setp: 4700, Loss: 0.31954219937324524
setp: 4800, Loss: 0.31956401467323303
setp: 4900, Loss: 0.3196629285812378
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8881578947368421
precision: 0.8
recall: 0.9056603773584906
F_score: 0.8495575221238938
model saved.
avg_acc: 0.8907894736842106, avg_f_score: 0.849769795349445
-------------subject: 8-------------
==========valence==========
******fold 0******
[275, 333]
training...
setp: 0, Loss: 0.6913024187088013
setp: 100, Loss: 0.6652993559837341
setp: 200, Loss: 0.41185906529426575
setp: 300, Loss: 0.3544866442680359
setp: 400, Loss: 0.34628239274024963
setp: 500, Loss: 0.3315013349056244
setp: 600, Loss: 0.3489396870136261
setp: 700, Loss: 0.3513394296169281
setp: 800, Loss: 0.320478618144989
setp: 900, Loss: 0.32232654094696045
setp: 1000, Loss: 0.31993013620376587
setp: 1100, Loss: 0.35982397198677063
setp: 1200, Loss: 0.32644063234329224
setp: 1300, Loss: 0.3230783939361572
setp: 1400, Loss: 0.317435085773468
setp: 1500, Loss: 0.3211889863014221
setp: 1600, Loss: 0.3484824299812317
setp: 1700, Loss: 0.31780049204826355
setp: 1800, Loss: 0.31743478775024414
setp: 1900, Loss: 0.3190857172012329
setp: 2000, Loss: 0.3184124529361725
setp: 2100, Loss: 0.3185234069824219
setp: 2200, Loss: 0.3180958032608032
setp: 2300, Loss: 0.3195013105869293
setp: 2400, Loss: 0.3200032413005829
setp: 2500, Loss: 0.318534255027771
setp: 2600, Loss: 0.31946688890457153
setp: 2700, Loss: 0.3190840482711792
setp: 2800, Loss: 0.3188343048095703
setp: 2900, Loss: 0.3221138119697571
setp: 3000, Loss: 0.4160135090351105
setp: 3100, Loss: 0.3506658375263214
setp: 3200, Loss: 0.33188989758491516
setp: 3300, Loss: 0.3276340663433075
setp: 3400, Loss: 0.32456767559051514
setp: 3500, Loss: 0.3250581622123718
setp: 3600, Loss: 0.32253360748291016
setp: 3700, Loss: 0.3208385109901428
setp: 3800, Loss: 0.3231203258037567
setp: 3900, Loss: 0.32754260301589966
setp: 4000, Loss: 0.3254297375679016
setp: 4100, Loss: 0.3223244845867157
setp: 4200, Loss: 0.3216918706893921
setp: 4300, Loss: 0.3220388889312744
setp: 4400, Loss: 0.32180023193359375
setp: 4500, Loss: 0.32349979877471924
setp: 4600, Loss: 0.322878360748291
setp: 4700, Loss: 0.32179561257362366
setp: 4800, Loss: 0.322018027305603
setp: 4900, Loss: 0.32447516918182373
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8947368421052632
precision: 0.8591549295774648
recall: 0.9104477611940298
F_score: 0.8840579710144928
******fold 1******
[275, 333]
training...
setp: 0, Loss: 0.6930533647537231
setp: 100, Loss: 0.6912024021148682
setp: 200, Loss: 0.6730040907859802
setp: 300, Loss: 0.6860198974609375
setp: 400, Loss: 0.6922134757041931
setp: 500, Loss: 0.6912922859191895
setp: 600, Loss: 0.6912745833396912
setp: 700, Loss: 0.6856688857078552
setp: 800, Loss: 0.6919984221458435
setp: 900, Loss: 0.6822473406791687
setp: 1000, Loss: 0.6862423419952393
setp: 1100, Loss: 0.6855555176734924
setp: 1200, Loss: 0.7043405175209045
setp: 1300, Loss: 0.6782622337341309
setp: 1400, Loss: 0.697029709815979
setp: 1500, Loss: 0.7058830261230469
setp: 1600, Loss: 0.691636323928833
setp: 1700, Loss: 0.6868855357170105
setp: 1800, Loss: 0.6858804225921631
setp: 1900, Loss: 0.692254364490509
setp: 2000, Loss: 0.6914457082748413
setp: 2100, Loss: 0.6735870242118835
setp: 2200, Loss: 0.6857495903968811
setp: 2300, Loss: 0.6921824216842651
setp: 2400, Loss: 0.6913108825683594
setp: 2500, Loss: 0.6912932395935059
setp: 2600, Loss: 0.6856250762939453
setp: 2700, Loss: 0.6920603513717651
setp: 2800, Loss: 0.6823163032531738
setp: 2900, Loss: 0.6862346529960632
setp: 3000, Loss: 0.6855158805847168
setp: 3100, Loss: 0.7044777274131775
setp: 3200, Loss: 0.6784941554069519
setp: 3300, Loss: 0.6970851421356201
setp: 3400, Loss: 0.7061221599578857
setp: 3500, Loss: 0.6916424036026001
setp: 3600, Loss: 0.6869375705718994
setp: 3700, Loss: 0.68585604429245
setp: 3800, Loss: 0.692297637462616
setp: 3900, Loss: 0.6914426684379578
setp: 4000, Loss: 0.6737041473388672
setp: 4100, Loss: 0.685728907585144
setp: 4200, Loss: 0.6922038197517395
setp: 4300, Loss: 0.6913073062896729
setp: 4400, Loss: 0.6912932395935059
setp: 4500, Loss: 0.6856086254119873
setp: 4600, Loss: 0.6920667290687561
setp: 4700, Loss: 0.682342529296875
setp: 4800, Loss: 0.6862214207649231
setp: 4900, Loss: 0.6855052709579468
training successfully ended.
validating...
acc: 0.5476973684210527
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.5592105263157895
precision: 0
recall: 0.0
F_score: 0
******fold 2******
[276, 332]
training...
setp: 0, Loss: 0.7386595606803894
setp: 100, Loss: 0.6744507551193237
setp: 200, Loss: 0.6592593193054199
setp: 300, Loss: 0.5880704522132874
setp: 400, Loss: 0.43326452374458313
setp: 500, Loss: 0.37602221965789795
setp: 600, Loss: 0.3450120687484741
setp: 700, Loss: 0.35675522685050964
setp: 800, Loss: 0.32781147956848145
setp: 900, Loss: 0.32776281237602234
setp: 1000, Loss: 0.3237587809562683
setp: 1100, Loss: 0.3520314693450928
setp: 1200, Loss: 0.31921952962875366
setp: 1300, Loss: 0.32357653975486755
setp: 1400, Loss: 0.33407631516456604
setp: 1500, Loss: 0.3448043167591095
setp: 1600, Loss: 0.31980112195014954
setp: 1700, Loss: 0.31740519404411316
setp: 1800, Loss: 0.3184928596019745
setp: 1900, Loss: 0.3205019533634186
setp: 2000, Loss: 0.317934513092041
setp: 2100, Loss: 0.3189796209335327
setp: 2200, Loss: 0.3175020217895508
setp: 2300, Loss: 0.32029426097869873
setp: 2400, Loss: 0.3186517357826233
setp: 2500, Loss: 0.3185822367668152
setp: 2600, Loss: 0.3261946439743042
setp: 2700, Loss: 0.3385162055492401
setp: 2800, Loss: 0.32341238856315613
setp: 2900, Loss: 0.318287193775177
setp: 3000, Loss: 0.31791952252388
setp: 3100, Loss: 0.31810593605041504
setp: 3200, Loss: 0.3188636898994446
setp: 3300, Loss: 0.3170180022716522
setp: 3400, Loss: 0.318941205739975
setp: 3500, Loss: 0.3180999457836151
setp: 3600, Loss: 0.317414253950119
setp: 3700, Loss: 0.3186857998371124
setp: 3800, Loss: 0.32036226987838745
setp: 3900, Loss: 0.31805819272994995
setp: 4000, Loss: 0.31890735030174255
setp: 4100, Loss: 0.3176294267177582
setp: 4200, Loss: 0.3203357756137848
setp: 4300, Loss: 0.6647323966026306
setp: 4400, Loss: 0.37476277351379395
setp: 4500, Loss: 0.3389410376548767
setp: 4600, Loss: 0.3327966034412384
setp: 4700, Loss: 0.33661264181137085
setp: 4800, Loss: 0.3878413736820221
setp: 4900, Loss: 0.3302934169769287
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9963898916967509
recall: 1.0
F_score: 0.9981916817359855
validating...
acc: 0.9407894736842105
precision: 0.9253731343283582
recall: 0.9393939393939394
F_score: 0.9323308270676692
******fold 3******
[272, 336]
training...
setp: 0, Loss: 0.6757196187973022
setp: 100, Loss: 0.665163516998291
setp: 200, Loss: 0.6264965534210205
setp: 300, Loss: 0.5101062655448914
setp: 400, Loss: 0.3892459273338318
setp: 500, Loss: 0.35354045033454895
setp: 600, Loss: 0.32668107748031616
setp: 700, Loss: 0.33738207817077637
setp: 800, Loss: 0.32474249601364136
setp: 900, Loss: 0.3219602406024933
setp: 1000, Loss: 0.3206039071083069
setp: 1100, Loss: 0.31992557644844055
setp: 1200, Loss: 0.3211005926132202
setp: 1300, Loss: 0.3192480504512787
setp: 1400, Loss: 0.31874367594718933
setp: 1500, Loss: 0.32066336274147034
setp: 1600, Loss: 0.3177899420261383
setp: 1700, Loss: 0.42844685912132263
setp: 1800, Loss: 0.3713432550430298
setp: 1900, Loss: 0.3228153884410858
setp: 2000, Loss: 0.3187994956970215
setp: 2100, Loss: 0.3238517642021179
setp: 2200, Loss: 0.31961560249328613
setp: 2300, Loss: 0.3194434940814972
setp: 2400, Loss: 0.31915032863616943
setp: 2500, Loss: 0.3178291916847229
setp: 2600, Loss: 0.32005253434181213
setp: 2700, Loss: 0.32067611813545227
setp: 2800, Loss: 0.3189670741558075
setp: 2900, Loss: 0.3193936347961426
setp: 3000, Loss: 0.31926479935646057
setp: 3100, Loss: 0.3196094334125519
setp: 3200, Loss: 0.319174200296402
setp: 3300, Loss: 0.3192674517631531
setp: 3400, Loss: 0.3200555145740509
setp: 3500, Loss: 0.3352910876274109
setp: 3600, Loss: 0.3246830701828003
setp: 3700, Loss: 0.31831637024879456
setp: 3800, Loss: 0.32009315490722656
setp: 3900, Loss: 0.31794100999832153
setp: 4000, Loss: 0.3199126720428467
setp: 4100, Loss: 0.3185548186302185
setp: 4200, Loss: 0.32012712955474854
setp: 4300, Loss: 0.3197149634361267
setp: 4400, Loss: 0.3177299499511719
setp: 4500, Loss: 0.3197556138038635
setp: 4600, Loss: 0.3204854428768158
setp: 4700, Loss: 0.3191124200820923
setp: 4800, Loss: 0.3190781772136688
setp: 4900, Loss: 0.31923380494117737
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.9393939393939394
recall: 0.8857142857142857
F_score: 0.9117647058823529
******fold 4******
[270, 338]
training...
setp: 0, Loss: 0.7490992546081543
setp: 100, Loss: 0.7027912735939026
setp: 200, Loss: 0.6980936527252197
setp: 300, Loss: 0.6635059714317322
setp: 400, Loss: 0.49417439103126526
setp: 500, Loss: 0.403554230928421
setp: 600, Loss: 0.3285972774028778
setp: 700, Loss: 0.32812929153442383
setp: 800, Loss: 0.3245898485183716
setp: 900, Loss: 0.3250718414783478
setp: 1000, Loss: 0.3198496103286743
setp: 1100, Loss: 0.3546484112739563
setp: 1200, Loss: 0.3203355669975281
setp: 1300, Loss: 0.35124728083610535
setp: 1400, Loss: 0.3182825446128845
setp: 1500, Loss: 0.32162147760391235
setp: 1600, Loss: 0.3494810163974762
setp: 1700, Loss: 0.3180500566959381
setp: 1800, Loss: 0.318644642829895
setp: 1900, Loss: 0.35105031728744507
setp: 2000, Loss: 0.31917887926101685
setp: 2100, Loss: 0.3192313611507416
setp: 2200, Loss: 0.3199915885925293
setp: 2300, Loss: 0.31980523467063904
setp: 2400, Loss: 0.3188336491584778
setp: 2500, Loss: 0.37376028299331665
setp: 2600, Loss: 0.3429920971393585
setp: 2700, Loss: 0.32921576499938965
setp: 2800, Loss: 0.32834774255752563
setp: 2900, Loss: 0.32448771595954895
setp: 3000, Loss: 0.3262276351451874
setp: 3100, Loss: 0.3247299790382385
setp: 3200, Loss: 0.35434022545814514
setp: 3300, Loss: 0.32782313227653503
setp: 3400, Loss: 0.3256257176399231
setp: 3500, Loss: 0.32478952407836914
setp: 3600, Loss: 0.32334911823272705
setp: 3700, Loss: 0.3246006667613983
setp: 3800, Loss: 0.35714221000671387
setp: 3900, Loss: 0.3256092369556427
setp: 4000, Loss: 0.32477518916130066
setp: 4100, Loss: 0.32441776990890503
setp: 4200, Loss: 0.32522788643836975
setp: 4300, Loss: 0.3237651288509369
setp: 4400, Loss: 0.32377496361732483
setp: 4500, Loss: 0.3250074088573456
setp: 4600, Loss: 0.32423296570777893
setp: 4700, Loss: 0.3243015706539154
setp: 4800, Loss: 0.3233008086681366
setp: 4900, Loss: 0.3251105546951294
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9962825278810409
recall: 0.9925925925925926
F_score: 0.9944341372912802
validating...
acc: 0.8618421052631579
precision: 0.9473684210526315
recall: 0.75
F_score: 0.8372093023255814
model saved.
avg_acc: 0.8355263157894737, avg_f_score: 0.7130725612580193
==========arousal==========
******fold 0******
[242, 366]
training...
setp: 0, Loss: 0.6762723326683044
setp: 100, Loss: 0.6541178226470947
setp: 200, Loss: 0.5236635208129883
setp: 300, Loss: 0.547291100025177
setp: 400, Loss: 0.48733267188072205
setp: 500, Loss: 0.5053150653839111
setp: 600, Loss: 0.4475043714046478
setp: 700, Loss: 0.4939188063144684
setp: 800, Loss: 0.5213915109634399
setp: 900, Loss: 0.48007363080978394
setp: 1000, Loss: 0.5482909679412842
setp: 1100, Loss: 0.4637463092803955
setp: 1200, Loss: 0.44506898522377014
setp: 1300, Loss: 0.44432032108306885
setp: 1400, Loss: 0.4563220739364624
setp: 1500, Loss: 0.34942418336868286
setp: 1600, Loss: 0.3552853763103485
setp: 1700, Loss: 0.46817246079444885
setp: 1800, Loss: 0.35573434829711914
setp: 1900, Loss: 0.46667537093162537
setp: 2000, Loss: 0.4606487452983856
setp: 2100, Loss: 0.34900063276290894
setp: 2200, Loss: 0.47869089245796204
setp: 2300, Loss: 0.32057854533195496
setp: 2400, Loss: 0.4713239073753357
setp: 2500, Loss: 0.3961069881916046
setp: 2600, Loss: 0.40971115231513977
setp: 2700, Loss: 0.4396163523197174
setp: 2800, Loss: 0.3883017301559448
setp: 2900, Loss: 0.37005606293678284
setp: 3000, Loss: 0.37274929881095886
setp: 3100, Loss: 0.3817391097545624
setp: 3200, Loss: 0.35786956548690796
setp: 3300, Loss: 0.36507344245910645
setp: 3400, Loss: 0.3473756015300751
setp: 3500, Loss: 0.31735771894454956
setp: 3600, Loss: 0.3194286823272705
setp: 3700, Loss: 0.3386252224445343
setp: 3800, Loss: 0.34017398953437805
setp: 3900, Loss: 0.34434425830841064
setp: 4000, Loss: 0.3163805305957794
setp: 4100, Loss: 0.37464702129364014
setp: 4200, Loss: 0.31636306643486023
setp: 4300, Loss: 0.32299092411994934
setp: 4400, Loss: 0.4040766954421997
setp: 4500, Loss: 0.38637658953666687
setp: 4600, Loss: 0.40378525853157043
setp: 4700, Loss: 0.3856264650821686
setp: 4800, Loss: 0.31774091720581055
setp: 4900, Loss: 0.34693068265914917
training successfully ended.
validating...
acc: 0.9868421052631579
precision: 1.0
recall: 0.9669421487603306
F_score: 0.9831932773109243
validating...
acc: 0.8421052631578947
precision: 0.8166666666666667
recall: 0.7903225806451613
F_score: 0.8032786885245902
******fold 1******
[245, 363]
training...
setp: 0, Loss: 0.6633307337760925
setp: 100, Loss: 0.6631181240081787
setp: 200, Loss: 0.5450786352157593
setp: 300, Loss: 0.48924723267555237
setp: 400, Loss: 0.38261252641677856
setp: 500, Loss: 0.35135388374328613
setp: 600, Loss: 0.3244667947292328
setp: 700, Loss: 0.35113000869750977
setp: 800, Loss: 0.3226999342441559
setp: 900, Loss: 0.3208219110965729
setp: 1000, Loss: 0.3525264859199524
setp: 1100, Loss: 0.3873986005783081
setp: 1200, Loss: 0.32772621512413025
setp: 1300, Loss: 0.34937822818756104
setp: 1400, Loss: 0.31920045614242554
setp: 1500, Loss: 0.3201194703578949
setp: 1600, Loss: 0.31889405846595764
setp: 1700, Loss: 0.3199060261249542
setp: 1800, Loss: 0.3213554322719574
setp: 1900, Loss: 0.32078787684440613
setp: 2000, Loss: 0.3192293643951416
setp: 2100, Loss: 0.31862571835517883
setp: 2200, Loss: 0.3246602416038513
setp: 2300, Loss: 0.3271082639694214
setp: 2400, Loss: 0.3191671669483185
setp: 2500, Loss: 0.32082948088645935
setp: 2600, Loss: 0.3203756809234619
setp: 2700, Loss: 0.3193667232990265
setp: 2800, Loss: 0.31956470012664795
setp: 2900, Loss: 0.3235210180282593
setp: 3000, Loss: 0.3231342136859894
setp: 3100, Loss: 0.31790536642074585
setp: 3200, Loss: 0.3200382590293884
setp: 3300, Loss: 0.3192181885242462
setp: 3400, Loss: 0.31996116042137146
setp: 3500, Loss: 0.3185786306858063
setp: 3600, Loss: 0.3185197412967682
setp: 3700, Loss: 0.3186098337173462
setp: 3800, Loss: 0.31889674067497253
setp: 3900, Loss: 0.31859374046325684
setp: 4000, Loss: 0.3191756010055542
setp: 4100, Loss: 0.32648923993110657
setp: 4200, Loss: 0.3187115490436554
setp: 4300, Loss: 0.32093048095703125
setp: 4400, Loss: 0.3184310495853424
setp: 4500, Loss: 0.3185350000858307
setp: 4600, Loss: 0.3184037208557129
setp: 4700, Loss: 0.31812238693237305
setp: 4800, Loss: 0.3191539943218231
setp: 4900, Loss: 0.319334477186203
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9144736842105263
precision: 0.9791666666666666
recall: 0.7966101694915254
F_score: 0.8785046728971962
******fold 2******
[242, 366]
training...
setp: 0, Loss: 0.7006158828735352
setp: 100, Loss: 0.6324787735939026
setp: 200, Loss: 0.5014569759368896
setp: 300, Loss: 0.416714072227478
setp: 400, Loss: 0.34959760308265686
setp: 500, Loss: 0.369648814201355
setp: 600, Loss: 0.3353583514690399
setp: 700, Loss: 0.3280664384365082
setp: 800, Loss: 0.32510247826576233
setp: 900, Loss: 0.32199087738990784
setp: 1000, Loss: 0.32942715287208557
setp: 1100, Loss: 0.33511969447135925
setp: 1200, Loss: 0.33535683155059814
setp: 1300, Loss: 0.32682865858078003
setp: 1400, Loss: 0.31713998317718506
setp: 1500, Loss: 0.31857821345329285
setp: 1600, Loss: 0.3179716467857361
setp: 1700, Loss: 0.31858763098716736
setp: 1800, Loss: 0.31918153166770935
setp: 1900, Loss: 0.32053887844085693
setp: 2000, Loss: 0.3184410631656647
setp: 2100, Loss: 0.3177371621131897
setp: 2200, Loss: 0.31867292523384094
setp: 2300, Loss: 0.31890255212783813
setp: 2400, Loss: 0.3188082277774811
setp: 2500, Loss: 0.31847628951072693
setp: 2600, Loss: 0.3194848597049713
setp: 2700, Loss: 0.31833183765411377
setp: 2800, Loss: 0.318323016166687
setp: 2900, Loss: 0.3187809884548187
setp: 3000, Loss: 0.3202269375324249
setp: 3100, Loss: 0.5159305334091187
setp: 3200, Loss: 0.3665520250797272
setp: 3300, Loss: 0.32973307371139526
setp: 3400, Loss: 0.33393236994743347
setp: 3500, Loss: 0.3237152099609375
setp: 3600, Loss: 0.33305245637893677
setp: 3700, Loss: 0.32403966784477234
setp: 3800, Loss: 0.332819402217865
setp: 3900, Loss: 0.3256284296512604
setp: 4000, Loss: 0.32079243659973145
setp: 4100, Loss: 0.3517198860645294
setp: 4200, Loss: 0.32654890418052673
setp: 4300, Loss: 0.32162803411483765
setp: 4400, Loss: 0.32273560762405396
setp: 4500, Loss: 0.3242252767086029
setp: 4600, Loss: 0.3220193386077881
setp: 4700, Loss: 0.3218616843223572
setp: 4800, Loss: 0.3237549066543579
setp: 4900, Loss: 0.32466837763786316
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9013157894736842
precision: 0.8615384615384616
recall: 0.9032258064516129
F_score: 0.8818897637795274
******fold 3******
[248, 360]
training...
setp: 0, Loss: 0.6784699559211731
setp: 100, Loss: 0.5806655287742615
setp: 200, Loss: 0.5577536225318909
setp: 300, Loss: 0.6224061250686646
setp: 400, Loss: 0.38278692960739136
setp: 500, Loss: 0.3687328100204468
setp: 600, Loss: 0.38782423734664917
setp: 700, Loss: 0.3614747226238251
setp: 800, Loss: 0.3199480175971985
setp: 900, Loss: 0.3325231671333313
setp: 1000, Loss: 0.3193221390247345
setp: 1100, Loss: 0.3247118294239044
setp: 1200, Loss: 0.318146288394928
setp: 1300, Loss: 0.31827181577682495
setp: 1400, Loss: 0.32151827216148376
setp: 1500, Loss: 0.3183128535747528
setp: 1600, Loss: 0.3197273015975952
setp: 1700, Loss: 0.3172166347503662
setp: 1800, Loss: 0.3200533092021942
setp: 1900, Loss: 0.32012510299682617
setp: 2000, Loss: 0.3168785572052002
setp: 2100, Loss: 0.317617803812027
setp: 2200, Loss: 0.3177657425403595
setp: 2300, Loss: 0.3182273507118225
setp: 2400, Loss: 0.31725552678108215
setp: 2500, Loss: 0.31703805923461914
setp: 2600, Loss: 0.6134591698646545
setp: 2700, Loss: 0.36866113543510437
setp: 2800, Loss: 0.3436306416988373
setp: 2900, Loss: 0.3358762264251709
setp: 3000, Loss: 0.33125290274620056
setp: 3100, Loss: 0.3271523714065552
setp: 3200, Loss: 0.3297586441040039
setp: 3300, Loss: 0.32863232493400574
setp: 3400, Loss: 0.3375171720981598
setp: 3500, Loss: 0.3365077078342438
setp: 3600, Loss: 0.328899621963501
setp: 3700, Loss: 0.3274654746055603
setp: 3800, Loss: 0.32638421654701233
setp: 3900, Loss: 0.3227968215942383
setp: 4000, Loss: 0.3233794867992401
setp: 4100, Loss: 0.3235069215297699
setp: 4200, Loss: 0.32336920499801636
setp: 4300, Loss: 0.3235521912574768
setp: 4400, Loss: 0.3249104619026184
setp: 4500, Loss: 0.32567277550697327
setp: 4600, Loss: 0.32416850328445435
setp: 4700, Loss: 0.3232012987136841
setp: 4800, Loss: 0.3242371082305908
setp: 4900, Loss: 0.32522180676460266
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8947368421052632
precision: 0.8225806451612904
recall: 0.9107142857142857
F_score: 0.864406779661017
******fold 4******
[239, 369]
training...
setp: 0, Loss: 0.6748347282409668
setp: 100, Loss: 0.5915894508361816
setp: 200, Loss: 0.6249198913574219
setp: 300, Loss: 0.6582704782485962
setp: 400, Loss: 0.5626855492591858
setp: 500, Loss: 0.3878864347934723
setp: 600, Loss: 0.3696301579475403
setp: 700, Loss: 0.36013007164001465
setp: 800, Loss: 0.3616962730884552
setp: 900, Loss: 0.3222491443157196
setp: 1000, Loss: 0.3203868865966797
setp: 1100, Loss: 0.32209137082099915
setp: 1200, Loss: 0.3194233775138855
setp: 1300, Loss: 0.32649317383766174
setp: 1400, Loss: 0.4103963375091553
setp: 1500, Loss: 0.3189795911312103
setp: 1600, Loss: 0.3179378807544708
setp: 1700, Loss: 0.31652435660362244
setp: 1800, Loss: 0.34774309396743774
setp: 1900, Loss: 0.31954020261764526
setp: 2000, Loss: 0.31636661291122437
setp: 2100, Loss: 0.31750425696372986
setp: 2200, Loss: 0.3173028528690338
setp: 2300, Loss: 0.3315596282482147
setp: 2400, Loss: 0.3210658133029938
setp: 2500, Loss: 0.3161044418811798
setp: 2600, Loss: 0.31860142946243286
setp: 2700, Loss: 0.31606602668762207
setp: 2800, Loss: 0.31973305344581604
setp: 2900, Loss: 0.31694871187210083
setp: 3000, Loss: 0.3184925615787506
setp: 3100, Loss: 0.3179594576358795
setp: 3200, Loss: 0.3183589577674866
setp: 3300, Loss: 0.31643226742744446
setp: 3400, Loss: 0.3183988034725189
setp: 3500, Loss: 0.3171885907649994
setp: 3600, Loss: 0.31729501485824585
setp: 3700, Loss: 0.36559605598449707
setp: 3800, Loss: 0.3330560028553009
setp: 3900, Loss: 0.316588431596756
setp: 4000, Loss: 0.3172762095928192
setp: 4100, Loss: 0.3184514045715332
setp: 4200, Loss: 0.317223459482193
setp: 4300, Loss: 0.31617966294288635
setp: 4400, Loss: 0.31585693359375
setp: 4500, Loss: 0.31719544529914856
setp: 4600, Loss: 0.31668806076049805
setp: 4700, Loss: 0.31750616431236267
setp: 4800, Loss: 0.316759318113327
setp: 4900, Loss: 0.41621822118759155
training successfully ended.
validating...
acc: 0.9917763157894737
precision: 0.9795081967213115
recall: 1.0
F_score: 0.989648033126294
validating...
acc: 0.868421052631579
precision: 0.7777777777777778
recall: 0.9692307692307692
F_score: 0.863013698630137
model saved.
avg_acc: 0.8842105263157896, avg_f_score: 0.8582187206984935
-------------subject: 9-------------
==========valence==========
******fold 0******
[292, 316]
training...
setp: 0, Loss: 0.7060950398445129
setp: 100, Loss: 0.6949488520622253
setp: 200, Loss: 0.6969092488288879
setp: 300, Loss: 0.6913871765136719
setp: 400, Loss: 0.688099205493927
setp: 500, Loss: 0.6898311376571655
setp: 600, Loss: 0.6887744665145874
setp: 700, Loss: 0.6939087510108948
setp: 800, Loss: 0.691764771938324
setp: 900, Loss: 0.6897000670433044
setp: 1000, Loss: 0.6997639536857605
setp: 1100, Loss: 0.6961513161659241
setp: 1200, Loss: 0.6917993426322937
setp: 1300, Loss: 0.6916123628616333
setp: 1400, Loss: 0.699810266494751
setp: 1500, Loss: 0.6916053295135498
setp: 1600, Loss: 0.688454806804657
setp: 1700, Loss: 0.6892944574356079
setp: 1800, Loss: 0.6913954019546509
setp: 1900, Loss: 0.6955738663673401
setp: 2000, Loss: 0.6952973008155823
setp: 2100, Loss: 0.6964325904846191
setp: 2200, Loss: 0.6914447546005249
setp: 2300, Loss: 0.6883630156517029
setp: 2400, Loss: 0.6900131106376648
setp: 2500, Loss: 0.6887760162353516
setp: 2600, Loss: 0.6938803195953369
setp: 2700, Loss: 0.6918531060218811
setp: 2800, Loss: 0.6898756623268127
setp: 2900, Loss: 0.6998945474624634
setp: 3000, Loss: 0.696049153804779
setp: 3100, Loss: 0.691881000995636
setp: 3200, Loss: 0.6916476488113403
setp: 3300, Loss: 0.6999050974845886
setp: 3400, Loss: 0.6916406154632568
setp: 3500, Loss: 0.688740611076355
setp: 3600, Loss: 0.6893461346626282
setp: 3700, Loss: 0.69139564037323
setp: 3800, Loss: 0.6954721808433533
setp: 3900, Loss: 0.6951903104782104
setp: 4000, Loss: 0.6964316368103027
setp: 4100, Loss: 0.6914510726928711
setp: 4200, Loss: 0.6885154843330383
setp: 4300, Loss: 0.6900919079780579
setp: 4400, Loss: 0.6887604594230652
setp: 4500, Loss: 0.6938658952713013
setp: 4600, Loss: 0.6918805837631226
setp: 4700, Loss: 0.6899215579032898
setp: 4800, Loss: 0.6999519467353821
setp: 4900, Loss: 0.6960143446922302
training successfully ended.
validating...
acc: 0.5197368421052632
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.5460526315789473
precision: 0
recall: 0.0
F_score: 0
******fold 1******
[286, 322]
training...
setp: 0, Loss: 0.7070941925048828
setp: 100, Loss: 0.6681486964225769
setp: 200, Loss: 0.6078175902366638
setp: 300, Loss: 0.5347605347633362
setp: 400, Loss: 0.5748680233955383
setp: 500, Loss: 0.5426729321479797
setp: 600, Loss: 0.6374317407608032
setp: 700, Loss: 0.47773170471191406
setp: 800, Loss: 0.585379958152771
setp: 900, Loss: 0.4626651108264923
setp: 1000, Loss: 0.5768321752548218
setp: 1100, Loss: 0.5095198750495911
setp: 1200, Loss: 0.5046097636222839
setp: 1300, Loss: 0.44498008489608765
setp: 1400, Loss: 0.5078440308570862
setp: 1500, Loss: 0.5257366895675659
setp: 1600, Loss: 0.48873960971832275
setp: 1700, Loss: 0.48109596967697144
setp: 1800, Loss: 0.5657060146331787
setp: 1900, Loss: 0.533354640007019
setp: 2000, Loss: 0.5673738718032837
setp: 2100, Loss: 0.5414557456970215
setp: 2200, Loss: 0.4914594292640686
setp: 2300, Loss: 0.5105335712432861
setp: 2400, Loss: 0.44866299629211426
setp: 2500, Loss: 0.5517991185188293
setp: 2600, Loss: 0.4254670739173889
setp: 2700, Loss: 0.4620112180709839
setp: 2800, Loss: 0.3989409804344177
setp: 2900, Loss: 0.4657723605632782
setp: 3000, Loss: 0.38143858313560486
setp: 3100, Loss: 0.3804614543914795
setp: 3200, Loss: 0.35029175877571106
setp: 3300, Loss: 0.3541943430900574
setp: 3400, Loss: 0.39061683416366577
setp: 3500, Loss: 0.38165831565856934
setp: 3600, Loss: 0.3491344451904297
setp: 3700, Loss: 0.37887540459632874
setp: 3800, Loss: 0.3866841197013855
setp: 3900, Loss: 0.35162875056266785
setp: 4000, Loss: 0.32466840744018555
setp: 4100, Loss: 0.3500090539455414
setp: 4200, Loss: 0.3823920488357544
setp: 4300, Loss: 0.3244197368621826
setp: 4400, Loss: 0.35947084426879883
setp: 4500, Loss: 0.34966352581977844
setp: 4600, Loss: 0.34930458664894104
setp: 4700, Loss: 0.3201250731945038
setp: 4800, Loss: 0.35117581486701965
setp: 4900, Loss: 0.3502233922481537
training successfully ended.
validating...
acc: 0.9736842105263158
precision: 0.9963235294117647
recall: 0.9475524475524476
F_score: 0.971326164874552
validating...
acc: 0.881578947368421
precision: 0.9384615384615385
recall: 0.8133333333333334
F_score: 0.8714285714285714
******fold 2******
[286, 322]
training...
setp: 0, Loss: 0.6937541365623474
setp: 100, Loss: 0.683059811592102
setp: 200, Loss: 0.6914098858833313
setp: 300, Loss: 0.5657453536987305
setp: 400, Loss: 0.5491105318069458
setp: 500, Loss: 0.5067542791366577
setp: 600, Loss: 0.5753957629203796
setp: 700, Loss: 0.5203984975814819
setp: 800, Loss: 0.504230260848999
setp: 900, Loss: 0.499027281999588
setp: 1000, Loss: 0.5639786124229431
setp: 1100, Loss: 0.4248714745044708
setp: 1200, Loss: 0.505800724029541
setp: 1300, Loss: 0.5275238752365112
setp: 1400, Loss: 0.4523281157016754
setp: 1500, Loss: 0.4525107741355896
setp: 1600, Loss: 0.536777138710022
setp: 1700, Loss: 0.4657994210720062
setp: 1800, Loss: 0.42208799719810486
setp: 1900, Loss: 0.5003423094749451
setp: 2000, Loss: 0.4784083068370819
setp: 2100, Loss: 0.5496063232421875
setp: 2200, Loss: 0.4353269636631012
setp: 2300, Loss: 0.39121192693710327
setp: 2400, Loss: 0.37503716349601746
setp: 2500, Loss: 0.4840638339519501
setp: 2600, Loss: 0.4504053294658661
setp: 2700, Loss: 0.41501694917678833
setp: 2800, Loss: 0.4538390338420868
setp: 2900, Loss: 0.4618931710720062
setp: 3000, Loss: 0.4122963547706604
setp: 3100, Loss: 0.4468255639076233
setp: 3200, Loss: 0.45006945729255676
setp: 3300, Loss: 0.3805112838745117
setp: 3400, Loss: 0.48892849683761597
setp: 3500, Loss: 0.427355021238327
setp: 3600, Loss: 0.38019657135009766
setp: 3700, Loss: 0.3817961513996124
setp: 3800, Loss: 0.42060092091560364
setp: 3900, Loss: 0.39243656396865845
setp: 4000, Loss: 0.4149267077445984
setp: 4100, Loss: 0.3500470817089081
setp: 4200, Loss: 0.34966301918029785
setp: 4300, Loss: 0.3477737009525299
setp: 4400, Loss: 0.35111895203590393
setp: 4500, Loss: 0.3512312173843384
setp: 4600, Loss: 0.34900787472724915
setp: 4700, Loss: 0.37886902689933777
setp: 4800, Loss: 0.34935683012008667
setp: 4900, Loss: 0.34817835688591003
training successfully ended.
validating...
acc: 0.9703947368421053
precision: 0.9926470588235294
recall: 0.9440559440559441
F_score: 0.967741935483871
validating...
acc: 0.9013157894736842
precision: 0.8846153846153846
recall: 0.92
F_score: 0.9019607843137256
******fold 3******
[294, 314]
training...
setp: 0, Loss: 0.7183810472488403
setp: 100, Loss: 0.6974337100982666
setp: 200, Loss: 0.6351443529129028
setp: 300, Loss: 0.592217743396759
setp: 400, Loss: 0.525557816028595
setp: 500, Loss: 0.5437526106834412
setp: 600, Loss: 0.5642404556274414
setp: 700, Loss: 0.5605321526527405
setp: 800, Loss: 0.5164312720298767
setp: 900, Loss: 0.5317152738571167
setp: 1000, Loss: 0.4833381474018097
setp: 1100, Loss: 0.4086611866950989
setp: 1200, Loss: 0.48066771030426025
setp: 1300, Loss: 0.5117924809455872
setp: 1400, Loss: 0.3914811611175537
setp: 1500, Loss: 0.3906582295894623
setp: 1600, Loss: 0.47156286239624023
setp: 1700, Loss: 0.3633340299129486
setp: 1800, Loss: 0.48125043511390686
setp: 1900, Loss: 0.47735396027565
setp: 2000, Loss: 0.3572332561016083
setp: 2100, Loss: 0.480337530374527
setp: 2200, Loss: 0.3924616277217865
setp: 2300, Loss: 0.3953215181827545
setp: 2400, Loss: 0.38355737924575806
setp: 2500, Loss: 0.38876914978027344
setp: 2600, Loss: 0.3444606363773346
setp: 2700, Loss: 0.4189220666885376
setp: 2800, Loss: 0.5056061744689941
setp: 2900, Loss: 0.4338604509830475
setp: 3000, Loss: 0.3491588234901428
setp: 3100, Loss: 0.37986624240875244
setp: 3200, Loss: 0.4725942611694336
setp: 3300, Loss: 0.3881973326206207
setp: 3400, Loss: 0.4825759828090668
setp: 3500, Loss: 0.4428419768810272
setp: 3600, Loss: 0.3585432767868042
setp: 3700, Loss: 0.43100643157958984
setp: 3800, Loss: 0.437006413936615
setp: 3900, Loss: 0.3479198217391968
setp: 4000, Loss: 0.3550305962562561
setp: 4100, Loss: 0.34823235869407654
setp: 4200, Loss: 0.379927396774292
setp: 4300, Loss: 0.351243793964386
setp: 4400, Loss: 0.38112056255340576
setp: 4500, Loss: 0.3213284909725189
setp: 4600, Loss: 0.40398404002189636
setp: 4700, Loss: 0.394136905670166
setp: 4800, Loss: 0.37899431586265564
setp: 4900, Loss: 0.34813690185546875
training successfully ended.
validating...
acc: 0.9605263157894737
precision: 0.972027972027972
recall: 0.9455782312925171
F_score: 0.9586206896551724
validating...
acc: 0.8947368421052632
precision: 0.8923076923076924
recall: 0.8656716417910447
F_score: 0.8787878787878788
******fold 4******
[286, 322]
training...
setp: 0, Loss: 0.714626669883728
setp: 100, Loss: 0.6323099732398987
setp: 200, Loss: 0.6422897577285767
setp: 300, Loss: 0.5658366084098816
setp: 400, Loss: 0.4607934057712555
setp: 500, Loss: 0.5204325914382935
setp: 600, Loss: 0.5961530804634094
setp: 700, Loss: 0.5248493552207947
setp: 800, Loss: 0.558404803276062
setp: 900, Loss: 0.5061083436012268
setp: 1000, Loss: 0.4785851240158081
setp: 1100, Loss: 0.47586166858673096
setp: 1200, Loss: 0.4722364544868469
setp: 1300, Loss: 0.49097704887390137
setp: 1400, Loss: 0.5006052255630493
setp: 1500, Loss: 0.3964930772781372
setp: 1600, Loss: 0.48190954327583313
setp: 1700, Loss: 0.44313785433769226
setp: 1800, Loss: 0.4630192816257477
setp: 1900, Loss: 0.4558320641517639
setp: 2000, Loss: 0.3938177525997162
setp: 2100, Loss: 0.4146231412887573
setp: 2200, Loss: 0.42472270131111145
setp: 2300, Loss: 0.38364771008491516
setp: 2400, Loss: 0.4089382290840149
setp: 2500, Loss: 0.45794209837913513
setp: 2600, Loss: 0.36489322781562805
setp: 2700, Loss: 0.3603343963623047
setp: 2800, Loss: 0.4236402213573456
setp: 2900, Loss: 0.3659515976905823
setp: 3000, Loss: 0.32129642367362976
setp: 3100, Loss: 0.37799370288848877
setp: 3200, Loss: 0.3511107861995697
setp: 3300, Loss: 0.35068726539611816
setp: 3400, Loss: 0.34786325693130493
setp: 3500, Loss: 0.37949851155281067
setp: 3600, Loss: 0.3496856987476349
setp: 3700, Loss: 0.37160825729370117
setp: 3800, Loss: 0.3587122857570648
setp: 3900, Loss: 0.3817891776561737
setp: 4000, Loss: 0.32151588797569275
setp: 4100, Loss: 0.34837666153907776
setp: 4200, Loss: 0.35022273659706116
setp: 4300, Loss: 0.3787630498409271
setp: 4400, Loss: 0.34943118691444397
setp: 4500, Loss: 0.43951937556266785
setp: 4600, Loss: 0.32227903604507446
setp: 4700, Loss: 0.37878766655921936
setp: 4800, Loss: 0.34815046191215515
setp: 4900, Loss: 0.31685635447502136
training successfully ended.
validating...
acc: 0.962171052631579
precision: 0.9962264150943396
recall: 0.9230769230769231
F_score: 0.9582577132486388
validating...
acc: 0.8486842105263158
precision: 0.9333333333333333
recall: 0.7466666666666667
F_score: 0.8296296296296296
model saved.
avg_acc: 0.8144736842105263, avg_f_score: 0.6963613728319611
==========arousal==========
******fold 0******
[229, 379]
training...
setp: 0, Loss: 0.6752203702926636
setp: 100, Loss: 0.6614820957183838
setp: 200, Loss: 0.6797067523002625
setp: 300, Loss: 0.6719697117805481
setp: 400, Loss: 0.5837506651878357
setp: 500, Loss: 0.5200613737106323
setp: 600, Loss: 0.4449790418148041
setp: 700, Loss: 0.3773654103279114
setp: 800, Loss: 0.36502254009246826
setp: 900, Loss: 0.3355906009674072
setp: 1000, Loss: 0.3256438970565796
setp: 1100, Loss: 0.35264667868614197
setp: 1200, Loss: 0.32359224557876587
setp: 1300, Loss: 0.32428497076034546
setp: 1400, Loss: 0.324929416179657
setp: 1500, Loss: 0.31994932889938354
setp: 1600, Loss: 0.32390087842941284
setp: 1700, Loss: 0.3192984163761139
setp: 1800, Loss: 0.32189488410949707
setp: 1900, Loss: 0.3212948739528656
setp: 2000, Loss: 0.3201083838939667
setp: 2100, Loss: 0.3683475852012634
setp: 2200, Loss: 0.3222496509552002
setp: 2300, Loss: 0.318852961063385
setp: 2400, Loss: 0.31990066170692444
setp: 2500, Loss: 0.31950947642326355
setp: 2600, Loss: 0.31897300481796265
setp: 2700, Loss: 0.31853005290031433
setp: 2800, Loss: 0.3200320899486542
setp: 2900, Loss: 0.31872129440307617
setp: 3000, Loss: 0.3194262683391571
setp: 3100, Loss: 0.3205602765083313
setp: 3200, Loss: 0.3192170262336731
setp: 3300, Loss: 0.3467307686805725
setp: 3400, Loss: 0.3355059027671814
setp: 3500, Loss: 0.32945674657821655
setp: 3600, Loss: 0.31831109523773193
setp: 3700, Loss: 0.31962355971336365
setp: 3800, Loss: 0.32015112042427063
setp: 3900, Loss: 0.3194725513458252
setp: 4000, Loss: 0.3189084827899933
setp: 4100, Loss: 0.32003557682037354
setp: 4200, Loss: 0.3192260265350342
setp: 4300, Loss: 0.3192552924156189
setp: 4400, Loss: 0.3195004165172577
setp: 4500, Loss: 0.3195807933807373
setp: 4600, Loss: 0.32251742482185364
setp: 4700, Loss: 0.3393360674381256
setp: 4800, Loss: 0.32953161001205444
setp: 4900, Loss: 0.32087135314941406
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.875
precision: 0.8627450980392157
recall: 0.7857142857142857
F_score: 0.8224299065420562
******fold 1******
[228, 380]
training...
setp: 0, Loss: 0.6884720325469971
setp: 100, Loss: 0.6883556246757507
setp: 200, Loss: 0.6483105421066284
setp: 300, Loss: 0.5695747137069702
setp: 400, Loss: 0.48729899525642395
setp: 500, Loss: 0.46915215253829956
setp: 600, Loss: 0.38747310638427734
setp: 700, Loss: 0.3342895209789276
setp: 800, Loss: 0.3612155616283417
setp: 900, Loss: 0.36182400584220886
setp: 1000, Loss: 0.3245304524898529
setp: 1100, Loss: 0.3244377672672272
setp: 1200, Loss: 0.32381799817085266
setp: 1300, Loss: 0.3257574737071991
setp: 1400, Loss: 0.41697216033935547
setp: 1500, Loss: 0.3283340036869049
setp: 1600, Loss: 0.32777872681617737
setp: 1700, Loss: 0.33084094524383545
setp: 1800, Loss: 0.32144585251808167
setp: 1900, Loss: 0.3214060068130493
setp: 2000, Loss: 0.32138508558273315
setp: 2100, Loss: 0.3202226459980011
setp: 2200, Loss: 0.3202422857284546
setp: 2300, Loss: 0.32038265466690063
setp: 2400, Loss: 0.3218837380409241
setp: 2500, Loss: 0.320883184671402
setp: 2600, Loss: 0.3199567496776581
setp: 2700, Loss: 0.31949707865715027
setp: 2800, Loss: 0.3691522777080536
setp: 2900, Loss: 0.37743157148361206
setp: 3000, Loss: 0.3721068203449249
setp: 3100, Loss: 0.3476544916629791
setp: 3200, Loss: 0.32098686695098877
setp: 3300, Loss: 0.32077622413635254
setp: 3400, Loss: 0.3196864426136017
setp: 3500, Loss: 0.3210585415363312
setp: 3600, Loss: 0.3216152489185333
setp: 3700, Loss: 0.3213193714618683
setp: 3800, Loss: 0.3218142092227936
setp: 3900, Loss: 0.3211327791213989
setp: 4000, Loss: 0.32000672817230225
setp: 4100, Loss: 0.3200817108154297
setp: 4200, Loss: 0.3203853368759155
setp: 4300, Loss: 0.3209029734134674
setp: 4400, Loss: 0.32033079862594604
setp: 4500, Loss: 0.3200056552886963
setp: 4600, Loss: 0.31924760341644287
setp: 4700, Loss: 0.4506380558013916
setp: 4800, Loss: 0.3238334059715271
setp: 4900, Loss: 0.3418111801147461
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9444444444444444
recall: 0.8947368421052632
F_score: 0.918918918918919
******fold 2******
[221, 387]
training...
setp: 0, Loss: 0.74795001745224
setp: 100, Loss: 0.6014325022697449
setp: 200, Loss: 0.6432779431343079
setp: 300, Loss: 0.6427850127220154
setp: 400, Loss: 0.6918140649795532
setp: 500, Loss: 0.5946639776229858
setp: 600, Loss: 0.5449358820915222
setp: 700, Loss: 0.4063802659511566
setp: 800, Loss: 0.3852280080318451
setp: 900, Loss: 0.35584187507629395
setp: 1000, Loss: 0.3978229761123657
setp: 1100, Loss: 0.33618828654289246
setp: 1200, Loss: 0.3813135325908661
setp: 1300, Loss: 0.32735392451286316
setp: 1400, Loss: 0.32313573360443115
setp: 1500, Loss: 0.321053147315979
setp: 1600, Loss: 0.32091984152793884
setp: 1700, Loss: 0.32159557938575745
setp: 1800, Loss: 0.3211912512779236
setp: 1900, Loss: 0.32051828503608704
setp: 2000, Loss: 0.3221336603164673
setp: 2100, Loss: 0.32001861929893494
setp: 2200, Loss: 0.31917762756347656
setp: 2300, Loss: 0.321476012468338
setp: 2400, Loss: 0.33938363194465637
setp: 2500, Loss: 0.3554891049861908
setp: 2600, Loss: 0.32185593247413635
setp: 2700, Loss: 0.33802446722984314
setp: 2800, Loss: 0.3212469518184662
setp: 2900, Loss: 0.3200090229511261
setp: 3000, Loss: 0.3208199143409729
setp: 3100, Loss: 0.32418423891067505
setp: 3200, Loss: 0.32071277499198914
setp: 3300, Loss: 0.31971612572669983
setp: 3400, Loss: 0.31946197152137756
setp: 3500, Loss: 0.3203934133052826
setp: 3600, Loss: 0.32071453332901
setp: 3700, Loss: 0.3213156461715698
setp: 3800, Loss: 0.320200115442276
setp: 3900, Loss: 0.3211473822593689
setp: 4000, Loss: 0.3197539150714874
setp: 4100, Loss: 0.3189423084259033
setp: 4200, Loss: 0.32059723138809204
setp: 4300, Loss: 0.32034870982170105
setp: 4400, Loss: 0.32140064239501953
setp: 4500, Loss: 0.580508828163147
setp: 4600, Loss: 0.3372596800327301
setp: 4700, Loss: 0.35663920640945435
setp: 4800, Loss: 0.3404789865016937
setp: 4900, Loss: 0.32873907685279846
training successfully ended.
validating...
acc: 0.975328947368421
precision: 0.9401709401709402
recall: 0.995475113122172
F_score: 0.967032967032967
validating...
acc: 0.9539473684210527
precision: 0.9130434782608695
recall: 0.984375
F_score: 0.9473684210526315
******fold 3******
[227, 381]
training...
setp: 0, Loss: 0.6814368963241577
setp: 100, Loss: 0.6605336666107178
setp: 200, Loss: 0.575284481048584
setp: 300, Loss: 0.5068191289901733
setp: 400, Loss: 0.40861397981643677
setp: 500, Loss: 0.3413117229938507
setp: 600, Loss: 0.358659029006958
setp: 700, Loss: 0.32040169835090637
setp: 800, Loss: 0.32186681032180786
setp: 900, Loss: 0.33066457509994507
setp: 1000, Loss: 0.3227333724498749
setp: 1100, Loss: 0.32125940918922424
setp: 1200, Loss: 0.3215179741382599
setp: 1300, Loss: 0.3193072974681854
setp: 1400, Loss: 0.31862473487854004
setp: 1500, Loss: 0.31923240423202515
setp: 1600, Loss: 0.3190147876739502
setp: 1700, Loss: 0.3215442895889282
setp: 1800, Loss: 0.31975069642066956
setp: 1900, Loss: 0.626774251461029
setp: 2000, Loss: 0.3582455813884735
setp: 2100, Loss: 0.3274560868740082
setp: 2200, Loss: 0.335379421710968
setp: 2300, Loss: 0.3211148977279663
setp: 2400, Loss: 0.3234008848667145
setp: 2500, Loss: 0.3419207036495209
setp: 2600, Loss: 0.32045498490333557
setp: 2700, Loss: 0.32019108533859253
setp: 2800, Loss: 0.3233509957790375
setp: 2900, Loss: 0.31994232535362244
setp: 3000, Loss: 0.320461243391037
setp: 3100, Loss: 0.3220856785774231
setp: 3200, Loss: 0.32086804509162903
setp: 3300, Loss: 0.32133597135543823
setp: 3400, Loss: 0.321266770362854
setp: 3500, Loss: 0.32144707441329956
setp: 3600, Loss: 0.32336148619651794
setp: 3700, Loss: 0.3429901599884033
setp: 3800, Loss: 0.34057435393333435
setp: 3900, Loss: 0.32022392749786377
setp: 4000, Loss: 0.32016903162002563
setp: 4100, Loss: 0.3206120729446411
setp: 4200, Loss: 0.3205530643463135
setp: 4300, Loss: 0.32150113582611084
setp: 4400, Loss: 0.3214546740055084
setp: 4500, Loss: 0.3205592930316925
setp: 4600, Loss: 0.3205927908420563
setp: 4700, Loss: 0.32418614625930786
setp: 4800, Loss: 0.32012033462524414
setp: 4900, Loss: 0.32123690843582153
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.881578947368421
precision: 0.8125
recall: 0.896551724137931
F_score: 0.8524590163934426
******fold 4******
[235, 373]
training...
setp: 0, Loss: 0.7931958436965942
setp: 100, Loss: 0.6781378388404846
setp: 200, Loss: 0.6884695887565613
setp: 300, Loss: 0.6278665065765381
setp: 400, Loss: 0.5333439111709595
setp: 500, Loss: 0.5446710586547852
setp: 600, Loss: 0.4646543562412262
setp: 700, Loss: 0.3593099117279053
setp: 800, Loss: 0.3299083113670349
setp: 900, Loss: 0.38403740525245667
setp: 1000, Loss: 0.3209370970726013
setp: 1100, Loss: 0.32244157791137695
setp: 1200, Loss: 0.32591718435287476
setp: 1300, Loss: 0.3270496428012848
setp: 1400, Loss: 0.32181960344314575
setp: 1500, Loss: 0.3205682933330536
setp: 1600, Loss: 0.32216885685920715
setp: 1700, Loss: 0.3213168978691101
setp: 1800, Loss: 0.32053273916244507
setp: 1900, Loss: 0.32145506143569946
setp: 2000, Loss: 0.3211252689361572
setp: 2100, Loss: 0.319889634847641
setp: 2200, Loss: 0.3192620575428009
setp: 2300, Loss: 0.3199995458126068
setp: 2400, Loss: 0.3211922347545624
setp: 2500, Loss: 0.3467247784137726
setp: 2600, Loss: 0.3417017161846161
setp: 2700, Loss: 0.3225466012954712
setp: 2800, Loss: 0.32210153341293335
setp: 2900, Loss: 0.3188227415084839
setp: 3000, Loss: 0.3193114399909973
setp: 3100, Loss: 0.3204786479473114
setp: 3200, Loss: 0.3205452859401703
setp: 3300, Loss: 0.3196154236793518
setp: 3400, Loss: 0.31959912180900574
setp: 3500, Loss: 0.4017637073993683
setp: 3600, Loss: 0.32398468255996704
setp: 3700, Loss: 0.3198404312133789
setp: 3800, Loss: 0.334733247756958
setp: 3900, Loss: 0.3224920332431793
setp: 4000, Loss: 0.3235752284526825
setp: 4100, Loss: 0.3201470971107483
setp: 4200, Loss: 0.3228118419647217
setp: 4300, Loss: 0.3327198624610901
setp: 4400, Loss: 0.33833980560302734
setp: 4500, Loss: 0.3179737329483032
setp: 4600, Loss: 0.31879809498786926
setp: 4700, Loss: 0.3209533393383026
setp: 4800, Loss: 0.3190232813358307
setp: 4900, Loss: 0.3190632462501526
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9078947368421053
precision: 0.8461538461538461
recall: 0.88
F_score: 0.8627450980392156
model saved.
avg_acc: 0.9118421052631579, avg_f_score: 0.8807842721892529
-------------subject: 10-------------
==========valence==========
******fold 0******
[300, 308]
training...
setp: 0, Loss: 0.7569659948348999
setp: 100, Loss: 0.6084436178207397
setp: 200, Loss: 0.6344113349914551
setp: 300, Loss: 0.611565351486206
setp: 400, Loss: 0.48507165908813477
setp: 500, Loss: 0.4441377520561218
setp: 600, Loss: 0.4625994861125946
setp: 700, Loss: 0.4465063512325287
setp: 800, Loss: 0.42104315757751465
setp: 900, Loss: 0.391355961561203
setp: 1000, Loss: 0.35284754633903503
setp: 1100, Loss: 0.3816477954387665
setp: 1200, Loss: 0.38765019178390503
setp: 1300, Loss: 0.38074639439582825
setp: 1400, Loss: 0.36078575253486633
setp: 1500, Loss: 0.3558409810066223
setp: 1600, Loss: 0.3827402889728546
setp: 1700, Loss: 0.3791285455226898
setp: 1800, Loss: 0.3792710304260254
setp: 1900, Loss: 0.4393389821052551
setp: 2000, Loss: 0.3473808169364929
setp: 2100, Loss: 0.34850019216537476
setp: 2200, Loss: 0.3799166679382324
setp: 2300, Loss: 0.37868285179138184
setp: 2400, Loss: 0.34822380542755127
setp: 2500, Loss: 0.3804715871810913
setp: 2600, Loss: 0.3184758722782135
setp: 2700, Loss: 0.37978264689445496
setp: 2800, Loss: 0.3823249042034149
setp: 2900, Loss: 0.3492974638938904
setp: 3000, Loss: 0.32794779539108276
setp: 3100, Loss: 0.38576963543891907
setp: 3200, Loss: 0.37900179624557495
setp: 3300, Loss: 0.3471962809562683
setp: 3400, Loss: 0.34904301166534424
setp: 3500, Loss: 0.35009193420410156
setp: 3600, Loss: 0.31789103150367737
setp: 3700, Loss: 0.3307025134563446
setp: 3800, Loss: 0.4293555021286011
setp: 3900, Loss: 0.31593266129493713
setp: 4000, Loss: 0.3244755268096924
setp: 4100, Loss: 0.36045992374420166
setp: 4200, Loss: 0.3311834931373596
setp: 4300, Loss: 0.32079872488975525
setp: 4400, Loss: 0.31973427534103394
setp: 4500, Loss: 0.31700700521469116
setp: 4600, Loss: 0.3155952990055084
setp: 4700, Loss: 0.31784680485725403
setp: 4800, Loss: 0.3161078691482544
setp: 4900, Loss: 0.33466270565986633
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9966555183946488
recall: 0.9933333333333333
F_score: 0.9949916527545909
validating...
acc: 0.9342105263157895
precision: 0.9166666666666666
recall: 0.9625
F_score: 0.9390243902439025
******fold 1******
[301, 307]
training...
setp: 0, Loss: 0.6912650465965271
setp: 100, Loss: 0.607877790927887
setp: 200, Loss: 0.5430183410644531
setp: 300, Loss: 0.44909071922302246
setp: 400, Loss: 0.5363963842391968
setp: 500, Loss: 0.4122318923473358
setp: 600, Loss: 0.3756313920021057
setp: 700, Loss: 0.49288782477378845
setp: 800, Loss: 0.37757056951522827
setp: 900, Loss: 0.3234274089336395
setp: 1000, Loss: 0.3212294578552246
setp: 1100, Loss: 0.3523651659488678
setp: 1200, Loss: 0.3829304873943329
setp: 1300, Loss: 0.34825098514556885
setp: 1400, Loss: 0.3173295855522156
setp: 1500, Loss: 0.32114681601524353
setp: 1600, Loss: 0.35212594270706177
setp: 1700, Loss: 0.31977465748786926
setp: 1800, Loss: 0.3203912675380707
setp: 1900, Loss: 0.3238450586795807
setp: 2000, Loss: 0.3192124664783478
setp: 2100, Loss: 0.3502367436885834
setp: 2200, Loss: 0.35581138730049133
setp: 2300, Loss: 0.34958788752555847
setp: 2400, Loss: 0.35203975439071655
setp: 2500, Loss: 0.31739524006843567
setp: 2600, Loss: 0.31704315543174744
setp: 2700, Loss: 0.31755635142326355
setp: 2800, Loss: 0.3168293535709381
setp: 2900, Loss: 0.3173024654388428
setp: 3000, Loss: 0.31672003865242004
setp: 3100, Loss: 0.3334108889102936
setp: 3200, Loss: 0.34718453884124756
setp: 3300, Loss: 0.3162972331047058
setp: 3400, Loss: 0.3200279772281647
setp: 3500, Loss: 0.31688863039016724
setp: 3600, Loss: 0.31679272651672363
setp: 3700, Loss: 0.31608107686042786
setp: 3800, Loss: 0.32359036803245544
setp: 3900, Loss: 0.3245314955711365
setp: 4000, Loss: 0.322117418050766
setp: 4100, Loss: 0.3159826397895813
setp: 4200, Loss: 0.3270881175994873
setp: 4300, Loss: 0.31981247663497925
setp: 4400, Loss: 0.3171767294406891
setp: 4500, Loss: 0.32589420676231384
setp: 4600, Loss: 0.3180127739906311
setp: 4700, Loss: 0.3157669007778168
setp: 4800, Loss: 0.3171702027320862
setp: 4900, Loss: 0.3166689872741699
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9487179487179487
recall: 0.9367088607594937
F_score: 0.9426751592356688
******fold 2******
[314, 294]
training...
setp: 0, Loss: 0.6929997205734253
setp: 100, Loss: 0.5802738070487976
setp: 200, Loss: 0.5503447651863098
setp: 300, Loss: 0.49568942189216614
setp: 400, Loss: 0.4541422128677368
setp: 500, Loss: 0.40841662883758545
setp: 600, Loss: 0.49439167976379395
setp: 700, Loss: 0.36085835099220276
setp: 800, Loss: 0.38996607065200806
setp: 900, Loss: 0.3910711705684662
setp: 1000, Loss: 0.36750900745391846
setp: 1100, Loss: 0.31898778676986694
setp: 1200, Loss: 0.33297526836395264
setp: 1300, Loss: 0.35597750544548035
setp: 1400, Loss: 0.36064764857292175
setp: 1500, Loss: 0.3835994005203247
setp: 1600, Loss: 0.36659491062164307
setp: 1700, Loss: 0.3244389295578003
setp: 1800, Loss: 0.3531181812286377
setp: 1900, Loss: 0.3295036256313324
setp: 2000, Loss: 0.40651175379753113
setp: 2100, Loss: 0.3185037076473236
setp: 2200, Loss: 0.3763350248336792
setp: 2300, Loss: 0.37835970520973206
setp: 2400, Loss: 0.3467547297477722
setp: 2500, Loss: 0.31660449504852295
setp: 2600, Loss: 0.318359375
setp: 2700, Loss: 0.34693458676338196
setp: 2800, Loss: 0.3174872100353241
setp: 2900, Loss: 0.3217073082923889
setp: 3000, Loss: 0.3154800832271576
setp: 3100, Loss: 0.3290051817893982
setp: 3200, Loss: 0.3160952925682068
setp: 3300, Loss: 0.3169555366039276
setp: 3400, Loss: 0.3815184533596039
setp: 3500, Loss: 0.32379406690597534
setp: 3600, Loss: 0.3198692202568054
setp: 3700, Loss: 0.34740006923675537
setp: 3800, Loss: 0.3569270670413971
setp: 3900, Loss: 0.32884082198143005
setp: 4000, Loss: 0.3190208375453949
setp: 4100, Loss: 0.320404052734375
setp: 4200, Loss: 0.3377287983894348
setp: 4300, Loss: 0.3190516531467438
setp: 4400, Loss: 0.32830801606178284
setp: 4500, Loss: 0.36279505491256714
setp: 4600, Loss: 0.3242068886756897
setp: 4700, Loss: 0.34408649802207947
setp: 4800, Loss: 0.3245764672756195
setp: 4900, Loss: 0.3365209996700287
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9968253968253968
recall: 1.0
F_score: 0.9984101748807631
validating...
acc: 0.9144736842105263
precision: 0.9076923076923077
recall: 0.8939393939393939
F_score: 0.900763358778626
******fold 3******
[310, 298]
training...
setp: 0, Loss: 0.6766752004623413
setp: 100, Loss: 0.6164357662200928
setp: 200, Loss: 0.6338054537773132
setp: 300, Loss: 0.4808700680732727
setp: 400, Loss: 0.5369319915771484
setp: 500, Loss: 0.4960457384586334
setp: 600, Loss: 0.4369255304336548
setp: 700, Loss: 0.35969775915145874
setp: 800, Loss: 0.39444661140441895
setp: 900, Loss: 0.33594489097595215
setp: 1000, Loss: 0.35501381754875183
setp: 1100, Loss: 0.43069469928741455
setp: 1200, Loss: 0.3670555055141449
setp: 1300, Loss: 0.38756734132766724
setp: 1400, Loss: 0.3219665288925171
setp: 1500, Loss: 0.33618149161338806
setp: 1600, Loss: 0.34949174523353577
setp: 1700, Loss: 0.31774404644966125
setp: 1800, Loss: 0.3168051838874817
setp: 1900, Loss: 0.34799081087112427
setp: 2000, Loss: 0.3199187219142914
setp: 2100, Loss: 0.33159640431404114
setp: 2200, Loss: 0.3561722934246063
setp: 2300, Loss: 0.3505687117576599
setp: 2400, Loss: 0.31822046637535095
setp: 2500, Loss: 0.31587326526641846
setp: 2600, Loss: 0.3166002333164215
setp: 2700, Loss: 0.31693968176841736
setp: 2800, Loss: 0.31625896692276
setp: 2900, Loss: 0.33109715580940247
setp: 3000, Loss: 0.317974716424942
setp: 3100, Loss: 0.3186381161212921
setp: 3200, Loss: 0.3168852925300598
setp: 3300, Loss: 0.3161718249320984
setp: 3400, Loss: 0.31655335426330566
setp: 3500, Loss: 0.31697922945022583
setp: 3600, Loss: 0.3164624571800232
setp: 3700, Loss: 0.31596481800079346
setp: 3800, Loss: 0.3481220006942749
setp: 3900, Loss: 0.3183322846889496
setp: 4000, Loss: 0.4154691994190216
setp: 4100, Loss: 0.3772788643836975
setp: 4200, Loss: 0.34696173667907715
setp: 4300, Loss: 0.319678395986557
setp: 4400, Loss: 0.31814566254615784
setp: 4500, Loss: 0.31527915596961975
setp: 4600, Loss: 0.3165019452571869
setp: 4700, Loss: 0.3156431019306183
setp: 4800, Loss: 0.31683409214019775
setp: 4900, Loss: 0.3160090148448944
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9967845659163987
recall: 1.0
F_score: 0.998389694041868
validating...
acc: 0.9144736842105263
precision: 0.9014084507042254
recall: 0.9142857142857143
F_score: 0.9078014184397163
******fold 4******
[295, 313]
training...
setp: 0, Loss: 0.6977152824401855
setp: 100, Loss: 0.6425755023956299
setp: 200, Loss: 0.633309006690979
setp: 300, Loss: 0.5046421885490417
setp: 400, Loss: 0.5158805251121521
setp: 500, Loss: 0.42292648553848267
setp: 600, Loss: 0.384024053812027
setp: 700, Loss: 0.3600669205188751
setp: 800, Loss: 0.360496461391449
setp: 900, Loss: 0.35730987787246704
setp: 1000, Loss: 0.32464468479156494
setp: 1100, Loss: 0.31953611969947815
setp: 1200, Loss: 0.31951904296875
setp: 1300, Loss: 0.31693941354751587
setp: 1400, Loss: 0.3217178285121918
setp: 1500, Loss: 0.3299027979373932
setp: 1600, Loss: 0.3213726282119751
setp: 1700, Loss: 0.3564777970314026
setp: 1800, Loss: 0.3163173198699951
setp: 1900, Loss: 0.31715327501296997
setp: 2000, Loss: 0.3217281103134155
setp: 2100, Loss: 0.319307804107666
setp: 2200, Loss: 0.32093241810798645
setp: 2300, Loss: 0.3565136790275574
setp: 2400, Loss: 0.3201214671134949
setp: 2500, Loss: 0.3188157081604004
setp: 2600, Loss: 0.3205760717391968
setp: 2700, Loss: 0.31838056445121765
setp: 2800, Loss: 0.3189825713634491
setp: 2900, Loss: 0.31801655888557434
setp: 3000, Loss: 0.3187792897224426
setp: 3100, Loss: 0.31912684440612793
setp: 3200, Loss: 0.31628113985061646
setp: 3300, Loss: 0.3543155789375305
setp: 3400, Loss: 0.3249483108520508
setp: 3500, Loss: 0.4235803484916687
setp: 3600, Loss: 0.3158488869667053
setp: 3700, Loss: 0.316228985786438
setp: 3800, Loss: 0.31675848364830017
setp: 3900, Loss: 0.3167293071746826
setp: 4000, Loss: 0.3181404769420624
setp: 4100, Loss: 0.3183220326900482
setp: 4200, Loss: 0.3179020881652832
setp: 4300, Loss: 0.32188624143600464
setp: 4400, Loss: 0.31579986214637756
setp: 4500, Loss: 0.3167804777622223
setp: 4600, Loss: 0.3162307143211365
setp: 4700, Loss: 0.3166714608669281
setp: 4800, Loss: 0.3168759346008301
setp: 4900, Loss: 0.31864675879478455
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9966101694915255
recall: 0.9966101694915255
F_score: 0.9966101694915255
validating...
acc: 0.9539473684210527
precision: 0.9642857142857143
recall: 0.9529411764705882
F_score: 0.9585798816568047
model saved.
avg_acc: 0.9315789473684213, avg_f_score: 0.9297688416709438
==========arousal==========
******fold 0******
[266, 342]
training...
setp: 0, Loss: 0.7153162956237793
setp: 100, Loss: 0.6646507382392883
setp: 200, Loss: 0.47129395604133606
setp: 300, Loss: 0.39335909485816956
setp: 400, Loss: 0.34210526943206787
setp: 500, Loss: 0.32883158326148987
setp: 600, Loss: 0.3238290250301361
setp: 700, Loss: 0.3562448024749756
setp: 800, Loss: 0.33843910694122314
setp: 900, Loss: 0.3197036683559418
setp: 1000, Loss: 0.32037386298179626
setp: 1100, Loss: 0.31832632422447205
setp: 1200, Loss: 0.32009515166282654
setp: 1300, Loss: 0.3196244239807129
setp: 1400, Loss: 0.3188138008117676
setp: 1500, Loss: 0.3192852735519409
setp: 1600, Loss: 0.31906071305274963
setp: 1700, Loss: 0.31912901997566223
setp: 1800, Loss: 0.31896960735321045
setp: 1900, Loss: 0.3197675943374634
setp: 2000, Loss: 0.31902623176574707
setp: 2100, Loss: 0.33626702427864075
setp: 2200, Loss: 0.3339237570762634
setp: 2300, Loss: 0.31891167163848877
setp: 2400, Loss: 0.31873267889022827
setp: 2500, Loss: 0.32019713521003723
setp: 2600, Loss: 0.3198556900024414
setp: 2700, Loss: 0.3198336660861969
setp: 2800, Loss: 0.31940585374832153
setp: 2900, Loss: 0.3199857473373413
setp: 3000, Loss: 0.3185042440891266
setp: 3100, Loss: 0.32007265090942383
setp: 3200, Loss: 0.3200039267539978
setp: 3300, Loss: 0.3191387355327606
setp: 3400, Loss: 0.31965407729148865
setp: 3500, Loss: 0.3195772171020508
setp: 3600, Loss: 0.31920886039733887
setp: 3700, Loss: 0.3196679353713989
setp: 3800, Loss: 0.32036638259887695
setp: 3900, Loss: 0.31961536407470703
setp: 4000, Loss: 0.38233599066734314
setp: 4100, Loss: 0.3252709209918976
setp: 4200, Loss: 0.42103463411331177
setp: 4300, Loss: 0.32228702306747437
setp: 4400, Loss: 0.31887486577033997
setp: 4500, Loss: 0.324200302362442
setp: 4600, Loss: 0.3215861916542053
setp: 4700, Loss: 0.32013073563575745
setp: 4800, Loss: 0.3186597228050232
setp: 4900, Loss: 0.31797927618026733
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9144736842105263
precision: 0.9315068493150684
recall: 0.8947368421052632
F_score: 0.912751677852349
******fold 1******
[275, 333]
training...
setp: 0, Loss: 0.7171542644500732
setp: 100, Loss: 0.6971558928489685
setp: 200, Loss: 0.5765103697776794
setp: 300, Loss: 0.42614221572875977
setp: 400, Loss: 0.3725886344909668
setp: 500, Loss: 0.32938647270202637
setp: 600, Loss: 0.341734915971756
setp: 700, Loss: 0.37733912467956543
setp: 800, Loss: 0.32607290148735046
setp: 900, Loss: 0.32614830136299133
setp: 1000, Loss: 0.32160985469818115
setp: 1100, Loss: 0.31993788480758667
setp: 1200, Loss: 0.31900089979171753
setp: 1300, Loss: 0.3207249343395233
setp: 1400, Loss: 0.3193652331829071
setp: 1500, Loss: 0.31965184211730957
setp: 1600, Loss: 0.3196672797203064
setp: 1700, Loss: 0.31846103072166443
setp: 1800, Loss: 0.3193919360637665
setp: 1900, Loss: 0.3507850468158722
setp: 2000, Loss: 0.32039961218833923
setp: 2100, Loss: 0.36313220858573914
setp: 2200, Loss: 0.31839519739151
setp: 2300, Loss: 0.39213013648986816
setp: 2400, Loss: 0.32004064321517944
setp: 2500, Loss: 0.31952303647994995
setp: 2600, Loss: 0.319003164768219
setp: 2700, Loss: 0.3191814720630646
setp: 2800, Loss: 0.31845712661743164
setp: 2900, Loss: 0.31853342056274414
setp: 3000, Loss: 0.3187973201274872
setp: 3100, Loss: 0.31847766041755676
setp: 3200, Loss: 0.31929370760917664
setp: 3300, Loss: 0.3193036913871765
setp: 3400, Loss: 0.31864532828330994
setp: 3500, Loss: 0.3191075026988983
setp: 3600, Loss: 0.3183642029762268
setp: 3700, Loss: 0.3654002845287323
setp: 3800, Loss: 0.32474565505981445
setp: 3900, Loss: 0.346748411655426
setp: 4000, Loss: 0.32172706723213196
setp: 4100, Loss: 0.3237611949443817
setp: 4200, Loss: 0.329571396112442
setp: 4300, Loss: 0.318318635225296
setp: 4400, Loss: 0.31951433420181274
setp: 4500, Loss: 0.3197992742061615
setp: 4600, Loss: 0.3202694356441498
setp: 4700, Loss: 0.3192054033279419
setp: 4800, Loss: 0.3193785548210144
setp: 4900, Loss: 0.31961843371391296
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9393939393939394
recall: 0.9253731343283582
F_score: 0.9323308270676692
******fold 2******
[281, 327]
training...
setp: 0, Loss: 0.6946820020675659
setp: 100, Loss: 0.6813927292823792
setp: 200, Loss: 0.5936075448989868
setp: 300, Loss: 0.3784896433353424
setp: 400, Loss: 0.36746838688850403
setp: 500, Loss: 0.3893332779407501
setp: 600, Loss: 0.3231075704097748
setp: 700, Loss: 0.3392767012119293
setp: 800, Loss: 0.3769443929195404
setp: 900, Loss: 0.33400651812553406
setp: 1000, Loss: 0.320616215467453
setp: 1100, Loss: 0.3213469982147217
setp: 1200, Loss: 0.3818463981151581
setp: 1300, Loss: 0.31999900937080383
setp: 1400, Loss: 0.3180907666683197
setp: 1500, Loss: 0.3192228376865387
setp: 1600, Loss: 0.32098388671875
setp: 1700, Loss: 0.32534846663475037
setp: 1800, Loss: 0.319937139749527
setp: 1900, Loss: 0.32038068771362305
setp: 2000, Loss: 0.31884703040122986
setp: 2100, Loss: 0.31966736912727356
setp: 2200, Loss: 0.31877678632736206
setp: 2300, Loss: 0.3199988603591919
setp: 2400, Loss: 0.3187814950942993
setp: 2500, Loss: 0.3185603618621826
setp: 2600, Loss: 0.3202313184738159
setp: 2700, Loss: 0.31861984729766846
setp: 2800, Loss: 0.31904321908950806
setp: 2900, Loss: 0.3194590210914612
setp: 3000, Loss: 0.3199949562549591
setp: 3100, Loss: 0.319660484790802
setp: 3200, Loss: 0.32001370191574097
setp: 3300, Loss: 0.5456374883651733
setp: 3400, Loss: 0.3581051826477051
setp: 3500, Loss: 0.3426462411880493
setp: 3600, Loss: 0.3507055640220642
setp: 3700, Loss: 0.3346434235572815
setp: 3800, Loss: 0.3345297574996948
setp: 3900, Loss: 0.3354102075099945
setp: 4000, Loss: 0.3877468407154083
setp: 4100, Loss: 0.3566218912601471
setp: 4200, Loss: 0.34265050292015076
setp: 4300, Loss: 0.33083465695381165
setp: 4400, Loss: 0.32645654678344727
setp: 4500, Loss: 0.3423309028148651
setp: 4600, Loss: 0.3308829963207245
setp: 4700, Loss: 0.327421635389328
setp: 4800, Loss: 0.3293742537498474
setp: 4900, Loss: 0.3276270031929016
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.9032258064516129
recall: 0.9180327868852459
F_score: 0.9105691056910569
******fold 3******
[271, 337]
training...
setp: 0, Loss: 0.6630499362945557
setp: 100, Loss: 0.6900396943092346
setp: 200, Loss: 0.49304670095443726
setp: 300, Loss: 0.4156074523925781
setp: 400, Loss: 0.3938724398612976
setp: 500, Loss: 0.3800148069858551
setp: 600, Loss: 0.3232940435409546
setp: 700, Loss: 0.334159791469574
setp: 800, Loss: 0.33157217502593994
setp: 900, Loss: 0.3207736909389496
setp: 1000, Loss: 0.32736101746559143
setp: 1100, Loss: 0.3201461732387543
setp: 1200, Loss: 0.3278120160102844
setp: 1300, Loss: 0.31816011667251587
setp: 1400, Loss: 0.3188394606113434
setp: 1500, Loss: 0.3189552128314972
setp: 1600, Loss: 0.3195548355579376
setp: 1700, Loss: 0.31945672631263733
setp: 1800, Loss: 0.3491411805152893
setp: 1900, Loss: 0.32025912404060364
setp: 2000, Loss: 0.31932497024536133
setp: 2100, Loss: 0.32003188133239746
setp: 2200, Loss: 0.3187246322631836
setp: 2300, Loss: 0.3198103606700897
setp: 2400, Loss: 0.31900444626808167
setp: 2500, Loss: 0.318075954914093
setp: 2600, Loss: 0.38551393151283264
setp: 2700, Loss: 0.3304107189178467
setp: 2800, Loss: 0.3250254690647125
setp: 2900, Loss: 0.3330160081386566
setp: 3000, Loss: 0.32134923338890076
setp: 3100, Loss: 0.3230816125869751
setp: 3200, Loss: 0.32382965087890625
setp: 3300, Loss: 0.32166537642478943
setp: 3400, Loss: 0.32176557183265686
setp: 3500, Loss: 0.3219267427921295
setp: 3600, Loss: 0.32168692350387573
setp: 3700, Loss: 0.3518485128879547
setp: 3800, Loss: 0.32353854179382324
setp: 3900, Loss: 0.3216373324394226
setp: 4000, Loss: 0.3227313756942749
setp: 4100, Loss: 0.3211236596107483
setp: 4200, Loss: 0.32249611616134644
setp: 4300, Loss: 0.32091230154037476
setp: 4400, Loss: 0.6451431512832642
setp: 4500, Loss: 0.3825424313545227
setp: 4600, Loss: 0.33604711294174194
setp: 4700, Loss: 0.3301940858364105
setp: 4800, Loss: 0.33633169531822205
setp: 4900, Loss: 0.3259318470954895
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9926470588235294
recall: 0.996309963099631
F_score: 0.9944751381215471
validating...
acc: 0.881578947368421
precision: 0.8441558441558441
recall: 0.9154929577464789
F_score: 0.8783783783783784
******fold 4******
[275, 333]
training...
setp: 0, Loss: 0.7204873561859131
setp: 100, Loss: 0.6962724924087524
setp: 200, Loss: 0.5729723572731018
setp: 300, Loss: 0.4562043249607086
setp: 400, Loss: 0.3748592138290405
setp: 500, Loss: 0.33662450313568115
setp: 600, Loss: 0.32627400755882263
setp: 700, Loss: 0.3362680971622467
setp: 800, Loss: 0.3248448669910431
setp: 900, Loss: 0.3537745475769043
setp: 1000, Loss: 0.3333286643028259
setp: 1100, Loss: 0.32417774200439453
setp: 1200, Loss: 0.31856024265289307
setp: 1300, Loss: 0.35002660751342773
setp: 1400, Loss: 0.3200599253177643
setp: 1500, Loss: 0.31971436738967896
setp: 1600, Loss: 0.3202364146709442
setp: 1700, Loss: 0.3193809986114502
setp: 1800, Loss: 0.3187808394432068
setp: 1900, Loss: 0.3200206160545349
setp: 2000, Loss: 0.3193812072277069
setp: 2100, Loss: 0.3756412863731384
setp: 2200, Loss: 0.3236265480518341
setp: 2300, Loss: 0.33240413665771484
setp: 2400, Loss: 0.33233779668807983
setp: 2500, Loss: 0.3182808458805084
setp: 2600, Loss: 0.31933966279029846
setp: 2700, Loss: 0.3194735050201416
setp: 2800, Loss: 0.31939685344696045
setp: 2900, Loss: 0.3191918134689331
setp: 3000, Loss: 0.3195231556892395
setp: 3100, Loss: 0.31872686743736267
setp: 3200, Loss: 0.350687175989151
setp: 3300, Loss: 0.3200845420360565
setp: 3400, Loss: 0.3200828731060028
setp: 3500, Loss: 0.319814532995224
setp: 3600, Loss: 0.31917211413383484
setp: 3700, Loss: 0.31963425874710083
setp: 3800, Loss: 0.3205510675907135
setp: 3900, Loss: 0.3200926184654236
setp: 4000, Loss: 0.3208003044128418
setp: 4100, Loss: 0.6183595657348633
setp: 4200, Loss: 0.3906756043434143
setp: 4300, Loss: 0.33085349202156067
setp: 4400, Loss: 0.32955652475357056
setp: 4500, Loss: 0.3316088318824768
setp: 4600, Loss: 0.32907533645629883
setp: 4700, Loss: 0.35663020610809326
setp: 4800, Loss: 0.3241440951824188
setp: 4900, Loss: 0.3249853551387787
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8947368421052632
precision: 0.8591549295774648
recall: 0.9104477611940298
F_score: 0.8840579710144928
model saved.
avg_acc: 0.9118421052631579, avg_f_score: 0.9036175920007894
-------------subject: 11-------------
==========valence==========
******fold 0******
[246, 362]
training...
setp: 0, Loss: 0.732489824295044
setp: 100, Loss: 0.6823989748954773
setp: 200, Loss: 0.6231663823127747
setp: 300, Loss: 0.6080865859985352
setp: 400, Loss: 0.6550590395927429
setp: 500, Loss: 0.5942262411117554
setp: 600, Loss: 0.5781884789466858
setp: 700, Loss: 0.4728063941001892
setp: 800, Loss: 0.5315120220184326
setp: 900, Loss: 0.48490703105926514
setp: 1000, Loss: 0.5424475073814392
setp: 1100, Loss: 0.45439738035202026
setp: 1200, Loss: 0.577433168888092
setp: 1300, Loss: 0.449775367975235
setp: 1400, Loss: 0.38804200291633606
setp: 1500, Loss: 0.3810715675354004
setp: 1600, Loss: 0.3592262864112854
setp: 1700, Loss: 0.453885018825531
setp: 1800, Loss: 0.36826661229133606
setp: 1900, Loss: 0.3718966245651245
setp: 2000, Loss: 0.38176003098487854
setp: 2100, Loss: 0.3861997127532959
setp: 2200, Loss: 0.35995322465896606
setp: 2300, Loss: 0.3720027506351471
setp: 2400, Loss: 0.37162643671035767
setp: 2500, Loss: 0.3471671938896179
setp: 2600, Loss: 0.317575603723526
setp: 2700, Loss: 0.3542458713054657
setp: 2800, Loss: 0.35421887040138245
setp: 2900, Loss: 0.41667598485946655
setp: 3000, Loss: 0.3243260085582733
setp: 3100, Loss: 0.36472755670547485
setp: 3200, Loss: 0.34561488032341003
setp: 3300, Loss: 0.351924866437912
setp: 3400, Loss: 0.3712077736854553
setp: 3500, Loss: 0.3296375572681427
setp: 3600, Loss: 0.3822328448295593
setp: 3700, Loss: 0.3232159912586212
setp: 3800, Loss: 0.3461480736732483
setp: 3900, Loss: 0.3510793149471283
setp: 4000, Loss: 0.3805902302265167
setp: 4100, Loss: 0.373550683259964
setp: 4200, Loss: 0.3519710600376129
setp: 4300, Loss: 0.31918033957481384
setp: 4400, Loss: 0.32040101289749146
setp: 4500, Loss: 0.316977858543396
setp: 4600, Loss: 0.3272532522678375
setp: 4700, Loss: 0.32973772287368774
setp: 4800, Loss: 0.384147584438324
setp: 4900, Loss: 0.32241907715797424
training successfully ended.
validating...
acc: 0.787828947368421
precision: 0.6576819407008087
recall: 0.991869918699187
F_score: 0.7909238249594813
validating...
acc: 0.7236842105263158
precision: 0.5816326530612245
recall: 0.9827586206896551
F_score: 0.7307692307692307
******fold 1******
[239, 369]
training...
setp: 0, Loss: 0.677704930305481
setp: 100, Loss: 0.5938090085983276
setp: 200, Loss: 0.6615492701530457
setp: 300, Loss: 0.569527268409729
setp: 400, Loss: 0.5686421990394592
setp: 500, Loss: 0.6136180758476257
setp: 600, Loss: 0.5736718773841858
setp: 700, Loss: 0.5416126251220703
setp: 800, Loss: 0.5520181655883789
setp: 900, Loss: 0.5944392085075378
setp: 1000, Loss: 0.4451732039451599
setp: 1100, Loss: 0.44891780614852905
setp: 1200, Loss: 0.4211106300354004
setp: 1300, Loss: 0.4561648964881897
setp: 1400, Loss: 0.3978114426136017
setp: 1500, Loss: 0.3908271789550781
setp: 1600, Loss: 0.36093461513519287
setp: 1700, Loss: 0.40959104895591736
setp: 1800, Loss: 0.3961821496486664
setp: 1900, Loss: 0.34592655301094055
setp: 2000, Loss: 0.3958769738674164
setp: 2100, Loss: 0.3937015235424042
setp: 2200, Loss: 0.3340867757797241
setp: 2300, Loss: 0.32663559913635254
setp: 2400, Loss: 0.3514527678489685
setp: 2500, Loss: 0.32156020402908325
setp: 2600, Loss: 0.32241031527519226
setp: 2700, Loss: 0.32283449172973633
setp: 2800, Loss: 0.32512378692626953
setp: 2900, Loss: 0.361824095249176
setp: 3000, Loss: 0.39041242003440857
setp: 3100, Loss: 0.3217349052429199
setp: 3200, Loss: 0.3260708451271057
setp: 3300, Loss: 0.322628915309906
setp: 3400, Loss: 0.32066014409065247
setp: 3500, Loss: 0.3187561631202698
setp: 3600, Loss: 0.3206813335418701
setp: 3700, Loss: 0.3494415283203125
setp: 3800, Loss: 0.31994205713272095
setp: 3900, Loss: 0.3179602324962616
setp: 4000, Loss: 0.32065320014953613
setp: 4100, Loss: 0.33407971262931824
setp: 4200, Loss: 0.39480867981910706
setp: 4300, Loss: 0.32109707593917847
setp: 4400, Loss: 0.3179609179496765
setp: 4500, Loss: 0.31843000650405884
setp: 4600, Loss: 0.3191707134246826
setp: 4700, Loss: 0.3229854702949524
setp: 4800, Loss: 0.3175905644893646
setp: 4900, Loss: 0.31946656107902527
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.99581589958159
F_score: 0.9979035639412999
validating...
acc: 0.7960526315789473
precision: 0.7833333333333333
recall: 0.7230769230769231
F_score: 0.752
******fold 2******
[248, 360]
training...
setp: 0, Loss: 0.6628617644309998
setp: 100, Loss: 0.5973595380783081
setp: 200, Loss: 0.6365482807159424
setp: 300, Loss: 0.6092049479484558
setp: 400, Loss: 0.5550990104675293
setp: 500, Loss: 0.5953436493873596
setp: 600, Loss: 0.5739344358444214
setp: 700, Loss: 0.5876644253730774
setp: 800, Loss: 0.5649642944335938
setp: 900, Loss: 0.5941787958145142
setp: 1000, Loss: 0.5439585447311401
setp: 1100, Loss: 0.49823158979415894
setp: 1200, Loss: 0.49205848574638367
setp: 1300, Loss: 0.5647019743919373
setp: 1400, Loss: 0.6364871859550476
setp: 1500, Loss: 0.5944761633872986
setp: 1600, Loss: 0.39046376943588257
setp: 1700, Loss: 0.5474968552589417
setp: 1800, Loss: 0.3974170982837677
setp: 1900, Loss: 0.4458116590976715
setp: 2000, Loss: 0.4383239448070526
setp: 2100, Loss: 0.37511661648750305
setp: 2200, Loss: 0.393548846244812
setp: 2300, Loss: 0.39777788519859314
setp: 2400, Loss: 0.3610893785953522
setp: 2500, Loss: 0.3383064568042755
setp: 2600, Loss: 0.3438635766506195
setp: 2700, Loss: 0.34617820382118225
setp: 2800, Loss: 0.3720426857471466
setp: 2900, Loss: 0.3678756356239319
setp: 3000, Loss: 0.3264469504356384
setp: 3100, Loss: 0.3457915484905243
setp: 3200, Loss: 0.3333745002746582
setp: 3300, Loss: 0.37051209807395935
setp: 3400, Loss: 0.3253367245197296
setp: 3500, Loss: 0.3398463726043701
setp: 3600, Loss: 0.3293401300907135
setp: 3700, Loss: 0.33758169412612915
setp: 3800, Loss: 0.3737814724445343
setp: 3900, Loss: 0.38268765807151794
setp: 4000, Loss: 0.3214654326438904
setp: 4100, Loss: 0.3204297423362732
setp: 4200, Loss: 0.32633456587791443
setp: 4300, Loss: 0.323169082403183
setp: 4400, Loss: 0.33377280831336975
setp: 4500, Loss: 0.3328798711299896
setp: 4600, Loss: 0.32968929409980774
setp: 4700, Loss: 0.32564157247543335
setp: 4800, Loss: 0.3247787058353424
setp: 4900, Loss: 0.32254740595817566
training successfully ended.
validating...
acc: 0.9769736842105263
precision: 0.9465648854961832
recall: 1.0
F_score: 0.9725490196078431
validating...
acc: 0.7894736842105263
precision: 0.6463414634146342
recall: 0.9464285714285714
F_score: 0.7681159420289856
******fold 3******
[246, 362]
training...
setp: 0, Loss: 0.7081573009490967
setp: 100, Loss: 0.6227991580963135
setp: 200, Loss: 0.7096920609474182
setp: 300, Loss: 0.5928787589073181
setp: 400, Loss: 0.6156125068664551
setp: 500, Loss: 0.6309778690338135
setp: 600, Loss: 0.5667761564254761
setp: 700, Loss: 0.5567001700401306
setp: 800, Loss: 0.6371548771858215
setp: 900, Loss: 0.5502069592475891
setp: 1000, Loss: 0.4898763597011566
setp: 1100, Loss: 0.48627325892448425
setp: 1200, Loss: 0.5142890810966492
setp: 1300, Loss: 0.3936769962310791
setp: 1400, Loss: 0.43350324034690857
setp: 1500, Loss: 0.3743745982646942
setp: 1600, Loss: 0.3726450800895691
setp: 1700, Loss: 0.4540897607803345
setp: 1800, Loss: 0.4102866053581238
setp: 1900, Loss: 0.3773421347141266
setp: 2000, Loss: 0.4023358225822449
setp: 2100, Loss: 0.3575762212276459
setp: 2200, Loss: 0.3271956741809845
setp: 2300, Loss: 0.3453797698020935
setp: 2400, Loss: 0.35479074716567993
setp: 2500, Loss: 0.3224724531173706
setp: 2600, Loss: 0.3333815038204193
setp: 2700, Loss: 0.3315652906894684
setp: 2800, Loss: 0.32466572523117065
setp: 2900, Loss: 0.3252725303173065
setp: 3000, Loss: 0.3257023096084595
setp: 3100, Loss: 0.32492563128471375
setp: 3200, Loss: 0.3243044316768646
setp: 3300, Loss: 0.37992069125175476
setp: 3400, Loss: 0.32516488432884216
setp: 3500, Loss: 0.37947750091552734
setp: 3600, Loss: 0.36012783646583557
setp: 3700, Loss: 0.3249278664588928
setp: 3800, Loss: 0.33567824959754944
setp: 3900, Loss: 0.352774441242218
setp: 4000, Loss: 0.3259042203426361
setp: 4100, Loss: 0.32483750581741333
setp: 4200, Loss: 0.3299880027770996
setp: 4300, Loss: 0.32477647066116333
setp: 4400, Loss: 0.3207549750804901
setp: 4500, Loss: 0.3227897882461548
setp: 4600, Loss: 0.32413333654403687
setp: 4700, Loss: 0.3229667842388153
setp: 4800, Loss: 0.47289809584617615
setp: 4900, Loss: 0.36023569107055664
training successfully ended.
validating...
acc: 0.9917763157894737
precision: 0.9878542510121457
recall: 0.991869918699187
F_score: 0.9898580121703855
validating...
acc: 0.8881578947368421
precision: 0.847457627118644
recall: 0.8620689655172413
F_score: 0.8547008547008546
******fold 4******
[237, 371]
training...
setp: 0, Loss: 0.7093846797943115
setp: 100, Loss: 0.6552437543869019
setp: 200, Loss: 0.6664000749588013
setp: 300, Loss: 0.6061736941337585
setp: 400, Loss: 0.6225830316543579
setp: 500, Loss: 0.5918080806732178
setp: 600, Loss: 0.6084452271461487
setp: 700, Loss: 0.5785951018333435
setp: 800, Loss: 0.5272863507270813
setp: 900, Loss: 0.509899377822876
setp: 1000, Loss: 0.48858189582824707
setp: 1100, Loss: 0.5431177020072937
setp: 1200, Loss: 0.4671785831451416
setp: 1300, Loss: 0.47598153352737427
setp: 1400, Loss: 0.4855618178844452
setp: 1500, Loss: 0.44894763827323914
setp: 1600, Loss: 0.41257649660110474
setp: 1700, Loss: 0.5172687768936157
setp: 1800, Loss: 0.3631286323070526
setp: 1900, Loss: 0.47692981362342834
setp: 2000, Loss: 0.4201245903968811
setp: 2100, Loss: 0.4057064652442932
setp: 2200, Loss: 0.4081442356109619
setp: 2300, Loss: 0.3650231659412384
setp: 2400, Loss: 0.40035611391067505
setp: 2500, Loss: 0.3552536368370056
setp: 2600, Loss: 0.3236338198184967
setp: 2700, Loss: 0.31982943415641785
setp: 2800, Loss: 0.39201194047927856
setp: 2900, Loss: 0.31909939646720886
setp: 3000, Loss: 0.3500942289829254
setp: 3100, Loss: 0.3889937698841095
setp: 3200, Loss: 0.37893107533454895
setp: 3300, Loss: 0.32365456223487854
setp: 3400, Loss: 0.3183988928794861
setp: 3500, Loss: 0.31751877069473267
setp: 3600, Loss: 0.320082426071167
setp: 3700, Loss: 0.32016780972480774
setp: 3800, Loss: 0.31938931345939636
setp: 3900, Loss: 0.31920745968818665
setp: 4000, Loss: 0.38167691230773926
setp: 4100, Loss: 0.35344749689102173
setp: 4200, Loss: 0.3930080235004425
setp: 4300, Loss: 0.41479769349098206
setp: 4400, Loss: 0.34993037581443787
setp: 4500, Loss: 0.4398052990436554
setp: 4600, Loss: 0.33397313952445984
setp: 4700, Loss: 0.35159409046173096
setp: 4800, Loss: 0.3169461488723755
setp: 4900, Loss: 0.34776678681373596
training successfully ended.
validating...
acc: 0.9884868421052632
precision: 0.9831932773109243
recall: 0.9873417721518988
F_score: 0.9852631578947368
validating...
acc: 0.8486842105263158
precision: 0.8333333333333334
recall: 0.8208955223880597
F_score: 0.8270676691729324
model saved.
avg_acc: 0.8092105263157894, avg_f_score: 0.7865307393344005
==========arousal==========
******fold 0******
[379, 229]
training...
setp: 0, Loss: 0.6867655515670776
setp: 100, Loss: 0.6615595817565918
setp: 200, Loss: 0.6412920355796814
setp: 300, Loss: 0.6319403648376465
setp: 400, Loss: 0.6690654754638672
setp: 500, Loss: 0.5819973349571228
setp: 600, Loss: 0.5810030698776245
setp: 700, Loss: 0.5797759890556335
setp: 800, Loss: 0.5430004000663757
setp: 900, Loss: 0.49061429500579834
setp: 1000, Loss: 0.4179474711418152
setp: 1100, Loss: 0.5211623907089233
setp: 1200, Loss: 0.41905906796455383
setp: 1300, Loss: 0.3940277397632599
setp: 1400, Loss: 0.3747342824935913
setp: 1500, Loss: 0.3600969910621643
setp: 1600, Loss: 0.4562041163444519
setp: 1700, Loss: 0.35670745372772217
setp: 1800, Loss: 0.33257728815078735
setp: 1900, Loss: 0.3215005099773407
setp: 2000, Loss: 0.3294687867164612
setp: 2100, Loss: 0.3199039399623871
setp: 2200, Loss: 0.31902649998664856
setp: 2300, Loss: 0.34826695919036865
setp: 2400, Loss: 0.3426528573036194
setp: 2500, Loss: 0.3329024910926819
setp: 2600, Loss: 0.3288239538669586
setp: 2700, Loss: 0.32553231716156006
setp: 2800, Loss: 0.32088953256607056
setp: 2900, Loss: 0.31938472390174866
setp: 3000, Loss: 0.3210921287536621
setp: 3100, Loss: 0.32295674085617065
setp: 3200, Loss: 0.3524220585823059
setp: 3300, Loss: 0.3199044466018677
setp: 3400, Loss: 0.32206836342811584
setp: 3500, Loss: 0.3208402395248413
setp: 3600, Loss: 0.32182785868644714
setp: 3700, Loss: 0.31993329524993896
setp: 3800, Loss: 0.31870919466018677
setp: 3900, Loss: 0.3185971677303314
setp: 4000, Loss: 0.3197563588619232
setp: 4100, Loss: 0.3182606101036072
setp: 4200, Loss: 0.6700732707977295
setp: 4300, Loss: 0.4902004301548004
setp: 4400, Loss: 0.36961880326271057
setp: 4500, Loss: 0.36215776205062866
setp: 4600, Loss: 0.3499484360218048
setp: 4700, Loss: 0.37345975637435913
setp: 4800, Loss: 0.33659160137176514
setp: 4900, Loss: 0.3298468291759491
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9973684210526316
recall: 1.0
F_score: 0.9986824769433464
validating...
acc: 0.9013157894736842
precision: 0.9354838709677419
recall: 0.90625
F_score: 0.9206349206349206
******fold 1******
[383, 225]
training...
setp: 0, Loss: 0.7298758029937744
setp: 100, Loss: 0.6426798701286316
setp: 200, Loss: 0.6081732511520386
setp: 300, Loss: 0.5667771697044373
setp: 400, Loss: 0.6292193531990051
setp: 500, Loss: 0.5526400804519653
setp: 600, Loss: 0.5897918939590454
setp: 700, Loss: 0.5475627779960632
setp: 800, Loss: 0.5388674736022949
setp: 900, Loss: 0.46219927072525024
setp: 1000, Loss: 0.47112032771110535
setp: 1100, Loss: 0.39841902256011963
setp: 1200, Loss: 0.4639831781387329
setp: 1300, Loss: 0.3834139108657837
setp: 1400, Loss: 0.36946234107017517
setp: 1500, Loss: 0.341169536113739
setp: 1600, Loss: 0.3328538239002228
setp: 1700, Loss: 0.3393755555152893
setp: 1800, Loss: 0.3296966850757599
setp: 1900, Loss: 0.38859355449676514
setp: 2000, Loss: 0.32906150817871094
setp: 2100, Loss: 0.32867807149887085
setp: 2200, Loss: 0.32367846369743347
setp: 2300, Loss: 0.3293377161026001
setp: 2400, Loss: 0.32466602325439453
setp: 2500, Loss: 0.39047035574913025
setp: 2600, Loss: 0.3356529176235199
setp: 2700, Loss: 0.3267962336540222
setp: 2800, Loss: 0.3539925515651703
setp: 2900, Loss: 0.32677900791168213
setp: 3000, Loss: 0.32320111989974976
setp: 3100, Loss: 0.32477614283561707
setp: 3200, Loss: 0.3255802094936371
setp: 3300, Loss: 0.32318055629730225
setp: 3400, Loss: 0.3215063512325287
setp: 3500, Loss: 0.32253238558769226
setp: 3600, Loss: 0.32380127906799316
setp: 3700, Loss: 0.32238030433654785
setp: 3800, Loss: 0.32189565896987915
setp: 3900, Loss: 0.32120972871780396
setp: 4000, Loss: 0.32538527250289917
setp: 4100, Loss: 0.3199293315410614
setp: 4200, Loss: 0.32354938983917236
setp: 4300, Loss: 0.3220287263393402
setp: 4400, Loss: 0.6454403400421143
setp: 4500, Loss: 0.48774421215057373
setp: 4600, Loss: 0.3864752948284149
setp: 4700, Loss: 0.3749196529388428
setp: 4800, Loss: 0.352491557598114
setp: 4900, Loss: 0.3416043817996979
training successfully ended.
validating...
acc: 0.9901315789473685
precision: 0.9870801033591732
recall: 0.9973890339425587
F_score: 0.9922077922077922
validating...
acc: 0.8947368421052632
precision: 0.9222222222222223
recall: 0.9021739130434783
F_score: 0.9120879120879122
******fold 2******
[392, 216]
training...
setp: 0, Loss: 0.7073228359222412
setp: 100, Loss: 0.6617493629455566
setp: 200, Loss: 0.6328909397125244
setp: 300, Loss: 0.585050642490387
setp: 400, Loss: 0.5478622317314148
setp: 500, Loss: 0.6020510792732239
setp: 600, Loss: 0.5256407856941223
setp: 700, Loss: 0.5493909120559692
setp: 800, Loss: 0.5650710463523865
setp: 900, Loss: 0.5286190509796143
setp: 1000, Loss: 0.5074595212936401
setp: 1100, Loss: 0.3720506429672241
setp: 1200, Loss: 0.36351317167282104
setp: 1300, Loss: 0.43453067541122437
setp: 1400, Loss: 0.35844284296035767
setp: 1500, Loss: 0.40087342262268066
setp: 1600, Loss: 0.33271104097366333
setp: 1700, Loss: 0.3375564217567444
setp: 1800, Loss: 0.347281813621521
setp: 1900, Loss: 0.3337937295436859
setp: 2000, Loss: 0.3504955470561981
setp: 2100, Loss: 0.33838677406311035
setp: 2200, Loss: 0.3318421542644501
setp: 2300, Loss: 0.3495130240917206
setp: 2400, Loss: 0.3260343670845032
setp: 2500, Loss: 0.3206716775894165
setp: 2600, Loss: 0.3681032657623291
setp: 2700, Loss: 0.3700307607650757
setp: 2800, Loss: 0.3429643213748932
setp: 2900, Loss: 0.33031949400901794
setp: 3000, Loss: 0.327351838350296
setp: 3100, Loss: 0.33516889810562134
setp: 3200, Loss: 0.33464550971984863
setp: 3300, Loss: 0.333492249250412
setp: 3400, Loss: 0.3223511874675751
setp: 3500, Loss: 0.32383009791374207
setp: 3600, Loss: 0.3228212893009186
setp: 3700, Loss: 0.3220928907394409
setp: 3800, Loss: 0.32051271200180054
setp: 3900, Loss: 0.31893229484558105
setp: 4000, Loss: 0.32075250148773193
setp: 4100, Loss: 0.3201013505458832
setp: 4200, Loss: 0.3221631646156311
setp: 4300, Loss: 0.32067275047302246
setp: 4400, Loss: 0.3194268047809601
setp: 4500, Loss: 0.32194221019744873
setp: 4600, Loss: 0.3212517201900482
setp: 4700, Loss: 0.32398295402526855
setp: 4800, Loss: 0.3210415542125702
setp: 4900, Loss: 0.31999310851097107
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8486842105263158
precision: 0.8488372093023255
recall: 0.8795180722891566
F_score: 0.863905325443787
******fold 3******
[370, 238]
training...
setp: 0, Loss: 0.7268429398536682
setp: 100, Loss: 0.6516236662864685
setp: 200, Loss: 0.6310791373252869
setp: 300, Loss: 0.5993489623069763
setp: 400, Loss: 0.5868415236473083
setp: 500, Loss: 0.5925936102867126
setp: 600, Loss: 0.5130875706672668
setp: 700, Loss: 0.5639758110046387
setp: 800, Loss: 0.4807353913784027
setp: 900, Loss: 0.4659004211425781
setp: 1000, Loss: 0.385709285736084
setp: 1100, Loss: 0.38134247064590454
setp: 1200, Loss: 0.372402161359787
setp: 1300, Loss: 0.34946849942207336
setp: 1400, Loss: 0.3756865859031677
setp: 1500, Loss: 0.3262583613395691
setp: 1600, Loss: 0.3615345060825348
setp: 1700, Loss: 0.4085410237312317
setp: 1800, Loss: 0.31965893507003784
setp: 1900, Loss: 0.3214643597602844
setp: 2000, Loss: 0.3364395201206207
setp: 2100, Loss: 0.32782962918281555
setp: 2200, Loss: 0.32284706830978394
setp: 2300, Loss: 0.38452857732772827
setp: 2400, Loss: 0.3518688678741455
setp: 2500, Loss: 0.3475511968135834
setp: 2600, Loss: 0.32237839698791504
setp: 2700, Loss: 0.3195525109767914
setp: 2800, Loss: 0.3515092134475708
setp: 2900, Loss: 0.3247523605823517
setp: 3000, Loss: 0.36651208996772766
setp: 3100, Loss: 0.3189580738544464
setp: 3200, Loss: 0.3266935646533966
setp: 3300, Loss: 0.3512115478515625
setp: 3400, Loss: 0.3173726499080658
setp: 3500, Loss: 0.3213360607624054
setp: 3600, Loss: 0.35253337025642395
setp: 3700, Loss: 0.35157209634780884
setp: 3800, Loss: 0.3521011471748352
setp: 3900, Loss: 0.3219211995601654
setp: 4000, Loss: 0.31941473484039307
setp: 4100, Loss: 0.31842541694641113
setp: 4200, Loss: 0.32467183470726013
setp: 4300, Loss: 0.331936240196228
setp: 4400, Loss: 0.3204748034477234
setp: 4500, Loss: 0.37698474526405334
setp: 4600, Loss: 0.41899046301841736
setp: 4700, Loss: 0.3560981750488281
setp: 4800, Loss: 0.31794503331184387
setp: 4900, Loss: 0.35120269656181335
training successfully ended.
validating...
acc: 0.9917763157894737
precision: 0.9919137466307277
recall: 0.9945945945945946
F_score: 0.9932523616734144
validating...
acc: 0.881578947368421
precision: 0.9142857142857143
recall: 0.9142857142857143
F_score: 0.9142857142857143
******fold 4******
[376, 232]
training...
setp: 0, Loss: 0.6939737796783447
setp: 100, Loss: 0.6602033376693726
setp: 200, Loss: 0.6138253211975098
setp: 300, Loss: 0.6409175395965576
setp: 400, Loss: 0.5780760645866394
setp: 500, Loss: 0.583855152130127
setp: 600, Loss: 0.557584285736084
setp: 700, Loss: 0.5870416760444641
setp: 800, Loss: 0.47915124893188477
setp: 900, Loss: 0.44987648725509644
setp: 1000, Loss: 0.3463941514492035
setp: 1100, Loss: 0.40513181686401367
setp: 1200, Loss: 0.3916565179824829
setp: 1300, Loss: 0.3418259620666504
setp: 1400, Loss: 0.40064001083374023
setp: 1500, Loss: 0.32986685633659363
setp: 1600, Loss: 0.3246687054634094
setp: 1700, Loss: 0.3306356370449066
setp: 1800, Loss: 0.342553973197937
setp: 1900, Loss: 0.33381086587905884
setp: 2000, Loss: 0.33164992928504944
setp: 2100, Loss: 0.36498451232910156
setp: 2200, Loss: 0.32277244329452515
setp: 2300, Loss: 0.33098235726356506
setp: 2400, Loss: 0.3265687823295593
setp: 2500, Loss: 0.3279034495353699
setp: 2600, Loss: 0.3251924514770508
setp: 2700, Loss: 0.3409287929534912
setp: 2800, Loss: 0.3352137506008148
setp: 2900, Loss: 0.3251218795776367
setp: 3000, Loss: 0.36723336577415466
setp: 3100, Loss: 0.3277716636657715
setp: 3200, Loss: 0.3241484463214874
setp: 3300, Loss: 0.35422876477241516
setp: 3400, Loss: 0.31921255588531494
setp: 3500, Loss: 0.330840528011322
setp: 3600, Loss: 0.3479520082473755
setp: 3700, Loss: 0.35773012042045593
setp: 3800, Loss: 0.32063814997673035
setp: 3900, Loss: 0.3191560208797455
setp: 4000, Loss: 0.3214695453643799
setp: 4100, Loss: 0.31981176137924194
setp: 4200, Loss: 0.32146155834198
setp: 4300, Loss: 0.32112064957618713
setp: 4400, Loss: 0.32072049379348755
setp: 4500, Loss: 0.3207610249519348
setp: 4600, Loss: 0.322842538356781
setp: 4700, Loss: 0.3208528459072113
setp: 4800, Loss: 0.3202619254589081
setp: 4900, Loss: 0.3422769606113434
training successfully ended.
validating...
acc: 0.9259868421052632
precision: 1.0
recall: 0.8803191489361702
F_score: 0.9363507779349364
validating...
acc: 0.7894736842105263
precision: 0.935064935064935
recall: 0.7272727272727273
F_score: 0.8181818181818181
model saved.
avg_acc: 0.8631578947368421, avg_f_score: 0.8858191381268303
-------------subject: 12-------------
==========valence==========
******fold 0******
[296, 312]
training...
setp: 0, Loss: 0.6931838989257812
setp: 100, Loss: 0.5951791405677795
setp: 200, Loss: 0.574232816696167
setp: 300, Loss: 0.4323371648788452
setp: 400, Loss: 0.46035024523735046
setp: 500, Loss: 0.5273729562759399
setp: 600, Loss: 0.4326093792915344
setp: 700, Loss: 0.346863716840744
setp: 800, Loss: 0.3902154862880707
setp: 900, Loss: 0.3588579297065735
setp: 1000, Loss: 0.36430495977401733
setp: 1100, Loss: 0.4916096329689026
setp: 1200, Loss: 0.3665306568145752
setp: 1300, Loss: 0.39116930961608887
setp: 1400, Loss: 0.3669021725654602
setp: 1500, Loss: 0.3685486316680908
setp: 1600, Loss: 0.3557887673377991
setp: 1700, Loss: 0.366513729095459
setp: 1800, Loss: 0.3875390291213989
setp: 1900, Loss: 0.3655160665512085
setp: 2000, Loss: 0.32628241181373596
setp: 2100, Loss: 0.3295515477657318
setp: 2200, Loss: 0.3284060060977936
setp: 2300, Loss: 0.3238775134086609
setp: 2400, Loss: 0.31818869709968567
setp: 2500, Loss: 0.3212353587150574
setp: 2600, Loss: 0.3182048499584198
setp: 2700, Loss: 0.3178086280822754
setp: 2800, Loss: 0.31986597180366516
setp: 2900, Loss: 0.3171420395374298
setp: 3000, Loss: 0.3235580027103424
setp: 3100, Loss: 0.3197665214538574
setp: 3200, Loss: 0.31880736351013184
setp: 3300, Loss: 0.31933990120887756
setp: 3400, Loss: 0.3224491775035858
setp: 3500, Loss: 0.3178269863128662
setp: 3600, Loss: 0.3187752366065979
setp: 3700, Loss: 0.3197175860404968
setp: 3800, Loss: 0.31948122382164
setp: 3900, Loss: 0.31939223408699036
setp: 4000, Loss: 0.35608911514282227
setp: 4100, Loss: 0.35577693581581116
setp: 4200, Loss: 0.3182680010795593
setp: 4300, Loss: 0.31939244270324707
setp: 4400, Loss: 0.32419008016586304
setp: 4500, Loss: 0.321216881275177
setp: 4600, Loss: 0.3185599744319916
setp: 4700, Loss: 0.319792777299881
setp: 4800, Loss: 0.3174181282520294
setp: 4900, Loss: 0.32436618208885193
training successfully ended.
validating...
acc: 0.9786184210526315
precision: 1.0
recall: 0.956081081081081
F_score: 0.9775474956822107
validating...
acc: 0.8947368421052632
precision: 1.0
recall: 0.7538461538461538
F_score: 0.8596491228070176
******fold 1******
[289, 319]
training...
setp: 0, Loss: 0.6958575248718262
setp: 100, Loss: 0.6556209921836853
setp: 200, Loss: 0.5109780430793762
setp: 300, Loss: 0.4608655571937561
setp: 400, Loss: 0.46828219294548035
setp: 500, Loss: 0.4643518328666687
setp: 600, Loss: 0.4209551513195038
setp: 700, Loss: 0.3917487859725952
setp: 800, Loss: 0.39212900400161743
setp: 900, Loss: 0.3618452548980713
setp: 1000, Loss: 0.3209647238254547
setp: 1100, Loss: 0.361362099647522
setp: 1200, Loss: 0.34134432673454285
setp: 1300, Loss: 0.31921738386154175
setp: 1400, Loss: 0.33888816833496094
setp: 1500, Loss: 0.37329745292663574
setp: 1600, Loss: 0.31752580404281616
setp: 1700, Loss: 0.38051965832710266
setp: 1800, Loss: 0.3198074698448181
setp: 1900, Loss: 0.3286638557910919
setp: 2000, Loss: 0.31937283277511597
setp: 2100, Loss: 0.3166099786758423
setp: 2200, Loss: 0.3199083209037781
setp: 2300, Loss: 0.32984763383865356
setp: 2400, Loss: 0.31906288862228394
setp: 2500, Loss: 0.34008756279945374
setp: 2600, Loss: 0.319211483001709
setp: 2700, Loss: 0.32540541887283325
setp: 2800, Loss: 0.31846123933792114
setp: 2900, Loss: 0.33984190225601196
setp: 3000, Loss: 0.3173596262931824
setp: 3100, Loss: 0.3251769542694092
setp: 3200, Loss: 0.3195575177669525
setp: 3300, Loss: 0.32796743512153625
setp: 3400, Loss: 0.3419116139411926
setp: 3500, Loss: 0.33640530705451965
setp: 3600, Loss: 0.32210206985473633
setp: 3700, Loss: 0.31775805354118347
setp: 3800, Loss: 0.3175143301486969
setp: 3900, Loss: 0.31801360845565796
setp: 4000, Loss: 0.3164750933647156
setp: 4100, Loss: 0.31651729345321655
setp: 4200, Loss: 0.317870169878006
setp: 4300, Loss: 0.3175666034221649
setp: 4400, Loss: 0.31787723302841187
setp: 4500, Loss: 0.3170168399810791
setp: 4600, Loss: 0.3187780976295471
setp: 4700, Loss: 0.31749242544174194
setp: 4800, Loss: 0.3177889883518219
setp: 4900, Loss: 0.31991851329803467
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9144736842105263
precision: 0.927536231884058
recall: 0.8888888888888888
F_score: 0.9078014184397163
******fold 2******
[294, 314]
training...
setp: 0, Loss: 0.7174744606018066
setp: 100, Loss: 0.6751748919487
setp: 200, Loss: 0.5530698299407959
setp: 300, Loss: 0.5007820725440979
setp: 400, Loss: 0.5233194828033447
setp: 500, Loss: 0.45709964632987976
setp: 600, Loss: 0.39093017578125
setp: 700, Loss: 0.3986218273639679
setp: 800, Loss: 0.3925707936286926
setp: 900, Loss: 0.3360183537006378
setp: 1000, Loss: 0.3370845913887024
setp: 1100, Loss: 0.3531040549278259
setp: 1200, Loss: 0.3305701017379761
setp: 1300, Loss: 0.3267141878604889
setp: 1400, Loss: 0.37492451071739197
setp: 1500, Loss: 0.3931581676006317
setp: 1600, Loss: 0.3479422330856323
setp: 1700, Loss: 0.31892675161361694
setp: 1800, Loss: 0.323116272687912
setp: 1900, Loss: 0.3407188057899475
setp: 2000, Loss: 0.36916297674179077
setp: 2100, Loss: 0.33135852217674255
setp: 2200, Loss: 0.38453540205955505
setp: 2300, Loss: 0.35947203636169434
setp: 2400, Loss: 0.39163339138031006
setp: 2500, Loss: 0.3377460241317749
setp: 2600, Loss: 0.3206615447998047
setp: 2700, Loss: 0.34754785895347595
setp: 2800, Loss: 0.3171607255935669
setp: 2900, Loss: 0.31679001450538635
setp: 3000, Loss: 0.3206962049007416
setp: 3100, Loss: 0.3171743154525757
setp: 3200, Loss: 0.3181139826774597
setp: 3300, Loss: 0.3237130641937256
setp: 3400, Loss: 0.31980687379837036
setp: 3500, Loss: 0.31958723068237305
setp: 3600, Loss: 0.3159334361553192
setp: 3700, Loss: 0.3196544349193573
setp: 3800, Loss: 0.3174231946468353
setp: 3900, Loss: 0.31861743330955505
setp: 4000, Loss: 0.3192005753517151
setp: 4100, Loss: 0.31800687313079834
setp: 4200, Loss: 0.3480597734451294
setp: 4300, Loss: 0.34302690625190735
setp: 4400, Loss: 0.37756991386413574
setp: 4500, Loss: 0.35785865783691406
setp: 4600, Loss: 0.3488442003726959
setp: 4700, Loss: 0.31880736351013184
setp: 4800, Loss: 0.3195773959159851
setp: 4900, Loss: 0.3249351978302002
training successfully ended.
validating...
acc: 0.9407894736842105
precision: 1.0
recall: 0.8775510204081632
F_score: 0.9347826086956522
validating...
acc: 0.8552631578947368
precision: 1.0
recall: 0.6716417910447762
F_score: 0.8035714285714287
******fold 3******
[288, 320]
training...
setp: 0, Loss: 0.6910329461097717
setp: 100, Loss: 0.6285942196846008
setp: 200, Loss: 0.5702807903289795
setp: 300, Loss: 0.5023311972618103
setp: 400, Loss: 0.47577765583992004
setp: 500, Loss: 0.5276483297348022
setp: 600, Loss: 0.39179113507270813
setp: 700, Loss: 0.3662557601928711
setp: 800, Loss: 0.45265641808509827
setp: 900, Loss: 0.34480902552604675
setp: 1000, Loss: 0.4246174097061157
setp: 1100, Loss: 0.36963292956352234
setp: 1200, Loss: 0.3554215729236603
setp: 1300, Loss: 0.32199957966804504
setp: 1400, Loss: 0.3489772379398346
setp: 1500, Loss: 0.4109249711036682
setp: 1600, Loss: 0.31880638003349304
setp: 1700, Loss: 0.32712632417678833
setp: 1800, Loss: 0.33093881607055664
setp: 1900, Loss: 0.3398340940475464
setp: 2000, Loss: 0.3247945308685303
setp: 2100, Loss: 0.31715014576911926
setp: 2200, Loss: 0.3171451985836029
setp: 2300, Loss: 0.31757259368896484
setp: 2400, Loss: 0.3188519775867462
setp: 2500, Loss: 0.3159946799278259
setp: 2600, Loss: 0.35552749037742615
setp: 2700, Loss: 0.35183730721473694
setp: 2800, Loss: 0.33100625872612
setp: 2900, Loss: 0.3188628554344177
setp: 3000, Loss: 0.3174894154071808
setp: 3100, Loss: 0.31770333647727966
setp: 3200, Loss: 0.31827813386917114
setp: 3300, Loss: 0.3228898048400879
setp: 3400, Loss: 0.3209761679172516
setp: 3500, Loss: 0.31674817204475403
setp: 3600, Loss: 0.31770971417427063
setp: 3700, Loss: 0.31747567653656006
setp: 3800, Loss: 0.31968459486961365
setp: 3900, Loss: 0.3183485269546509
setp: 4000, Loss: 0.3589315712451935
setp: 4100, Loss: 0.37421807646751404
setp: 4200, Loss: 0.31900447607040405
setp: 4300, Loss: 0.3603285253047943
setp: 4400, Loss: 0.3159099817276001
setp: 4500, Loss: 0.3183070421218872
setp: 4600, Loss: 0.3185897767543793
setp: 4700, Loss: 0.3173273503780365
setp: 4800, Loss: 0.31835928559303284
setp: 4900, Loss: 0.3189373016357422
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.8860759493670886
recall: 0.958904109589041
F_score: 0.9210526315789473
******fold 4******
[277, 331]
training...
setp: 0, Loss: 0.6990659832954407
setp: 100, Loss: 0.5863600373268127
setp: 200, Loss: 0.557389497756958
setp: 300, Loss: 0.47593897581100464
setp: 400, Loss: 0.48206672072410583
setp: 500, Loss: 0.5731932520866394
setp: 600, Loss: 0.38894662261009216
setp: 700, Loss: 0.3641176223754883
setp: 800, Loss: 0.43256738781929016
setp: 900, Loss: 0.3689202666282654
setp: 1000, Loss: 0.3378790020942688
setp: 1100, Loss: 0.38623809814453125
setp: 1200, Loss: 0.3333498239517212
setp: 1300, Loss: 0.3275606334209442
setp: 1400, Loss: 0.33091580867767334
setp: 1500, Loss: 0.34361693263053894
setp: 1600, Loss: 0.33007046580314636
setp: 1700, Loss: 0.36575207114219666
setp: 1800, Loss: 0.32668086886405945
setp: 1900, Loss: 0.327445387840271
setp: 2000, Loss: 0.32580047845840454
setp: 2100, Loss: 0.3170415163040161
setp: 2200, Loss: 0.3162224292755127
setp: 2300, Loss: 0.32249799370765686
setp: 2400, Loss: 0.36853498220443726
setp: 2500, Loss: 0.31624817848205566
setp: 2600, Loss: 0.32127436995506287
setp: 2700, Loss: 0.3199768662452698
setp: 2800, Loss: 0.3220444917678833
setp: 2900, Loss: 0.31887903809547424
setp: 3000, Loss: 0.3314560055732727
setp: 3100, Loss: 0.3182319700717926
setp: 3200, Loss: 0.31770387291908264
setp: 3300, Loss: 0.3179994523525238
setp: 3400, Loss: 0.31856682896614075
setp: 3500, Loss: 0.3174740672111511
setp: 3600, Loss: 0.3478442132472992
setp: 3700, Loss: 0.32259508967399597
setp: 3800, Loss: 0.32060977816581726
setp: 3900, Loss: 0.31995129585266113
setp: 4000, Loss: 0.3181529939174652
setp: 4100, Loss: 0.33380305767059326
setp: 4200, Loss: 0.31646180152893066
setp: 4300, Loss: 0.3479395806789398
setp: 4400, Loss: 0.31760963797569275
setp: 4500, Loss: 0.3166760802268982
setp: 4600, Loss: 0.3179630637168884
setp: 4700, Loss: 0.3181276023387909
setp: 4800, Loss: 0.3171271085739136
setp: 4900, Loss: 0.318054735660553
training successfully ended.
validating...
acc: 0.9901315789473685
precision: 0.9822064056939501
recall: 0.9963898916967509
F_score: 0.989247311827957
validating...
acc: 0.9276315789473685
precision: 0.9010989010989011
recall: 0.9761904761904762
F_score: 0.937142857142857
model saved.
avg_acc: 0.9026315789473683, avg_f_score: 0.8858434917079935
==========arousal==========
******fold 0******
[104, 504]
training...
setp: 0, Loss: 0.6925544142723083
setp: 100, Loss: 0.6170365810394287
setp: 200, Loss: 0.658593475818634
setp: 300, Loss: 0.5543206334114075
setp: 400, Loss: 0.5779333710670471
setp: 500, Loss: 0.4481062889099121
setp: 600, Loss: 0.44172561168670654
setp: 700, Loss: 0.32759448885917664
setp: 800, Loss: 0.3729126453399658
setp: 900, Loss: 0.3674454391002655
setp: 1000, Loss: 0.32039469480514526
setp: 1100, Loss: 0.38027846813201904
setp: 1200, Loss: 0.3265484869480133
setp: 1300, Loss: 0.3202991187572479
setp: 1400, Loss: 0.32427674531936646
setp: 1500, Loss: 0.31716397404670715
setp: 1600, Loss: 0.3177601993083954
setp: 1700, Loss: 0.34780871868133545
setp: 1800, Loss: 0.31732863187789917
setp: 1900, Loss: 0.3786272704601288
setp: 2000, Loss: 0.3217308521270752
setp: 2100, Loss: 0.32215067744255066
setp: 2200, Loss: 0.334977388381958
setp: 2300, Loss: 0.3160549998283386
setp: 2400, Loss: 0.3160759508609772
setp: 2500, Loss: 0.3472989499568939
setp: 2600, Loss: 0.3166128993034363
setp: 2700, Loss: 0.37784188985824585
setp: 2800, Loss: 0.32286006212234497
setp: 2900, Loss: 0.3757609724998474
setp: 3000, Loss: 0.3195401430130005
setp: 3100, Loss: 0.3162655532360077
setp: 3200, Loss: 0.31614434719085693
setp: 3300, Loss: 0.3473169505596161
setp: 3400, Loss: 0.3174736499786377
setp: 3500, Loss: 0.3785223066806793
setp: 3600, Loss: 0.3191111385822296
setp: 3700, Loss: 0.31955021619796753
setp: 3800, Loss: 0.3162967264652252
setp: 3900, Loss: 0.33159390091896057
setp: 4000, Loss: 0.31547266244888306
setp: 4100, Loss: 0.3468145430088043
setp: 4200, Loss: 0.31642448902130127
setp: 4300, Loss: 0.37810176610946655
setp: 4400, Loss: 0.3186895251274109
setp: 4500, Loss: 0.3202427327632904
setp: 4600, Loss: 0.31673508882522583
setp: 4700, Loss: 0.31665438413619995
setp: 4800, Loss: 0.316827654838562
setp: 4900, Loss: 0.3474309742450714
training successfully ended.
validating...
acc: 0.9900793650793651
precision: 0.9979838709677419
recall: 0.9821428571428571
F_score: 0.99
validating...
acc: 0.9276315789473685
precision: 0.8461538461538461
recall: 0.7586206896551724
F_score: 0.8
******fold 1******
[106, 502]
training...
setp: 0, Loss: 0.7411726713180542
setp: 100, Loss: 0.6411250829696655
setp: 200, Loss: 0.46111372113227844
setp: 300, Loss: 0.47652336955070496
setp: 400, Loss: 0.4758983850479126
setp: 500, Loss: 0.40293750166893005
setp: 600, Loss: 0.41491541266441345
setp: 700, Loss: 0.45010286569595337
setp: 800, Loss: 0.35139352083206177
setp: 900, Loss: 0.38009998202323914
setp: 1000, Loss: 0.3496248424053192
setp: 1100, Loss: 0.4129289388656616
setp: 1200, Loss: 0.416890949010849
setp: 1300, Loss: 0.3912631869316101
setp: 1400, Loss: 0.4128115177154541
setp: 1500, Loss: 0.3785149157047272
setp: 1600, Loss: 0.3468211591243744
setp: 1700, Loss: 0.378261536359787
setp: 1800, Loss: 0.34779828786849976
setp: 1900, Loss: 0.4034382700920105
setp: 2000, Loss: 0.4125446081161499
setp: 2100, Loss: 0.3481469452381134
setp: 2200, Loss: 0.3911772668361664
setp: 2300, Loss: 0.37914663553237915
setp: 2400, Loss: 0.3480963408946991
setp: 2500, Loss: 0.3779951333999634
setp: 2600, Loss: 0.3488503694534302
setp: 2700, Loss: 0.3506430685520172
setp: 2800, Loss: 0.41083717346191406
setp: 2900, Loss: 0.34810760617256165
setp: 3000, Loss: 0.37932088971138
setp: 3100, Loss: 0.3784518241882324
setp: 3200, Loss: 0.3490805923938751
setp: 3300, Loss: 0.37930163741111755
setp: 3400, Loss: 0.3474285900592804
setp: 3500, Loss: 0.3489338457584381
setp: 3600, Loss: 0.4101201891899109
setp: 3700, Loss: 0.3475755453109741
setp: 3800, Loss: 0.3788054585456848
setp: 3900, Loss: 0.37922772765159607
setp: 4000, Loss: 0.3465266525745392
setp: 4100, Loss: 0.3775681257247925
setp: 4200, Loss: 0.34889331459999084
setp: 4300, Loss: 0.3484359085559845
setp: 4400, Loss: 0.7807499766349792
setp: 4500, Loss: 0.34159088134765625
setp: 4600, Loss: 0.37116292119026184
setp: 4700, Loss: 0.3750724792480469
setp: 4800, Loss: 0.3150947093963623
setp: 4900, Loss: 0.34678348898887634
training successfully ended.
validating...
acc: 0.9711155378486056
precision: 0.9958071278825996
recall: 0.9462151394422311
F_score: 0.9703779366700714
validating...
acc: 0.9013157894736842
precision: 0.7727272727272727
recall: 0.6296296296296297
F_score: 0.6938775510204083
******fold 2******
[116, 492]
training...
setp: 0, Loss: 0.6945856809616089
setp: 100, Loss: 0.6618762016296387
setp: 200, Loss: 0.5562233328819275
setp: 300, Loss: 0.4739711284637451
setp: 400, Loss: 0.3609631359577179
setp: 500, Loss: 0.33419090509414673
setp: 600, Loss: 0.3743063807487488
setp: 700, Loss: 0.321601539850235
setp: 800, Loss: 0.3222348093986511
setp: 900, Loss: 0.3189152777194977
setp: 1000, Loss: 0.31843554973602295
setp: 1100, Loss: 0.32058101892471313
setp: 1200, Loss: 0.3204364478588104
setp: 1300, Loss: 0.31976911425590515
setp: 1400, Loss: 0.3199668228626251
setp: 1500, Loss: 0.3201887309551239
setp: 1600, Loss: 0.31789082288742065
setp: 1700, Loss: 0.32183408737182617
setp: 1800, Loss: 0.4587273597717285
setp: 1900, Loss: 0.3176797032356262
setp: 2000, Loss: 0.3175651729106903
setp: 2100, Loss: 0.31814104318618774
setp: 2200, Loss: 0.3171657621860504
setp: 2300, Loss: 0.3175402581691742
setp: 2400, Loss: 0.31802624464035034
setp: 2500, Loss: 0.31809839606285095
setp: 2600, Loss: 0.3187572956085205
setp: 2700, Loss: 0.32242345809936523
setp: 2800, Loss: 0.31725120544433594
setp: 2900, Loss: 0.317911297082901
setp: 3000, Loss: 0.3173554241657257
setp: 3100, Loss: 0.3170420527458191
setp: 3200, Loss: 0.31772854924201965
setp: 3300, Loss: 0.5334842205047607
setp: 3400, Loss: 0.40844228863716125
setp: 3500, Loss: 0.3354780375957489
setp: 3600, Loss: 0.32892322540283203
setp: 3700, Loss: 0.32816898822784424
setp: 3800, Loss: 0.324282705783844
setp: 3900, Loss: 0.3238525092601776
setp: 4000, Loss: 0.32398441433906555
setp: 4100, Loss: 0.32392188906669617
setp: 4200, Loss: 0.32703423500061035
setp: 4300, Loss: 0.3239491581916809
setp: 4400, Loss: 0.3238335847854614
setp: 4500, Loss: 0.3242221474647522
setp: 4600, Loss: 0.3243832588195801
setp: 4700, Loss: 0.3253827691078186
setp: 4800, Loss: 0.33800897002220154
setp: 4900, Loss: 0.3249993920326233
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9605263157894737
precision: 0.7894736842105263
recall: 0.8823529411764706
F_score: 0.8333333333333333
******fold 3******
[100, 508]
training...
setp: 0, Loss: 0.7275587320327759
setp: 100, Loss: 0.6684704422950745
setp: 200, Loss: 0.5848081707954407
setp: 300, Loss: 0.4910055100917816
setp: 400, Loss: 0.5454077124595642
setp: 500, Loss: 0.3496897518634796
setp: 600, Loss: 0.3765052556991577
setp: 700, Loss: 0.3629782199859619
setp: 800, Loss: 0.32721036672592163
setp: 900, Loss: 0.34814438223838806
setp: 1000, Loss: 0.32049059867858887
setp: 1100, Loss: 0.32415005564689636
setp: 1200, Loss: 0.42984965443611145
setp: 1300, Loss: 0.3160994350910187
setp: 1400, Loss: 0.31706199049949646
setp: 1500, Loss: 0.3205091953277588
setp: 1600, Loss: 0.31928297877311707
setp: 1700, Loss: 0.3546682596206665
setp: 1800, Loss: 0.31963038444519043
setp: 1900, Loss: 0.3172295391559601
setp: 2000, Loss: 0.34941190481185913
setp: 2100, Loss: 0.3158598840236664
setp: 2200, Loss: 0.31804996728897095
setp: 2300, Loss: 0.31678614020347595
setp: 2400, Loss: 0.31587448716163635
setp: 2500, Loss: 0.3473183512687683
setp: 2600, Loss: 0.3164568245410919
setp: 2700, Loss: 0.31917625665664673
setp: 2800, Loss: 0.3490128517150879
setp: 2900, Loss: 0.4539283215999603
setp: 3000, Loss: 0.4125375747680664
setp: 3100, Loss: 0.3563896715641022
setp: 3200, Loss: 0.3210783004760742
setp: 3300, Loss: 0.348642498254776
setp: 3400, Loss: 0.3178611397743225
setp: 3500, Loss: 0.3189077079296112
setp: 3600, Loss: 0.34970417618751526
setp: 3700, Loss: 0.317084401845932
setp: 3800, Loss: 0.31837257742881775
setp: 3900, Loss: 0.317887544631958
setp: 4000, Loss: 0.31655097007751465
setp: 4100, Loss: 0.34774088859558105
setp: 4200, Loss: 0.31635794043540955
setp: 4300, Loss: 0.5692335367202759
setp: 4400, Loss: 0.5137321352958679
setp: 4500, Loss: 0.3191409409046173
setp: 4600, Loss: 0.3193775713443756
setp: 4700, Loss: 0.31722304224967957
setp: 4800, Loss: 0.31651008129119873
setp: 4900, Loss: 0.34737998247146606
training successfully ended.
validating...
acc: 0.9950787401574803
precision: 1.0
recall: 0.9901574803149606
F_score: 0.9950544015825915
validating...
acc: 0.881578947368421
precision: 0.7027027027027027
recall: 0.7878787878787878
F_score: 0.7428571428571429
******fold 4******
[106, 502]
training...
setp: 0, Loss: 0.6927072405815125
setp: 100, Loss: 0.5913790464401245
setp: 200, Loss: 0.5357118248939514
setp: 300, Loss: 0.494916707277298
setp: 400, Loss: 0.38118669390678406
setp: 500, Loss: 0.37671124935150146
setp: 600, Loss: 0.45368117094039917
setp: 700, Loss: 0.3316240906715393
setp: 800, Loss: 0.3442268371582031
setp: 900, Loss: 0.31768932938575745
setp: 1000, Loss: 0.3313702940940857
setp: 1100, Loss: 0.5348142385482788
setp: 1200, Loss: 0.31838297843933105
setp: 1300, Loss: 0.31855568289756775
setp: 1400, Loss: 0.31939640641212463
setp: 1500, Loss: 0.32081377506256104
setp: 1600, Loss: 0.3160211443901062
setp: 1700, Loss: 0.316098153591156
setp: 1800, Loss: 0.31843647360801697
setp: 1900, Loss: 0.32144635915756226
setp: 2000, Loss: 0.31591880321502686
setp: 2100, Loss: 0.3353019058704376
setp: 2200, Loss: 0.3199109435081482
setp: 2300, Loss: 0.39464083313941956
setp: 2400, Loss: 0.3171684145927429
setp: 2500, Loss: 0.31606853008270264
setp: 2600, Loss: 0.3176426887512207
setp: 2700, Loss: 0.3177807331085205
setp: 2800, Loss: 0.3167451024055481
setp: 2900, Loss: 0.3161058723926544
setp: 3000, Loss: 0.31756699085235596
setp: 3100, Loss: 0.3186880052089691
setp: 3200, Loss: 0.31589651107788086
setp: 3300, Loss: 0.31627756357192993
setp: 3400, Loss: 0.3976193070411682
setp: 3500, Loss: 0.32179343700408936
setp: 3600, Loss: 0.31639039516448975
setp: 3700, Loss: 0.3165769577026367
setp: 3800, Loss: 0.3173764646053314
setp: 3900, Loss: 0.31804826855659485
setp: 4000, Loss: 0.3174392580986023
setp: 4100, Loss: 0.31573283672332764
setp: 4200, Loss: 0.31567931175231934
setp: 4300, Loss: 0.31888967752456665
setp: 4400, Loss: 0.43559205532073975
setp: 4500, Loss: 0.41345342993736267
setp: 4600, Loss: 0.32420608401298523
setp: 4700, Loss: 0.3447282612323761
setp: 4800, Loss: 0.3159601390361786
setp: 4900, Loss: 0.3155563473701477
training successfully ended.
validating...
acc: 0.999003984063745
precision: 0.9980119284294234
recall: 1.0
F_score: 0.999004975124378
validating...
acc: 0.9342105263157895
precision: 0.8148148148148148
recall: 0.8148148148148148
F_score: 0.8148148148148148
model saved.
avg_acc: 0.9210526315789475, avg_f_score: 0.7769765684051398
-------------subject: 13-------------
==========valence==========
******fold 0******
[338, 270]
training...
setp: 0, Loss: 0.7973447442054749
setp: 100, Loss: 0.6433374881744385
setp: 200, Loss: 0.5300301313400269
setp: 300, Loss: 0.5533186197280884
setp: 400, Loss: 0.5086943507194519
setp: 500, Loss: 0.4710264205932617
setp: 600, Loss: 0.44779157638549805
setp: 700, Loss: 0.5678002238273621
setp: 800, Loss: 0.436381995677948
setp: 900, Loss: 0.43792301416397095
setp: 1000, Loss: 0.35163089632987976
setp: 1100, Loss: 0.43812263011932373
setp: 1200, Loss: 0.39467039704322815
setp: 1300, Loss: 0.37530872225761414
setp: 1400, Loss: 0.3261691927909851
setp: 1500, Loss: 0.3782036602497101
setp: 1600, Loss: 0.36052602529525757
setp: 1700, Loss: 0.38878133893013
setp: 1800, Loss: 0.32361844182014465
setp: 1900, Loss: 0.3573879301548004
setp: 2000, Loss: 0.31727856397628784
setp: 2100, Loss: 0.33052319288253784
setp: 2200, Loss: 0.35699889063835144
setp: 2300, Loss: 0.32702207565307617
setp: 2400, Loss: 0.33572623133659363
setp: 2500, Loss: 0.3198760449886322
setp: 2600, Loss: 0.320865273475647
setp: 2700, Loss: 0.317005455493927
setp: 2800, Loss: 0.32163095474243164
setp: 2900, Loss: 0.3165900409221649
setp: 3000, Loss: 0.3182528018951416
setp: 3100, Loss: 0.352175772190094
setp: 3200, Loss: 0.3212844729423523
setp: 3300, Loss: 0.3206157386302948
setp: 3400, Loss: 0.3177730143070221
setp: 3500, Loss: 0.32636141777038574
setp: 3600, Loss: 0.34853944182395935
setp: 3700, Loss: 0.3303854465484619
setp: 3800, Loss: 0.3545510470867157
setp: 3900, Loss: 0.31829890608787537
setp: 4000, Loss: 0.31661850214004517
setp: 4100, Loss: 0.3222019672393799
setp: 4200, Loss: 0.31761375069618225
setp: 4300, Loss: 0.31754282116889954
setp: 4400, Loss: 0.3268795907497406
setp: 4500, Loss: 0.31911227107048035
setp: 4600, Loss: 0.31716102361679077
setp: 4700, Loss: 0.3201722800731659
setp: 4800, Loss: 0.31559890508651733
setp: 4900, Loss: 0.31774669885635376
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9941176470588236
recall: 1.0
F_score: 0.9970501474926253
validating...
acc: 0.9078947368421053
precision: 0.9024390243902439
recall: 0.925
F_score: 0.9135802469135802
******fold 1******
[328, 280]
training...
setp: 0, Loss: 0.7353208065032959
setp: 100, Loss: 0.7003211379051208
setp: 200, Loss: 0.596301794052124
setp: 300, Loss: 0.5207287073135376
setp: 400, Loss: 0.5538756847381592
setp: 500, Loss: 0.48951637744903564
setp: 600, Loss: 0.45082390308380127
setp: 700, Loss: 0.41693758964538574
setp: 800, Loss: 0.41511964797973633
setp: 900, Loss: 0.44125744700431824
setp: 1000, Loss: 0.43055516481399536
setp: 1100, Loss: 0.4297451078891754
setp: 1200, Loss: 0.40784040093421936
setp: 1300, Loss: 0.3847796618938446
setp: 1400, Loss: 0.3766568601131439
setp: 1500, Loss: 0.3685706853866577
setp: 1600, Loss: 0.32360777258872986
setp: 1700, Loss: 0.3501855432987213
setp: 1800, Loss: 0.352847158908844
setp: 1900, Loss: 0.3730210065841675
setp: 2000, Loss: 0.33194947242736816
setp: 2100, Loss: 0.3225499391555786
setp: 2200, Loss: 0.3245929479598999
setp: 2300, Loss: 0.35602760314941406
setp: 2400, Loss: 0.384857714176178
setp: 2500, Loss: 0.3165682852268219
setp: 2600, Loss: 0.3502209782600403
setp: 2700, Loss: 0.31750553846359253
setp: 2800, Loss: 0.3224336504936218
setp: 2900, Loss: 0.3190080225467682
setp: 3000, Loss: 0.3160324990749359
setp: 3100, Loss: 0.3490102291107178
setp: 3200, Loss: 0.3290651738643646
setp: 3300, Loss: 0.324189692735672
setp: 3400, Loss: 0.3926251232624054
setp: 3500, Loss: 0.3232393264770508
setp: 3600, Loss: 0.3408048450946808
setp: 3700, Loss: 0.3465789258480072
setp: 3800, Loss: 0.35676199197769165
setp: 3900, Loss: 0.3166394829750061
setp: 4000, Loss: 0.3163568377494812
setp: 4100, Loss: 0.31829947233200073
setp: 4200, Loss: 0.34112492203712463
setp: 4300, Loss: 0.3188830614089966
setp: 4400, Loss: 0.32107070088386536
setp: 4500, Loss: 0.38924965262413025
setp: 4600, Loss: 0.3442976176738739
setp: 4700, Loss: 0.3193616271018982
setp: 4800, Loss: 0.3189547657966614
setp: 4900, Loss: 0.3261355459690094
training successfully ended.
validating...
acc: 0.9851973684210527
precision: 1.0
recall: 0.9725609756097561
F_score: 0.9860896445131375
validating...
acc: 0.7894736842105263
precision: 0.9142857142857143
recall: 0.7111111111111111
F_score: 0.8
******fold 2******
[345, 263]
training...
setp: 0, Loss: 0.6924591064453125
setp: 100, Loss: 0.6083911061286926
setp: 200, Loss: 0.49347996711730957
setp: 300, Loss: 0.4998469054698944
setp: 400, Loss: 0.45280733704566956
setp: 500, Loss: 0.4901568591594696
setp: 600, Loss: 0.45354557037353516
setp: 700, Loss: 0.43522900342941284
setp: 800, Loss: 0.3968145549297333
setp: 900, Loss: 0.39361193776130676
setp: 1000, Loss: 0.3801785409450531
setp: 1100, Loss: 0.4081958830356598
setp: 1200, Loss: 0.3236077129840851
setp: 1300, Loss: 0.38410961627960205
setp: 1400, Loss: 0.3213704526424408
setp: 1500, Loss: 0.35252895951271057
setp: 1600, Loss: 0.31897595524787903
setp: 1700, Loss: 0.3169345557689667
setp: 1800, Loss: 0.37454846501350403
setp: 1900, Loss: 0.38363566994667053
setp: 2000, Loss: 0.3187919855117798
setp: 2100, Loss: 0.31730687618255615
setp: 2200, Loss: 0.34777966141700745
setp: 2300, Loss: 0.3492562770843506
setp: 2400, Loss: 0.36642128229141235
setp: 2500, Loss: 0.3500312268733978
setp: 2600, Loss: 0.3249552845954895
setp: 2700, Loss: 0.3186768591403961
setp: 2800, Loss: 0.320920467376709
setp: 2900, Loss: 0.3154158294200897
setp: 3000, Loss: 0.3174280822277069
setp: 3100, Loss: 0.31847378611564636
setp: 3200, Loss: 0.34756138920783997
setp: 3300, Loss: 0.31630730628967285
setp: 3400, Loss: 0.3264312744140625
setp: 3500, Loss: 0.31746596097946167
setp: 3600, Loss: 0.31592652201652527
setp: 3700, Loss: 0.3203108012676239
setp: 3800, Loss: 0.3678588271141052
setp: 3900, Loss: 0.3180195987224579
setp: 4000, Loss: 0.3164029121398926
setp: 4100, Loss: 0.34701237082481384
setp: 4200, Loss: 0.3190757930278778
setp: 4300, Loss: 0.31899404525756836
setp: 4400, Loss: 0.3464497923851013
setp: 4500, Loss: 0.32999518513679504
setp: 4600, Loss: 0.31711339950561523
setp: 4700, Loss: 0.3175269365310669
setp: 4800, Loss: 0.31897759437561035
setp: 4900, Loss: 0.3342933654785156
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9971014492753624
F_score: 0.9985486211901305
validating...
acc: 0.8947368421052632
precision: 0.8607594936708861
recall: 0.9315068493150684
F_score: 0.8947368421052632
******fold 3******
[338, 270]
training...
setp: 0, Loss: 0.6916487812995911
setp: 100, Loss: 0.6249718070030212
setp: 200, Loss: 0.5458194613456726
setp: 300, Loss: 0.5620054006576538
setp: 400, Loss: 0.6156817674636841
setp: 500, Loss: 0.5882527232170105
setp: 600, Loss: 0.5279428958892822
setp: 700, Loss: 0.4655037224292755
setp: 800, Loss: 0.4641572833061218
setp: 900, Loss: 0.49026164412498474
setp: 1000, Loss: 0.3926066756248474
setp: 1100, Loss: 0.4085457921028137
setp: 1200, Loss: 0.3936590254306793
setp: 1300, Loss: 0.43041008710861206
setp: 1400, Loss: 0.322131872177124
setp: 1500, Loss: 0.4014039933681488
setp: 1600, Loss: 0.3937852680683136
setp: 1700, Loss: 0.3586964011192322
setp: 1800, Loss: 0.32136330008506775
setp: 1900, Loss: 0.42859554290771484
setp: 2000, Loss: 0.37584561109542847
setp: 2100, Loss: 0.32113736867904663
setp: 2200, Loss: 0.326037734746933
setp: 2300, Loss: 0.35368454456329346
setp: 2400, Loss: 0.3272813856601715
setp: 2500, Loss: 0.3192092180252075
setp: 2600, Loss: 0.32964879274368286
setp: 2700, Loss: 0.39573174715042114
setp: 2800, Loss: 0.35138577222824097
setp: 2900, Loss: 0.3179270625114441
setp: 3000, Loss: 0.37180978059768677
setp: 3100, Loss: 0.3291548490524292
setp: 3200, Loss: 0.38393208384513855
setp: 3300, Loss: 0.32514041662216187
setp: 3400, Loss: 0.4542301297187805
setp: 3500, Loss: 0.32198670506477356
setp: 3600, Loss: 0.4257929027080536
setp: 3700, Loss: 0.3649064898490906
setp: 3800, Loss: 0.37311047315597534
setp: 3900, Loss: 0.3389533758163452
setp: 4000, Loss: 0.4071212708950043
setp: 4100, Loss: 0.31610745191574097
setp: 4200, Loss: 0.3481023609638214
setp: 4300, Loss: 0.3194520175457001
setp: 4400, Loss: 0.31552743911743164
setp: 4500, Loss: 0.31857165694236755
setp: 4600, Loss: 0.3479868769645691
setp: 4700, Loss: 0.3185786306858063
setp: 4800, Loss: 0.3163333535194397
setp: 4900, Loss: 0.31924545764923096
training successfully ended.
validating...
acc: 0.993421052631579
precision: 0.9883040935672515
recall: 1.0
F_score: 0.9941176470588234
validating...
acc: 0.9342105263157895
precision: 0.926829268292683
recall: 0.95
F_score: 0.9382716049382716
******fold 4******
[323, 285]
training...
setp: 0, Loss: 0.7120707035064697
setp: 100, Loss: 0.6321443319320679
setp: 200, Loss: 0.6091829538345337
setp: 300, Loss: 0.502297580242157
setp: 400, Loss: 0.588188648223877
setp: 500, Loss: 0.5707100033760071
setp: 600, Loss: 0.5072532892227173
setp: 700, Loss: 0.39438891410827637
setp: 800, Loss: 0.4061052203178406
setp: 900, Loss: 0.5314857959747314
setp: 1000, Loss: 0.3883657157421112
setp: 1100, Loss: 0.41618672013282776
setp: 1200, Loss: 0.4752318859100342
setp: 1300, Loss: 0.3689351975917816
setp: 1400, Loss: 0.32617390155792236
setp: 1500, Loss: 0.38466596603393555
setp: 1600, Loss: 0.34278416633605957
setp: 1700, Loss: 0.3662957549095154
setp: 1800, Loss: 0.34505364298820496
setp: 1900, Loss: 0.3310178518295288
setp: 2000, Loss: 0.32260823249816895
setp: 2100, Loss: 0.32896190881729126
setp: 2200, Loss: 0.3216399848461151
setp: 2300, Loss: 0.3264368176460266
setp: 2400, Loss: 0.35514798760414124
setp: 2500, Loss: 0.31808242201805115
setp: 2600, Loss: 0.3217819631099701
setp: 2700, Loss: 0.3201726973056793
setp: 2800, Loss: 0.3565361201763153
setp: 2900, Loss: 0.31849169731140137
setp: 3000, Loss: 0.31793034076690674
setp: 3100, Loss: 0.32639050483703613
setp: 3200, Loss: 0.32343339920043945
setp: 3300, Loss: 0.3165707290172577
setp: 3400, Loss: 0.3171946704387665
setp: 3500, Loss: 0.3457808792591095
setp: 3600, Loss: 0.3512726426124573
setp: 3700, Loss: 0.3237059414386749
setp: 3800, Loss: 0.31882795691490173
setp: 3900, Loss: 0.3266909122467041
setp: 4000, Loss: 0.3225487470626831
setp: 4100, Loss: 0.32452479004859924
setp: 4200, Loss: 0.31766605377197266
setp: 4300, Loss: 0.34504830837249756
setp: 4400, Loss: 0.32250669598579407
setp: 4500, Loss: 0.3182525932788849
setp: 4600, Loss: 0.3167554438114166
setp: 4700, Loss: 0.32000336050987244
setp: 4800, Loss: 0.31899502873420715
setp: 4900, Loss: 0.3198874294757843
training successfully ended.
validating...
acc: 0.9802631578947368
precision: 0.9669669669669669
recall: 0.9969040247678018
F_score: 0.9817073170731707
validating...
acc: 0.9013157894736842
precision: 0.8773584905660378
recall: 0.9789473684210527
F_score: 0.9253731343283582
model saved.
avg_acc: 0.8855263157894736, avg_f_score: 0.8943923656570945
==========arousal==========
******fold 0******
[87, 521]
training...
setp: 0, Loss: 0.6941149830818176
setp: 100, Loss: 0.6716105937957764
setp: 200, Loss: 0.5532070994377136
setp: 300, Loss: 0.4070409834384918
setp: 400, Loss: 0.3821474611759186
setp: 500, Loss: 0.36648279428482056
setp: 600, Loss: 0.3333453834056854
setp: 700, Loss: 0.3244228661060333
setp: 800, Loss: 0.32544320821762085
setp: 900, Loss: 0.3240010440349579
setp: 1000, Loss: 0.32721075415611267
setp: 1100, Loss: 0.31982797384262085
setp: 1200, Loss: 0.32013675570487976
setp: 1300, Loss: 0.3192390203475952
setp: 1400, Loss: 0.31872764229774475
setp: 1500, Loss: 0.31971973180770874
setp: 1600, Loss: 0.3484921157360077
setp: 1700, Loss: 0.31990084052085876
setp: 1800, Loss: 0.3189752697944641
setp: 1900, Loss: 0.31941041350364685
setp: 2000, Loss: 0.356324702501297
setp: 2100, Loss: 0.3231357932090759
setp: 2200, Loss: 0.32648786902427673
setp: 2300, Loss: 0.3222096860408783
setp: 2400, Loss: 0.3228687644004822
setp: 2500, Loss: 0.3209674656391144
setp: 2600, Loss: 0.3209502696990967
setp: 2700, Loss: 0.3219679296016693
setp: 2800, Loss: 0.32112178206443787
setp: 2900, Loss: 0.31911543011665344
setp: 3000, Loss: 0.32143712043762207
setp: 3100, Loss: 0.33868899941444397
setp: 3200, Loss: 0.32130199670791626
setp: 3300, Loss: 0.3185785710811615
setp: 3400, Loss: 0.3196480870246887
setp: 3500, Loss: 0.3202090561389923
setp: 3600, Loss: 0.3216315805912018
setp: 3700, Loss: 0.3249996304512024
setp: 3800, Loss: 0.31934401392936707
setp: 3900, Loss: 0.3196190595626831
setp: 4000, Loss: 0.3188086748123169
setp: 4100, Loss: 0.3228318393230438
setp: 4200, Loss: 0.3193223774433136
setp: 4300, Loss: 0.3175300359725952
setp: 4400, Loss: 0.32368484139442444
setp: 4500, Loss: 0.31966039538383484
setp: 4600, Loss: 0.32082343101501465
setp: 4700, Loss: 0.31720083951950073
setp: 4800, Loss: 0.3181295692920685
setp: 4900, Loss: 0.3174963593482971
training successfully ended.
validating...
acc: 0.9990403071017274
precision: 0.9980842911877394
recall: 1.0
F_score: 0.9990412272291467
validating...
acc: 0.9605263157894737
precision: 0.8888888888888888
recall: 0.8888888888888888
F_score: 0.8888888888888888
******fold 1******
[89, 519]
training...
setp: 0, Loss: 0.7022320032119751
setp: 100, Loss: 0.7012748122215271
setp: 200, Loss: 0.6156965494155884
setp: 300, Loss: 0.5126864910125732
setp: 400, Loss: 0.4800032675266266
setp: 500, Loss: 0.42189162969589233
setp: 600, Loss: 0.35396891832351685
setp: 700, Loss: 0.3317568302154541
setp: 800, Loss: 0.3246305584907532
setp: 900, Loss: 0.31751978397369385
setp: 1000, Loss: 0.3188536763191223
setp: 1100, Loss: 0.3203948140144348
setp: 1200, Loss: 0.3495361804962158
setp: 1300, Loss: 0.3210327923297882
setp: 1400, Loss: 0.3175813853740692
setp: 1500, Loss: 0.31896892189979553
setp: 1600, Loss: 0.3169536590576172
setp: 1700, Loss: 0.31701767444610596
setp: 1800, Loss: 0.31818777322769165
setp: 1900, Loss: 0.31924688816070557
setp: 2000, Loss: 0.31756308674812317
setp: 2100, Loss: 0.31651222705841064
setp: 2200, Loss: 0.7896553874015808
setp: 2300, Loss: 0.4751107096672058
setp: 2400, Loss: 0.4413743019104004
setp: 2500, Loss: 0.43480581045150757
setp: 2600, Loss: 0.3464040458202362
setp: 2700, Loss: 0.32646816968917847
setp: 2800, Loss: 0.3310075104236603
setp: 2900, Loss: 0.31921127438545227
setp: 3000, Loss: 0.33814117312431335
setp: 3100, Loss: 0.3199906647205353
setp: 3200, Loss: 0.4923688471317291
setp: 3300, Loss: 0.3188437223434448
setp: 3400, Loss: 0.3424961566925049
setp: 3500, Loss: 0.5004401803016663
setp: 3600, Loss: 0.3180167078971863
setp: 3700, Loss: 0.3249600827693939
setp: 3800, Loss: 0.31810787320137024
setp: 3900, Loss: 0.31821486353874207
setp: 4000, Loss: 0.3172340393066406
setp: 4100, Loss: 0.32504013180732727
setp: 4200, Loss: 0.31742560863494873
setp: 4300, Loss: 0.31729912757873535
setp: 4400, Loss: 0.409045547246933
setp: 4500, Loss: 0.3399299085140228
setp: 4600, Loss: 0.31697604060173035
setp: 4700, Loss: 0.3184892237186432
setp: 4800, Loss: 0.32121941447257996
setp: 4900, Loss: 0.3159555494785309
training successfully ended.
validating...
acc: 0.9990366088631984
precision: 0.9980769230769231
recall: 1.0
F_score: 0.9990375360923965
validating...
acc: 0.9210526315789473
precision: 0.76
recall: 0.76
F_score: 0.76
******fold 2******
[100, 508]
training...
setp: 0, Loss: 0.7240664958953857
setp: 100, Loss: 0.5477041006088257
setp: 200, Loss: 0.42851799726486206
setp: 300, Loss: 0.3960469365119934
setp: 400, Loss: 0.32788804173469543
setp: 500, Loss: 0.33461228013038635
setp: 600, Loss: 0.3356628715991974
setp: 700, Loss: 0.32192468643188477
setp: 800, Loss: 0.3184208273887634
setp: 900, Loss: 0.3193731904029846
setp: 1000, Loss: 0.3261978328227997
setp: 1100, Loss: 0.32380691170692444
setp: 1200, Loss: 0.3175731301307678
setp: 1300, Loss: 0.3200724422931671
setp: 1400, Loss: 0.31892845034599304
setp: 1500, Loss: 0.31785014271736145
setp: 1600, Loss: 0.40354713797569275
setp: 1700, Loss: 0.3166877031326294
setp: 1800, Loss: 0.31794944405555725
setp: 1900, Loss: 0.3180994391441345
setp: 2000, Loss: 0.31750887632369995
setp: 2100, Loss: 0.3178212642669678
setp: 2200, Loss: 0.31807762384414673
setp: 2300, Loss: 0.3201814591884613
setp: 2400, Loss: 0.3351924419403076
setp: 2500, Loss: 0.3212018609046936
setp: 2600, Loss: 0.31864479184150696
setp: 2700, Loss: 0.3186730444431305
setp: 2800, Loss: 0.31854888796806335
setp: 2900, Loss: 0.3192445933818817
setp: 3000, Loss: 0.31844019889831543
setp: 3100, Loss: 0.31807124614715576
setp: 3200, Loss: 0.3166193962097168
setp: 3300, Loss: 0.3249944746494293
setp: 3400, Loss: 0.3211134076118469
setp: 3500, Loss: 0.31761595606803894
setp: 3600, Loss: 0.31717756390571594
setp: 3700, Loss: 0.3184841275215149
setp: 3800, Loss: 0.31834977865219116
setp: 3900, Loss: 0.3177972733974457
setp: 4000, Loss: 0.316707044839859
setp: 4100, Loss: 0.3400616943836212
setp: 4200, Loss: 0.31663841009140015
setp: 4300, Loss: 0.3171553909778595
setp: 4400, Loss: 0.31657686829566956
setp: 4500, Loss: 0.3180917501449585
setp: 4600, Loss: 0.3182171583175659
setp: 4700, Loss: 0.3184022605419159
setp: 4800, Loss: 0.3165104389190674
setp: 4900, Loss: 0.32022637128829956
training successfully ended.
validating...
acc: 0.9990157480314961
precision: 0.9980353634577603
recall: 1.0
F_score: 0.9990167158308751
validating...
acc: 0.9868421052631579
precision: 0.9285714285714286
recall: 0.9285714285714286
F_score: 0.9285714285714286
******fold 3******
[86, 522]
training...
setp: 0, Loss: 0.6936779022216797
setp: 100, Loss: 0.7100366950035095
setp: 200, Loss: 0.5927470326423645
setp: 300, Loss: 0.6587501764297485
setp: 400, Loss: 0.7106328010559082
setp: 500, Loss: 0.40884241461753845
setp: 600, Loss: 0.4155389964580536
setp: 700, Loss: 0.42786818742752075
setp: 800, Loss: 0.4297162592411041
setp: 900, Loss: 0.354316383600235
setp: 1000, Loss: 0.32965555787086487
setp: 1100, Loss: 0.3762453496456146
setp: 1200, Loss: 0.3510182797908783
setp: 1300, Loss: 0.35219982266426086
setp: 1400, Loss: 0.319842129945755
setp: 1500, Loss: 0.3225115239620209
setp: 1600, Loss: 0.3343208432197571
setp: 1700, Loss: 0.352033406496048
setp: 1800, Loss: 0.3182269036769867
setp: 1900, Loss: 0.32022422552108765
setp: 2000, Loss: 0.3412802815437317
setp: 2100, Loss: 0.3164721727371216
setp: 2200, Loss: 0.37745314836502075
setp: 2300, Loss: 0.3201400339603424
setp: 2400, Loss: 0.3156672418117523
setp: 2500, Loss: 0.3157806396484375
setp: 2600, Loss: 0.3522992432117462
setp: 2700, Loss: 0.3162671625614166
setp: 2800, Loss: 0.3164392113685608
setp: 2900, Loss: 0.3153710663318634
setp: 3000, Loss: 0.3155086040496826
setp: 3100, Loss: 0.3164534568786621
setp: 3200, Loss: 0.31633079051971436
setp: 3300, Loss: 0.3164712190628052
setp: 3400, Loss: 0.33307695388793945
setp: 3500, Loss: 0.318924218416214
setp: 3600, Loss: 0.33168819546699524
setp: 3700, Loss: 0.43286946415901184
setp: 3800, Loss: 0.3279035687446594
setp: 3900, Loss: 0.3192565143108368
setp: 4000, Loss: 0.3938708007335663
setp: 4100, Loss: 0.3173026740550995
setp: 4200, Loss: 0.3152121603488922
setp: 4300, Loss: 0.3161945343017578
setp: 4400, Loss: 0.3169052004814148
setp: 4500, Loss: 0.31569239497184753
setp: 4600, Loss: 0.3162729740142822
setp: 4700, Loss: 0.31648650765419006
setp: 4800, Loss: 0.3175528645515442
setp: 4900, Loss: 0.31662851572036743
training successfully ended.
validating...
acc: 0.9971264367816092
precision: 0.9942857142857143
recall: 1.0
F_score: 0.9971346704871061
validating...
acc: 0.9276315789473685
precision: 0.8695652173913043
recall: 0.7142857142857143
F_score: 0.7843137254901961
******fold 4******
[94, 514]
training...
setp: 0, Loss: 0.6940886974334717
setp: 100, Loss: 0.6933647394180298
setp: 200, Loss: 0.4881128668785095
setp: 300, Loss: 0.5580921769142151
setp: 400, Loss: 0.526183545589447
setp: 500, Loss: 0.47277361154556274
setp: 600, Loss: 0.3624999225139618
setp: 700, Loss: 0.3884146213531494
setp: 800, Loss: 0.4222274124622345
setp: 900, Loss: 0.34909671545028687
setp: 1000, Loss: 0.4310033321380615
setp: 1100, Loss: 0.35704001784324646
setp: 1200, Loss: 0.3221503794193268
setp: 1300, Loss: 0.3689534366130829
setp: 1400, Loss: 0.34737738966941833
setp: 1500, Loss: 0.34738847613334656
setp: 1600, Loss: 0.32393163442611694
setp: 1700, Loss: 0.3206159472465515
setp: 1800, Loss: 0.31787535548210144
setp: 1900, Loss: 0.3157272934913635
setp: 2000, Loss: 0.35002490878105164
setp: 2100, Loss: 0.316343754529953
setp: 2200, Loss: 0.3344448208808899
setp: 2300, Loss: 0.3202838599681854
setp: 2400, Loss: 0.3157931864261627
setp: 2500, Loss: 0.3176845908164978
setp: 2600, Loss: 0.33022379875183105
setp: 2700, Loss: 0.31679630279541016
setp: 2800, Loss: 0.31795650720596313
setp: 2900, Loss: 0.3153337836265564
setp: 3000, Loss: 0.3471966087818146
setp: 3100, Loss: 0.31533756852149963
setp: 3200, Loss: 0.3147684931755066
setp: 3300, Loss: 0.31607586145401
setp: 3400, Loss: 0.31868138909339905
setp: 3500, Loss: 0.32712578773498535
setp: 3600, Loss: 0.31627824902534485
setp: 3700, Loss: 0.32031935453414917
setp: 3800, Loss: 0.3169533610343933
setp: 3900, Loss: 0.31687578558921814
setp: 4000, Loss: 0.31699755787849426
setp: 4100, Loss: 0.3192788064479828
setp: 4200, Loss: 0.31677791476249695
setp: 4300, Loss: 0.3184507489204407
setp: 4400, Loss: 0.31624293327331543
setp: 4500, Loss: 0.3227486312389374
setp: 4600, Loss: 0.328655868768692
setp: 4700, Loss: 0.31625494360923767
setp: 4800, Loss: 0.3157792389392853
setp: 4900, Loss: 0.3181961476802826
training successfully ended.
validating...
acc: 0.9990272373540856
precision: 0.9980582524271845
recall: 1.0
F_score: 0.9990281827016522
validating...
acc: 0.9605263157894737
precision: 0.9375
recall: 0.75
F_score: 0.8333333333333334
model saved.
avg_acc: 0.9513157894736842, avg_f_score: 0.8390214752567694
-------------subject: 14-------------
==========valence==========
******fold 0******
[308, 300]
training...
setp: 0, Loss: 0.7157474160194397
setp: 100, Loss: 0.6290408968925476
setp: 200, Loss: 0.5342890620231628
setp: 300, Loss: 0.431763231754303
setp: 400, Loss: 0.43939831852912903
setp: 500, Loss: 0.35783690214157104
setp: 600, Loss: 0.41356176137924194
setp: 700, Loss: 0.36192771792411804
setp: 800, Loss: 0.3308732807636261
setp: 900, Loss: 0.3747464120388031
setp: 1000, Loss: 0.32085901498794556
setp: 1100, Loss: 0.39815664291381836
setp: 1200, Loss: 0.3526822030544281
setp: 1300, Loss: 0.34879451990127563
setp: 1400, Loss: 0.3191260099411011
setp: 1500, Loss: 0.320165753364563
setp: 1600, Loss: 0.3185381293296814
setp: 1700, Loss: 0.3175784945487976
setp: 1800, Loss: 0.319700688123703
setp: 1900, Loss: 0.5065398812294006
setp: 2000, Loss: 0.31875938177108765
setp: 2100, Loss: 0.31952008605003357
setp: 2200, Loss: 0.31694209575653076
setp: 2300, Loss: 0.31857651472091675
setp: 2400, Loss: 0.3187049627304077
setp: 2500, Loss: 0.31843364238739014
setp: 2600, Loss: 0.31703421473503113
setp: 2700, Loss: 0.3468306064605713
setp: 2800, Loss: 0.3210650384426117
setp: 2900, Loss: 0.32182949781417847
setp: 3000, Loss: 0.31938010454177856
setp: 3100, Loss: 0.31942662596702576
setp: 3200, Loss: 0.31813302636146545
setp: 3300, Loss: 0.3181116580963135
setp: 3400, Loss: 0.3185107111930847
setp: 3500, Loss: 0.3182309865951538
setp: 3600, Loss: 0.3171508014202118
setp: 3700, Loss: 0.31690484285354614
setp: 3800, Loss: 0.349662721157074
setp: 3900, Loss: 0.3185007572174072
setp: 4000, Loss: 0.3178095519542694
setp: 4100, Loss: 0.3938359320163727
setp: 4200, Loss: 0.3656131327152252
setp: 4300, Loss: 0.3187095820903778
setp: 4400, Loss: 0.3167554438114166
setp: 4500, Loss: 0.3161015808582306
setp: 4600, Loss: 0.3173883259296417
setp: 4700, Loss: 0.3185364603996277
setp: 4800, Loss: 0.31877216696739197
setp: 4900, Loss: 0.31912463903427124
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9078947368421053
precision: 0.953125
recall: 0.8472222222222222
F_score: 0.8970588235294118
******fold 1******
[298, 310]
training...
setp: 0, Loss: 0.7002983093261719
setp: 100, Loss: 0.691326916217804
setp: 200, Loss: 0.5460525751113892
setp: 300, Loss: 0.5716295838356018
setp: 400, Loss: 0.5047456622123718
setp: 500, Loss: 0.4857349991798401
setp: 600, Loss: 0.4209466278553009
setp: 700, Loss: 0.382907897233963
setp: 800, Loss: 0.38916507363319397
setp: 900, Loss: 0.384106308221817
setp: 1000, Loss: 0.3527756929397583
setp: 1100, Loss: 0.36118844151496887
setp: 1200, Loss: 0.3843541145324707
setp: 1300, Loss: 0.3441951274871826
setp: 1400, Loss: 0.3216189444065094
setp: 1500, Loss: 0.36368924379348755
setp: 1600, Loss: 0.31983959674835205
setp: 1700, Loss: 0.3161472976207733
setp: 1800, Loss: 0.34194666147232056
setp: 1900, Loss: 0.31783899664878845
setp: 2000, Loss: 0.3179817795753479
setp: 2100, Loss: 0.3165402114391327
setp: 2200, Loss: 0.31838902831077576
setp: 2300, Loss: 0.349245548248291
setp: 2400, Loss: 0.32429608702659607
setp: 2500, Loss: 0.3168107569217682
setp: 2600, Loss: 0.35621321201324463
setp: 2700, Loss: 0.31944069266319275
setp: 2800, Loss: 0.339365690946579
setp: 2900, Loss: 0.316571444272995
setp: 3000, Loss: 0.33632585406303406
setp: 3100, Loss: 0.3749857544898987
setp: 3200, Loss: 0.32892823219299316
setp: 3300, Loss: 0.319061279296875
setp: 3400, Loss: 0.31764063239097595
setp: 3500, Loss: 0.3175305426120758
setp: 3600, Loss: 0.31605401635169983
setp: 3700, Loss: 0.3159842789173126
setp: 3800, Loss: 0.317330926656723
setp: 3900, Loss: 0.31789830327033997
setp: 4000, Loss: 0.3186947703361511
setp: 4100, Loss: 0.31662169098854065
setp: 4200, Loss: 0.3191293478012085
setp: 4300, Loss: 0.3174615204334259
setp: 4400, Loss: 0.3166465163230896
setp: 4500, Loss: 0.3196755647659302
setp: 4600, Loss: 0.3175533413887024
setp: 4700, Loss: 0.32488319277763367
setp: 4800, Loss: 0.3168649971485138
setp: 4900, Loss: 0.31716352701187134
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9078947368421053
precision: 0.9146341463414634
recall: 0.9146341463414634
F_score: 0.9146341463414634
******fold 2******
[309, 299]
training...
setp: 0, Loss: 0.7280323505401611
setp: 100, Loss: 0.7059895396232605
setp: 200, Loss: 0.5779857039451599
setp: 300, Loss: 0.5127037167549133
setp: 400, Loss: 0.4860489070415497
setp: 500, Loss: 0.47735080122947693
setp: 600, Loss: 0.4719071686267853
setp: 700, Loss: 0.43130138516426086
setp: 800, Loss: 0.34592363238334656
setp: 900, Loss: 0.33334630727767944
setp: 1000, Loss: 0.35827532410621643
setp: 1100, Loss: 0.3306494951248169
setp: 1200, Loss: 0.3254137933254242
setp: 1300, Loss: 0.3883917033672333
setp: 1400, Loss: 0.3211880326271057
setp: 1500, Loss: 0.351515531539917
setp: 1600, Loss: 0.3333228826522827
setp: 1700, Loss: 0.3179122805595398
setp: 1800, Loss: 0.31960248947143555
setp: 1900, Loss: 0.31961825489997864
setp: 2000, Loss: 0.3197999596595764
setp: 2100, Loss: 0.3200068175792694
setp: 2200, Loss: 0.3187113404273987
setp: 2300, Loss: 0.32135844230651855
setp: 2400, Loss: 0.3187621235847473
setp: 2500, Loss: 0.3181917667388916
setp: 2600, Loss: 0.31958436965942383
setp: 2700, Loss: 0.3199271857738495
setp: 2800, Loss: 0.3193220794200897
setp: 2900, Loss: 0.32173120975494385
setp: 3000, Loss: 0.34876617789268494
setp: 3100, Loss: 0.36140239238739014
setp: 3200, Loss: 0.35203075408935547
setp: 3300, Loss: 0.3197121322154999
setp: 3400, Loss: 0.32026514410972595
setp: 3500, Loss: 0.319085031747818
setp: 3600, Loss: 0.3176901042461395
setp: 3700, Loss: 0.31959640979766846
setp: 3800, Loss: 0.31962689757347107
setp: 3900, Loss: 0.31993567943573
setp: 4000, Loss: 0.3531779646873474
setp: 4100, Loss: 0.32965612411499023
setp: 4200, Loss: 0.32313257455825806
setp: 4300, Loss: 0.322235643863678
setp: 4400, Loss: 0.3237641453742981
setp: 4500, Loss: 0.35338833928108215
setp: 4600, Loss: 0.3211827874183655
setp: 4700, Loss: 0.329801082611084
setp: 4800, Loss: 0.31932389736175537
setp: 4900, Loss: 0.3188813328742981
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.868421052631579
precision: 0.8695652173913043
recall: 0.8450704225352113
F_score: 0.8571428571428571
******fold 3******
[308, 300]
training...
setp: 0, Loss: 0.7159374952316284
setp: 100, Loss: 0.6290132999420166
setp: 200, Loss: 0.5053126215934753
setp: 300, Loss: 0.39769795536994934
setp: 400, Loss: 0.48060843348503113
setp: 500, Loss: 0.39852622151374817
setp: 600, Loss: 0.34351685643196106
setp: 700, Loss: 0.3696663975715637
setp: 800, Loss: 0.34470221400260925
setp: 900, Loss: 0.3432738482952118
setp: 1000, Loss: 0.3415164053440094
setp: 1100, Loss: 0.3209741413593292
setp: 1200, Loss: 0.32671985030174255
setp: 1300, Loss: 0.3207797110080719
setp: 1400, Loss: 0.3185504078865051
setp: 1500, Loss: 0.34625354409217834
setp: 1600, Loss: 0.33969351649284363
setp: 1700, Loss: 0.3294510841369629
setp: 1800, Loss: 0.3181172311306
setp: 1900, Loss: 0.3550117611885071
setp: 2000, Loss: 0.32019564509391785
setp: 2100, Loss: 0.3510679006576538
setp: 2200, Loss: 0.3166974186897278
setp: 2300, Loss: 0.3577239513397217
setp: 2400, Loss: 0.3200395703315735
setp: 2500, Loss: 0.3294982314109802
setp: 2600, Loss: 0.31784817576408386
setp: 2700, Loss: 0.33436211943626404
setp: 2800, Loss: 0.3399338126182556
setp: 2900, Loss: 0.3356487452983856
setp: 3000, Loss: 0.3207239806652069
setp: 3100, Loss: 0.3351800739765167
setp: 3200, Loss: 0.456681489944458
setp: 3300, Loss: 0.3166695833206177
setp: 3400, Loss: 0.31784310936927795
setp: 3500, Loss: 0.31611189246177673
setp: 3600, Loss: 0.31838637590408325
setp: 3700, Loss: 0.3164900243282318
setp: 3800, Loss: 0.31866568326950073
setp: 3900, Loss: 0.317770779132843
setp: 4000, Loss: 0.31809383630752563
setp: 4100, Loss: 0.31691768765449524
setp: 4200, Loss: 0.3186759650707245
setp: 4300, Loss: 0.3192771077156067
setp: 4400, Loss: 0.33904916048049927
setp: 4500, Loss: 0.3244488537311554
setp: 4600, Loss: 0.3783890902996063
setp: 4700, Loss: 0.31986838579177856
setp: 4800, Loss: 0.31739386916160583
setp: 4900, Loss: 0.3171045780181885
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.9420289855072463
recall: 0.9027777777777778
F_score: 0.9219858156028369
******fold 4******
[297, 311]
training...
setp: 0, Loss: 0.6921980381011963
setp: 100, Loss: 0.6102095246315002
setp: 200, Loss: 0.5437855124473572
setp: 300, Loss: 0.45371341705322266
setp: 400, Loss: 0.42140820622444153
setp: 500, Loss: 0.38257741928100586
setp: 600, Loss: 0.420033723115921
setp: 700, Loss: 0.40038633346557617
setp: 800, Loss: 0.32896628975868225
setp: 900, Loss: 0.3573409616947174
setp: 1000, Loss: 0.3271401524543762
setp: 1100, Loss: 0.3222198784351349
setp: 1200, Loss: 0.3208083510398865
setp: 1300, Loss: 0.3199842870235443
setp: 1400, Loss: 0.31835752725601196
setp: 1500, Loss: 0.32350894808769226
setp: 1600, Loss: 0.3251804709434509
setp: 1700, Loss: 0.3229331970214844
setp: 1800, Loss: 0.32548466324806213
setp: 1900, Loss: 0.3492126166820526
setp: 2000, Loss: 0.31949278712272644
setp: 2100, Loss: 0.3207518458366394
setp: 2200, Loss: 0.32168862223625183
setp: 2300, Loss: 0.3179306387901306
setp: 2400, Loss: 0.3207228183746338
setp: 2500, Loss: 0.3173428475856781
setp: 2600, Loss: 0.3206934928894043
setp: 2700, Loss: 0.3208593726158142
setp: 2800, Loss: 0.32244738936424255
setp: 2900, Loss: 0.3175674378871918
setp: 3000, Loss: 0.31960636377334595
setp: 3100, Loss: 0.3190074861049652
setp: 3200, Loss: 0.3169810473918915
setp: 3300, Loss: 0.3245563209056854
setp: 3400, Loss: 0.36135298013687134
setp: 3500, Loss: 0.32105135917663574
setp: 3600, Loss: 0.316725492477417
setp: 3700, Loss: 0.31704726815223694
setp: 3800, Loss: 0.31787213683128357
setp: 3900, Loss: 0.31830164790153503
setp: 4000, Loss: 0.3186717629432678
setp: 4100, Loss: 0.3167361915111542
setp: 4200, Loss: 0.3170243799686432
setp: 4300, Loss: 0.3192581236362457
setp: 4400, Loss: 0.31896647810935974
setp: 4500, Loss: 0.3177992105484009
setp: 4600, Loss: 0.3477472960948944
setp: 4700, Loss: 0.31949636340141296
setp: 4800, Loss: 0.32072916626930237
setp: 4900, Loss: 0.3182781934738159
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9743589743589743
recall: 0.9156626506024096
F_score: 0.9440993788819876
model saved.
avg_acc: 0.9105263157894736, avg_f_score: 0.9069842042997113
==========arousal==========
******fold 0******
[194, 414]
training...
setp: 0, Loss: 0.7150623798370361
setp: 100, Loss: 0.6453266739845276
setp: 200, Loss: 0.6208851337432861
setp: 300, Loss: 0.6432281136512756
setp: 400, Loss: 0.5367770195007324
setp: 500, Loss: 0.6059299111366272
setp: 600, Loss: 0.6148702502250671
setp: 700, Loss: 0.6010123491287231
setp: 800, Loss: 0.5872712731361389
setp: 900, Loss: 0.5817376375198364
setp: 1000, Loss: 0.6004948616027832
setp: 1100, Loss: 0.5419785976409912
setp: 1200, Loss: 0.5450694561004639
setp: 1300, Loss: 0.5442671775817871
setp: 1400, Loss: 0.46519598364830017
setp: 1500, Loss: 0.4716213643550873
setp: 1600, Loss: 0.5275137424468994
setp: 1700, Loss: 0.4780399203300476
setp: 1800, Loss: 0.49100252985954285
setp: 1900, Loss: 0.5227599143981934
setp: 2000, Loss: 0.5670011639595032
setp: 2100, Loss: 0.48072293400764465
setp: 2200, Loss: 0.4620749056339264
setp: 2300, Loss: 0.41143468022346497
setp: 2400, Loss: 0.4244517385959625
setp: 2500, Loss: 0.440140962600708
setp: 2600, Loss: 0.4822936952114105
setp: 2700, Loss: 0.4659923017024994
setp: 2800, Loss: 0.46529707312583923
setp: 2900, Loss: 0.32640770077705383
setp: 3000, Loss: 0.38629579544067383
setp: 3100, Loss: 0.35783618688583374
setp: 3200, Loss: 0.4597359299659729
setp: 3300, Loss: 0.346644788980484
setp: 3400, Loss: 0.4301891028881073
setp: 3500, Loss: 0.3978685140609741
setp: 3600, Loss: 0.3778385519981384
setp: 3700, Loss: 0.37274083495140076
setp: 3800, Loss: 0.36814460158348083
setp: 3900, Loss: 0.385489821434021
setp: 4000, Loss: 0.41153067350387573
setp: 4100, Loss: 0.390597939491272
setp: 4200, Loss: 0.34758511185646057
setp: 4300, Loss: 0.3704600930213928
setp: 4400, Loss: 0.40831825137138367
setp: 4500, Loss: 0.38659363985061646
setp: 4600, Loss: 0.3937032222747803
setp: 4700, Loss: 0.38078898191452026
setp: 4800, Loss: 0.31916120648384094
setp: 4900, Loss: 0.35116177797317505
training successfully ended.
validating...
acc: 0.9884868421052632
precision: 0.9947089947089947
recall: 0.9690721649484536
F_score: 0.9817232375979112
validating...
acc: 0.881578947368421
precision: 0.8301886792452831
recall: 0.8301886792452831
F_score: 0.8301886792452831
******fold 1******
[202, 406]
training...
setp: 0, Loss: 0.7619310617446899
setp: 100, Loss: 0.6641443967819214
setp: 200, Loss: 0.61421799659729
setp: 300, Loss: 0.6389570832252502
setp: 400, Loss: 0.5910155773162842
setp: 500, Loss: 0.5857917666435242
setp: 600, Loss: 0.6113711595535278
setp: 700, Loss: 0.5857992172241211
setp: 800, Loss: 0.5589404106140137
setp: 900, Loss: 0.5154040455818176
setp: 1000, Loss: 0.5684748888015747
setp: 1100, Loss: 0.5298096537590027
setp: 1200, Loss: 0.5502136945724487
setp: 1300, Loss: 0.42596015334129333
setp: 1400, Loss: 0.5152413249015808
setp: 1500, Loss: 0.610358476638794
setp: 1600, Loss: 0.47285810112953186
setp: 1700, Loss: 0.4948921799659729
setp: 1800, Loss: 0.4527498483657837
setp: 1900, Loss: 0.48344579339027405
setp: 2000, Loss: 0.4672941565513611
setp: 2100, Loss: 0.4298265874385834
setp: 2200, Loss: 0.42447924613952637
setp: 2300, Loss: 0.38597530126571655
setp: 2400, Loss: 0.4284763038158417
setp: 2500, Loss: 0.3546820282936096
setp: 2600, Loss: 0.4211513102054596
setp: 2700, Loss: 0.34932342171669006
setp: 2800, Loss: 0.3269102871417999
setp: 2900, Loss: 0.3692493438720703
setp: 3000, Loss: 0.3285095989704132
setp: 3100, Loss: 0.3667354881763458
setp: 3200, Loss: 0.3174586594104767
setp: 3300, Loss: 0.3217957317829132
setp: 3400, Loss: 0.32671332359313965
setp: 3500, Loss: 0.31850019097328186
setp: 3600, Loss: 0.3198084235191345
setp: 3700, Loss: 0.343279093503952
setp: 3800, Loss: 0.32755815982818604
setp: 3900, Loss: 0.3182007670402527
setp: 4000, Loss: 0.31762099266052246
setp: 4100, Loss: 0.31760352849960327
setp: 4200, Loss: 0.31757229566574097
setp: 4300, Loss: 0.3175007402896881
setp: 4400, Loss: 0.3173394203186035
setp: 4500, Loss: 0.3178497850894928
setp: 4600, Loss: 0.31743448972702026
setp: 4700, Loss: 0.3189660906791687
setp: 4800, Loss: 0.31765666604042053
setp: 4900, Loss: 0.31964340806007385
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9950738916256158
recall: 1.0
F_score: 0.9975308641975309
validating...
acc: 0.9210526315789473
precision: 0.8666666666666667
recall: 0.8666666666666667
F_score: 0.8666666666666667
******fold 2******
[198, 410]
training...
setp: 0, Loss: 0.7044695019721985
setp: 100, Loss: 0.5778451561927795
setp: 200, Loss: 0.630551278591156
setp: 300, Loss: 0.5453877449035645
setp: 400, Loss: 0.6180760264396667
setp: 500, Loss: 0.5664688944816589
setp: 600, Loss: 0.6063595414161682
setp: 700, Loss: 0.5962551236152649
setp: 800, Loss: 0.5299717783927917
setp: 900, Loss: 0.4880785048007965
setp: 1000, Loss: 0.5243841409683228
setp: 1100, Loss: 0.4927653968334198
setp: 1200, Loss: 0.5352746844291687
setp: 1300, Loss: 0.3821678161621094
setp: 1400, Loss: 0.4431343674659729
setp: 1500, Loss: 0.4133757948875427
setp: 1600, Loss: 0.3731142282485962
setp: 1700, Loss: 0.4208352863788605
setp: 1800, Loss: 0.40960216522216797
setp: 1900, Loss: 0.35213977098464966
setp: 2000, Loss: 0.3203524053096771
setp: 2100, Loss: 0.35107165575027466
setp: 2200, Loss: 0.3478459417819977
setp: 2300, Loss: 0.3487813174724579
setp: 2400, Loss: 0.3340517282485962
setp: 2500, Loss: 0.3389431834220886
setp: 2600, Loss: 0.410200297832489
setp: 2700, Loss: 0.32218044996261597
setp: 2800, Loss: 0.3180176019668579
setp: 2900, Loss: 0.3203390836715698
setp: 3000, Loss: 0.32040131092071533
setp: 3100, Loss: 0.3189356327056885
setp: 3200, Loss: 0.3189504146575928
setp: 3300, Loss: 0.3170967102050781
setp: 3400, Loss: 0.3203936815261841
setp: 3500, Loss: 0.31831076741218567
setp: 3600, Loss: 0.31867632269859314
setp: 3700, Loss: 0.318088173866272
setp: 3800, Loss: 0.3182142972946167
setp: 3900, Loss: 0.31704235076904297
setp: 4000, Loss: 0.3186359405517578
setp: 4100, Loss: 0.38284584879875183
setp: 4200, Loss: 0.36367595195770264
setp: 4300, Loss: 0.3513129949569702
setp: 4400, Loss: 0.3192482590675354
setp: 4500, Loss: 0.3170229196548462
setp: 4600, Loss: 0.3158717453479767
setp: 4700, Loss: 0.3181600570678711
setp: 4800, Loss: 0.31730613112449646
setp: 4900, Loss: 0.31909650564193726
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9473684210526315
precision: 0.9019607843137255
recall: 0.9387755102040817
F_score: 0.92
******fold 3******
[194, 414]
training...
setp: 0, Loss: 0.749407947063446
setp: 100, Loss: 0.645435094833374
setp: 200, Loss: 0.603986382484436
setp: 300, Loss: 0.6118671894073486
setp: 400, Loss: 0.5671826004981995
setp: 500, Loss: 0.5441410541534424
setp: 600, Loss: 0.5142186284065247
setp: 700, Loss: 0.5186461806297302
setp: 800, Loss: 0.4605865478515625
setp: 900, Loss: 0.5424452424049377
setp: 1000, Loss: 0.43941783905029297
setp: 1100, Loss: 0.4034089148044586
setp: 1200, Loss: 0.41691890358924866
setp: 1300, Loss: 0.44173023104667664
setp: 1400, Loss: 0.375798761844635
setp: 1500, Loss: 0.39311593770980835
setp: 1600, Loss: 0.4146524965763092
setp: 1700, Loss: 0.39061272144317627
setp: 1800, Loss: 0.4025164842605591
setp: 1900, Loss: 0.38610073924064636
setp: 2000, Loss: 0.3590620160102844
setp: 2100, Loss: 0.3582818806171417
setp: 2200, Loss: 0.3568319082260132
setp: 2300, Loss: 0.3476889431476593
setp: 2400, Loss: 0.35232678055763245
setp: 2500, Loss: 0.36017632484436035
setp: 2600, Loss: 0.32077479362487793
setp: 2700, Loss: 0.38015931844711304
setp: 2800, Loss: 0.3882123529911041
setp: 2900, Loss: 0.3603532910346985
setp: 3000, Loss: 0.36844685673713684
setp: 3100, Loss: 0.353447824716568
setp: 3200, Loss: 0.3806613087654114
setp: 3300, Loss: 0.31814315915107727
setp: 3400, Loss: 0.35836613178253174
setp: 3500, Loss: 0.3539358377456665
setp: 3600, Loss: 0.37818029522895813
setp: 3700, Loss: 0.34968239068984985
setp: 3800, Loss: 0.319182425737381
setp: 3900, Loss: 0.36748388409614563
setp: 4000, Loss: 0.3616214394569397
setp: 4100, Loss: 0.35020601749420166
setp: 4200, Loss: 0.34828075766563416
setp: 4300, Loss: 0.31835809350013733
setp: 4400, Loss: 0.3486691117286682
setp: 4500, Loss: 0.31696492433547974
setp: 4600, Loss: 0.37981081008911133
setp: 4700, Loss: 0.34923917055130005
setp: 4800, Loss: 0.3493797183036804
setp: 4900, Loss: 0.34972742199897766
training successfully ended.
validating...
acc: 0.9671052631578947
precision: 1.0
recall: 0.8969072164948454
F_score: 0.9456521739130436
validating...
acc: 0.8486842105263158
precision: 0.8409090909090909
recall: 0.6981132075471698
F_score: 0.7628865979381443
******fold 4******
[200, 408]
training...
setp: 0, Loss: 0.8331406116485596
setp: 100, Loss: 0.6215111613273621
setp: 200, Loss: 0.6604779958724976
setp: 300, Loss: 0.5611695051193237
setp: 400, Loss: 0.6245425939559937
setp: 500, Loss: 0.5379900932312012
setp: 600, Loss: 0.5135477781295776
setp: 700, Loss: 0.5151044726371765
setp: 800, Loss: 0.583263635635376
setp: 900, Loss: 0.5863101482391357
setp: 1000, Loss: 0.5434526801109314
setp: 1100, Loss: 0.5503615736961365
setp: 1200, Loss: 0.5661004185676575
setp: 1300, Loss: 0.5740780234336853
setp: 1400, Loss: 0.5595657229423523
setp: 1500, Loss: 0.6114605665206909
setp: 1600, Loss: 0.5971155166625977
setp: 1700, Loss: 0.5503533482551575
setp: 1800, Loss: 0.5443520545959473
setp: 1900, Loss: 0.46485623717308044
setp: 2000, Loss: 0.5654383897781372
setp: 2100, Loss: 0.4506487548351288
setp: 2200, Loss: 0.42096638679504395
setp: 2300, Loss: 0.521342933177948
setp: 2400, Loss: 0.4879007041454315
setp: 2500, Loss: 0.4574984610080719
setp: 2600, Loss: 0.4067980945110321
setp: 2700, Loss: 0.43352022767066956
setp: 2800, Loss: 0.4438328444957733
setp: 2900, Loss: 0.37254682183265686
setp: 3000, Loss: 0.3266933560371399
setp: 3100, Loss: 0.3533935248851776
setp: 3200, Loss: 0.343869149684906
setp: 3300, Loss: 0.3697616755962372
setp: 3400, Loss: 0.353280246257782
setp: 3500, Loss: 0.35448241233825684
setp: 3600, Loss: 0.33448678255081177
setp: 3700, Loss: 0.3237098157405853
setp: 3800, Loss: 0.3186628222465515
setp: 3900, Loss: 0.32028061151504517
setp: 4000, Loss: 0.3260383605957031
setp: 4100, Loss: 0.31634292006492615
setp: 4200, Loss: 0.31782177090644836
setp: 4300, Loss: 0.3196410536766052
setp: 4400, Loss: 0.3179587125778198
setp: 4500, Loss: 0.3178170919418335
setp: 4600, Loss: 0.3198375105857849
setp: 4700, Loss: 0.35056865215301514
setp: 4800, Loss: 0.31831133365631104
setp: 4900, Loss: 0.3926513195037842
training successfully ended.
validating...
acc: 0.9884868421052632
precision: 0.9707317073170731
recall: 0.995
F_score: 0.982716049382716
validating...
acc: 0.9605263157894737
precision: 0.8867924528301887
recall: 1.0
F_score: 0.9400000000000001
model saved.
avg_acc: 0.9118421052631579, avg_f_score: 0.8639483887700189
-------------subject: 15-------------
==========valence==========
******fold 0******
[305, 303]
training...
setp: 0, Loss: 0.7739998698234558
setp: 100, Loss: 0.6934983730316162
setp: 200, Loss: 0.6888399124145508
setp: 300, Loss: 0.5567641854286194
setp: 400, Loss: 0.5759915709495544
setp: 500, Loss: 0.5267570614814758
setp: 600, Loss: 0.5246646404266357
setp: 700, Loss: 0.4374510645866394
setp: 800, Loss: 0.44829216599464417
setp: 900, Loss: 0.4387001097202301
setp: 1000, Loss: 0.3564540147781372
setp: 1100, Loss: 0.4462142288684845
setp: 1200, Loss: 0.3510825037956238
setp: 1300, Loss: 0.3345457911491394
setp: 1400, Loss: 0.32323554158210754
setp: 1500, Loss: 0.3237397372722626
setp: 1600, Loss: 0.32399722933769226
setp: 1700, Loss: 0.3198716342449188
setp: 1800, Loss: 0.35632795095443726
setp: 1900, Loss: 0.33191224932670593
setp: 2000, Loss: 0.32236430048942566
setp: 2100, Loss: 0.35400524735450745
setp: 2200, Loss: 0.31635797023773193
setp: 2300, Loss: 0.3326044976711273
setp: 2400, Loss: 0.36870694160461426
setp: 2500, Loss: 0.32276010513305664
setp: 2600, Loss: 0.3331588804721832
setp: 2700, Loss: 0.40185970067977905
setp: 2800, Loss: 0.3252548277378082
setp: 2900, Loss: 0.3166881203651428
setp: 3000, Loss: 0.3196128010749817
setp: 3100, Loss: 0.3165818750858307
setp: 3200, Loss: 0.3162837326526642
setp: 3300, Loss: 0.31638798117637634
setp: 3400, Loss: 0.31817102432250977
setp: 3500, Loss: 0.317198783159256
setp: 3600, Loss: 0.31602245569229126
setp: 3700, Loss: 0.31724879145622253
setp: 3800, Loss: 0.3191896080970764
setp: 3900, Loss: 0.31818097829818726
setp: 4000, Loss: 0.3175870478153229
setp: 4100, Loss: 0.31631335616111755
setp: 4200, Loss: 0.31919580698013306
setp: 4300, Loss: 0.3584039807319641
setp: 4400, Loss: 0.327230304479599
setp: 4500, Loss: 0.32782813906669617
setp: 4600, Loss: 0.350974440574646
setp: 4700, Loss: 0.31765198707580566
setp: 4800, Loss: 0.3161557614803314
setp: 4900, Loss: 0.3189232647418976
training successfully ended.
validating...
acc: 0.9786184210526315
precision: 1.0
recall: 0.9573770491803278
F_score: 0.9782244556113903
validating...
acc: 0.8947368421052632
precision: 0.9682539682539683
recall: 0.8133333333333334
F_score: 0.8840579710144927
******fold 1******
[302, 306]
training...
setp: 0, Loss: 0.7464560866355896
setp: 100, Loss: 0.6842799782752991
setp: 200, Loss: 0.5919955968856812
setp: 300, Loss: 0.5059723854064941
setp: 400, Loss: 0.5934659838676453
setp: 500, Loss: 0.5923986434936523
setp: 600, Loss: 0.49029892683029175
setp: 700, Loss: 0.4318050146102905
setp: 800, Loss: 0.4201837480068207
setp: 900, Loss: 0.4507048428058624
setp: 1000, Loss: 0.36193832755088806
setp: 1100, Loss: 0.45074599981307983
setp: 1200, Loss: 0.363056480884552
setp: 1300, Loss: 0.3287886381149292
setp: 1400, Loss: 0.364665150642395
setp: 1500, Loss: 0.322654128074646
setp: 1600, Loss: 0.35006991028785706
setp: 1700, Loss: 0.31955477595329285
setp: 1800, Loss: 0.33154764771461487
setp: 1900, Loss: 0.3495073914527893
setp: 2000, Loss: 0.32729870080947876
setp: 2100, Loss: 0.3395569622516632
setp: 2200, Loss: 0.320873886346817
setp: 2300, Loss: 0.38858774304389954
setp: 2400, Loss: 0.3205348253250122
setp: 2500, Loss: 0.32751235365867615
setp: 2600, Loss: 0.33760514855384827
setp: 2700, Loss: 0.3178320825099945
setp: 2800, Loss: 0.34808075428009033
setp: 2900, Loss: 0.3173377811908722
setp: 3000, Loss: 0.31815168261528015
setp: 3100, Loss: 0.31883522868156433
setp: 3200, Loss: 0.31644222140312195
setp: 3300, Loss: 0.3193868398666382
setp: 3400, Loss: 0.317812979221344
setp: 3500, Loss: 0.3172687590122223
setp: 3600, Loss: 0.31621867418289185
setp: 3700, Loss: 0.31871476769447327
setp: 3800, Loss: 0.3255796730518341
setp: 3900, Loss: 0.32024773955345154
setp: 4000, Loss: 0.31738805770874023
setp: 4100, Loss: 0.32878339290618896
setp: 4200, Loss: 0.3511037230491638
setp: 4300, Loss: 0.3205992877483368
setp: 4400, Loss: 0.3358887732028961
setp: 4500, Loss: 0.319804847240448
setp: 4600, Loss: 0.3468148410320282
setp: 4700, Loss: 0.3479907810688019
setp: 4800, Loss: 0.3167581558227539
setp: 4900, Loss: 0.31898239254951477
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9605263157894737
precision: 0.9864864864864865
recall: 0.9358974358974359
F_score: 0.9605263157894737
******fold 2******
[314, 294]
training...
setp: 0, Loss: 0.6932157278060913
setp: 100, Loss: 0.6133242845535278
setp: 200, Loss: 0.5372728705406189
setp: 300, Loss: 0.4915146827697754
setp: 400, Loss: 0.5311416387557983
setp: 500, Loss: 0.5433509945869446
setp: 600, Loss: 0.4306088387966156
setp: 700, Loss: 0.4086948335170746
setp: 800, Loss: 0.45400112867355347
setp: 900, Loss: 0.44578760862350464
setp: 1000, Loss: 0.36655962467193604
setp: 1100, Loss: 0.41870442032814026
setp: 1200, Loss: 0.3544543981552124
setp: 1300, Loss: 0.33595502376556396
setp: 1400, Loss: 0.3592129051685333
setp: 1500, Loss: 0.3507167100906372
setp: 1600, Loss: 0.3162846267223358
setp: 1700, Loss: 0.36851438879966736
setp: 1800, Loss: 0.3258005380630493
setp: 1900, Loss: 0.35971882939338684
setp: 2000, Loss: 0.38310983777046204
setp: 2100, Loss: 0.3204442858695984
setp: 2200, Loss: 0.32166552543640137
setp: 2300, Loss: 0.37810230255126953
setp: 2400, Loss: 0.321665495634079
setp: 2500, Loss: 0.3159882724285126
setp: 2600, Loss: 0.31747692823410034
setp: 2700, Loss: 0.37299415469169617
setp: 2800, Loss: 0.32824641466140747
setp: 2900, Loss: 0.3175038695335388
setp: 3000, Loss: 0.3220960199832916
setp: 3100, Loss: 0.3156866133213043
setp: 3200, Loss: 0.3198956549167633
setp: 3300, Loss: 0.3182183802127838
setp: 3400, Loss: 0.3253527879714966
setp: 3500, Loss: 0.3359563648700714
setp: 3600, Loss: 0.31636565923690796
setp: 3700, Loss: 0.3293437957763672
setp: 3800, Loss: 0.34977713227272034
setp: 3900, Loss: 0.31640470027923584
setp: 4000, Loss: 0.31572628021240234
setp: 4100, Loss: 0.32161155343055725
setp: 4200, Loss: 0.3852182626724243
setp: 4300, Loss: 0.3180655539035797
setp: 4400, Loss: 0.31579071283340454
setp: 4500, Loss: 0.3187321722507477
setp: 4600, Loss: 0.3533242344856262
setp: 4700, Loss: 0.32087355852127075
setp: 4800, Loss: 0.31742754578590393
setp: 4900, Loss: 0.3220917284488678
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.9936305732484076
F_score: 0.9968051118210862
validating...
acc: 0.9539473684210527
precision: 0.927536231884058
recall: 0.9696969696969697
F_score: 0.9481481481481481
******fold 3******
[305, 303]
training...
setp: 0, Loss: 0.7672278881072998
setp: 100, Loss: 0.6826018691062927
setp: 200, Loss: 0.5684709548950195
setp: 300, Loss: 0.40619152784347534
setp: 400, Loss: 0.41056716442108154
setp: 500, Loss: 0.3848680257797241
setp: 600, Loss: 0.3726617693901062
setp: 700, Loss: 0.32640281319618225
setp: 800, Loss: 0.33722469210624695
setp: 900, Loss: 0.350225567817688
setp: 1000, Loss: 0.32700732350349426
setp: 1100, Loss: 0.3230024576187134
setp: 1200, Loss: 0.32034438848495483
setp: 1300, Loss: 0.3193928003311157
setp: 1400, Loss: 0.32111114263534546
setp: 1500, Loss: 0.32030102610588074
setp: 1600, Loss: 0.3205672800540924
setp: 1700, Loss: 0.3185308575630188
setp: 1800, Loss: 0.3196136951446533
setp: 1900, Loss: 0.32180556654930115
setp: 2000, Loss: 0.32222551107406616
setp: 2100, Loss: 0.3183424174785614
setp: 2200, Loss: 0.3185122013092041
setp: 2300, Loss: 0.32022517919540405
setp: 2400, Loss: 0.3198780119419098
setp: 2500, Loss: 0.3181118965148926
setp: 2600, Loss: 0.3190006911754608
setp: 2700, Loss: 0.3209784924983978
setp: 2800, Loss: 0.39251285791397095
setp: 2900, Loss: 0.3863127529621124
setp: 3000, Loss: 0.321025013923645
setp: 3100, Loss: 0.3199855089187622
setp: 3200, Loss: 0.325917512178421
setp: 3300, Loss: 0.3214489221572876
setp: 3400, Loss: 0.3290906846523285
setp: 3500, Loss: 0.3282542824745178
setp: 3600, Loss: 0.31960952281951904
setp: 3700, Loss: 0.31913143396377563
setp: 3800, Loss: 0.3226909637451172
setp: 3900, Loss: 0.3187033534049988
setp: 4000, Loss: 0.31966596841812134
setp: 4100, Loss: 0.3189847767353058
setp: 4200, Loss: 0.3200487494468689
setp: 4300, Loss: 0.32045120000839233
setp: 4400, Loss: 0.3185495436191559
setp: 4500, Loss: 0.31951385736465454
setp: 4600, Loss: 0.32064828276634216
setp: 4700, Loss: 0.3185483515262604
setp: 4800, Loss: 0.3193609118461609
setp: 4900, Loss: 0.31949177384376526
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.8987341772151899
recall: 0.9466666666666667
F_score: 0.9220779220779222
******fold 4******
[294, 314]
training...
setp: 0, Loss: 0.6968547105789185
setp: 100, Loss: 0.6287209391593933
setp: 200, Loss: 0.5111900568008423
setp: 300, Loss: 0.47480902075767517
setp: 400, Loss: 0.5075457096099854
setp: 500, Loss: 0.4846388101577759
setp: 600, Loss: 0.47586214542388916
setp: 700, Loss: 0.4250561594963074
setp: 800, Loss: 0.4558251202106476
setp: 900, Loss: 0.35199594497680664
setp: 1000, Loss: 0.36386236548423767
setp: 1100, Loss: 0.3337715268135071
setp: 1200, Loss: 0.3685201406478882
setp: 1300, Loss: 0.3200746774673462
setp: 1400, Loss: 0.3237386643886566
setp: 1500, Loss: 0.3193168640136719
setp: 1600, Loss: 0.3183777928352356
setp: 1700, Loss: 0.3198937475681305
setp: 1800, Loss: 0.3266489803791046
setp: 1900, Loss: 0.35076087713241577
setp: 2000, Loss: 0.31852757930755615
setp: 2100, Loss: 0.3205562233924866
setp: 2200, Loss: 0.32181164622306824
setp: 2300, Loss: 0.32076093554496765
setp: 2400, Loss: 0.31705087423324585
setp: 2500, Loss: 0.3174578845500946
setp: 2600, Loss: 0.3182436525821686
setp: 2700, Loss: 0.3208577036857605
setp: 2800, Loss: 0.31907370686531067
setp: 2900, Loss: 0.3164151906967163
setp: 3000, Loss: 0.316912978887558
setp: 3100, Loss: 0.31890788674354553
setp: 3200, Loss: 0.3251695930957794
setp: 3300, Loss: 0.38359755277633667
setp: 3400, Loss: 0.3170209527015686
setp: 3500, Loss: 0.3163960576057434
setp: 3600, Loss: 0.31770703196525574
setp: 3700, Loss: 0.3175638020038605
setp: 3800, Loss: 0.3490099310874939
setp: 3900, Loss: 0.31669729948043823
setp: 4000, Loss: 0.317061185836792
setp: 4100, Loss: 0.3163280785083771
setp: 4200, Loss: 0.31930407881736755
setp: 4300, Loss: 0.31780219078063965
setp: 4400, Loss: 0.3188997507095337
setp: 4500, Loss: 0.3173030614852905
setp: 4600, Loss: 0.31964215636253357
setp: 4700, Loss: 0.3234606981277466
setp: 4800, Loss: 0.31575512886047363
setp: 4900, Loss: 0.31635403633117676
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9965986394557823
F_score: 0.9982964224872232
validating...
acc: 0.9407894736842105
precision: 0.9529411764705882
recall: 0.9418604651162791
F_score: 0.9473684210526314
model saved.
avg_acc: 0.9342105263157894, avg_f_score: 0.9324357556165335
==========arousal==========
******fold 0******
[289, 319]
training...
setp: 0, Loss: 0.6920194029808044
setp: 100, Loss: 0.6812888383865356
setp: 200, Loss: 0.5964407920837402
setp: 300, Loss: 0.503609836101532
setp: 400, Loss: 0.4991590976715088
setp: 500, Loss: 0.39953845739364624
setp: 600, Loss: 0.36106032133102417
setp: 700, Loss: 0.38654637336730957
setp: 800, Loss: 0.35248568654060364
setp: 900, Loss: 0.3508835434913635
setp: 1000, Loss: 0.32779476046562195
setp: 1100, Loss: 0.32516855001449585
setp: 1200, Loss: 0.32517215609550476
setp: 1300, Loss: 0.3416076898574829
setp: 1400, Loss: 0.3272136151790619
setp: 1500, Loss: 0.3282625675201416
setp: 1600, Loss: 0.3262121081352234
setp: 1700, Loss: 0.3206694722175598
setp: 1800, Loss: 0.3247339725494385
setp: 1900, Loss: 0.32529422640800476
setp: 2000, Loss: 0.3224121630191803
setp: 2100, Loss: 0.3235747814178467
setp: 2200, Loss: 0.3203682601451874
setp: 2300, Loss: 0.3316580355167389
setp: 2400, Loss: 0.3190920948982239
setp: 2500, Loss: 0.3191116452217102
setp: 2600, Loss: 0.3205227255821228
setp: 2700, Loss: 0.32140329480171204
setp: 2800, Loss: 0.3210866153240204
setp: 2900, Loss: 0.31932365894317627
setp: 3000, Loss: 0.31948038935661316
setp: 3100, Loss: 0.31998902559280396
setp: 3200, Loss: 0.32059237360954285
setp: 3300, Loss: 0.32023462653160095
setp: 3400, Loss: 0.3208655118942261
setp: 3500, Loss: 0.3202936351299286
setp: 3600, Loss: 0.6213901042938232
setp: 3700, Loss: 0.34071847796440125
setp: 3800, Loss: 0.36166343092918396
setp: 3900, Loss: 0.32425665855407715
setp: 4000, Loss: 0.3231395483016968
setp: 4100, Loss: 0.32227015495300293
setp: 4200, Loss: 0.3233613073825836
setp: 4300, Loss: 0.32169005274772644
setp: 4400, Loss: 0.3215411901473999
setp: 4500, Loss: 0.32278794050216675
setp: 4600, Loss: 0.32310837507247925
setp: 4700, Loss: 0.32163482904434204
setp: 4800, Loss: 0.33162447810173035
setp: 4900, Loss: 0.32251161336898804
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.8987341772151899
recall: 0.9861111111111112
F_score: 0.9403973509933775
******fold 1******
[287, 321]
training...
setp: 0, Loss: 0.6959089040756226
setp: 100, Loss: 0.6755868792533875
setp: 200, Loss: 0.593885600566864
setp: 300, Loss: 0.5713598728179932
setp: 400, Loss: 0.4421023428440094
setp: 500, Loss: 0.5083670020103455
setp: 600, Loss: 0.3827558159828186
setp: 700, Loss: 0.37794309854507446
setp: 800, Loss: 0.3515649735927582
setp: 900, Loss: 0.37453150749206543
setp: 1000, Loss: 0.32656461000442505
setp: 1100, Loss: 0.36362069845199585
setp: 1200, Loss: 0.32328614592552185
setp: 1300, Loss: 0.36092644929885864
setp: 1400, Loss: 0.31829699873924255
setp: 1500, Loss: 0.3273051381111145
setp: 1600, Loss: 0.31640955805778503
setp: 1700, Loss: 0.3155476152896881
setp: 1800, Loss: 0.3190939724445343
setp: 1900, Loss: 0.3233674466609955
setp: 2000, Loss: 0.3162825405597687
setp: 2100, Loss: 0.3232201337814331
setp: 2200, Loss: 0.31643059849739075
setp: 2300, Loss: 0.3213929831981659
setp: 2400, Loss: 0.3159800171852112
setp: 2500, Loss: 0.3188147246837616
setp: 2600, Loss: 0.31689438223838806
setp: 2700, Loss: 0.33477309346199036
setp: 2800, Loss: 0.3345150053501129
setp: 2900, Loss: 0.3165925145149231
setp: 3000, Loss: 0.32300350069999695
setp: 3100, Loss: 0.3163153827190399
setp: 3200, Loss: 0.31908321380615234
setp: 3300, Loss: 0.3194965124130249
setp: 3400, Loss: 0.31680750846862793
setp: 3500, Loss: 0.31597742438316345
setp: 3600, Loss: 0.31671473383903503
setp: 3700, Loss: 0.3159620761871338
setp: 3800, Loss: 0.3171994984149933
setp: 3900, Loss: 0.3226209580898285
setp: 4000, Loss: 0.3161109983921051
setp: 4100, Loss: 0.31919825077056885
setp: 4200, Loss: 0.31594279408454895
setp: 4300, Loss: 0.31878846883773804
setp: 4400, Loss: 0.31801244616508484
setp: 4500, Loss: 0.31822633743286133
setp: 4600, Loss: 0.34423723816871643
setp: 4700, Loss: 0.3197828233242035
setp: 4800, Loss: 0.31648439168930054
setp: 4900, Loss: 0.3723318576812744
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9965277777777778
recall: 1.0
F_score: 0.9982608695652174
validating...
acc: 0.9210526315789473
precision: 0.9078947368421053
recall: 0.9324324324324325
F_score: 0.92
******fold 2******
[288, 320]
training...
setp: 0, Loss: 0.6926793456077576
setp: 100, Loss: 0.6863332390785217
setp: 200, Loss: 0.6872832775115967
setp: 300, Loss: 0.6504880785942078
setp: 400, Loss: 0.5704667568206787
setp: 500, Loss: 0.5896458625793457
setp: 600, Loss: 0.4698971211910248
setp: 700, Loss: 0.48223677277565
setp: 800, Loss: 0.5985378623008728
setp: 900, Loss: 0.40279898047447205
setp: 1000, Loss: 0.4231584370136261
setp: 1100, Loss: 0.42004328966140747
setp: 1200, Loss: 0.32966288924217224
setp: 1300, Loss: 0.4631577432155609
setp: 1400, Loss: 0.3422720730304718
setp: 1500, Loss: 0.33932092785835266
setp: 1600, Loss: 0.32384976744651794
setp: 1700, Loss: 0.32691138982772827
setp: 1800, Loss: 0.346158504486084
setp: 1900, Loss: 0.35349684953689575
setp: 2000, Loss: 0.3364860713481903
setp: 2100, Loss: 0.33851271867752075
setp: 2200, Loss: 0.3201478123664856
setp: 2300, Loss: 0.32078462839126587
setp: 2400, Loss: 0.31871119141578674
setp: 2500, Loss: 0.31664660573005676
setp: 2600, Loss: 0.40162307024002075
setp: 2700, Loss: 0.3238481283187866
setp: 2800, Loss: 0.3165305256843567
setp: 2900, Loss: 0.31813499331474304
setp: 3000, Loss: 0.3670712411403656
setp: 3100, Loss: 0.3187878429889679
setp: 3200, Loss: 0.3548540771007538
setp: 3300, Loss: 0.3234187662601471
setp: 3400, Loss: 0.31856295466423035
setp: 3500, Loss: 0.31880220770835876
setp: 3600, Loss: 0.31889960169792175
setp: 3700, Loss: 0.31656771898269653
setp: 3800, Loss: 0.3510357439517975
setp: 3900, Loss: 0.3192295730113983
setp: 4000, Loss: 0.3245587944984436
setp: 4100, Loss: 0.3226768374443054
setp: 4200, Loss: 0.319394052028656
setp: 4300, Loss: 0.3229735791683197
setp: 4400, Loss: 0.31881627440452576
setp: 4500, Loss: 0.4153895080089569
setp: 4600, Loss: 0.32232555747032166
setp: 4700, Loss: 0.3170572519302368
setp: 4800, Loss: 0.3286502957344055
setp: 4900, Loss: 0.3351963460445404
training successfully ended.
validating...
acc: 0.9671052631578947
precision: 0.935064935064935
recall: 1.0
F_score: 0.9664429530201343
validating...
acc: 0.9210526315789473
precision: 0.8588235294117647
recall: 1.0
F_score: 0.9240506329113923
******fold 3******
[288, 320]
training...
setp: 0, Loss: 0.6690264940261841
setp: 100, Loss: 0.6905447244644165
setp: 200, Loss: 0.6424789428710938
setp: 300, Loss: 0.6353940367698669
setp: 400, Loss: 0.6399821639060974
setp: 500, Loss: 0.6320575475692749
setp: 600, Loss: 0.5218825340270996
setp: 700, Loss: 0.5172678232192993
setp: 800, Loss: 0.5082931518554688
setp: 900, Loss: 0.46714460849761963
setp: 1000, Loss: 0.449558287858963
setp: 1100, Loss: 0.43467721343040466
setp: 1200, Loss: 0.43169334530830383
setp: 1300, Loss: 0.38112327456474304
setp: 1400, Loss: 0.3266635835170746
setp: 1500, Loss: 0.36338263750076294
setp: 1600, Loss: 0.3323333263397217
setp: 1700, Loss: 0.333245187997818
setp: 1800, Loss: 0.3194529414176941
setp: 1900, Loss: 0.3186931312084198
setp: 2000, Loss: 0.31867432594299316
setp: 2100, Loss: 0.3179723918437958
setp: 2200, Loss: 0.32508763670921326
setp: 2300, Loss: 0.37409746646881104
setp: 2400, Loss: 0.3204173147678375
setp: 2500, Loss: 0.338936984539032
setp: 2600, Loss: 0.3157367408275604
setp: 2700, Loss: 0.3419804871082306
setp: 2800, Loss: 0.3218749165534973
setp: 2900, Loss: 0.31755009293556213
setp: 3000, Loss: 0.3173053562641144
setp: 3100, Loss: 0.3194570243358612
setp: 3200, Loss: 0.3170579969882965
setp: 3300, Loss: 0.3168087303638458
setp: 3400, Loss: 0.34896764159202576
setp: 3500, Loss: 0.31729114055633545
setp: 3600, Loss: 0.31798598170280457
setp: 3700, Loss: 0.32166945934295654
setp: 3800, Loss: 0.3954244554042816
setp: 3900, Loss: 0.3381428122520447
setp: 4000, Loss: 0.3204194903373718
setp: 4100, Loss: 0.3185652196407318
setp: 4200, Loss: 0.31660884618759155
setp: 4300, Loss: 0.3162809908390045
setp: 4400, Loss: 0.31626269221305847
setp: 4500, Loss: 0.31614869832992554
setp: 4600, Loss: 0.31649303436279297
setp: 4700, Loss: 0.31735798716545105
setp: 4800, Loss: 0.31982165575027466
setp: 4900, Loss: 0.32992762327194214
training successfully ended.
validating...
acc: 0.868421052631579
precision: 0.782608695652174
recall: 1.0
F_score: 0.878048780487805
validating...
acc: 0.8092105263157895
precision: 0.7244897959183674
recall: 0.9726027397260274
F_score: 0.8304093567251463
******fold 4******
[292, 316]
training...
setp: 0, Loss: 0.6877641081809998
setp: 100, Loss: 0.6933350563049316
setp: 200, Loss: 0.5624924898147583
setp: 300, Loss: 0.6362876296043396
setp: 400, Loss: 0.5839197039604187
setp: 500, Loss: 0.4818803071975708
setp: 600, Loss: 0.47810813784599304
setp: 700, Loss: 0.4847947657108307
setp: 800, Loss: 0.4442504346370697
setp: 900, Loss: 0.4250415861606598
setp: 1000, Loss: 0.4413416385650635
setp: 1100, Loss: 0.44251808524131775
setp: 1200, Loss: 0.39535054564476013
setp: 1300, Loss: 0.39412182569503784
setp: 1400, Loss: 0.32340070605278015
setp: 1500, Loss: 0.4330313205718994
setp: 1600, Loss: 0.33911338448524475
setp: 1700, Loss: 0.35618263483047485
setp: 1800, Loss: 0.31655481457710266
setp: 1900, Loss: 0.3203309178352356
setp: 2000, Loss: 0.3183988630771637
setp: 2100, Loss: 0.32037070393562317
setp: 2200, Loss: 0.3511020839214325
setp: 2300, Loss: 0.33815082907676697
setp: 2400, Loss: 0.31607016921043396
setp: 2500, Loss: 0.3170110583305359
setp: 2600, Loss: 0.31796351075172424
setp: 2700, Loss: 0.3158514201641083
setp: 2800, Loss: 0.3176347613334656
setp: 2900, Loss: 0.3178781270980835
setp: 3000, Loss: 0.31854286789894104
setp: 3100, Loss: 0.3178514242172241
setp: 3200, Loss: 0.3173668682575226
setp: 3300, Loss: 0.31640738248825073
setp: 3400, Loss: 0.3662041425704956
setp: 3500, Loss: 0.32285264134407043
setp: 3600, Loss: 0.32475408911705017
setp: 3700, Loss: 0.3161602020263672
setp: 3800, Loss: 0.3166213035583496
setp: 3900, Loss: 0.3202245533466339
setp: 4000, Loss: 0.3169836103916168
setp: 4100, Loss: 0.31724128127098083
setp: 4200, Loss: 0.31549975275993347
setp: 4300, Loss: 0.3158971071243286
setp: 4400, Loss: 0.3172704577445984
setp: 4500, Loss: 0.3170948624610901
setp: 4600, Loss: 0.31653130054473877
setp: 4700, Loss: 0.3181057870388031
setp: 4800, Loss: 0.3171469271183014
setp: 4900, Loss: 0.3248208463191986
training successfully ended.
validating...
acc: 0.993421052631579
precision: 1.0
recall: 0.9863013698630136
F_score: 0.993103448275862
validating...
acc: 0.9605263157894737
precision: 0.9701492537313433
recall: 0.9420289855072463
F_score: 0.9558823529411764
model saved.
avg_acc: 0.9105263157894736, avg_f_score: 0.9141479387142185
-------------subject: 16-------------
==========valence==========
******fold 0******
[378, 230]
training...
setp: 0, Loss: 0.7061134576797485
setp: 100, Loss: 0.6521217823028564
setp: 200, Loss: 0.5306721329689026
setp: 300, Loss: 0.4986940026283264
setp: 400, Loss: 0.49664589762687683
setp: 500, Loss: 0.4417870342731476
setp: 600, Loss: 0.44701841473579407
setp: 700, Loss: 0.3811313509941101
setp: 800, Loss: 0.49926358461380005
setp: 900, Loss: 0.40048086643218994
setp: 1000, Loss: 0.406413197517395
setp: 1100, Loss: 0.4106561243534088
setp: 1200, Loss: 0.4184958338737488
setp: 1300, Loss: 0.3790450990200043
setp: 1400, Loss: 0.34770622849464417
setp: 1500, Loss: 0.4104953110218048
setp: 1600, Loss: 0.3167809844017029
setp: 1700, Loss: 0.380391925573349
setp: 1800, Loss: 0.41089561581611633
setp: 1900, Loss: 0.4103003144264221
setp: 2000, Loss: 0.37865525484085083
setp: 2100, Loss: 0.3788801431655884
setp: 2200, Loss: 0.42070168256759644
setp: 2300, Loss: 0.3795323073863983
setp: 2400, Loss: 0.3621777892112732
setp: 2500, Loss: 0.3424178957939148
setp: 2600, Loss: 0.32277002930641174
setp: 2700, Loss: 0.36936572194099426
setp: 2800, Loss: 0.32749348878860474
setp: 2900, Loss: 0.3784707188606262
setp: 3000, Loss: 0.3854374587535858
setp: 3100, Loss: 0.347443163394928
setp: 3200, Loss: 0.373675674200058
setp: 3300, Loss: 0.316315621137619
setp: 3400, Loss: 0.34865865111351013
setp: 3500, Loss: 0.32660239934921265
setp: 3600, Loss: 0.3518351912498474
setp: 3700, Loss: 0.3463709056377411
setp: 3800, Loss: 0.31912821531295776
setp: 3900, Loss: 0.348006933927536
setp: 4000, Loss: 0.34719300270080566
setp: 4100, Loss: 0.3484479784965515
setp: 4200, Loss: 0.3718460500240326
setp: 4300, Loss: 0.35039278864860535
setp: 4400, Loss: 0.31703850626945496
setp: 4500, Loss: 0.3153122663497925
setp: 4600, Loss: 0.3470277786254883
setp: 4700, Loss: 0.31607499718666077
setp: 4800, Loss: 0.3819994032382965
setp: 4900, Loss: 0.3800957202911377
training successfully ended.
validating...
acc: 0.9654605263157895
precision: 0.9612403100775194
recall: 0.9841269841269841
F_score: 0.9725490196078431
validating...
acc: 0.9276315789473685
precision: 0.9479166666666666
recall: 0.9381443298969072
F_score: 0.9430051813471502
******fold 1******
[376, 232]
training...
setp: 0, Loss: 0.6478053331375122
setp: 100, Loss: 0.6077619791030884
setp: 200, Loss: 0.5262773633003235
setp: 300, Loss: 0.4630637764930725
setp: 400, Loss: 0.5353500247001648
setp: 500, Loss: 0.4744201898574829
setp: 600, Loss: 0.4931502342224121
setp: 700, Loss: 0.4104333817958832
setp: 800, Loss: 0.5240188837051392
setp: 900, Loss: 0.4938926100730896
setp: 1000, Loss: 0.4170396327972412
setp: 1100, Loss: 0.4423326551914215
setp: 1200, Loss: 0.41606420278549194
setp: 1300, Loss: 0.44390955567359924
setp: 1400, Loss: 0.4470691680908203
setp: 1500, Loss: 0.46168991923332214
setp: 1600, Loss: 0.44074547290802
setp: 1700, Loss: 0.4093532860279083
setp: 1800, Loss: 0.44797420501708984
setp: 1900, Loss: 0.40628543496131897
setp: 2000, Loss: 0.3786761462688446
setp: 2100, Loss: 0.38252899050712585
setp: 2200, Loss: 0.3798018991947174
setp: 2300, Loss: 0.4136485457420349
setp: 2400, Loss: 0.44009503722190857
setp: 2500, Loss: 0.4131809175014496
setp: 2600, Loss: 0.3821452856063843
setp: 2700, Loss: 0.4107581079006195
setp: 2800, Loss: 0.4110035002231598
setp: 2900, Loss: 0.3909572660923004
setp: 3000, Loss: 0.42101147770881653
setp: 3100, Loss: 0.3783816397190094
setp: 3200, Loss: 0.37818825244903564
setp: 3300, Loss: 0.4102367162704468
setp: 3400, Loss: 0.40971726179122925
setp: 3500, Loss: 0.3811543881893158
setp: 3600, Loss: 0.4088203012943268
setp: 3700, Loss: 0.704778254032135
setp: 3800, Loss: 0.4594237506389618
setp: 3900, Loss: 0.3943023979663849
setp: 4000, Loss: 0.38811618089675903
setp: 4100, Loss: 0.41017577052116394
setp: 4200, Loss: 0.396884560585022
setp: 4300, Loss: 0.4404970407485962
setp: 4400, Loss: 0.41374480724334717
setp: 4500, Loss: 0.37818533182144165
setp: 4600, Loss: 0.4095168113708496
setp: 4700, Loss: 0.4276828169822693
setp: 4800, Loss: 0.3804073929786682
setp: 4900, Loss: 0.41257941722869873
training successfully ended.
validating...
acc: 0.9210526315789473
precision: 0.8867924528301887
recall: 1.0
F_score: 0.9400000000000001
validating...
acc: 0.9276315789473685
precision: 0.9
recall: 1.0
F_score: 0.9473684210526316
******fold 2******
[387, 221]
training...
setp: 0, Loss: 0.6773125529289246
setp: 100, Loss: 0.7085423469543457
setp: 200, Loss: 0.5603983402252197
setp: 300, Loss: 0.6486406326293945
setp: 400, Loss: 0.5354290008544922
setp: 500, Loss: 0.46091532707214355
setp: 600, Loss: 0.6031853556632996
setp: 700, Loss: 0.37082475423812866
setp: 800, Loss: 0.440230131149292
setp: 900, Loss: 0.37993869185447693
setp: 1000, Loss: 0.356932669878006
setp: 1100, Loss: 0.35582879185676575
setp: 1200, Loss: 0.35054078698158264
setp: 1300, Loss: 0.39221081137657166
setp: 1400, Loss: 0.377916157245636
setp: 1500, Loss: 0.35610517859458923
setp: 1600, Loss: 0.38126900792121887
setp: 1700, Loss: 0.31702303886413574
setp: 1800, Loss: 0.35059505701065063
setp: 1900, Loss: 0.3602556586265564
setp: 2000, Loss: 0.378889262676239
setp: 2100, Loss: 0.3474723696708679
setp: 2200, Loss: 0.3180643916130066
setp: 2300, Loss: 0.37979409098625183
setp: 2400, Loss: 0.37784817814826965
setp: 2500, Loss: 0.3579491972923279
setp: 2600, Loss: 0.34671732783317566
setp: 2700, Loss: 0.31946471333503723
setp: 2800, Loss: 0.34751245379447937
setp: 2900, Loss: 0.3488409221172333
setp: 3000, Loss: 0.3475596010684967
setp: 3100, Loss: 0.3464859426021576
setp: 3200, Loss: 0.34674376249313354
setp: 3300, Loss: 0.3477464020252228
setp: 3400, Loss: 0.38265517354011536
setp: 3500, Loss: 0.3508990406990051
setp: 3600, Loss: 0.31755542755126953
setp: 3700, Loss: 0.3464193344116211
setp: 3800, Loss: 0.3579446077346802
setp: 3900, Loss: 0.34974440932273865
setp: 4000, Loss: 0.3633628785610199
setp: 4100, Loss: 0.42707616090774536
setp: 4200, Loss: 0.3531516194343567
setp: 4300, Loss: 0.34770429134368896
setp: 4400, Loss: 0.42079296708106995
setp: 4500, Loss: 0.3479064106941223
setp: 4600, Loss: 0.316211462020874
setp: 4700, Loss: 0.3472077548503876
setp: 4800, Loss: 0.34694942831993103
setp: 4900, Loss: 0.3465765118598938
training successfully ended.
validating...
acc: 0.9736842105263158
precision: 0.9602977667493796
recall: 1.0
F_score: 0.9797468354430379
validating...
acc: 0.8947368421052632
precision: 0.8913043478260869
recall: 0.9318181818181818
F_score: 0.9111111111111111
******fold 3******
[376, 232]
training...
setp: 0, Loss: 0.6759642958641052
setp: 100, Loss: 0.6252878904342651
setp: 200, Loss: 0.5593437552452087
setp: 300, Loss: 0.5461915731430054
setp: 400, Loss: 0.47346386313438416
setp: 500, Loss: 0.4105457067489624
setp: 600, Loss: 0.38045787811279297
setp: 700, Loss: 0.426388680934906
setp: 800, Loss: 0.3983633518218994
setp: 900, Loss: 0.37785398960113525
setp: 1000, Loss: 0.4409199357032776
setp: 1100, Loss: 0.43915122747421265
setp: 1200, Loss: 0.49353697896003723
setp: 1300, Loss: 0.4411700665950775
setp: 1400, Loss: 0.44051212072372437
setp: 1500, Loss: 0.3778100609779358
setp: 1600, Loss: 0.41299155354499817
setp: 1700, Loss: 0.43992024660110474
setp: 1800, Loss: 0.3780258297920227
setp: 1900, Loss: 0.47255995869636536
setp: 2000, Loss: 0.3789393901824951
setp: 2100, Loss: 0.37821415066719055
setp: 2200, Loss: 0.441078782081604
setp: 2300, Loss: 0.37982529401779175
setp: 2400, Loss: 0.3783245384693146
setp: 2500, Loss: 0.6692430377006531
setp: 2600, Loss: 0.42787444591522217
setp: 2700, Loss: 0.3858613967895508
setp: 2800, Loss: 0.3478621542453766
setp: 2900, Loss: 0.4410710036754608
setp: 3000, Loss: 0.43863505125045776
setp: 3100, Loss: 0.44162875413894653
setp: 3200, Loss: 0.43990030884742737
setp: 3300, Loss: 0.46295398473739624
setp: 3400, Loss: 0.39460495114326477
setp: 3500, Loss: 0.41186782717704773
setp: 3600, Loss: 0.4334155023097992
setp: 3700, Loss: 0.3777249753475189
setp: 3800, Loss: 0.41843652725219727
setp: 3900, Loss: 0.34835875034332275
setp: 4000, Loss: 0.3761139512062073
setp: 4100, Loss: 0.3798680305480957
setp: 4200, Loss: 0.35700279474258423
setp: 4300, Loss: 0.3254992663860321
setp: 4400, Loss: 0.3244357705116272
setp: 4500, Loss: 0.3849799633026123
setp: 4600, Loss: 0.35346630215644836
setp: 4700, Loss: 0.3466127812862396
setp: 4800, Loss: 0.3799881339073181
setp: 4900, Loss: 0.34631142020225525
training successfully ended.
validating...
acc: 0.9555921052631579
precision: 0.9373433583959899
recall: 0.9946808510638298
F_score: 0.9651612903225806
validating...
acc: 0.9013157894736842
precision: 0.8962264150943396
recall: 0.9595959595959596
F_score: 0.926829268292683
******fold 4******
[383, 225]
training...
setp: 0, Loss: 0.6962448954582214
setp: 100, Loss: 0.5721781253814697
setp: 200, Loss: 0.5338460206985474
setp: 300, Loss: 0.5230630040168762
setp: 400, Loss: 0.5455870032310486
setp: 500, Loss: 0.4870196282863617
setp: 600, Loss: 0.49406003952026367
setp: 700, Loss: 0.422147661447525
setp: 800, Loss: 0.3829568922519684
setp: 900, Loss: 0.40328454971313477
setp: 1000, Loss: 0.39929279685020447
setp: 1100, Loss: 0.35726118087768555
setp: 1200, Loss: 0.42489156126976013
setp: 1300, Loss: 0.39636003971099854
setp: 1400, Loss: 0.35248565673828125
setp: 1500, Loss: 0.36702653765678406
setp: 1600, Loss: 0.340986430644989
setp: 1700, Loss: 0.3496801555156708
setp: 1800, Loss: 0.36611005663871765
setp: 1900, Loss: 0.37977397441864014
setp: 2000, Loss: 0.34713873267173767
setp: 2100, Loss: 0.31797337532043457
setp: 2200, Loss: 0.3469077944755554
setp: 2300, Loss: 0.34929075837135315
setp: 2400, Loss: 0.32663241028785706
setp: 2500, Loss: 0.3671584725379944
setp: 2600, Loss: 0.34757453203201294
setp: 2700, Loss: 0.31809139251708984
setp: 2800, Loss: 0.3478628098964691
setp: 2900, Loss: 0.3548036217689514
setp: 3000, Loss: 0.3155694901943207
setp: 3100, Loss: 0.3465745449066162
setp: 3200, Loss: 0.388130784034729
setp: 3300, Loss: 0.3178597390651703
setp: 3400, Loss: 0.35103708505630493
setp: 3500, Loss: 0.32127127051353455
setp: 3600, Loss: 0.3521912395954132
setp: 3700, Loss: 0.3482065200805664
setp: 3800, Loss: 0.34936219453811646
setp: 3900, Loss: 0.35822156071662903
setp: 4000, Loss: 0.3300725221633911
setp: 4100, Loss: 0.34689202904701233
setp: 4200, Loss: 0.35239991545677185
setp: 4300, Loss: 0.3220573663711548
setp: 4400, Loss: 0.3469924032688141
setp: 4500, Loss: 0.35363873839378357
setp: 4600, Loss: 0.3185570240020752
setp: 4700, Loss: 0.36694756150245667
setp: 4800, Loss: 0.34760937094688416
setp: 4900, Loss: 0.3154287040233612
training successfully ended.
validating...
acc: 0.9786184210526315
precision: 0.9671717171717171
recall: 1.0
F_score: 0.9833119383825417
validating...
acc: 0.9473684210526315
precision: 0.92
recall: 1.0
F_score: 0.9583333333333334
model saved.
avg_acc: 0.9197368421052632, avg_f_score: 0.9373294630273818
==========arousal==========
******fold 0******
[307, 301]
training...
setp: 0, Loss: 0.7625753283500671
setp: 100, Loss: 0.5230675339698792
setp: 200, Loss: 0.5805065631866455
setp: 300, Loss: 0.5901395082473755
setp: 400, Loss: 0.5591909289360046
setp: 500, Loss: 0.5285266041755676
setp: 600, Loss: 0.5527654886245728
setp: 700, Loss: 0.5598576068878174
setp: 800, Loss: 0.4951570928096771
setp: 900, Loss: 0.46132758259773254
setp: 1000, Loss: 0.5987516045570374
setp: 1100, Loss: 0.4653027653694153
setp: 1200, Loss: 0.4633481800556183
setp: 1300, Loss: 0.4388241767883301
setp: 1400, Loss: 0.42054831981658936
setp: 1500, Loss: 0.3816784620285034
setp: 1600, Loss: 0.4951780140399933
setp: 1700, Loss: 0.3481217622756958
setp: 1800, Loss: 0.4188203513622284
setp: 1900, Loss: 0.3404160737991333
setp: 2000, Loss: 0.4971238970756531
setp: 2100, Loss: 0.3795391917228699
setp: 2200, Loss: 0.3581436574459076
setp: 2300, Loss: 0.5214561820030212
setp: 2400, Loss: 0.3793637454509735
setp: 2500, Loss: 0.4714413285255432
setp: 2600, Loss: 0.4724816083908081
setp: 2700, Loss: 0.4342522621154785
setp: 2800, Loss: 0.38004451990127563
setp: 2900, Loss: 0.4219822585582733
setp: 3000, Loss: 0.3497721254825592
setp: 3100, Loss: 0.47017496824264526
setp: 3200, Loss: 0.38096708059310913
setp: 3300, Loss: 0.4109043478965759
setp: 3400, Loss: 0.6038597226142883
setp: 3500, Loss: 0.43906083703041077
setp: 3600, Loss: 0.35496440529823303
setp: 3700, Loss: 0.378827840089798
setp: 3800, Loss: 0.499186247587204
setp: 3900, Loss: 0.4385235607624054
setp: 4000, Loss: 0.40838390588760376
setp: 4100, Loss: 0.3509445786476135
setp: 4200, Loss: 0.37839218974113464
setp: 4300, Loss: 0.37767305970191956
setp: 4400, Loss: 0.3472002148628235
setp: 4500, Loss: 0.44084224104881287
setp: 4600, Loss: 0.40968260169029236
setp: 4700, Loss: 0.3716222941875458
setp: 4800, Loss: 0.42029932141304016
setp: 4900, Loss: 0.3482952415943146
training successfully ended.
validating...
acc: 0.944078947368421
precision: 0.9074626865671642
recall: 0.990228013029316
F_score: 0.9470404984423676
validating...
acc: 0.881578947368421
precision: 0.8235294117647058
recall: 0.958904109589041
F_score: 0.8860759493670886
******fold 1******
[304, 304]
training...
setp: 0, Loss: 0.6971427202224731
setp: 100, Loss: 0.5128339529037476
setp: 200, Loss: 0.564118504524231
setp: 300, Loss: 0.4568162262439728
setp: 400, Loss: 0.48988446593284607
setp: 500, Loss: 0.40547025203704834
setp: 600, Loss: 0.44474413990974426
setp: 700, Loss: 0.43537789583206177
setp: 800, Loss: 0.4193176329135895
setp: 900, Loss: 0.4756281077861786
setp: 1000, Loss: 0.37770283222198486
setp: 1100, Loss: 0.3439454734325409
setp: 1200, Loss: 0.3835737109184265
setp: 1300, Loss: 0.515529215335846
setp: 1400, Loss: 0.37512141466140747
setp: 1500, Loss: 0.4083932340145111
setp: 1600, Loss: 0.4373811185359955
setp: 1700, Loss: 0.35183393955230713
setp: 1800, Loss: 0.431425541639328
setp: 1900, Loss: 0.43663087487220764
setp: 2000, Loss: 0.34751588106155396
setp: 2100, Loss: 0.36922487616539
setp: 2200, Loss: 0.4114857017993927
setp: 2300, Loss: 0.4095524847507477
setp: 2400, Loss: 0.3570781350135803
setp: 2500, Loss: 0.3477291166782379
setp: 2600, Loss: 0.40094879269599915
setp: 2700, Loss: 0.4341891407966614
setp: 2800, Loss: 0.4087848365306854
setp: 2900, Loss: 0.3476994037628174
setp: 3000, Loss: 0.3670755624771118
setp: 3100, Loss: 0.39172235131263733
setp: 3200, Loss: 0.38811787962913513
setp: 3300, Loss: 0.37207627296447754
setp: 3400, Loss: 0.34873315691947937
setp: 3500, Loss: 0.37909454107284546
setp: 3600, Loss: 0.34251976013183594
setp: 3700, Loss: 0.41089296340942383
setp: 3800, Loss: 0.35060131549835205
setp: 3900, Loss: 0.36637043952941895
setp: 4000, Loss: 0.34972772002220154
setp: 4100, Loss: 0.3471919298171997
setp: 4200, Loss: 0.3783979117870331
setp: 4300, Loss: 0.34630417823791504
setp: 4400, Loss: 0.3468340039253235
setp: 4500, Loss: 0.31625717878341675
setp: 4600, Loss: 0.3156547546386719
setp: 4700, Loss: 0.34858763217926025
setp: 4800, Loss: 0.3471057116985321
setp: 4900, Loss: 0.3158201575279236
training successfully ended.
validating...
acc: 0.9720394736842105
precision: 0.9526813880126183
recall: 0.993421052631579
F_score: 0.9726247987117552
validating...
acc: 0.868421052631579
precision: 0.8589743589743589
recall: 0.881578947368421
F_score: 0.8701298701298701
******fold 2******
[310, 298]
training...
setp: 0, Loss: 0.703449010848999
setp: 100, Loss: 0.6955403685569763
setp: 200, Loss: 0.5842472910881042
setp: 300, Loss: 0.5729146599769592
setp: 400, Loss: 0.5632370114326477
setp: 500, Loss: 0.5320680141448975
setp: 600, Loss: 0.5412506461143494
setp: 700, Loss: 0.5818877816200256
setp: 800, Loss: 0.492256224155426
setp: 900, Loss: 0.525019109249115
setp: 1000, Loss: 0.4800564646720886
setp: 1100, Loss: 0.6003531217575073
setp: 1200, Loss: 0.47672393918037415
setp: 1300, Loss: 0.4519113302230835
setp: 1400, Loss: 0.4612760543823242
setp: 1500, Loss: 0.4446223974227905
setp: 1600, Loss: 0.4674767553806305
setp: 1700, Loss: 0.43597090244293213
setp: 1800, Loss: 0.4702466130256653
setp: 1900, Loss: 0.46784624457359314
setp: 2000, Loss: 0.4496087431907654
setp: 2100, Loss: 0.5517690777778625
setp: 2200, Loss: 0.4505064785480499
setp: 2300, Loss: 0.5288247466087341
setp: 2400, Loss: 0.4096713364124298
setp: 2500, Loss: 0.4286939799785614
setp: 2600, Loss: 0.39885231852531433
setp: 2700, Loss: 0.37777289748191833
setp: 2800, Loss: 0.43891608715057373
setp: 2900, Loss: 0.4092765152454376
setp: 3000, Loss: 0.4107637107372284
setp: 3100, Loss: 0.41749104857444763
setp: 3200, Loss: 0.37440159916877747
setp: 3300, Loss: 0.3560873568058014
setp: 3400, Loss: 0.3623826503753662
setp: 3500, Loss: 0.380750834941864
setp: 3600, Loss: 0.4099667966365814
setp: 3700, Loss: 0.4839133024215698
setp: 3800, Loss: 0.4841423034667969
setp: 3900, Loss: 0.40552419424057007
setp: 4000, Loss: 0.3779451847076416
setp: 4100, Loss: 0.40364429354667664
setp: 4200, Loss: 0.4107295572757721
setp: 4300, Loss: 0.3636738061904907
setp: 4400, Loss: 0.40152421593666077
setp: 4500, Loss: 0.38340163230895996
setp: 4600, Loss: 0.36571311950683594
setp: 4700, Loss: 0.4045524001121521
setp: 4800, Loss: 0.3876931071281433
setp: 4900, Loss: 0.34693601727485657
training successfully ended.
validating...
acc: 0.9523026315789473
precision: 0.9323076923076923
recall: 0.9774193548387097
F_score: 0.9543307086614172
validating...
acc: 0.9210526315789473
precision: 0.953125
recall: 0.8714285714285714
F_score: 0.9104477611940298
******fold 3******
[298, 310]
training...
setp: 0, Loss: 0.736279308795929
setp: 100, Loss: 0.6352654695510864
setp: 200, Loss: 0.5585514307022095
setp: 300, Loss: 0.46978917717933655
setp: 400, Loss: 0.4531288146972656
setp: 500, Loss: 0.4812689423561096
setp: 600, Loss: 0.44966357946395874
setp: 700, Loss: 0.5183553695678711
setp: 800, Loss: 0.4455913305282593
setp: 900, Loss: 0.3867100477218628
setp: 1000, Loss: 0.413165807723999
setp: 1100, Loss: 0.47590306401252747
setp: 1200, Loss: 0.44400259852409363
setp: 1300, Loss: 0.38500499725341797
setp: 1400, Loss: 0.6145715713500977
setp: 1500, Loss: 0.3484248220920563
setp: 1600, Loss: 0.42673438787460327
setp: 1700, Loss: 0.4750460982322693
setp: 1800, Loss: 0.39968520402908325
setp: 1900, Loss: 0.47523677349090576
setp: 2000, Loss: 0.34747114777565
setp: 2100, Loss: 0.41090458631515503
setp: 2200, Loss: 0.3806951344013214
setp: 2300, Loss: 0.40804070234298706
setp: 2400, Loss: 0.4322429895401001
setp: 2500, Loss: 0.44132596254348755
setp: 2600, Loss: 0.4127558171749115
setp: 2700, Loss: 0.4030039310455322
setp: 2800, Loss: 0.34718620777130127
setp: 2900, Loss: 0.3547399640083313
setp: 3000, Loss: 0.34891971945762634
setp: 3100, Loss: 0.3459550440311432
setp: 3200, Loss: 0.3780686557292938
setp: 3300, Loss: 0.3791404962539673
setp: 3400, Loss: 0.3156181573867798
setp: 3500, Loss: 0.4104918837547302
setp: 3600, Loss: 0.44876548647880554
setp: 3700, Loss: 0.4245980381965637
setp: 3800, Loss: 0.44058817625045776
setp: 3900, Loss: 0.3526992201805115
setp: 4000, Loss: 0.32038456201553345
setp: 4100, Loss: 0.3553425073623657
setp: 4200, Loss: 0.3337468206882477
setp: 4300, Loss: 0.38154423236846924
setp: 4400, Loss: 0.3777916431427002
setp: 4500, Loss: 0.37867170572280884
setp: 4600, Loss: 0.3468210697174072
setp: 4700, Loss: 0.3164208233356476
setp: 4800, Loss: 0.34688258171081543
setp: 4900, Loss: 0.34644344449043274
training successfully ended.
validating...
acc: 0.944078947368421
precision: 0.9
recall: 0.9966442953020134
F_score: 0.9458598726114651
validating...
acc: 0.8947368421052632
precision: 0.84375
recall: 0.9878048780487805
F_score: 0.9101123595505618
******fold 4******
[301, 307]
training...
setp: 0, Loss: 0.7162777781486511
setp: 100, Loss: 0.5698112845420837
setp: 200, Loss: 0.5691310167312622
setp: 300, Loss: 0.41404902935028076
setp: 400, Loss: 0.5432024598121643
setp: 500, Loss: 0.417174369096756
setp: 600, Loss: 0.5323030948638916
setp: 700, Loss: 0.5256975293159485
setp: 800, Loss: 0.5717320442199707
setp: 900, Loss: 0.4373466372489929
setp: 1000, Loss: 0.4109298884868622
setp: 1100, Loss: 0.4133382737636566
setp: 1200, Loss: 0.4414907693862915
setp: 1300, Loss: 0.4089374542236328
setp: 1400, Loss: 0.5333484411239624
setp: 1500, Loss: 0.40747812390327454
setp: 1600, Loss: 0.3796572685241699
setp: 1700, Loss: 0.4690083861351013
setp: 1800, Loss: 0.40621325373649597
setp: 1900, Loss: 0.385416716337204
setp: 2000, Loss: 0.5316664576530457
setp: 2100, Loss: 0.40632253885269165
setp: 2200, Loss: 0.35978731513023376
setp: 2300, Loss: 0.4718318581581116
setp: 2400, Loss: 0.4125222861766815
setp: 2500, Loss: 0.4364302158355713
setp: 2600, Loss: 0.44055697321891785
setp: 2700, Loss: 0.39703962206840515
setp: 2800, Loss: 0.379576712846756
setp: 2900, Loss: 0.40958842635154724
setp: 3000, Loss: 0.37824738025665283
setp: 3100, Loss: 0.5012264847755432
setp: 3200, Loss: 0.37826892733573914
setp: 3300, Loss: 0.4485566020011902
setp: 3400, Loss: 0.4062246084213257
setp: 3500, Loss: 0.37871241569519043
setp: 3600, Loss: 0.389244943857193
setp: 3700, Loss: 0.36155593395233154
setp: 3800, Loss: 0.44209444522857666
setp: 3900, Loss: 0.4094441831111908
setp: 4000, Loss: 0.3781781792640686
setp: 4100, Loss: 0.3468763828277588
setp: 4200, Loss: 0.40939095616340637
setp: 4300, Loss: 0.34741249680519104
setp: 4400, Loss: 0.37822389602661133
setp: 4500, Loss: 0.4097219407558441
setp: 4600, Loss: 0.40087029337882996
setp: 4700, Loss: 0.4356887638568878
setp: 4800, Loss: 0.40872064232826233
setp: 4900, Loss: 0.41114145517349243
training successfully ended.
validating...
acc: 0.9292763157894737
precision: 0.8909090909090909
recall: 0.9767441860465116
F_score: 0.9318541996830427
validating...
acc: 0.881578947368421
precision: 0.8765432098765432
recall: 0.8987341772151899
F_score: 0.8875
model saved.
avg_acc: 0.8894736842105264, avg_f_score: 0.8928531880483102
-------------subject: 17-------------
==========valence==========
******fold 0******
[275, 333]
training...
setp: 0, Loss: 0.6929850578308105
setp: 100, Loss: 0.6910774111747742
setp: 200, Loss: 0.6852871775627136
setp: 300, Loss: 0.6938491463661194
setp: 400, Loss: 0.594494104385376
setp: 500, Loss: 0.44154196977615356
setp: 600, Loss: 0.40501824021339417
setp: 700, Loss: 0.3911304771900177
setp: 800, Loss: 0.330586314201355
setp: 900, Loss: 0.32331201434135437
setp: 1000, Loss: 0.3435976505279541
setp: 1100, Loss: 0.32268938422203064
setp: 1200, Loss: 0.3277817368507385
setp: 1300, Loss: 0.3252125382423401
setp: 1400, Loss: 0.32044118642807007
setp: 1500, Loss: 0.3193436563014984
setp: 1600, Loss: 0.3496520221233368
setp: 1700, Loss: 0.32051682472229004
setp: 1800, Loss: 0.32323652505874634
setp: 1900, Loss: 0.3247573971748352
setp: 2000, Loss: 0.3813278079032898
setp: 2100, Loss: 0.3216233551502228
setp: 2200, Loss: 0.3179835379123688
setp: 2300, Loss: 0.3191361427307129
setp: 2400, Loss: 0.31835612654685974
setp: 2500, Loss: 0.3175954520702362
setp: 2600, Loss: 0.3191857933998108
setp: 2700, Loss: 0.3194817900657654
setp: 2800, Loss: 0.3188492953777313
setp: 2900, Loss: 0.31892967224121094
setp: 3000, Loss: 0.3183056116104126
setp: 3100, Loss: 0.31840425729751587
setp: 3200, Loss: 0.31928950548171997
setp: 3300, Loss: 0.3179822564125061
setp: 3400, Loss: 0.3190680146217346
setp: 3500, Loss: 0.3330397605895996
setp: 3600, Loss: 0.32461750507354736
setp: 3700, Loss: 0.3178645968437195
setp: 3800, Loss: 0.3177296221256256
setp: 3900, Loss: 0.31775423884391785
setp: 4000, Loss: 0.3191993534564972
setp: 4100, Loss: 0.3186553418636322
setp: 4200, Loss: 0.3195604383945465
setp: 4300, Loss: 0.31904253363609314
setp: 4400, Loss: 0.31798580288887024
setp: 4500, Loss: 0.31866928935050964
setp: 4600, Loss: 0.3190246820449829
setp: 4700, Loss: 0.3730575740337372
setp: 4800, Loss: 0.32244637608528137
setp: 4900, Loss: 0.3185120224952698
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.875
precision: 0.8333333333333334
recall: 0.8955223880597015
F_score: 0.8633093525179857
******fold 1******
[270, 338]
training...
setp: 0, Loss: 0.7188937664031982
setp: 100, Loss: 0.6802696585655212
setp: 200, Loss: 0.6748762130737305
setp: 300, Loss: 0.6747426390647888
setp: 400, Loss: 0.6932528018951416
setp: 500, Loss: 0.6410140991210938
setp: 600, Loss: 0.6160635948181152
setp: 700, Loss: 0.6176692247390747
setp: 800, Loss: 0.6623959541320801
setp: 900, Loss: 0.4681946337223053
setp: 1000, Loss: 0.3944855332374573
setp: 1100, Loss: 0.42833229899406433
setp: 1200, Loss: 0.35065749287605286
setp: 1300, Loss: 0.4814417362213135
setp: 1400, Loss: 0.4599509537220001
setp: 1500, Loss: 0.4585050642490387
setp: 1600, Loss: 0.43634456396102905
setp: 1700, Loss: 0.3580056130886078
setp: 1800, Loss: 0.3513742685317993
setp: 1900, Loss: 0.3507336378097534
setp: 2000, Loss: 0.3530005216598511
setp: 2100, Loss: 0.3843824565410614
setp: 2200, Loss: 0.3193821310997009
setp: 2300, Loss: 0.31762051582336426
setp: 2400, Loss: 0.3184562027454376
setp: 2500, Loss: 0.3184567093849182
setp: 2600, Loss: 0.34785231947898865
setp: 2700, Loss: 0.3614616096019745
setp: 2800, Loss: 0.31779325008392334
setp: 2900, Loss: 0.3224511742591858
setp: 3000, Loss: 0.348112016916275
setp: 3100, Loss: 0.3167325556278229
setp: 3200, Loss: 0.33801016211509705
setp: 3300, Loss: 0.3460942506790161
setp: 3400, Loss: 0.34845104813575745
setp: 3500, Loss: 0.3500024974346161
setp: 3600, Loss: 0.33453816175460815
setp: 3700, Loss: 0.3475601375102997
setp: 3800, Loss: 0.34822845458984375
setp: 3900, Loss: 0.3241221308708191
setp: 4000, Loss: 0.41168636083602905
setp: 4100, Loss: 0.3162210285663605
setp: 4200, Loss: 0.3170943260192871
setp: 4300, Loss: 0.3165930509567261
setp: 4400, Loss: 0.31829458475112915
setp: 4500, Loss: 0.31724822521209717
setp: 4600, Loss: 0.3171024024486542
setp: 4700, Loss: 0.31819218397140503
setp: 4800, Loss: 0.3178249001502991
setp: 4900, Loss: 0.34857863187789917
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.9925925925925926
F_score: 0.996282527881041
validating...
acc: 0.8881578947368421
precision: 0.9365079365079365
recall: 0.8194444444444444
F_score: 0.874074074074074
******fold 2******
[283, 325]
training...
setp: 0, Loss: 0.6949691772460938
setp: 100, Loss: 0.6846916675567627
setp: 200, Loss: 0.6908670663833618
setp: 300, Loss: 0.6774972081184387
setp: 400, Loss: 0.6719454526901245
setp: 500, Loss: 0.6262186169624329
setp: 600, Loss: 0.5785859823226929
setp: 700, Loss: 0.6156857013702393
setp: 800, Loss: 0.45824456214904785
setp: 900, Loss: 0.3719639778137207
setp: 1000, Loss: 0.36599719524383545
setp: 1100, Loss: 0.3835945725440979
setp: 1200, Loss: 0.3629215359687805
setp: 1300, Loss: 0.3247125744819641
setp: 1400, Loss: 0.38426774740219116
setp: 1500, Loss: 0.38681283593177795
setp: 1600, Loss: 0.36204150319099426
setp: 1700, Loss: 0.3513859212398529
setp: 1800, Loss: 0.3499983549118042
setp: 1900, Loss: 0.3484047055244446
setp: 2000, Loss: 0.325675904750824
setp: 2100, Loss: 0.3483358919620514
setp: 2200, Loss: 0.35481831431388855
setp: 2300, Loss: 0.34898510575294495
setp: 2400, Loss: 0.3290484845638275
setp: 2500, Loss: 0.34836143255233765
setp: 2600, Loss: 0.34896135330200195
setp: 2700, Loss: 0.3548245131969452
setp: 2800, Loss: 0.3609043061733246
setp: 2900, Loss: 0.31995445489883423
setp: 3000, Loss: 0.3556674122810364
setp: 3100, Loss: 0.3513643741607666
setp: 3200, Loss: 0.31879401206970215
setp: 3300, Loss: 0.3176426887512207
setp: 3400, Loss: 0.3476425111293793
setp: 3500, Loss: 0.31692931056022644
setp: 3600, Loss: 0.3188629746437073
setp: 3700, Loss: 0.31847643852233887
setp: 3800, Loss: 0.3196974992752075
setp: 3900, Loss: 0.31696322560310364
setp: 4000, Loss: 0.3179692029953003
setp: 4100, Loss: 0.31826749444007874
setp: 4200, Loss: 0.3200446367263794
setp: 4300, Loss: 0.31910568475723267
setp: 4400, Loss: 0.3168822228908539
setp: 4500, Loss: 0.3179045021533966
setp: 4600, Loss: 0.31717154383659363
setp: 4700, Loss: 0.3215005397796631
setp: 4800, Loss: 0.3186022639274597
setp: 4900, Loss: 0.3239589333534241
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9964788732394366
recall: 1.0
F_score: 0.998236331569665
validating...
acc: 0.9013157894736842
precision: 0.8142857142857143
recall: 0.9661016949152542
F_score: 0.8837209302325583
******fold 3******
[275, 333]
training...
setp: 0, Loss: 0.6934271454811096
setp: 100, Loss: 0.6634873151779175
setp: 200, Loss: 0.6578323245048523
setp: 300, Loss: 0.5862836837768555
setp: 400, Loss: 0.5050346851348877
setp: 500, Loss: 0.5160135626792908
setp: 600, Loss: 0.4826696217060089
setp: 700, Loss: 0.43942221999168396
setp: 800, Loss: 0.47929099202156067
setp: 900, Loss: 0.4648505449295044
setp: 1000, Loss: 0.3966802656650543
setp: 1100, Loss: 0.34443098306655884
setp: 1200, Loss: 0.3353133499622345
setp: 1300, Loss: 0.5355392098426819
setp: 1400, Loss: 0.32723692059516907
setp: 1500, Loss: 0.327612042427063
setp: 1600, Loss: 0.3867686986923218
setp: 1700, Loss: 0.3263252377510071
setp: 1800, Loss: 0.33825498819351196
setp: 1900, Loss: 0.35342884063720703
setp: 2000, Loss: 0.3221628665924072
setp: 2100, Loss: 0.32077863812446594
setp: 2200, Loss: 0.3188186287879944
setp: 2300, Loss: 0.3189038634300232
setp: 2400, Loss: 0.3346936106681824
setp: 2500, Loss: 0.3496752679347992
setp: 2600, Loss: 0.32392778992652893
setp: 2700, Loss: 0.31837746500968933
setp: 2800, Loss: 0.3206420838832855
setp: 2900, Loss: 0.31644460558891296
setp: 3000, Loss: 0.32026785612106323
setp: 3100, Loss: 0.3249388337135315
setp: 3200, Loss: 0.34991031885147095
setp: 3300, Loss: 0.3174125850200653
setp: 3400, Loss: 0.31853073835372925
setp: 3500, Loss: 0.31992101669311523
setp: 3600, Loss: 0.3182900846004486
setp: 3700, Loss: 0.31730416417121887
setp: 3800, Loss: 0.3190069794654846
setp: 3900, Loss: 0.318215936422348
setp: 4000, Loss: 0.3180799186229706
setp: 4100, Loss: 0.31796029210090637
setp: 4200, Loss: 0.3170865774154663
setp: 4300, Loss: 0.3175289034843445
setp: 4400, Loss: 0.31703731417655945
setp: 4500, Loss: 0.3494270443916321
setp: 4600, Loss: 0.3361585736274719
setp: 4700, Loss: 0.3291175067424774
setp: 4800, Loss: 0.3253825902938843
setp: 4900, Loss: 0.3216225206851959
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9963768115942029
recall: 1.0
F_score: 0.9981851179673321
validating...
acc: 0.881578947368421
precision: 0.9152542372881356
recall: 0.8059701492537313
F_score: 0.8571428571428572
******fold 4******
[265, 343]
training...
setp: 0, Loss: 0.6902605891227722
setp: 100, Loss: 0.6851845383644104
setp: 200, Loss: 0.6851800680160522
setp: 300, Loss: 0.6934639811515808
setp: 400, Loss: 0.6858150362968445
setp: 500, Loss: 0.6747509241104126
setp: 600, Loss: 0.5966469049453735
setp: 700, Loss: 0.645979106426239
setp: 800, Loss: 0.6471008062362671
setp: 900, Loss: 0.492765337228775
setp: 1000, Loss: 0.4334302842617035
setp: 1100, Loss: 0.43821609020233154
setp: 1200, Loss: 0.401667982339859
setp: 1300, Loss: 0.36444181203842163
setp: 1400, Loss: 0.39896160364151
setp: 1500, Loss: 0.38024210929870605
setp: 1600, Loss: 0.3920280337333679
setp: 1700, Loss: 0.3306502401828766
setp: 1800, Loss: 0.3534245491027832
setp: 1900, Loss: 0.3516480326652527
setp: 2000, Loss: 0.3248067796230316
setp: 2100, Loss: 0.35557302832603455
setp: 2200, Loss: 0.318785160779953
setp: 2300, Loss: 0.3257084786891937
setp: 2400, Loss: 0.32201576232910156
setp: 2500, Loss: 0.318916916847229
setp: 2600, Loss: 0.39529353380203247
setp: 2700, Loss: 0.320734441280365
setp: 2800, Loss: 0.3616907596588135
setp: 2900, Loss: 0.3176639974117279
setp: 3000, Loss: 0.31748905777931213
setp: 3100, Loss: 0.31947505474090576
setp: 3200, Loss: 0.317850261926651
setp: 3300, Loss: 0.3161727786064148
setp: 3400, Loss: 0.3480042815208435
setp: 3500, Loss: 0.31761297583580017
setp: 3600, Loss: 0.3193376362323761
setp: 3700, Loss: 0.31771528720855713
setp: 3800, Loss: 0.3178916871547699
setp: 3900, Loss: 0.319120854139328
setp: 4000, Loss: 0.3198060393333435
setp: 4100, Loss: 0.3183012008666992
setp: 4200, Loss: 0.31832045316696167
setp: 4300, Loss: 0.3192535936832428
setp: 4400, Loss: 0.36140018701553345
setp: 4500, Loss: 0.33638277649879456
setp: 4600, Loss: 0.33184441924095154
setp: 4700, Loss: 0.3236077129840851
setp: 4800, Loss: 0.3169993460178375
setp: 4900, Loss: 0.3167683184146881
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9962264150943396
F_score: 0.998109640831758
validating...
acc: 0.9276315789473685
precision: 0.9230769230769231
recall: 0.935064935064935
F_score: 0.9290322580645162
model saved.
avg_acc: 0.894736842105263, avg_f_score: 0.8814558944063983
==========arousal==========
******fold 0******
[218, 390]
training...
setp: 0, Loss: 0.7350069284439087
setp: 100, Loss: 0.6428833603858948
setp: 200, Loss: 0.646206796169281
setp: 300, Loss: 0.6336617469787598
setp: 400, Loss: 0.583420991897583
setp: 500, Loss: 0.5827853083610535
setp: 600, Loss: 0.5440390706062317
setp: 700, Loss: 0.4685716927051544
setp: 800, Loss: 0.4462635815143585
setp: 900, Loss: 0.525750458240509
setp: 1000, Loss: 0.38985711336135864
setp: 1100, Loss: 0.3545098602771759
setp: 1200, Loss: 0.36227747797966003
setp: 1300, Loss: 0.3283695578575134
setp: 1400, Loss: 0.32586827874183655
setp: 1500, Loss: 0.32185328006744385
setp: 1600, Loss: 0.3203578591346741
setp: 1700, Loss: 0.3218022286891937
setp: 1800, Loss: 0.33082717657089233
setp: 1900, Loss: 0.34135544300079346
setp: 2000, Loss: 0.3222316801548004
setp: 2100, Loss: 0.3217214345932007
setp: 2200, Loss: 0.3204514980316162
setp: 2300, Loss: 0.32104507088661194
setp: 2400, Loss: 0.31926003098487854
setp: 2500, Loss: 0.3187708854675293
setp: 2600, Loss: 0.31996598839759827
setp: 2700, Loss: 0.3189751207828522
setp: 2800, Loss: 0.32229241728782654
setp: 2900, Loss: 0.3199017345905304
setp: 3000, Loss: 0.3204807937145233
setp: 3100, Loss: 0.3220536708831787
setp: 3200, Loss: 0.31960177421569824
setp: 3300, Loss: 0.42291614413261414
setp: 3400, Loss: 0.33483678102493286
setp: 3500, Loss: 0.3319140374660492
setp: 3600, Loss: 0.32502496242523193
setp: 3700, Loss: 0.3514951765537262
setp: 3800, Loss: 0.3302990794181824
setp: 3900, Loss: 0.3344779312610626
setp: 4000, Loss: 0.33019769191741943
setp: 4100, Loss: 0.35495731234550476
setp: 4200, Loss: 0.3238862156867981
setp: 4300, Loss: 0.32203900814056396
setp: 4400, Loss: 0.32149675488471985
setp: 4500, Loss: 0.32204681634902954
setp: 4600, Loss: 0.32182928919792175
setp: 4700, Loss: 0.32370519638061523
setp: 4800, Loss: 0.32310739159584045
setp: 4900, Loss: 0.3233616352081299
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.881578947368421
precision: 0.9016393442622951
recall: 0.8208955223880597
F_score: 0.8593750000000001
******fold 1******
[233, 375]
training...
setp: 0, Loss: 0.7871622443199158
setp: 100, Loss: 0.6886768341064453
setp: 200, Loss: 0.6605828404426575
setp: 300, Loss: 0.6793515682220459
setp: 400, Loss: 0.628606379032135
setp: 500, Loss: 0.5777115821838379
setp: 600, Loss: 0.5786226987838745
setp: 700, Loss: 0.6110588312149048
setp: 800, Loss: 0.5599282383918762
setp: 900, Loss: 0.5003023743629456
setp: 1000, Loss: 0.5289797186851501
setp: 1100, Loss: 0.44420623779296875
setp: 1200, Loss: 0.45551756024360657
setp: 1300, Loss: 0.41320446133613586
setp: 1400, Loss: 0.4161752462387085
setp: 1500, Loss: 0.33354049921035767
setp: 1600, Loss: 0.4327443540096283
setp: 1700, Loss: 0.4604869782924652
setp: 1800, Loss: 0.375590443611145
setp: 1900, Loss: 0.3966982364654541
setp: 2000, Loss: 0.4875202775001526
setp: 2100, Loss: 0.33560842275619507
setp: 2200, Loss: 0.3489023447036743
setp: 2300, Loss: 0.31716087460517883
setp: 2400, Loss: 0.3502613306045532
setp: 2500, Loss: 0.37953317165374756
setp: 2600, Loss: 0.35082975029945374
setp: 2700, Loss: 0.36557164788246155
setp: 2800, Loss: 0.3233356773853302
setp: 2900, Loss: 0.32169339060783386
setp: 3000, Loss: 0.33917030692100525
setp: 3100, Loss: 0.3489716053009033
setp: 3200, Loss: 0.3219471871852875
setp: 3300, Loss: 0.3494678735733032
setp: 3400, Loss: 0.337682843208313
setp: 3500, Loss: 0.3490946292877197
setp: 3600, Loss: 0.3293539583683014
setp: 3700, Loss: 0.34285858273506165
setp: 3800, Loss: 0.3188398480415344
setp: 3900, Loss: 0.324115127325058
setp: 4000, Loss: 0.34426945447921753
setp: 4100, Loss: 0.3502373695373535
setp: 4200, Loss: 0.323246568441391
setp: 4300, Loss: 0.32929226756095886
setp: 4400, Loss: 0.3855014443397522
setp: 4500, Loss: 0.3215906322002411
setp: 4600, Loss: 0.3172932267189026
setp: 4700, Loss: 0.34916162490844727
setp: 4800, Loss: 0.318224161863327
setp: 4900, Loss: 0.32116588950157166
training successfully ended.
validating...
acc: 0.9819078947368421
precision: 0.9955357142857143
recall: 0.9570815450643777
F_score: 0.975929978118162
validating...
acc: 0.8552631578947368
precision: 0.8947368421052632
recall: 0.6538461538461539
F_score: 0.7555555555555555
******fold 2******
[237, 371]
training...
setp: 0, Loss: 0.693709671497345
setp: 100, Loss: 0.6395871639251709
setp: 200, Loss: 0.6277395486831665
setp: 300, Loss: 0.44528988003730774
setp: 400, Loss: 0.45272958278656006
setp: 500, Loss: 0.35421279072761536
setp: 600, Loss: 0.3432014584541321
setp: 700, Loss: 0.3277814984321594
setp: 800, Loss: 0.3208484947681427
setp: 900, Loss: 0.3368419408798218
setp: 1000, Loss: 0.39516666531562805
setp: 1100, Loss: 0.3241558074951172
setp: 1200, Loss: 0.3276960253715515
setp: 1300, Loss: 0.3214136064052582
setp: 1400, Loss: 0.3259966969490051
setp: 1500, Loss: 0.31855490803718567
setp: 1600, Loss: 0.31897813081741333
setp: 1700, Loss: 0.3193097412586212
setp: 1800, Loss: 0.3207572102546692
setp: 1900, Loss: 0.31915655732154846
setp: 2000, Loss: 0.31870192289352417
setp: 2100, Loss: 0.3498118221759796
setp: 2200, Loss: 0.32001152634620667
setp: 2300, Loss: 0.3202654719352722
setp: 2400, Loss: 0.31964796781539917
setp: 2500, Loss: 0.32007813453674316
setp: 2600, Loss: 0.3997197151184082
setp: 2700, Loss: 0.42831650376319885
setp: 2800, Loss: 0.34950825572013855
setp: 2900, Loss: 0.32731854915618896
setp: 3000, Loss: 0.3230292797088623
setp: 3100, Loss: 0.32399946451187134
setp: 3200, Loss: 0.32190486788749695
setp: 3300, Loss: 0.325721800327301
setp: 3400, Loss: 0.32298243045806885
setp: 3500, Loss: 0.3255235552787781
setp: 3600, Loss: 0.3225928843021393
setp: 3700, Loss: 0.3266954720020294
setp: 3800, Loss: 0.322812020778656
setp: 3900, Loss: 0.32523706555366516
setp: 4000, Loss: 0.3230445384979248
setp: 4100, Loss: 0.32175251841545105
setp: 4200, Loss: 0.322887122631073
setp: 4300, Loss: 0.3222464323043823
setp: 4400, Loss: 0.32209035754203796
setp: 4500, Loss: 0.32367175817489624
setp: 4600, Loss: 0.3208964467048645
setp: 4700, Loss: 0.32201218605041504
setp: 4800, Loss: 0.3232724070549011
setp: 4900, Loss: 0.322365403175354
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9078947368421053
precision: 0.8269230769230769
recall: 0.8958333333333334
F_score: 0.86
******fold 3******
[219, 389]
training...
setp: 0, Loss: 0.6476923823356628
setp: 100, Loss: 0.6589800119400024
setp: 200, Loss: 0.5757490992546082
setp: 300, Loss: 0.6106724739074707
setp: 400, Loss: 0.5400487780570984
setp: 500, Loss: 0.49168816208839417
setp: 600, Loss: 0.432217001914978
setp: 700, Loss: 0.4070574939250946
setp: 800, Loss: 0.4519091248512268
setp: 900, Loss: 0.47602513432502747
setp: 1000, Loss: 0.4179961383342743
setp: 1100, Loss: 0.357554167509079
setp: 1200, Loss: 0.4120559096336365
setp: 1300, Loss: 0.36136606335639954
setp: 1400, Loss: 0.37079063057899475
setp: 1500, Loss: 0.32463979721069336
setp: 1600, Loss: 0.32238441705703735
setp: 1700, Loss: 0.36174464225769043
setp: 1800, Loss: 0.32314109802246094
setp: 1900, Loss: 0.3509984612464905
setp: 2000, Loss: 0.3510375916957855
setp: 2100, Loss: 0.317991703748703
setp: 2200, Loss: 0.320915549993515
setp: 2300, Loss: 0.41483160853385925
setp: 2400, Loss: 0.39173364639282227
setp: 2500, Loss: 0.33042004704475403
setp: 2600, Loss: 0.3174703121185303
setp: 2700, Loss: 0.31959930062294006
setp: 2800, Loss: 0.3495458662509918
setp: 2900, Loss: 0.3193812668323517
setp: 3000, Loss: 0.3221254348754883
setp: 3100, Loss: 0.4084639251232147
setp: 3200, Loss: 0.3489930033683777
setp: 3300, Loss: 0.31722837686538696
setp: 3400, Loss: 0.31748533248901367
setp: 3500, Loss: 0.31953394412994385
setp: 3600, Loss: 0.3187534213066101
setp: 3700, Loss: 0.32650184631347656
setp: 3800, Loss: 0.35070446133613586
setp: 3900, Loss: 0.34706270694732666
setp: 4000, Loss: 0.3170517385005951
setp: 4100, Loss: 0.3176144063472748
setp: 4200, Loss: 0.3199177086353302
setp: 4300, Loss: 0.3193683624267578
setp: 4400, Loss: 0.3188987970352173
setp: 4500, Loss: 0.3172704577445984
setp: 4600, Loss: 0.3179672062397003
setp: 4700, Loss: 0.3183096647262573
setp: 4800, Loss: 0.32005640864372253
setp: 4900, Loss: 0.31807973980903625
training successfully ended.
validating...
acc: 0.993421052631579
precision: 1.0
recall: 0.9817351598173516
F_score: 0.9907834101382489
validating...
acc: 0.8157894736842105
precision: 0.88
recall: 0.6666666666666666
F_score: 0.7586206896551725
******fold 4******
[233, 375]
training...
setp: 0, Loss: 0.6940993070602417
setp: 100, Loss: 0.6952765583992004
setp: 200, Loss: 0.6467275619506836
setp: 300, Loss: 0.6143571138381958
setp: 400, Loss: 0.587390661239624
setp: 500, Loss: 0.4978840947151184
setp: 600, Loss: 0.5514433979988098
setp: 700, Loss: 0.4955962300300598
setp: 800, Loss: 0.5467151999473572
setp: 900, Loss: 0.4451906085014343
setp: 1000, Loss: 0.5322060585021973
setp: 1100, Loss: 0.5451597571372986
setp: 1200, Loss: 0.43164417147636414
setp: 1300, Loss: 0.4001445472240448
setp: 1400, Loss: 0.49467504024505615
setp: 1500, Loss: 0.41466274857521057
setp: 1600, Loss: 0.45178520679473877
setp: 1700, Loss: 0.38565734028816223
setp: 1800, Loss: 0.35591721534729004
setp: 1900, Loss: 0.37146463990211487
setp: 2000, Loss: 0.42120087146759033
setp: 2100, Loss: 0.38745617866516113
setp: 2200, Loss: 0.3513614535331726
setp: 2300, Loss: 0.3642871379852295
setp: 2400, Loss: 0.3646402060985565
setp: 2500, Loss: 0.38213130831718445
setp: 2600, Loss: 0.3247867524623871
setp: 2700, Loss: 0.3186210095882416
setp: 2800, Loss: 0.38134974241256714
setp: 2900, Loss: 0.39085066318511963
setp: 3000, Loss: 0.3206425607204437
setp: 3100, Loss: 0.35516083240509033
setp: 3200, Loss: 0.39175066351890564
setp: 3300, Loss: 0.36860886216163635
setp: 3400, Loss: 0.355837345123291
setp: 3500, Loss: 0.38442811369895935
setp: 3600, Loss: 0.3277914822101593
setp: 3700, Loss: 0.33374646306037903
setp: 3800, Loss: 0.32008835673332214
setp: 3900, Loss: 0.3603896498680115
setp: 4000, Loss: 0.3212642967700958
setp: 4100, Loss: 0.319965124130249
setp: 4200, Loss: 0.31952401995658875
setp: 4300, Loss: 0.36375337839126587
setp: 4400, Loss: 0.3780921399593353
setp: 4500, Loss: 0.3173106610774994
setp: 4600, Loss: 0.31650853157043457
setp: 4700, Loss: 0.3187887966632843
setp: 4800, Loss: 0.35012584924697876
setp: 4900, Loss: 0.31849026679992676
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9956896551724138
recall: 0.9914163090128756
F_score: 0.9935483870967742
validating...
acc: 0.8289473684210527
precision: 0.76
recall: 0.7307692307692307
F_score: 0.7450980392156863
model saved.
avg_acc: 0.8578947368421053, avg_f_score: 0.7957298568852829
-------------subject: 18-------------
==========valence==========
******fold 0******
[215, 393]
training...
setp: 0, Loss: 0.7075895667076111
setp: 100, Loss: 0.6617102026939392
setp: 200, Loss: 0.6581050753593445
setp: 300, Loss: 0.5703043937683105
setp: 400, Loss: 0.4815548062324524
setp: 500, Loss: 0.41344696283340454
setp: 600, Loss: 0.35817617177963257
setp: 700, Loss: 0.4190558195114136
setp: 800, Loss: 0.35688021779060364
setp: 900, Loss: 0.3847726285457611
setp: 1000, Loss: 0.3525887429714203
setp: 1100, Loss: 0.35032010078430176
setp: 1200, Loss: 0.3520866930484772
setp: 1300, Loss: 0.3195742964744568
setp: 1400, Loss: 0.378675639629364
setp: 1500, Loss: 0.33502697944641113
setp: 1600, Loss: 0.319338321685791
setp: 1700, Loss: 0.31891512870788574
setp: 1800, Loss: 0.3340017795562744
setp: 1900, Loss: 0.3800343871116638
setp: 2000, Loss: 0.3213079273700714
setp: 2100, Loss: 0.34759968519210815
setp: 2200, Loss: 0.32205450534820557
setp: 2300, Loss: 0.3483959436416626
setp: 2400, Loss: 0.35157519578933716
setp: 2500, Loss: 0.3154739439487457
setp: 2600, Loss: 0.3163496255874634
setp: 2700, Loss: 0.3480510711669922
setp: 2800, Loss: 0.3181001842021942
setp: 2900, Loss: 0.3175897002220154
setp: 3000, Loss: 0.31782495975494385
setp: 3100, Loss: 0.32497167587280273
setp: 3200, Loss: 0.32918328046798706
setp: 3300, Loss: 0.3159330487251282
setp: 3400, Loss: 0.31650233268737793
setp: 3500, Loss: 0.3166451156139374
setp: 3600, Loss: 0.3176112771034241
setp: 3700, Loss: 0.3167341351509094
setp: 3800, Loss: 0.31930461525917053
setp: 3900, Loss: 0.3213638961315155
setp: 4000, Loss: 0.31776151061058044
setp: 4100, Loss: 0.3208461403846741
setp: 4200, Loss: 0.32637614011764526
setp: 4300, Loss: 0.34854820370674133
setp: 4400, Loss: 0.31605714559555054
setp: 4500, Loss: 0.31615394353866577
setp: 4600, Loss: 0.3190782070159912
setp: 4700, Loss: 0.3179033398628235
setp: 4800, Loss: 0.31645604968070984
setp: 4900, Loss: 0.3174055814743042
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.9183673469387755
recall: 0.8823529411764706
F_score: 0.9
******fold 1******
[211, 397]
training...
setp: 0, Loss: 0.6960190534591675
setp: 100, Loss: 0.620155930519104
setp: 200, Loss: 0.5185732841491699
setp: 300, Loss: 0.49014899134635925
setp: 400, Loss: 0.4492844343185425
setp: 500, Loss: 0.5754122734069824
setp: 600, Loss: 0.4784509539604187
setp: 700, Loss: 0.511650025844574
setp: 800, Loss: 0.4823049306869507
setp: 900, Loss: 0.5847417712211609
setp: 1000, Loss: 0.42672964930534363
setp: 1100, Loss: 0.47543713450431824
setp: 1200, Loss: 0.5376543998718262
setp: 1300, Loss: 0.48178794980049133
setp: 1400, Loss: 0.42592936754226685
setp: 1500, Loss: 0.4588182866573334
setp: 1600, Loss: 0.5445611476898193
setp: 1700, Loss: 0.5372228026390076
setp: 1800, Loss: 0.4520045518875122
setp: 1900, Loss: 0.5022683143615723
setp: 2000, Loss: 0.48300889134407043
setp: 2100, Loss: 0.44360747933387756
setp: 2200, Loss: 0.4509061276912689
setp: 2300, Loss: 0.4463095963001251
setp: 2400, Loss: 0.5144328474998474
setp: 2500, Loss: 0.47243502736091614
setp: 2600, Loss: 0.47676360607147217
setp: 2700, Loss: 0.4503767490386963
setp: 2800, Loss: 0.4631398022174835
setp: 2900, Loss: 0.4098573327064514
setp: 3000, Loss: 0.47196653485298157
setp: 3100, Loss: 0.49732860922813416
setp: 3200, Loss: 0.41603198647499084
setp: 3300, Loss: 0.38605549931526184
setp: 3400, Loss: 0.41132304072380066
setp: 3500, Loss: 0.4960372745990753
setp: 3600, Loss: 0.4925917387008667
setp: 3700, Loss: 0.44810009002685547
setp: 3800, Loss: 0.3914966285228729
setp: 3900, Loss: 0.44258829951286316
setp: 4000, Loss: 0.3854275047779083
setp: 4100, Loss: 0.37927958369255066
setp: 4200, Loss: 0.4280690550804138
setp: 4300, Loss: 0.38447585701942444
setp: 4400, Loss: 0.41045311093330383
setp: 4500, Loss: 0.4180696904659271
setp: 4600, Loss: 0.41141563653945923
setp: 4700, Loss: 0.4192279875278473
setp: 4800, Loss: 0.3529520332813263
setp: 4900, Loss: 0.420741468667984
training successfully ended.
validating...
acc: 0.8865131578947368
precision: 0.8901098901098901
recall: 0.7677725118483413
F_score: 0.8244274809160305
validating...
acc: 0.8421052631578947
precision: 0.8163265306122449
recall: 0.7272727272727273
F_score: 0.7692307692307693
******fold 2******
[219, 389]
training...
setp: 0, Loss: 0.7055671215057373
setp: 100, Loss: 0.6273514628410339
setp: 200, Loss: 0.5933949947357178
setp: 300, Loss: 0.4336450695991516
setp: 400, Loss: 0.38720688223838806
setp: 500, Loss: 0.3441026508808136
setp: 600, Loss: 0.35928457975387573
setp: 700, Loss: 0.3586614727973938
setp: 800, Loss: 0.3564320206642151
setp: 900, Loss: 0.3251214921474457
setp: 1000, Loss: 0.32646867632865906
setp: 1100, Loss: 0.3224262595176697
setp: 1200, Loss: 0.32704970240592957
setp: 1300, Loss: 0.32001689076423645
setp: 1400, Loss: 0.3235560953617096
setp: 1500, Loss: 0.3287394642829895
setp: 1600, Loss: 0.3212120532989502
setp: 1700, Loss: 0.3214415907859802
setp: 1800, Loss: 0.3214375674724579
setp: 1900, Loss: 0.32381200790405273
setp: 2000, Loss: 0.3212261497974396
setp: 2100, Loss: 0.3193749487400055
setp: 2200, Loss: 0.31828543543815613
setp: 2300, Loss: 0.31898513436317444
setp: 2400, Loss: 0.31996631622314453
setp: 2500, Loss: 0.32030001282691956
setp: 2600, Loss: 0.3251133859157562
setp: 2700, Loss: 0.33461257815361023
setp: 2800, Loss: 0.3176165521144867
setp: 2900, Loss: 0.31794050335884094
setp: 3000, Loss: 0.3184952139854431
setp: 3100, Loss: 0.31871911883354187
setp: 3200, Loss: 0.31867924332618713
setp: 3300, Loss: 0.3186786472797394
setp: 3400, Loss: 0.3194999396800995
setp: 3500, Loss: 0.4200199842453003
setp: 3600, Loss: 0.32684507966041565
setp: 3700, Loss: 0.3187314569950104
setp: 3800, Loss: 0.3194379210472107
setp: 3900, Loss: 0.31890612840652466
setp: 4000, Loss: 0.3193548917770386
setp: 4100, Loss: 0.31879764795303345
setp: 4200, Loss: 0.3190862238407135
setp: 4300, Loss: 0.31969502568244934
setp: 4400, Loss: 0.32009267807006836
setp: 4500, Loss: 0.38814637064933777
setp: 4600, Loss: 0.32224923372268677
setp: 4700, Loss: 0.3187108039855957
setp: 4800, Loss: 0.31793081760406494
setp: 4900, Loss: 0.31860730051994324
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9539473684210527
precision: 0.9545454545454546
recall: 0.8936170212765957
F_score: 0.9230769230769231
******fold 3******
[215, 393]
training...
setp: 0, Loss: 0.6872730255126953
setp: 100, Loss: 0.605992317199707
setp: 200, Loss: 0.49829962849617004
setp: 300, Loss: 0.512547492980957
setp: 400, Loss: 0.45979559421539307
setp: 500, Loss: 0.5415688753128052
setp: 600, Loss: 0.4925946593284607
setp: 700, Loss: 0.4436798095703125
setp: 800, Loss: 0.5224609971046448
setp: 900, Loss: 0.47765111923217773
setp: 1000, Loss: 0.4856452941894531
setp: 1100, Loss: 0.47660091519355774
setp: 1200, Loss: 0.5020539164543152
setp: 1300, Loss: 0.4958575963973999
setp: 1400, Loss: 0.44128742814064026
setp: 1500, Loss: 0.5038214325904846
setp: 1600, Loss: 0.4144991934299469
setp: 1700, Loss: 0.4117100238800049
setp: 1800, Loss: 0.4116780757904053
setp: 1900, Loss: 0.3817742168903351
setp: 2000, Loss: 0.49627602100372314
setp: 2100, Loss: 0.41121378540992737
setp: 2200, Loss: 0.42344561219215393
setp: 2300, Loss: 0.4473895728588104
setp: 2400, Loss: 0.4117301106452942
setp: 2500, Loss: 0.44093650579452515
setp: 2600, Loss: 0.4408853352069855
setp: 2700, Loss: 0.4405192732810974
setp: 2800, Loss: 0.41068035364151
setp: 2900, Loss: 0.4406777024269104
setp: 3000, Loss: 0.44129329919815063
setp: 3100, Loss: 0.48017174005508423
setp: 3200, Loss: 0.4673158824443817
setp: 3300, Loss: 0.442414790391922
setp: 3400, Loss: 0.4710668921470642
setp: 3500, Loss: 0.4204980731010437
setp: 3600, Loss: 0.4095322787761688
setp: 3700, Loss: 0.4280151128768921
setp: 3800, Loss: 0.3866010904312134
setp: 3900, Loss: 0.4132068455219269
setp: 4000, Loss: 0.40954717993736267
setp: 4100, Loss: 0.4104483723640442
setp: 4200, Loss: 0.447715699672699
setp: 4300, Loss: 0.41288331151008606
setp: 4400, Loss: 0.4412771761417389
setp: 4500, Loss: 0.41386210918426514
setp: 4600, Loss: 0.4182247817516327
setp: 4700, Loss: 0.40926697850227356
setp: 4800, Loss: 0.4422927796840668
setp: 4900, Loss: 0.44133031368255615
training successfully ended.
validating...
acc: 0.8963815789473685
precision: 0.9935064935064936
recall: 0.7116279069767442
F_score: 0.8292682926829268
validating...
acc: 0.8552631578947368
precision: 0.8536585365853658
recall: 0.6862745098039216
F_score: 0.7608695652173912
******fold 4******
[204, 404]
training...
setp: 0, Loss: 0.6386414766311646
setp: 100, Loss: 0.5951271653175354
setp: 200, Loss: 0.6041857004165649
setp: 300, Loss: 0.51473069190979
setp: 400, Loss: 0.4195614457130432
setp: 500, Loss: 0.5385450124740601
setp: 600, Loss: 0.4365578889846802
setp: 700, Loss: 0.44579628109931946
setp: 800, Loss: 0.5083621740341187
setp: 900, Loss: 0.48597192764282227
setp: 1000, Loss: 0.43937239050865173
setp: 1100, Loss: 0.4120837152004242
setp: 1200, Loss: 0.525434136390686
setp: 1300, Loss: 0.5494585633277893
setp: 1400, Loss: 0.4352264404296875
setp: 1500, Loss: 0.544134795665741
setp: 1600, Loss: 0.48067352175712585
setp: 1700, Loss: 0.4798838496208191
setp: 1800, Loss: 0.4070623517036438
setp: 1900, Loss: 0.44413134455680847
setp: 2000, Loss: 0.4421458840370178
setp: 2100, Loss: 0.3791734278202057
setp: 2200, Loss: 0.41545936465263367
setp: 2300, Loss: 0.34618186950683594
setp: 2400, Loss: 0.415975421667099
setp: 2500, Loss: 0.3524562120437622
setp: 2600, Loss: 0.4017300605773926
setp: 2700, Loss: 0.4987490177154541
setp: 2800, Loss: 0.44067737460136414
setp: 2900, Loss: 0.39056751132011414
setp: 3000, Loss: 0.3828502893447876
setp: 3100, Loss: 0.42675355076789856
setp: 3200, Loss: 0.3872040808200836
setp: 3300, Loss: 0.37922146916389465
setp: 3400, Loss: 0.411196231842041
setp: 3500, Loss: 0.44311708211898804
setp: 3600, Loss: 0.4112439453601837
setp: 3700, Loss: 0.38178151845932007
setp: 3800, Loss: 0.4415925145149231
setp: 3900, Loss: 0.38114190101623535
setp: 4000, Loss: 0.37845492362976074
setp: 4100, Loss: 0.41172900795936584
setp: 4200, Loss: 0.3467085063457489
setp: 4300, Loss: 0.4294012784957886
setp: 4400, Loss: 0.3676024079322815
setp: 4500, Loss: 0.379624605178833
setp: 4600, Loss: 0.4426550567150116
setp: 4700, Loss: 0.4104464650154114
setp: 4800, Loss: 0.3809064030647278
setp: 4900, Loss: 0.37830761075019836
training successfully ended.
validating...
acc: 0.9210526315789473
precision: 0.9936708860759493
recall: 0.7696078431372549
F_score: 0.8674033149171272
validating...
acc: 0.8026315789473685
precision: 0.8076923076923077
recall: 0.6774193548387096
F_score: 0.7368421052631579
model saved.
avg_acc: 0.8776315789473683, avg_f_score: 0.8180038725576482
==========arousal==========
******fold 0******
[234, 374]
training...
setp: 0, Loss: 0.6929183006286621
setp: 100, Loss: 0.6601044535636902
setp: 200, Loss: 0.5901959538459778
setp: 300, Loss: 0.552510678768158
setp: 400, Loss: 0.4497518539428711
setp: 500, Loss: 0.4289706349372864
setp: 600, Loss: 0.34378474950790405
setp: 700, Loss: 0.33608078956604004
setp: 800, Loss: 0.3246077299118042
setp: 900, Loss: 0.3305984139442444
setp: 1000, Loss: 0.3231959044933319
setp: 1100, Loss: 0.3237711489200592
setp: 1200, Loss: 0.31926846504211426
setp: 1300, Loss: 0.3201318085193634
setp: 1400, Loss: 0.3537946343421936
setp: 1500, Loss: 0.3240610659122467
setp: 1600, Loss: 0.333573579788208
setp: 1700, Loss: 0.32512998580932617
setp: 1800, Loss: 0.3208298087120056
setp: 1900, Loss: 0.31928297877311707
setp: 2000, Loss: 0.31891950964927673
setp: 2100, Loss: 0.31923189759254456
setp: 2200, Loss: 0.3188469409942627
setp: 2300, Loss: 0.319611132144928
setp: 2400, Loss: 0.5515934824943542
setp: 2500, Loss: 0.3572577238082886
setp: 2600, Loss: 0.3563080132007599
setp: 2700, Loss: 0.3373444080352783
setp: 2800, Loss: 0.3320627212524414
setp: 2900, Loss: 0.32662931084632874
setp: 3000, Loss: 0.34389957785606384
setp: 3100, Loss: 0.32625824213027954
setp: 3200, Loss: 0.32778945565223694
setp: 3300, Loss: 0.33998215198516846
setp: 3400, Loss: 0.3261567950248718
setp: 3500, Loss: 0.324741005897522
setp: 3600, Loss: 0.32379627227783203
setp: 3700, Loss: 0.3217346966266632
setp: 3800, Loss: 0.3228151500225067
setp: 3900, Loss: 0.32222771644592285
setp: 4000, Loss: 0.327583909034729
setp: 4100, Loss: 0.3352973461151123
setp: 4200, Loss: 0.3202325403690338
setp: 4300, Loss: 0.31901994347572327
setp: 4400, Loss: 0.3188208341598511
setp: 4500, Loss: 0.31920143961906433
setp: 4600, Loss: 0.31987684965133667
setp: 4700, Loss: 0.3249996304512024
setp: 4800, Loss: 0.32005083560943604
setp: 4900, Loss: 0.32023826241493225
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9605263157894737
precision: 0.9245283018867925
recall: 0.9607843137254902
F_score: 0.9423076923076923
******fold 1******
[219, 389]
training...
setp: 0, Loss: 0.6570547223091125
setp: 100, Loss: 0.6266642808914185
setp: 200, Loss: 0.5554258823394775
setp: 300, Loss: 0.595075786113739
setp: 400, Loss: 0.5497446060180664
setp: 500, Loss: 0.603386640548706
setp: 600, Loss: 0.47941628098487854
setp: 700, Loss: 0.45922955870628357
setp: 800, Loss: 0.48054760694503784
setp: 900, Loss: 0.4389575123786926
setp: 1000, Loss: 0.3938879072666168
setp: 1100, Loss: 0.40784507989883423
setp: 1200, Loss: 0.38430073857307434
setp: 1300, Loss: 0.3960566222667694
setp: 1400, Loss: 0.38393786549568176
setp: 1500, Loss: 0.32632750272750854
setp: 1600, Loss: 0.34056785702705383
setp: 1700, Loss: 0.3398095369338989
setp: 1800, Loss: 0.32560545206069946
setp: 1900, Loss: 0.33427074551582336
setp: 2000, Loss: 0.3244527280330658
setp: 2100, Loss: 0.32285410165786743
setp: 2200, Loss: 0.32039985060691833
setp: 2300, Loss: 0.32762184739112854
setp: 2400, Loss: 0.35243135690689087
setp: 2500, Loss: 0.31827014684677124
setp: 2600, Loss: 0.3207361698150635
setp: 2700, Loss: 0.31930720806121826
setp: 2800, Loss: 0.3302268087863922
setp: 2900, Loss: 0.31902265548706055
setp: 3000, Loss: 0.3190573751926422
setp: 3100, Loss: 0.31729352474212646
setp: 3200, Loss: 0.3214957118034363
setp: 3300, Loss: 0.33597221970558167
setp: 3400, Loss: 0.31747058033943176
setp: 3500, Loss: 0.317599356174469
setp: 3600, Loss: 0.3187255859375
setp: 3700, Loss: 0.3181547224521637
setp: 3800, Loss: 0.3199932277202606
setp: 3900, Loss: 0.3225034773349762
setp: 4000, Loss: 0.3180665969848633
setp: 4100, Loss: 0.31741607189178467
setp: 4200, Loss: 0.3173002600669861
setp: 4300, Loss: 0.31973350048065186
setp: 4400, Loss: 0.3370268642902374
setp: 4500, Loss: 0.3325665593147278
setp: 4600, Loss: 0.31642991304397583
setp: 4700, Loss: 0.31746017932891846
setp: 4800, Loss: 0.31854191422462463
setp: 4900, Loss: 0.31875985860824585
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9671052631578947
precision: 0.9841269841269841
recall: 0.9393939393939394
F_score: 0.9612403100775193
******fold 2******
[230, 378]
training...
setp: 0, Loss: 0.7240707874298096
setp: 100, Loss: 0.5980916023254395
setp: 200, Loss: 0.5918996334075928
setp: 300, Loss: 0.5663818120956421
setp: 400, Loss: 0.5529321432113647
setp: 500, Loss: 0.59296053647995
setp: 600, Loss: 0.47572556138038635
setp: 700, Loss: 0.36344417929649353
setp: 800, Loss: 0.3526631295681
setp: 900, Loss: 0.3610084056854248
setp: 1000, Loss: 0.3452779948711395
setp: 1100, Loss: 0.36341556906700134
setp: 1200, Loss: 0.32069358229637146
setp: 1300, Loss: 0.3199949860572815
setp: 1400, Loss: 0.32125401496887207
setp: 1500, Loss: 0.41852131485939026
setp: 1600, Loss: 0.3503304123878479
setp: 1700, Loss: 0.3194176256656647
setp: 1800, Loss: 0.3212478458881378
setp: 1900, Loss: 0.32352522015571594
setp: 2000, Loss: 0.34250032901763916
setp: 2100, Loss: 0.3264233469963074
setp: 2200, Loss: 0.3271511197090149
setp: 2300, Loss: 0.31890353560447693
setp: 2400, Loss: 0.35126635432243347
setp: 2500, Loss: 0.31670525670051575
setp: 2600, Loss: 0.3187674880027771
setp: 2700, Loss: 0.31764519214630127
setp: 2800, Loss: 0.3182350993156433
setp: 2900, Loss: 0.3190847933292389
setp: 3000, Loss: 0.3184731900691986
setp: 3100, Loss: 0.5845193862915039
setp: 3200, Loss: 0.40544185042381287
setp: 3300, Loss: 0.5021589994430542
setp: 3400, Loss: 0.3208373785018921
setp: 3500, Loss: 0.3191104531288147
setp: 3600, Loss: 0.319230854511261
setp: 3700, Loss: 0.3200755715370178
setp: 3800, Loss: 0.3194418251514435
setp: 3900, Loss: 0.41470155119895935
setp: 4000, Loss: 0.31603556871414185
setp: 4100, Loss: 0.3158501088619232
setp: 4200, Loss: 0.31720060110092163
setp: 4300, Loss: 0.3484288156032562
setp: 4400, Loss: 0.3163382112979889
setp: 4500, Loss: 0.3181958496570587
setp: 4600, Loss: 0.31716352701187134
setp: 4700, Loss: 0.3503789007663727
setp: 4800, Loss: 0.32146984338760376
setp: 4900, Loss: 0.3174601197242737
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.991304347826087
F_score: 0.9956331877729258
validating...
acc: 0.9539473684210527
precision: 0.9137931034482759
recall: 0.9636363636363636
F_score: 0.9380530973451328
******fold 3******
[229, 379]
training...
setp: 0, Loss: 0.6302902102470398
setp: 100, Loss: 0.6087542176246643
setp: 200, Loss: 0.5542263984680176
setp: 300, Loss: 0.548277735710144
setp: 400, Loss: 0.40980055928230286
setp: 500, Loss: 0.41419893503189087
setp: 600, Loss: 0.33228376507759094
setp: 700, Loss: 0.32933947443962097
setp: 800, Loss: 0.3852938413619995
setp: 900, Loss: 0.3242250978946686
setp: 1000, Loss: 0.3256327211856842
setp: 1100, Loss: 0.31988847255706787
setp: 1200, Loss: 0.32011252641677856
setp: 1300, Loss: 0.32725629210472107
setp: 1400, Loss: 0.33004695177078247
setp: 1500, Loss: 0.31823182106018066
setp: 1600, Loss: 0.3178921937942505
setp: 1700, Loss: 0.31972774863243103
setp: 1800, Loss: 0.3187348544597626
setp: 1900, Loss: 0.3265935182571411
setp: 2000, Loss: 0.35513272881507874
setp: 2100, Loss: 0.316832959651947
setp: 2200, Loss: 0.3193992078304291
setp: 2300, Loss: 0.31732797622680664
setp: 2400, Loss: 0.31945765018463135
setp: 2500, Loss: 0.3170771896839142
setp: 2600, Loss: 0.6803581118583679
setp: 2700, Loss: 0.33147746324539185
setp: 2800, Loss: 0.4229411482810974
setp: 2900, Loss: 0.32000675797462463
setp: 3000, Loss: 0.31729641556739807
setp: 3100, Loss: 0.32353678345680237
setp: 3200, Loss: 0.3187519907951355
setp: 3300, Loss: 0.31737348437309265
setp: 3400, Loss: 0.31910571455955505
setp: 3500, Loss: 0.31708452105522156
setp: 3600, Loss: 0.3183949887752533
setp: 3700, Loss: 0.3251577615737915
setp: 3800, Loss: 0.3180330693721771
setp: 3900, Loss: 0.3181228041648865
setp: 4000, Loss: 0.3175516426563263
setp: 4100, Loss: 0.31740400195121765
setp: 4200, Loss: 0.31681278347969055
setp: 4300, Loss: 0.749221682548523
setp: 4400, Loss: 0.38706621527671814
setp: 4500, Loss: 0.39655154943466187
setp: 4600, Loss: 0.35898029804229736
setp: 4700, Loss: 0.32597097754478455
setp: 4800, Loss: 0.3291564881801605
setp: 4900, Loss: 0.3390696048736572
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9870689655172413
recall: 1.0
F_score: 0.9934924078091106
validating...
acc: 0.9276315789473685
precision: 0.9090909090909091
recall: 0.8928571428571429
F_score: 0.9009009009009009
******fold 4******
[228, 380]
training...
setp: 0, Loss: 0.6251944899559021
setp: 100, Loss: 0.6398053765296936
setp: 200, Loss: 0.6080118417739868
setp: 300, Loss: 0.5219447016716003
setp: 400, Loss: 0.42543333768844604
setp: 500, Loss: 0.46275219321250916
setp: 600, Loss: 0.33151283860206604
setp: 700, Loss: 0.3367749750614166
setp: 800, Loss: 0.33118000626564026
setp: 900, Loss: 0.3463306128978729
setp: 1000, Loss: 0.3397628962993622
setp: 1100, Loss: 0.3226443827152252
setp: 1200, Loss: 0.3298278748989105
setp: 1300, Loss: 0.3221675455570221
setp: 1400, Loss: 0.35406139492988586
setp: 1500, Loss: 0.3218221068382263
setp: 1600, Loss: 0.32215309143066406
setp: 1700, Loss: 0.3185200095176697
setp: 1800, Loss: 0.32287782430648804
setp: 1900, Loss: 0.3185978829860687
setp: 2000, Loss: 0.31956249475479126
setp: 2100, Loss: 0.3209631145000458
setp: 2200, Loss: 0.3199351131916046
setp: 2300, Loss: 0.31894007325172424
setp: 2400, Loss: 0.3206934630870819
setp: 2500, Loss: 0.3185913860797882
setp: 2600, Loss: 0.3197474777698517
setp: 2700, Loss: 0.32077905535697937
setp: 2800, Loss: 0.31893011927604675
setp: 2900, Loss: 0.31847721338272095
setp: 3000, Loss: 0.3193466365337372
setp: 3100, Loss: 0.3191834092140198
setp: 3200, Loss: 0.31801438331604004
setp: 3300, Loss: 0.3203349709510803
setp: 3400, Loss: 0.5052559971809387
setp: 3500, Loss: 0.3653952181339264
setp: 3600, Loss: 0.351785808801651
setp: 3700, Loss: 0.32962343096733093
setp: 3800, Loss: 0.32221919298171997
setp: 3900, Loss: 0.3207334876060486
setp: 4000, Loss: 0.324740469455719
setp: 4100, Loss: 0.32217493653297424
setp: 4200, Loss: 0.3270982503890991
setp: 4300, Loss: 0.32135260105133057
setp: 4400, Loss: 0.32168644666671753
setp: 4500, Loss: 0.3211348354816437
setp: 4600, Loss: 0.3225489556789398
setp: 4700, Loss: 0.32113412022590637
setp: 4800, Loss: 0.3206309378147125
setp: 4900, Loss: 0.3214338421821594
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9539473684210527
precision: 0.9166666666666666
recall: 0.9649122807017544
F_score: 0.9401709401709402
model saved.
avg_acc: 0.9526315789473685, avg_f_score: 0.9365345881604371
-------------subject: 19-------------
==========valence==========
******fold 0******
[255, 353]
training...
setp: 0, Loss: 0.6942909955978394
setp: 100, Loss: 0.697304368019104
setp: 200, Loss: 0.6587103605270386
setp: 300, Loss: 0.5959402322769165
setp: 400, Loss: 0.5129976272583008
setp: 500, Loss: 0.6101319789886475
setp: 600, Loss: 0.499208927154541
setp: 700, Loss: 0.3320782780647278
setp: 800, Loss: 0.3693256080150604
setp: 900, Loss: 0.40029141306877136
setp: 1000, Loss: 0.3696073591709137
setp: 1100, Loss: 0.34859007596969604
setp: 1200, Loss: 0.34961947798728943
setp: 1300, Loss: 0.32247626781463623
setp: 1400, Loss: 0.3183537721633911
setp: 1500, Loss: 0.3626837730407715
setp: 1600, Loss: 0.3257456123828888
setp: 1700, Loss: 0.316636323928833
setp: 1800, Loss: 0.3193455636501312
setp: 1900, Loss: 0.31822440028190613
setp: 2000, Loss: 0.32594913244247437
setp: 2100, Loss: 0.326847642660141
setp: 2200, Loss: 0.3167107105255127
setp: 2300, Loss: 0.3455784022808075
setp: 2400, Loss: 0.32573646306991577
setp: 2500, Loss: 0.31651175022125244
setp: 2600, Loss: 0.3153211176395416
setp: 2700, Loss: 0.31599661707878113
setp: 2800, Loss: 0.31640851497650146
setp: 2900, Loss: 0.3176889717578888
setp: 3000, Loss: 0.32871153950691223
setp: 3100, Loss: 0.32071053981781006
setp: 3200, Loss: 0.3653201758861542
setp: 3300, Loss: 0.31553155183792114
setp: 3400, Loss: 0.354527086019516
setp: 3500, Loss: 0.3152925670146942
setp: 3600, Loss: 0.3215515911579132
setp: 3700, Loss: 0.31715840101242065
setp: 3800, Loss: 0.3367522656917572
setp: 3900, Loss: 0.33947426080703735
setp: 4000, Loss: 0.31631389260292053
setp: 4100, Loss: 0.3167012631893158
setp: 4200, Loss: 0.3187215030193329
setp: 4300, Loss: 0.3290017247200012
setp: 4400, Loss: 0.31554079055786133
setp: 4500, Loss: 0.3149975538253784
setp: 4600, Loss: 0.3158011734485626
setp: 4700, Loss: 0.31909558176994324
setp: 4800, Loss: 0.3193080723285675
setp: 4900, Loss: 0.3159247636795044
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.996078431372549
F_score: 0.9980353634577603
validating...
acc: 0.9144736842105263
precision: 0.9104477611940298
recall: 0.8970588235294118
F_score: 0.9037037037037037
******fold 1******
[257, 351]
training...
setp: 0, Loss: 0.6881911158561707
setp: 100, Loss: 0.6664960384368896
setp: 200, Loss: 0.6250251531600952
setp: 300, Loss: 0.49738892912864685
setp: 400, Loss: 0.4647209048271179
setp: 500, Loss: 0.5358172059059143
setp: 600, Loss: 0.4666602611541748
setp: 700, Loss: 0.393778532743454
setp: 800, Loss: 0.37637951970100403
setp: 900, Loss: 0.4104120433330536
setp: 1000, Loss: 0.33907759189605713
setp: 1100, Loss: 0.32611703872680664
setp: 1200, Loss: 0.32759562134742737
setp: 1300, Loss: 0.3274683356285095
setp: 1400, Loss: 0.3220619559288025
setp: 1500, Loss: 0.32005804777145386
setp: 1600, Loss: 0.3175937235355377
setp: 1700, Loss: 0.31701040267944336
setp: 1800, Loss: 0.31652626395225525
setp: 1900, Loss: 0.33136487007141113
setp: 2000, Loss: 0.3260114789009094
setp: 2100, Loss: 0.3582676649093628
setp: 2200, Loss: 0.3165670335292816
setp: 2300, Loss: 0.33174264430999756
setp: 2400, Loss: 0.3182035982608795
setp: 2500, Loss: 0.32620593905448914
setp: 2600, Loss: 0.3189457654953003
setp: 2700, Loss: 0.3491707146167755
setp: 2800, Loss: 0.3343132734298706
setp: 2900, Loss: 0.3146064579486847
setp: 3000, Loss: 0.325574666261673
setp: 3100, Loss: 0.3399431109428406
setp: 3200, Loss: 0.31607404351234436
setp: 3300, Loss: 0.335173636674881
setp: 3400, Loss: 0.31596624851226807
setp: 3500, Loss: 0.32194939255714417
setp: 3600, Loss: 0.35432547330856323
setp: 3700, Loss: 0.31749337911605835
setp: 3800, Loss: 0.32125231623649597
setp: 3900, Loss: 0.31778767704963684
setp: 4000, Loss: 0.31727850437164307
setp: 4100, Loss: 0.3168635070323944
setp: 4200, Loss: 0.3176247477531433
setp: 4300, Loss: 0.318462997674942
setp: 4400, Loss: 0.3168855309486389
setp: 4500, Loss: 0.31705158948898315
setp: 4600, Loss: 0.3477913439273834
setp: 4700, Loss: 0.3183809220790863
setp: 4800, Loss: 0.3148929476737976
setp: 4900, Loss: 0.31592145562171936
training successfully ended.
validating...
acc: 0.9901315789473685
precision: 0.9771863117870723
recall: 1.0
F_score: 0.9884615384615385
validating...
acc: 0.881578947368421
precision: 0.8157894736842105
recall: 0.9393939393939394
F_score: 0.8732394366197183
******fold 2******
[264, 344]
training...
setp: 0, Loss: 0.6873801946640015
setp: 100, Loss: 0.5961431264877319
setp: 200, Loss: 0.5007868409156799
setp: 300, Loss: 0.3734745979309082
setp: 400, Loss: 0.38233479857444763
setp: 500, Loss: 0.3653032183647156
setp: 600, Loss: 0.3433002233505249
setp: 700, Loss: 0.3326176404953003
setp: 800, Loss: 0.32072189450263977
setp: 900, Loss: 0.32435888051986694
setp: 1000, Loss: 0.3192274868488312
setp: 1100, Loss: 0.32113441824913025
setp: 1200, Loss: 0.32168376445770264
setp: 1300, Loss: 0.3178131580352783
setp: 1400, Loss: 0.3171929717063904
setp: 1500, Loss: 0.3193230926990509
setp: 1600, Loss: 0.35339057445526123
setp: 1700, Loss: 0.3528083860874176
setp: 1800, Loss: 0.3173142671585083
setp: 1900, Loss: 0.3176535367965698
setp: 2000, Loss: 0.3173919916152954
setp: 2100, Loss: 0.3173089027404785
setp: 2200, Loss: 0.31763315200805664
setp: 2300, Loss: 0.45266178250312805
setp: 2400, Loss: 0.3536033034324646
setp: 2500, Loss: 0.3177541494369507
setp: 2600, Loss: 0.31621843576431274
setp: 2700, Loss: 0.3163311779499054
setp: 2800, Loss: 0.3180451989173889
setp: 2900, Loss: 0.3177083432674408
setp: 3000, Loss: 0.31933116912841797
setp: 3100, Loss: 0.3654387891292572
setp: 3200, Loss: 0.3169386386871338
setp: 3300, Loss: 0.3166278898715973
setp: 3400, Loss: 0.3177946209907532
setp: 3500, Loss: 0.3172697126865387
setp: 3600, Loss: 0.3162520229816437
setp: 3700, Loss: 0.397521436214447
setp: 3800, Loss: 0.31804221868515015
setp: 3900, Loss: 0.31685686111450195
setp: 4000, Loss: 0.31714051961898804
setp: 4100, Loss: 0.31713125109672546
setp: 4200, Loss: 0.3173861801624298
setp: 4300, Loss: 0.3484565019607544
setp: 4400, Loss: 0.31617996096611023
setp: 4500, Loss: 0.31601056456565857
setp: 4600, Loss: 0.3159911632537842
setp: 4700, Loss: 0.3172585070133209
setp: 4800, Loss: 0.31745636463165283
setp: 4900, Loss: 0.3175262212753296
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9962264150943396
recall: 1.0
F_score: 0.998109640831758
validating...
acc: 0.9539473684210527
precision: 0.90625
recall: 0.9830508474576272
F_score: 0.943089430894309
******fold 3******
[263, 345]
training...
setp: 0, Loss: 0.6940464377403259
setp: 100, Loss: 0.6733203530311584
setp: 200, Loss: 0.6264002919197083
setp: 300, Loss: 0.5066843628883362
setp: 400, Loss: 0.502376914024353
setp: 500, Loss: 0.5196136832237244
setp: 600, Loss: 0.3291999399662018
setp: 700, Loss: 0.38206613063812256
setp: 800, Loss: 0.35435277223587036
setp: 900, Loss: 0.34438544511795044
setp: 1000, Loss: 0.3571595549583435
setp: 1100, Loss: 0.38470178842544556
setp: 1200, Loss: 0.355988085269928
setp: 1300, Loss: 0.3224428594112396
setp: 1400, Loss: 0.35303637385368347
setp: 1500, Loss: 0.34930527210235596
setp: 1600, Loss: 0.34982219338417053
setp: 1700, Loss: 0.31662654876708984
setp: 1800, Loss: 0.34691286087036133
setp: 1900, Loss: 0.3802606165409088
setp: 2000, Loss: 0.3512856662273407
setp: 2100, Loss: 0.3177412748336792
setp: 2200, Loss: 0.3667093515396118
setp: 2300, Loss: 0.3480968773365021
setp: 2400, Loss: 0.34948959946632385
setp: 2500, Loss: 0.3179762065410614
setp: 2600, Loss: 0.34645581245422363
setp: 2700, Loss: 0.3156167268753052
setp: 2800, Loss: 0.3168365955352783
setp: 2900, Loss: 0.3188219368457794
setp: 3000, Loss: 0.3660650849342346
setp: 3100, Loss: 0.31674984097480774
setp: 3200, Loss: 0.330345094203949
setp: 3300, Loss: 0.31979864835739136
setp: 3400, Loss: 0.3634803593158722
setp: 3500, Loss: 0.3199375867843628
setp: 3600, Loss: 0.3210262358188629
setp: 3700, Loss: 0.31890302896499634
setp: 3800, Loss: 0.33043795824050903
setp: 3900, Loss: 0.32799863815307617
setp: 4000, Loss: 0.33489570021629333
setp: 4100, Loss: 0.3257654309272766
setp: 4200, Loss: 0.3191051781177521
setp: 4300, Loss: 0.3198181688785553
setp: 4400, Loss: 0.3157002925872803
setp: 4500, Loss: 0.31567227840423584
setp: 4600, Loss: 0.3166571259498596
setp: 4700, Loss: 0.31591275334358215
setp: 4800, Loss: 0.31820765137672424
setp: 4900, Loss: 0.31825366616249084
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9144736842105263
precision: 0.873015873015873
recall: 0.9166666666666666
F_score: 0.894308943089431
******fold 4******
[253, 355]
training...
setp: 0, Loss: 0.6489588022232056
setp: 100, Loss: 0.6725258827209473
setp: 200, Loss: 0.6158276200294495
setp: 300, Loss: 0.5950173735618591
setp: 400, Loss: 0.4697768986225128
setp: 500, Loss: 0.5528692603111267
setp: 600, Loss: 0.5161168575286865
setp: 700, Loss: 0.3568198084831238
setp: 800, Loss: 0.34958964586257935
setp: 900, Loss: 0.43258237838745117
setp: 1000, Loss: 0.40567436814308167
setp: 1100, Loss: 0.346929669380188
setp: 1200, Loss: 0.32583707571029663
setp: 1300, Loss: 0.3180365264415741
setp: 1400, Loss: 0.3212986886501312
setp: 1500, Loss: 0.3730557858943939
setp: 1600, Loss: 0.3162277340888977
setp: 1700, Loss: 0.33561357855796814
setp: 1800, Loss: 0.3245377540588379
setp: 1900, Loss: 0.3198894262313843
setp: 2000, Loss: 0.3668866753578186
setp: 2100, Loss: 0.3196524381637573
setp: 2200, Loss: 0.31555092334747314
setp: 2300, Loss: 0.316942423582077
setp: 2400, Loss: 0.40963485836982727
setp: 2500, Loss: 0.3178705871105194
setp: 2600, Loss: 0.32422980666160583
setp: 2700, Loss: 0.31901976466178894
setp: 2800, Loss: 0.324392706155777
setp: 2900, Loss: 0.31676241755485535
setp: 3000, Loss: 0.41603827476501465
setp: 3100, Loss: 0.31587496399879456
setp: 3200, Loss: 0.3491264283657074
setp: 3300, Loss: 0.3225362002849579
setp: 3400, Loss: 0.3330744206905365
setp: 3500, Loss: 0.3167748749256134
setp: 3600, Loss: 0.38537928462028503
setp: 3700, Loss: 0.3154177665710449
setp: 3800, Loss: 0.334177166223526
setp: 3900, Loss: 0.32170987129211426
setp: 4000, Loss: 0.3167017102241516
setp: 4100, Loss: 0.3354690968990326
setp: 4200, Loss: 0.31651604175567627
setp: 4300, Loss: 0.3514423966407776
setp: 4400, Loss: 0.3163677155971527
setp: 4500, Loss: 0.3161928653717041
setp: 4600, Loss: 0.3157080411911011
setp: 4700, Loss: 0.3157646954059601
setp: 4800, Loss: 0.3162846565246582
setp: 4900, Loss: 0.31711411476135254
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9960474308300395
F_score: 0.998019801980198
validating...
acc: 0.9605263157894737
precision: 0.9705882352941176
recall: 0.9428571428571428
F_score: 0.9565217391304348
model saved.
avg_acc: 0.925, avg_f_score: 0.9141726506875193
==========arousal==========
******fold 0******
[190, 418]
training...
setp: 0, Loss: 0.6672926545143127
setp: 100, Loss: 0.6210107207298279
setp: 200, Loss: 0.6204440593719482
setp: 300, Loss: 0.6447915434837341
setp: 400, Loss: 0.4931754469871521
setp: 500, Loss: 0.4846777319908142
setp: 600, Loss: 0.3433300852775574
setp: 700, Loss: 0.4246436655521393
setp: 800, Loss: 0.3816658854484558
setp: 900, Loss: 0.39071157574653625
setp: 1000, Loss: 0.37190645933151245
setp: 1100, Loss: 0.35960277915000916
setp: 1200, Loss: 0.36479565501213074
setp: 1300, Loss: 0.355185866355896
setp: 1400, Loss: 0.3813447058200836
setp: 1500, Loss: 0.34639570116996765
setp: 1600, Loss: 0.3518315255641937
setp: 1700, Loss: 0.351209819316864
setp: 1800, Loss: 0.349660724401474
setp: 1900, Loss: 0.3363460302352905
setp: 2000, Loss: 0.42836347222328186
setp: 2100, Loss: 0.34758177399635315
setp: 2200, Loss: 0.31772539019584656
setp: 2300, Loss: 0.34754419326782227
setp: 2400, Loss: 0.34781596064567566
setp: 2500, Loss: 0.3166525661945343
setp: 2600, Loss: 0.34862929582595825
setp: 2700, Loss: 0.34720057249069214
setp: 2800, Loss: 0.35247135162353516
setp: 2900, Loss: 0.3516216278076172
setp: 3000, Loss: 0.3460957407951355
setp: 3100, Loss: 0.34909963607788086
setp: 3200, Loss: 0.34972161054611206
setp: 3300, Loss: 0.3842029571533203
setp: 3400, Loss: 0.3475308120250702
setp: 3500, Loss: 0.35094043612480164
setp: 3600, Loss: 0.34914475679397583
setp: 3700, Loss: 0.3497092127799988
setp: 3800, Loss: 0.3246685564517975
setp: 3900, Loss: 0.3287709653377533
setp: 4000, Loss: 0.34626054763793945
setp: 4100, Loss: 0.31574922800064087
setp: 4200, Loss: 0.35182589292526245
setp: 4300, Loss: 0.36620962619781494
setp: 4400, Loss: 0.3160198926925659
setp: 4500, Loss: 0.3471432328224182
setp: 4600, Loss: 0.35144364833831787
setp: 4700, Loss: 0.3518035113811493
setp: 4800, Loss: 0.3487096428871155
setp: 4900, Loss: 0.3474077880382538
training successfully ended.
validating...
acc: 0.9736842105263158
precision: 0.9943181818181818
recall: 0.9210526315789473
F_score: 0.9562841530054645
validating...
acc: 0.9078947368421053
precision: 0.9574468085106383
recall: 0.7894736842105263
F_score: 0.8653846153846154
******fold 1******
[201, 407]
training...
setp: 0, Loss: 0.6583386063575745
setp: 100, Loss: 0.6640365719795227
setp: 200, Loss: 0.5991467237472534
setp: 300, Loss: 0.5363726615905762
setp: 400, Loss: 0.48314425349235535
setp: 500, Loss: 0.3791773319244385
setp: 600, Loss: 0.40567174553871155
setp: 700, Loss: 0.3587522506713867
setp: 800, Loss: 0.32628360390663147
setp: 900, Loss: 0.35526102781295776
setp: 1000, Loss: 0.39544564485549927
setp: 1100, Loss: 0.31843364238739014
setp: 1200, Loss: 0.36846891045570374
setp: 1300, Loss: 0.34939250349998474
setp: 1400, Loss: 0.38304653763771057
setp: 1500, Loss: 0.35308051109313965
setp: 1600, Loss: 0.3490287661552429
setp: 1700, Loss: 0.31673216819763184
setp: 1800, Loss: 0.3508514165878296
setp: 1900, Loss: 0.35441306233406067
setp: 2000, Loss: 0.3481309711933136
setp: 2100, Loss: 0.3479458689689636
setp: 2200, Loss: 0.3690430521965027
setp: 2300, Loss: 0.34796634316444397
setp: 2400, Loss: 0.3516007959842682
setp: 2500, Loss: 0.35945355892181396
setp: 2600, Loss: 0.32184094190597534
setp: 2700, Loss: 0.31795448064804077
setp: 2800, Loss: 0.35002100467681885
setp: 2900, Loss: 0.3506912589073181
setp: 3000, Loss: 0.31631171703338623
setp: 3100, Loss: 0.34915322065353394
setp: 3200, Loss: 0.34799277782440186
setp: 3300, Loss: 0.35012581944465637
setp: 3400, Loss: 0.3507217764854431
setp: 3500, Loss: 0.34899580478668213
setp: 3600, Loss: 0.3272974491119385
setp: 3700, Loss: 0.3461059629917145
setp: 3800, Loss: 0.35019993782043457
setp: 3900, Loss: 0.34755679965019226
setp: 4000, Loss: 0.3471792936325073
setp: 4100, Loss: 0.3473135232925415
setp: 4200, Loss: 0.35021254420280457
setp: 4300, Loss: 0.4617651402950287
setp: 4400, Loss: 0.3502363860607147
setp: 4500, Loss: 0.3158292770385742
setp: 4600, Loss: 0.31661173701286316
setp: 4700, Loss: 0.35063666105270386
setp: 4800, Loss: 0.34872516989707947
setp: 4900, Loss: 0.31639599800109863
training successfully ended.
validating...
acc: 0.975328947368421
precision: 1.0
recall: 0.9253731343283582
F_score: 0.9612403100775194
validating...
acc: 0.9276315789473685
precision: 0.9069767441860465
recall: 0.8478260869565217
F_score: 0.8764044943820224
******fold 2******
[206, 402]
training...
setp: 0, Loss: 0.793511152267456
setp: 100, Loss: 0.5835089087486267
setp: 200, Loss: 0.643101692199707
setp: 300, Loss: 0.5609752535820007
setp: 400, Loss: 0.5286942720413208
setp: 500, Loss: 0.47228628396987915
setp: 600, Loss: 0.4251658320426941
setp: 700, Loss: 0.36105138063430786
setp: 800, Loss: 0.37542223930358887
setp: 900, Loss: 0.3535730838775635
setp: 1000, Loss: 0.3643593192100525
setp: 1100, Loss: 0.349743515253067
setp: 1200, Loss: 0.35139158368110657
setp: 1300, Loss: 0.3200621008872986
setp: 1400, Loss: 0.3188908100128174
setp: 1500, Loss: 0.32536157965660095
setp: 1600, Loss: 0.32454386353492737
setp: 1700, Loss: 0.3196888566017151
setp: 1800, Loss: 0.3169860541820526
setp: 1900, Loss: 0.3225035071372986
setp: 2000, Loss: 0.3175353705883026
setp: 2100, Loss: 0.3167497515678406
setp: 2200, Loss: 0.3158456087112427
setp: 2300, Loss: 0.33062317967414856
setp: 2400, Loss: 0.3216136395931244
setp: 2500, Loss: 0.3178696036338806
setp: 2600, Loss: 0.3158312439918518
setp: 2700, Loss: 0.3173331618309021
setp: 2800, Loss: 0.3206101655960083
setp: 2900, Loss: 0.317288339138031
setp: 3000, Loss: 0.3174034655094147
setp: 3100, Loss: 0.3449696898460388
setp: 3200, Loss: 0.3169572055339813
setp: 3300, Loss: 0.3160054385662079
setp: 3400, Loss: 0.3167473077774048
setp: 3500, Loss: 0.32079413533210754
setp: 3600, Loss: 0.31694522500038147
setp: 3700, Loss: 0.31543394923210144
setp: 3800, Loss: 0.3202405869960785
setp: 3900, Loss: 0.31667810678482056
setp: 4000, Loss: 0.315935879945755
setp: 4100, Loss: 0.315253347158432
setp: 4200, Loss: 0.3299955427646637
setp: 4300, Loss: 0.31845229864120483
setp: 4400, Loss: 0.31547489762306213
setp: 4500, Loss: 0.3149954676628113
setp: 4600, Loss: 0.3162533938884735
setp: 4700, Loss: 0.3199320435523987
setp: 4800, Loss: 0.3167944848537445
setp: 4900, Loss: 0.3176654279232025
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9671052631578947
precision: 0.8913043478260869
recall: 1.0
F_score: 0.9425287356321839
******fold 3******
[191, 417]
training...
setp: 0, Loss: 0.6890183091163635
setp: 100, Loss: 0.6210429668426514
setp: 200, Loss: 0.595539391040802
setp: 300, Loss: 0.6329004168510437
setp: 400, Loss: 0.585353672504425
setp: 500, Loss: 0.6070219874382019
setp: 600, Loss: 0.4054689109325409
setp: 700, Loss: 0.4002271294593811
setp: 800, Loss: 0.36713263392448425
setp: 900, Loss: 0.3695127069950104
setp: 1000, Loss: 0.3704567551612854
setp: 1100, Loss: 0.35869666934013367
setp: 1200, Loss: 0.3511263430118561
setp: 1300, Loss: 0.37098926305770874
setp: 1400, Loss: 0.35616055130958557
setp: 1500, Loss: 0.317180335521698
setp: 1600, Loss: 0.35195091366767883
setp: 1700, Loss: 0.37673160433769226
setp: 1800, Loss: 0.3468568027019501
setp: 1900, Loss: 0.3519732356071472
setp: 2000, Loss: 0.34997689723968506
setp: 2100, Loss: 0.3151259124279022
setp: 2200, Loss: 0.34823691844940186
setp: 2300, Loss: 0.3212537169456482
setp: 2400, Loss: 0.3515744209289551
setp: 2500, Loss: 0.34670770168304443
setp: 2600, Loss: 0.3470469117164612
setp: 2700, Loss: 0.3552795350551605
setp: 2800, Loss: 0.3186052441596985
setp: 2900, Loss: 0.34910258650779724
setp: 3000, Loss: 0.34695667028427124
setp: 3100, Loss: 0.3171587586402893
setp: 3200, Loss: 0.3483343720436096
setp: 3300, Loss: 0.3473682701587677
setp: 3400, Loss: 0.31704965233802795
setp: 3500, Loss: 0.34793993830680847
setp: 3600, Loss: 0.348080039024353
setp: 3700, Loss: 0.3563252389431
setp: 3800, Loss: 0.3481612801551819
setp: 3900, Loss: 0.3479599952697754
setp: 4000, Loss: 0.3152960538864136
setp: 4100, Loss: 0.34687259793281555
setp: 4200, Loss: 0.31825926899909973
setp: 4300, Loss: 0.3480094075202942
setp: 4400, Loss: 0.36314767599105835
setp: 4500, Loss: 0.38139981031417847
setp: 4600, Loss: 0.37687569856643677
setp: 4700, Loss: 0.3161703944206238
setp: 4800, Loss: 0.3481777608394623
setp: 4900, Loss: 0.3472350537776947
training successfully ended.
validating...
acc: 0.9769736842105263
precision: 1.0
recall: 0.9267015706806283
F_score: 0.9619565217391305
validating...
acc: 0.9473684210526315
precision: 0.98
recall: 0.875
F_score: 0.9245283018867924
******fold 4******
[200, 408]
training...
setp: 0, Loss: 0.7181577682495117
setp: 100, Loss: 0.6624913215637207
setp: 200, Loss: 0.6239564418792725
setp: 300, Loss: 0.5313815474510193
setp: 400, Loss: 0.5574678182601929
setp: 500, Loss: 0.4058826267719269
setp: 600, Loss: 0.35845935344696045
setp: 700, Loss: 0.34970512986183167
setp: 800, Loss: 0.353894978761673
setp: 900, Loss: 0.3864648938179016
setp: 1000, Loss: 0.3254639208316803
setp: 1100, Loss: 0.3485654294490814
setp: 1200, Loss: 0.43720558285713196
setp: 1300, Loss: 0.3485843241214752
setp: 1400, Loss: 0.31693559885025024
setp: 1500, Loss: 0.32082951068878174
setp: 1600, Loss: 0.3220953345298767
setp: 1700, Loss: 0.3671110272407532
setp: 1800, Loss: 0.31733471155166626
setp: 1900, Loss: 0.3186253607273102
setp: 2000, Loss: 0.34108057618141174
setp: 2100, Loss: 0.3155277371406555
setp: 2200, Loss: 0.3159981369972229
setp: 2300, Loss: 0.31822478771209717
setp: 2400, Loss: 0.34246826171875
setp: 2500, Loss: 0.31526219844818115
setp: 2600, Loss: 0.3148500323295593
setp: 2700, Loss: 0.3160378336906433
setp: 2800, Loss: 0.32082146406173706
setp: 2900, Loss: 0.3161192834377289
setp: 3000, Loss: 0.31903207302093506
setp: 3100, Loss: 0.3165871500968933
setp: 3200, Loss: 0.3473035991191864
setp: 3300, Loss: 0.31540703773498535
setp: 3400, Loss: 0.3174414038658142
setp: 3500, Loss: 0.31642580032348633
setp: 3600, Loss: 0.3215339779853821
setp: 3700, Loss: 0.37791603803634644
setp: 3800, Loss: 0.3222268521785736
setp: 3900, Loss: 0.3243890404701233
setp: 4000, Loss: 0.3157680630683899
setp: 4100, Loss: 0.31553423404693604
setp: 4200, Loss: 0.3177110254764557
setp: 4300, Loss: 0.3167553246021271
setp: 4400, Loss: 0.3152519762516022
setp: 4500, Loss: 0.3152419924736023
setp: 4600, Loss: 0.3156530559062958
setp: 4700, Loss: 0.31809327006340027
setp: 4800, Loss: 0.35896438360214233
setp: 4900, Loss: 0.3619832992553711
training successfully ended.
validating...
acc: 0.9358552631578947
precision: 0.8396624472573839
recall: 0.995
F_score: 0.9107551487414187
validating...
acc: 0.8486842105263158
precision: 0.6714285714285714
recall: 1.0
F_score: 0.8034188034188035
model saved.
avg_acc: 0.9197368421052632, avg_f_score: 0.8824529901408835
-------------subject: 20-------------
==========valence==========
******fold 0******
[257, 351]
training...
setp: 0, Loss: 0.6857867240905762
setp: 100, Loss: 0.6848333477973938
setp: 200, Loss: 0.624502420425415
setp: 300, Loss: 0.5185479521751404
setp: 400, Loss: 0.38338518142700195
setp: 500, Loss: 0.4995717406272888
setp: 600, Loss: 0.46557796001434326
setp: 700, Loss: 0.3355952799320221
setp: 800, Loss: 0.38262301683425903
setp: 900, Loss: 0.4625140428543091
setp: 1000, Loss: 0.41284242272377014
setp: 1100, Loss: 0.3223380148410797
setp: 1200, Loss: 0.44582706689834595
setp: 1300, Loss: 0.3567887544631958
setp: 1400, Loss: 0.321620911359787
setp: 1500, Loss: 0.37929677963256836
setp: 1600, Loss: 0.35276368260383606
setp: 1700, Loss: 0.34317880868911743
setp: 1800, Loss: 0.3185615837574005
setp: 1900, Loss: 0.3240635097026825
setp: 2000, Loss: 0.32049185037612915
setp: 2100, Loss: 0.31738153100013733
setp: 2200, Loss: 0.32930365204811096
setp: 2300, Loss: 0.3187427222728729
setp: 2400, Loss: 0.3202991485595703
setp: 2500, Loss: 0.3225681781768799
setp: 2600, Loss: 0.31636229157447815
setp: 2700, Loss: 0.31846413016319275
setp: 2800, Loss: 0.3183263838291168
setp: 2900, Loss: 0.3494797646999359
setp: 3000, Loss: 0.31687161326408386
setp: 3100, Loss: 0.3174242675304413
setp: 3200, Loss: 0.3177582323551178
setp: 3300, Loss: 0.31900912523269653
setp: 3400, Loss: 0.3452763557434082
setp: 3500, Loss: 0.322495698928833
setp: 3600, Loss: 0.3347327411174774
setp: 3700, Loss: 0.3186317980289459
setp: 3800, Loss: 0.32304486632347107
setp: 3900, Loss: 0.3175698518753052
setp: 4000, Loss: 0.3158324956893921
setp: 4100, Loss: 0.3191717267036438
setp: 4200, Loss: 0.31542399525642395
setp: 4300, Loss: 0.3164018988609314
setp: 4400, Loss: 0.3179812729358673
setp: 4500, Loss: 0.3167078197002411
setp: 4600, Loss: 0.31618422269821167
setp: 4700, Loss: 0.3174057602882385
setp: 4800, Loss: 0.37407657504081726
setp: 4900, Loss: 0.317028284072876
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8881578947368421
precision: 0.9454545454545454
recall: 0.7878787878787878
F_score: 0.8595041322314049
******fold 1******
[258, 350]
training...
setp: 0, Loss: 0.6925233602523804
setp: 100, Loss: 0.665668249130249
setp: 200, Loss: 0.6071875691413879
setp: 300, Loss: 0.5509145855903625
setp: 400, Loss: 0.4872796833515167
setp: 500, Loss: 0.40781277418136597
setp: 600, Loss: 0.3424157500267029
setp: 700, Loss: 0.3564058840274811
setp: 800, Loss: 0.3393287658691406
setp: 900, Loss: 0.3198312222957611
setp: 1000, Loss: 0.32615527510643005
setp: 1100, Loss: 0.32043638825416565
setp: 1200, Loss: 0.3210572898387909
setp: 1300, Loss: 0.3227400779724121
setp: 1400, Loss: 0.32562699913978577
setp: 1500, Loss: 0.33480551838874817
setp: 1600, Loss: 0.3180946707725525
setp: 1700, Loss: 0.3182935118675232
setp: 1800, Loss: 0.3183346390724182
setp: 1900, Loss: 0.31959840655326843
setp: 2000, Loss: 0.31867149472236633
setp: 2100, Loss: 0.31820783019065857
setp: 2200, Loss: 0.31778740882873535
setp: 2300, Loss: 0.3182520270347595
setp: 2400, Loss: 0.3184475600719452
setp: 2500, Loss: 0.31958770751953125
setp: 2600, Loss: 0.3802291452884674
setp: 2700, Loss: 0.3243248760700226
setp: 2800, Loss: 0.31663987040519714
setp: 2900, Loss: 0.3178684711456299
setp: 3000, Loss: 0.31803181767463684
setp: 3100, Loss: 0.31825926899909973
setp: 3200, Loss: 0.31821128726005554
setp: 3300, Loss: 0.3176535367965698
setp: 3400, Loss: 0.31819236278533936
setp: 3500, Loss: 0.3177315294742584
setp: 3600, Loss: 0.3180367946624756
setp: 3700, Loss: 0.31798139214515686
setp: 3800, Loss: 0.3179453909397125
setp: 3900, Loss: 0.31754955649375916
setp: 4000, Loss: 0.3174782693386078
setp: 4100, Loss: 0.31779736280441284
setp: 4200, Loss: 0.31760382652282715
setp: 4300, Loss: 0.3182056248188019
setp: 4400, Loss: 0.31897681951522827
setp: 4500, Loss: 0.3186684548854828
setp: 4600, Loss: 0.3367147743701935
setp: 4700, Loss: 0.31681087613105774
setp: 4800, Loss: 0.31794989109039307
setp: 4900, Loss: 0.31719210743904114
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.875
recall: 0.9692307692307692
F_score: 0.9197080291970802
******fold 2******
[267, 341]
training...
setp: 0, Loss: 0.6901031136512756
setp: 100, Loss: 0.6133719682693481
setp: 200, Loss: 0.6326794028282166
setp: 300, Loss: 0.5048680901527405
setp: 400, Loss: 0.5264956951141357
setp: 500, Loss: 0.49145519733428955
setp: 600, Loss: 0.4837471842765808
setp: 700, Loss: 0.3767280578613281
setp: 800, Loss: 0.5779704451560974
setp: 900, Loss: 0.40939199924468994
setp: 1000, Loss: 0.3905812203884125
setp: 1100, Loss: 0.36788347363471985
setp: 1200, Loss: 0.5227222442626953
setp: 1300, Loss: 0.3184700310230255
setp: 1400, Loss: 0.3446398675441742
setp: 1500, Loss: 0.414310485124588
setp: 1600, Loss: 0.3264312744140625
setp: 1700, Loss: 0.31706634163856506
setp: 1800, Loss: 0.3852807879447937
setp: 1900, Loss: 0.3740067780017853
setp: 2000, Loss: 0.3549353778362274
setp: 2100, Loss: 0.3165571093559265
setp: 2200, Loss: 0.31817060708999634
setp: 2300, Loss: 0.31749391555786133
setp: 2400, Loss: 0.3173750936985016
setp: 2500, Loss: 0.3440832495689392
setp: 2600, Loss: 0.3171157240867615
setp: 2700, Loss: 0.32439902424812317
setp: 2800, Loss: 0.32168179750442505
setp: 2900, Loss: 0.3781568109989166
setp: 3000, Loss: 0.320018470287323
setp: 3100, Loss: 0.3172381818294525
setp: 3200, Loss: 0.317035049200058
setp: 3300, Loss: 0.31680238246917725
setp: 3400, Loss: 0.3818822503089905
setp: 3500, Loss: 0.3647882044315338
setp: 3600, Loss: 0.3251584768295288
setp: 3700, Loss: 0.4340333640575409
setp: 3800, Loss: 0.3170223832130432
setp: 3900, Loss: 0.3186483085155487
setp: 4000, Loss: 0.3164670169353485
setp: 4100, Loss: 0.31596627831459045
setp: 4200, Loss: 0.3161035478115082
setp: 4300, Loss: 0.3156518340110779
setp: 4400, Loss: 0.3179449439048767
setp: 4500, Loss: 0.3172561526298523
setp: 4600, Loss: 0.31678086519241333
setp: 4700, Loss: 0.317649245262146
setp: 4800, Loss: 0.34793543815612793
setp: 4900, Loss: 0.316945344209671
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9962546816479401
recall: 0.9962546816479401
F_score: 0.9962546816479401
validating...
acc: 0.8947368421052632
precision: 0.8571428571428571
recall: 0.8571428571428571
F_score: 0.8571428571428571
******fold 3******
[257, 351]
training...
setp: 0, Loss: 0.7133775949478149
setp: 100, Loss: 0.608894407749176
setp: 200, Loss: 0.6184418797492981
setp: 300, Loss: 0.5191469192504883
setp: 400, Loss: 0.37156566977500916
setp: 500, Loss: 0.39905884861946106
setp: 600, Loss: 0.3834777772426605
setp: 700, Loss: 0.4015873968601227
setp: 800, Loss: 0.33989575505256653
setp: 900, Loss: 0.43117570877075195
setp: 1000, Loss: 0.3584516942501068
setp: 1100, Loss: 0.4680860638618469
setp: 1200, Loss: 0.3661792576313019
setp: 1300, Loss: 0.3556465804576874
setp: 1400, Loss: 0.3485177457332611
setp: 1500, Loss: 0.3798699676990509
setp: 1600, Loss: 0.3423471450805664
setp: 1700, Loss: 0.322224497795105
setp: 1800, Loss: 0.34794342517852783
setp: 1900, Loss: 0.3184579312801361
setp: 2000, Loss: 0.3664405643939972
setp: 2100, Loss: 0.31908944249153137
setp: 2200, Loss: 0.34785985946655273
setp: 2300, Loss: 0.3180920481681824
setp: 2400, Loss: 0.316242516040802
setp: 2500, Loss: 0.346780389547348
setp: 2600, Loss: 0.3166729509830475
setp: 2700, Loss: 0.3342105448246002
setp: 2800, Loss: 0.3186572790145874
setp: 2900, Loss: 0.3183749318122864
setp: 3000, Loss: 0.31765416264533997
setp: 3100, Loss: 0.3165593445301056
setp: 3200, Loss: 0.3187727630138397
setp: 3300, Loss: 0.32686367630958557
setp: 3400, Loss: 0.34819120168685913
setp: 3500, Loss: 0.32114940881729126
setp: 3600, Loss: 0.3181813359260559
setp: 3700, Loss: 0.34689757227897644
setp: 3800, Loss: 0.32717180252075195
setp: 3900, Loss: 0.31674468517303467
setp: 4000, Loss: 0.3172089755535126
setp: 4100, Loss: 0.320343554019928
setp: 4200, Loss: 0.31936195492744446
setp: 4300, Loss: 0.3167310953140259
setp: 4400, Loss: 0.3213183283805847
setp: 4500, Loss: 0.31530463695526123
setp: 4600, Loss: 0.316045880317688
setp: 4700, Loss: 0.3175915777683258
setp: 4800, Loss: 0.31749019026756287
setp: 4900, Loss: 0.3180873394012451
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9961089494163424
F_score: 0.9980506822612086
validating...
acc: 0.9276315789473685
precision: 0.9824561403508771
recall: 0.8484848484848485
F_score: 0.9105691056910568
******fold 4******
[253, 355]
training...
setp: 0, Loss: 0.6868186593055725
setp: 100, Loss: 0.5850260257720947
setp: 200, Loss: 0.6553147435188293
setp: 300, Loss: 0.41586148738861084
setp: 400, Loss: 0.4555116295814514
setp: 500, Loss: 0.36681485176086426
setp: 600, Loss: 0.372224897146225
setp: 700, Loss: 0.35667550563812256
setp: 800, Loss: 0.3367231488227844
setp: 900, Loss: 0.4249301552772522
setp: 1000, Loss: 0.3220774233341217
setp: 1100, Loss: 0.34924787282943726
setp: 1200, Loss: 0.3820246160030365
setp: 1300, Loss: 0.3325085937976837
setp: 1400, Loss: 0.33263692259788513
setp: 1500, Loss: 0.35164615511894226
setp: 1600, Loss: 0.31958848237991333
setp: 1700, Loss: 0.31801918148994446
setp: 1800, Loss: 0.3852651119232178
setp: 1900, Loss: 0.31841805577278137
setp: 2000, Loss: 0.3183826506137848
setp: 2100, Loss: 0.31912532448768616
setp: 2200, Loss: 0.3492732346057892
setp: 2300, Loss: 0.316155344247818
setp: 2400, Loss: 0.3176419734954834
setp: 2500, Loss: 0.3167524039745331
setp: 2600, Loss: 0.31748440861701965
setp: 2700, Loss: 0.3174251616001129
setp: 2800, Loss: 0.34944280982017517
setp: 2900, Loss: 0.31606781482696533
setp: 3000, Loss: 0.3229103684425354
setp: 3100, Loss: 0.31731125712394714
setp: 3200, Loss: 0.31667569279670715
setp: 3300, Loss: 0.31557130813598633
setp: 3400, Loss: 0.34920772910118103
setp: 3500, Loss: 0.3157905340194702
setp: 3600, Loss: 0.38222363591194153
setp: 3700, Loss: 0.349593847990036
setp: 3800, Loss: 0.33814120292663574
setp: 3900, Loss: 0.3163939118385315
setp: 4000, Loss: 0.3157237470149994
setp: 4100, Loss: 0.3470230996608734
setp: 4200, Loss: 0.31641918420791626
setp: 4300, Loss: 0.31907522678375244
setp: 4400, Loss: 0.31720781326293945
setp: 4500, Loss: 0.3184870183467865
setp: 4600, Loss: 0.31545084714889526
setp: 4700, Loss: 0.3158630132675171
setp: 4800, Loss: 0.31738951802253723
setp: 4900, Loss: 0.3194853365421295
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 1.0
recall: 0.9881422924901185
F_score: 0.9940357852882703
validating...
acc: 0.9210526315789473
precision: 0.9142857142857143
recall: 0.9142857142857143
F_score: 0.9142857142857143
model saved.
avg_acc: 0.9118421052631579, avg_f_score: 0.8922419677096227
==========arousal==========
******fold 0******
[128, 480]
training...
setp: 0, Loss: 0.6978897452354431
setp: 100, Loss: 0.5964003205299377
setp: 200, Loss: 0.5629149675369263
setp: 300, Loss: 0.39600661396980286
setp: 400, Loss: 0.3397354483604431
setp: 500, Loss: 0.35647639632225037
setp: 600, Loss: 0.32794618606567383
setp: 700, Loss: 0.3194022476673126
setp: 800, Loss: 0.3238571882247925
setp: 900, Loss: 0.31880587339401245
setp: 1000, Loss: 0.31816908717155457
setp: 1100, Loss: 0.31999942660331726
setp: 1200, Loss: 0.31729888916015625
setp: 1300, Loss: 0.3175070285797119
setp: 1400, Loss: 0.31806620955467224
setp: 1500, Loss: 0.3174847662448883
setp: 1600, Loss: 0.31694725155830383
setp: 1700, Loss: 0.3247097432613373
setp: 1800, Loss: 0.31710225343704224
setp: 1900, Loss: 0.31658172607421875
setp: 2000, Loss: 0.31703031063079834
setp: 2100, Loss: 0.3170318007469177
setp: 2200, Loss: 0.31714504957199097
setp: 2300, Loss: 0.31768059730529785
setp: 2400, Loss: 0.3169850707054138
setp: 2500, Loss: 0.3168177902698517
setp: 2600, Loss: 0.3175366222858429
setp: 2700, Loss: 0.31691792607307434
setp: 2800, Loss: 0.3346981108188629
setp: 2900, Loss: 0.3213958740234375
setp: 3000, Loss: 0.3166126310825348
setp: 3100, Loss: 0.31655755639076233
setp: 3200, Loss: 0.3168238699436188
setp: 3300, Loss: 0.3168722987174988
setp: 3400, Loss: 0.31658875942230225
setp: 3500, Loss: 0.31726792454719543
setp: 3600, Loss: 0.3168841600418091
setp: 3700, Loss: 0.3164884150028229
setp: 3800, Loss: 0.5706340670585632
setp: 3900, Loss: 0.342059850692749
setp: 4000, Loss: 0.3425503671169281
setp: 4100, Loss: 0.3222390413284302
setp: 4200, Loss: 0.32169824838638306
setp: 4300, Loss: 0.32052141427993774
setp: 4400, Loss: 0.3201569616794586
setp: 4500, Loss: 0.32009029388427734
setp: 4600, Loss: 0.31872570514678955
setp: 4700, Loss: 0.31876876950263977
setp: 4800, Loss: 0.3334517478942871
setp: 4900, Loss: 0.3206072449684143
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9868421052631579
precision: 1.0
recall: 0.9534883720930233
F_score: 0.9761904761904763
******fold 1******
[140, 468]
training...
setp: 0, Loss: 0.6930741667747498
setp: 100, Loss: 0.6513291597366333
setp: 200, Loss: 0.572012186050415
setp: 300, Loss: 0.37861835956573486
setp: 400, Loss: 0.3349783718585968
setp: 500, Loss: 0.33658567070961
setp: 600, Loss: 0.32195672392845154
setp: 700, Loss: 0.321245938539505
setp: 800, Loss: 0.32314494252204895
setp: 900, Loss: 0.31985971331596375
setp: 1000, Loss: 0.32834938168525696
setp: 1100, Loss: 0.3244096338748932
setp: 1200, Loss: 0.31939536333084106
setp: 1300, Loss: 0.3215908408164978
setp: 1400, Loss: 0.31859487295150757
setp: 1500, Loss: 0.31766876578330994
setp: 1600, Loss: 0.31773629784584045
setp: 1700, Loss: 0.3185824751853943
setp: 1800, Loss: 0.3178272247314453
setp: 1900, Loss: 0.33026403188705444
setp: 2000, Loss: 0.32074686884880066
setp: 2100, Loss: 0.31717801094055176
setp: 2200, Loss: 0.3172992467880249
setp: 2300, Loss: 0.3180467188358307
setp: 2400, Loss: 0.3175349533557892
setp: 2500, Loss: 0.3189117908477783
setp: 2600, Loss: 0.31798994541168213
setp: 2700, Loss: 0.31708982586860657
setp: 2800, Loss: 0.31708064675331116
setp: 2900, Loss: 0.31912508606910706
setp: 3000, Loss: 0.3172587752342224
setp: 3100, Loss: 0.3166152238845825
setp: 3200, Loss: 0.317972332239151
setp: 3300, Loss: 0.31859150528907776
setp: 3400, Loss: 0.316589891910553
setp: 3500, Loss: 0.3178704082965851
setp: 3600, Loss: 0.3185611069202423
setp: 3700, Loss: 0.34018760919570923
setp: 3800, Loss: 0.318878710269928
setp: 3900, Loss: 0.31649497151374817
setp: 4000, Loss: 0.31670263409614563
setp: 4100, Loss: 0.3176855146884918
setp: 4200, Loss: 0.3167775273323059
setp: 4300, Loss: 0.3182266652584076
setp: 4400, Loss: 0.31665724515914917
setp: 4500, Loss: 0.31652572751045227
setp: 4600, Loss: 0.3168277144432068
setp: 4700, Loss: 0.3173789978027344
setp: 4800, Loss: 0.3179168701171875
setp: 4900, Loss: 0.31687217950820923
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9605263157894737
precision: 0.9032258064516129
recall: 0.9032258064516129
F_score: 0.9032258064516129
******fold 2******
[140, 468]
training...
setp: 0, Loss: 0.7342084050178528
setp: 100, Loss: 0.6187567710876465
setp: 200, Loss: 0.4365999698638916
setp: 300, Loss: 0.3637169599533081
setp: 400, Loss: 0.3567334711551666
setp: 500, Loss: 0.3362595736980438
setp: 600, Loss: 0.3263775110244751
setp: 700, Loss: 0.3223535418510437
setp: 800, Loss: 0.3216066062450409
setp: 900, Loss: 0.32156121730804443
setp: 1000, Loss: 0.3212820589542389
setp: 1100, Loss: 0.319927453994751
setp: 1200, Loss: 0.31969088315963745
setp: 1300, Loss: 0.31997647881507874
setp: 1400, Loss: 0.3215410113334656
setp: 1500, Loss: 0.32074689865112305
setp: 1600, Loss: 0.3201458752155304
setp: 1700, Loss: 0.34458789229393005
setp: 1800, Loss: 0.3220486044883728
setp: 1900, Loss: 0.318693071603775
setp: 2000, Loss: 0.3192269504070282
setp: 2100, Loss: 0.31948956847190857
setp: 2200, Loss: 0.31809690594673157
setp: 2300, Loss: 0.31879618763923645
setp: 2400, Loss: 0.31927230954170227
setp: 2500, Loss: 0.318869411945343
setp: 2600, Loss: 0.3194274306297302
setp: 2700, Loss: 0.319578617811203
setp: 2800, Loss: 0.3187974691390991
setp: 2900, Loss: 0.31917932629585266
setp: 3000, Loss: 0.3197828531265259
setp: 3100, Loss: 0.374299019575119
setp: 3200, Loss: 0.3523307740688324
setp: 3300, Loss: 0.3207243084907532
setp: 3400, Loss: 0.3224567472934723
setp: 3500, Loss: 0.31957900524139404
setp: 3600, Loss: 0.31990423798561096
setp: 3700, Loss: 0.3195568025112152
setp: 3800, Loss: 0.3202255070209503
setp: 3900, Loss: 0.32020318508148193
setp: 4000, Loss: 0.31992870569229126
setp: 4100, Loss: 0.3201340436935425
setp: 4200, Loss: 0.32018885016441345
setp: 4300, Loss: 0.3194965720176697
setp: 4400, Loss: 0.3205726146697998
setp: 4500, Loss: 0.3205590844154358
setp: 4600, Loss: 0.3202250301837921
setp: 4700, Loss: 0.3540351688861847
setp: 4800, Loss: 0.33457428216934204
setp: 4900, Loss: 0.31835320591926575
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.7837837837837838
recall: 0.9354838709677419
F_score: 0.8529411764705881
******fold 3******
[132, 476]
training...
setp: 0, Loss: 0.7161325812339783
setp: 100, Loss: 0.6923457384109497
setp: 200, Loss: 0.545666515827179
setp: 300, Loss: 0.4482745826244354
setp: 400, Loss: 0.3609534800052643
setp: 500, Loss: 0.3301701247692108
setp: 600, Loss: 0.34069085121154785
setp: 700, Loss: 0.3205530345439911
setp: 800, Loss: 0.3191096782684326
setp: 900, Loss: 0.32131779193878174
setp: 1000, Loss: 0.3194708824157715
setp: 1100, Loss: 0.3184730112552643
setp: 1200, Loss: 0.3202027678489685
setp: 1300, Loss: 0.31907644867897034
setp: 1400, Loss: 0.31836333870887756
setp: 1500, Loss: 0.3190896213054657
setp: 1600, Loss: 0.31827303767204285
setp: 1700, Loss: 0.3179832398891449
setp: 1800, Loss: 0.32905319333076477
setp: 1900, Loss: 0.31740516424179077
setp: 2000, Loss: 0.3178347945213318
setp: 2100, Loss: 0.31868258118629456
setp: 2200, Loss: 0.3177994191646576
setp: 2300, Loss: 0.3181471824645996
setp: 2400, Loss: 0.3186572194099426
setp: 2500, Loss: 0.3183702230453491
setp: 2600, Loss: 0.31841349601745605
setp: 2700, Loss: 0.3189835846424103
setp: 2800, Loss: 0.31769537925720215
setp: 2900, Loss: 0.31795984506607056
setp: 3000, Loss: 0.3213101327419281
setp: 3100, Loss: 0.32031914591789246
setp: 3200, Loss: 0.3185608685016632
setp: 3300, Loss: 0.31896311044692993
setp: 3400, Loss: 0.31870654225349426
setp: 3500, Loss: 0.3183825612068176
setp: 3600, Loss: 0.3192322254180908
setp: 3700, Loss: 0.3182860016822815
setp: 3800, Loss: 0.3185287117958069
setp: 3900, Loss: 0.3191882371902466
setp: 4000, Loss: 0.3176634609699249
setp: 4100, Loss: 0.3595566153526306
setp: 4200, Loss: 0.32060229778289795
setp: 4300, Loss: 0.31870710849761963
setp: 4400, Loss: 0.31842097640037537
setp: 4500, Loss: 0.31969931721687317
setp: 4600, Loss: 0.3193705081939697
setp: 4700, Loss: 0.3185715973377228
setp: 4800, Loss: 0.31989315152168274
setp: 4900, Loss: 0.3189123272895813
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.868421052631579
recall: 0.8461538461538461
F_score: 0.8571428571428572
******fold 4******
[144, 464]
training...
setp: 0, Loss: 0.7069764733314514
setp: 100, Loss: 0.6926635503768921
setp: 200, Loss: 0.6025022864341736
setp: 300, Loss: 0.49958252906799316
setp: 400, Loss: 0.37100905179977417
setp: 500, Loss: 0.3448510766029358
setp: 600, Loss: 0.39807015657424927
setp: 700, Loss: 0.3254048228263855
setp: 800, Loss: 0.3227901756763458
setp: 900, Loss: 0.32336387038230896
setp: 1000, Loss: 0.31961625814437866
setp: 1100, Loss: 0.32040512561798096
setp: 1200, Loss: 0.31993985176086426
setp: 1300, Loss: 0.3239573538303375
setp: 1400, Loss: 0.3325040638446808
setp: 1500, Loss: 0.31937870383262634
setp: 1600, Loss: 0.32229289412498474
setp: 1700, Loss: 0.31816697120666504
setp: 1800, Loss: 0.31944653391838074
setp: 1900, Loss: 0.31916144490242004
setp: 2000, Loss: 0.318983256816864
setp: 2100, Loss: 0.31709396839141846
setp: 2200, Loss: 0.3200574517250061
setp: 2300, Loss: 0.32059797644615173
setp: 2400, Loss: 0.31991034746170044
setp: 2500, Loss: 0.3203786611557007
setp: 2600, Loss: 0.31934645771980286
setp: 2700, Loss: 0.31793949007987976
setp: 2800, Loss: 0.31868600845336914
setp: 2900, Loss: 0.4176158905029297
setp: 3000, Loss: 0.3843414783477783
setp: 3100, Loss: 0.3247785270214081
setp: 3200, Loss: 0.32129669189453125
setp: 3300, Loss: 0.32076069712638855
setp: 3400, Loss: 0.31985360383987427
setp: 3500, Loss: 0.3205719292163849
setp: 3600, Loss: 0.3204469680786133
setp: 3700, Loss: 0.3205937445163727
setp: 3800, Loss: 0.32096222043037415
setp: 3900, Loss: 0.3203885853290558
setp: 4000, Loss: 0.32042109966278076
setp: 4100, Loss: 0.3213408589363098
setp: 4200, Loss: 0.32145464420318604
setp: 4300, Loss: 0.3200027644634247
setp: 4400, Loss: 0.3207268714904785
setp: 4500, Loss: 0.3213712275028229
setp: 4600, Loss: 0.32053399085998535
setp: 4700, Loss: 0.32172125577926636
setp: 4800, Loss: 0.321133017539978
setp: 4900, Loss: 0.32065439224243164
training successfully ended.
validating...
acc: 0.9989224137931034
precision: 0.9978494623655914
recall: 1.0
F_score: 0.9989235737351991
validating...
acc: 0.9407894736842105
precision: 0.8461538461538461
recall: 0.8148148148148148
F_score: 0.830188679245283
model saved.
avg_acc: 0.95, avg_f_score: 0.8839377991001636
-------------subject: 21-------------
==========valence==========
******fold 0******
[284, 324]
training...
setp: 0, Loss: 0.6934038996696472
setp: 100, Loss: 0.6961960792541504
setp: 200, Loss: 0.6872330904006958
setp: 300, Loss: 0.6883494257926941
setp: 400, Loss: 0.6872630715370178
setp: 500, Loss: 0.6912651658058167
setp: 600, Loss: 0.6911975741386414
setp: 700, Loss: 0.6852571368217468
setp: 800, Loss: 0.6912027597427368
setp: 900, Loss: 0.695914089679718
setp: 1000, Loss: 0.6947357058525085
setp: 1100, Loss: 0.6880561709403992
setp: 1200, Loss: 0.6956679821014404
setp: 1300, Loss: 0.6912381052970886
setp: 1400, Loss: 0.6912518739700317
setp: 1500, Loss: 0.6912174224853516
setp: 1600, Loss: 0.6866939067840576
setp: 1700, Loss: 0.6999087929725647
setp: 1800, Loss: 0.6883504986763
setp: 1900, Loss: 0.694959282875061
setp: 2000, Loss: 0.6960548758506775
setp: 2100, Loss: 0.6872742772102356
setp: 2200, Loss: 0.6885268688201904
setp: 2300, Loss: 0.6872402429580688
setp: 2400, Loss: 0.691292941570282
setp: 2500, Loss: 0.6912025213241577
setp: 2600, Loss: 0.6857149600982666
setp: 2700, Loss: 0.6912083625793457
setp: 2800, Loss: 0.6960701942443848
setp: 2900, Loss: 0.6946092844009399
setp: 3000, Loss: 0.6882489323616028
setp: 3100, Loss: 0.6957844495773315
setp: 3200, Loss: 0.6912493109703064
setp: 3300, Loss: 0.6912843585014343
setp: 3400, Loss: 0.6912277936935425
setp: 3500, Loss: 0.6866199970245361
setp: 3600, Loss: 0.6999337673187256
setp: 3700, Loss: 0.6885377764701843
setp: 3800, Loss: 0.6949353218078613
setp: 3900, Loss: 0.6961523294448853
setp: 4000, Loss: 0.6873184442520142
setp: 4100, Loss: 0.6886633038520813
setp: 4200, Loss: 0.687247633934021
setp: 4300, Loss: 0.69130539894104
setp: 4400, Loss: 0.691206693649292
setp: 4500, Loss: 0.685858428478241
setp: 4600, Loss: 0.69120854139328
setp: 4700, Loss: 0.6961091160774231
setp: 4800, Loss: 0.6945688724517822
setp: 4900, Loss: 0.6882798075675964
training successfully ended.
validating...
acc: 0.5328947368421053
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.4934210526315789
precision: 0
recall: 0.0
F_score: 0
******fold 1******
[283, 325]
training...
setp: 0, Loss: 0.6936553716659546
setp: 100, Loss: 0.6906120181083679
setp: 200, Loss: 0.6769658923149109
setp: 300, Loss: 0.647491455078125
setp: 400, Loss: 0.5014975666999817
setp: 500, Loss: 0.42578041553497314
setp: 600, Loss: 0.36658668518066406
setp: 700, Loss: 0.3230597674846649
setp: 800, Loss: 0.3357817828655243
setp: 900, Loss: 0.32348230481147766
setp: 1000, Loss: 0.3224296569824219
setp: 1100, Loss: 0.32217633724212646
setp: 1200, Loss: 0.32405564188957214
setp: 1300, Loss: 0.31931841373443604
setp: 1400, Loss: 0.32481154799461365
setp: 1500, Loss: 0.3613418936729431
setp: 1600, Loss: 0.31925085186958313
setp: 1700, Loss: 0.3189060688018799
setp: 1800, Loss: 0.319044828414917
setp: 1900, Loss: 0.32108333706855774
setp: 2000, Loss: 0.3210923373699188
setp: 2100, Loss: 0.3181508183479309
setp: 2200, Loss: 0.3466915190219879
setp: 2300, Loss: 0.3219371438026428
setp: 2400, Loss: 0.319039911031723
setp: 2500, Loss: 0.3185593783855438
setp: 2600, Loss: 0.3167750835418701
setp: 2700, Loss: 0.31886202096939087
setp: 2800, Loss: 0.3189513385295868
setp: 2900, Loss: 0.5457939505577087
setp: 3000, Loss: 0.3254659175872803
setp: 3100, Loss: 0.33040666580200195
setp: 3200, Loss: 0.3172062635421753
setp: 3300, Loss: 0.3190300166606903
setp: 3400, Loss: 0.3192829191684723
setp: 3500, Loss: 0.31783759593963623
setp: 3600, Loss: 0.4002286195755005
setp: 3700, Loss: 0.3188345432281494
setp: 3800, Loss: 0.31854209303855896
setp: 3900, Loss: 0.31810399889945984
setp: 4000, Loss: 0.31696584820747375
setp: 4100, Loss: 0.3433004915714264
setp: 4200, Loss: 0.3213249742984772
setp: 4300, Loss: 0.3173910975456238
setp: 4400, Loss: 0.3180372714996338
setp: 4500, Loss: 0.31687793135643005
setp: 4600, Loss: 0.419764906167984
setp: 4700, Loss: 0.32575902342796326
setp: 4800, Loss: 0.3175393342971802
setp: 4900, Loss: 0.3165494501590729
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.9342105263157895
recall: 0.9102564102564102
F_score: 0.922077922077922
******fold 2******
[296, 312]
training...
setp: 0, Loss: 0.7163042426109314
setp: 100, Loss: 0.6743336319923401
setp: 200, Loss: 0.6123945116996765
setp: 300, Loss: 0.5019447803497314
setp: 400, Loss: 0.4485265612602234
setp: 500, Loss: 0.3689301908016205
setp: 600, Loss: 0.3899785876274109
setp: 700, Loss: 0.35313156247138977
setp: 800, Loss: 0.3548354208469391
setp: 900, Loss: 0.3224998712539673
setp: 1000, Loss: 0.3240743577480316
setp: 1100, Loss: 0.33209899067878723
setp: 1200, Loss: 0.3258092403411865
setp: 1300, Loss: 0.3189850449562073
setp: 1400, Loss: 0.32016199827194214
setp: 1500, Loss: 0.3190178871154785
setp: 1600, Loss: 0.31916725635528564
setp: 1700, Loss: 0.3201700448989868
setp: 1800, Loss: 0.3195963501930237
setp: 1900, Loss: 0.32034534215927124
setp: 2000, Loss: 0.32729780673980713
setp: 2100, Loss: 0.31692224740982056
setp: 2200, Loss: 0.317902147769928
setp: 2300, Loss: 0.32037797570228577
setp: 2400, Loss: 0.31759899854660034
setp: 2500, Loss: 0.3193540573120117
setp: 2600, Loss: 0.3177385628223419
setp: 2700, Loss: 0.3221028745174408
setp: 2800, Loss: 0.33446094393730164
setp: 2900, Loss: 0.3197005093097687
setp: 3000, Loss: 0.3197017014026642
setp: 3100, Loss: 0.3196355998516083
setp: 3200, Loss: 0.3177264630794525
setp: 3300, Loss: 0.3185654282569885
setp: 3400, Loss: 0.31866544485092163
setp: 3500, Loss: 0.3185679316520691
setp: 3600, Loss: 0.318303644657135
setp: 3700, Loss: 0.3179858922958374
setp: 3800, Loss: 0.3197192847728729
setp: 3900, Loss: 0.3196312189102173
setp: 4000, Loss: 0.3286411762237549
setp: 4100, Loss: 0.32440119981765747
setp: 4200, Loss: 0.3702715039253235
setp: 4300, Loss: 0.3198930025100708
setp: 4400, Loss: 0.32017847895622253
setp: 4500, Loss: 0.31782999634742737
setp: 4600, Loss: 0.3187410235404968
setp: 4700, Loss: 0.31651774048805237
setp: 4800, Loss: 0.3180910050868988
setp: 4900, Loss: 0.3181464374065399
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9078947368421053
precision: 0.8805970149253731
recall: 0.9076923076923077
F_score: 0.8939393939393939
******fold 3******
[291, 317]
training...
setp: 0, Loss: 0.7291343212127686
setp: 100, Loss: 0.6499579548835754
setp: 200, Loss: 0.604486882686615
setp: 300, Loss: 0.5121170878410339
setp: 400, Loss: 0.42736074328422546
setp: 500, Loss: 0.3591901957988739
setp: 600, Loss: 0.33430591225624084
setp: 700, Loss: 0.3273826837539673
setp: 800, Loss: 0.3284270763397217
setp: 900, Loss: 0.32929450273513794
setp: 1000, Loss: 0.34259340167045593
setp: 1100, Loss: 0.3249136805534363
setp: 1200, Loss: 0.31944432854652405
setp: 1300, Loss: 0.3190683126449585
setp: 1400, Loss: 0.3216363787651062
setp: 1500, Loss: 0.31944623589515686
setp: 1600, Loss: 0.3202965259552002
setp: 1700, Loss: 0.3192133605480194
setp: 1800, Loss: 0.31841275095939636
setp: 1900, Loss: 0.3208748996257782
setp: 2000, Loss: 0.3208272159099579
setp: 2100, Loss: 0.3183597922325134
setp: 2200, Loss: 0.3181963860988617
setp: 2300, Loss: 0.32006168365478516
setp: 2400, Loss: 0.3190952241420746
setp: 2500, Loss: 0.3669901192188263
setp: 2600, Loss: 0.31875789165496826
setp: 2700, Loss: 0.3182141184806824
setp: 2800, Loss: 0.31806787848472595
setp: 2900, Loss: 0.3191125988960266
setp: 3000, Loss: 0.3183775544166565
setp: 3100, Loss: 0.3187517523765564
setp: 3200, Loss: 0.31913667917251587
setp: 3300, Loss: 0.3192598819732666
setp: 3400, Loss: 0.3193717896938324
setp: 3500, Loss: 0.31794169545173645
setp: 3600, Loss: 0.3182920515537262
setp: 3700, Loss: 0.3301045000553131
setp: 3800, Loss: 0.3317320942878723
setp: 3900, Loss: 0.31875190138816833
setp: 4000, Loss: 0.31817862391471863
setp: 4100, Loss: 0.3166847825050354
setp: 4200, Loss: 0.31782200932502747
setp: 4300, Loss: 0.3183383047580719
setp: 4400, Loss: 0.3200189173221588
setp: 4500, Loss: 0.31812578439712524
setp: 4600, Loss: 0.3260970115661621
setp: 4700, Loss: 0.3209182322025299
setp: 4800, Loss: 0.32117950916290283
setp: 4900, Loss: 0.31748080253601074
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9013157894736842
precision: 0.8666666666666667
recall: 0.9285714285714286
F_score: 0.896551724137931
******fold 4******
[290, 318]
training...
setp: 0, Loss: 0.6869224905967712
setp: 100, Loss: 0.6948322057723999
setp: 200, Loss: 0.699095606803894
setp: 300, Loss: 0.6914385557174683
setp: 400, Loss: 0.6944364905357361
setp: 500, Loss: 0.6913173198699951
setp: 600, Loss: 0.691429853439331
setp: 700, Loss: 0.6967806816101074
setp: 800, Loss: 0.6912542581558228
setp: 900, Loss: 0.6913718581199646
setp: 1000, Loss: 0.691460132598877
setp: 1100, Loss: 0.6860823035240173
setp: 1200, Loss: 0.6945716142654419
setp: 1300, Loss: 0.6888361573219299
setp: 1400, Loss: 0.6890840530395508
setp: 1500, Loss: 0.6941711902618408
setp: 1600, Loss: 0.6979613304138184
setp: 1700, Loss: 0.6888583898544312
setp: 1800, Loss: 0.6914613842964172
setp: 1900, Loss: 0.6883921027183533
setp: 2000, Loss: 0.6944443583488464
setp: 2100, Loss: 0.6994132995605469
setp: 2200, Loss: 0.6914275884628296
setp: 2300, Loss: 0.6944189667701721
setp: 2400, Loss: 0.6913177371025085
setp: 2500, Loss: 0.6914441585540771
setp: 2600, Loss: 0.6967507004737854
setp: 2700, Loss: 0.6912432312965393
setp: 2800, Loss: 0.6913797855377197
setp: 2900, Loss: 0.6914892196655273
setp: 3000, Loss: 0.6861404180526733
setp: 3100, Loss: 0.6946471333503723
setp: 3200, Loss: 0.6889115571975708
setp: 3300, Loss: 0.6892011165618896
setp: 3400, Loss: 0.6941746473312378
setp: 3500, Loss: 0.6981098651885986
setp: 3600, Loss: 0.6889402270317078
setp: 3700, Loss: 0.6914858222007751
setp: 3800, Loss: 0.6883662939071655
setp: 3900, Loss: 0.6944766640663147
setp: 4000, Loss: 0.699286162853241
setp: 4100, Loss: 0.6914421916007996
setp: 4200, Loss: 0.6944543719291687
setp: 4300, Loss: 0.6913173794746399
setp: 4400, Loss: 0.6914637088775635
setp: 4500, Loss: 0.6967190504074097
setp: 4600, Loss: 0.6912360787391663
setp: 4700, Loss: 0.6913857460021973
setp: 4800, Loss: 0.6915084719657898
setp: 4900, Loss: 0.6861681938171387
training successfully ended.
validating...
acc: 0.5230263157894737
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.5328947368421053
precision: 0
recall: 0.0
F_score: 0
model saved.
avg_acc: 0.7513157894736843, avg_f_score: 0.5425138080310494
==========arousal==========
******fold 0******
[118, 490]
training...
setp: 0, Loss: 0.6932094693183899
setp: 100, Loss: 0.613465428352356
setp: 200, Loss: 0.5277575850486755
setp: 300, Loss: 0.44890186190605164
setp: 400, Loss: 0.38266199827194214
setp: 500, Loss: 0.39637893438339233
setp: 600, Loss: 0.33786526322364807
setp: 700, Loss: 0.33528783917427063
setp: 800, Loss: 0.3272683322429657
setp: 900, Loss: 0.355665385723114
setp: 1000, Loss: 0.3245917856693268
setp: 1100, Loss: 0.31799665093421936
setp: 1200, Loss: 0.3164977431297302
setp: 1300, Loss: 0.31767264008522034
setp: 1400, Loss: 0.3204237222671509
setp: 1500, Loss: 0.3178679943084717
setp: 1600, Loss: 0.3174647092819214
setp: 1700, Loss: 0.44660308957099915
setp: 1800, Loss: 0.3169398009777069
setp: 1900, Loss: 0.31637537479400635
setp: 2000, Loss: 0.31798264384269714
setp: 2100, Loss: 0.3172163665294647
setp: 2200, Loss: 0.3171396553516388
setp: 2300, Loss: 0.3157753050327301
setp: 2400, Loss: 0.31644925475120544
setp: 2500, Loss: 0.3178296387195587
setp: 2600, Loss: 0.3627530336380005
setp: 2700, Loss: 0.3164418041706085
setp: 2800, Loss: 0.31710749864578247
setp: 2900, Loss: 0.31638389825820923
setp: 3000, Loss: 0.31561070680618286
setp: 3100, Loss: 0.31627365946769714
setp: 3200, Loss: 0.31759214401245117
setp: 3300, Loss: 0.3165147304534912
setp: 3400, Loss: 0.315595418214798
setp: 3500, Loss: 0.3160969614982605
setp: 3600, Loss: 0.31607726216316223
setp: 3700, Loss: 0.3158334791660309
setp: 3800, Loss: 0.317702978849411
setp: 3900, Loss: 0.3184086084365845
setp: 4000, Loss: 0.31653833389282227
setp: 4100, Loss: 0.3165978789329529
setp: 4200, Loss: 0.3161035180091858
setp: 4300, Loss: 0.31526443362236023
setp: 4400, Loss: 0.31578171253204346
setp: 4500, Loss: 0.31821581721305847
setp: 4600, Loss: 0.3180943429470062
setp: 4700, Loss: 0.3163425922393799
setp: 4800, Loss: 0.333200067281723
setp: 4900, Loss: 0.31667208671569824
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9671052631578947
precision: 0.9393939393939394
recall: 0.9117647058823529
F_score: 0.9253731343283583
******fold 1******
[120, 488]
training...
setp: 0, Loss: 0.693496823310852
setp: 100, Loss: 0.6127204895019531
setp: 200, Loss: 0.6252143383026123
setp: 300, Loss: 0.5084553360939026
setp: 400, Loss: 0.44920021295547485
setp: 500, Loss: 0.48974621295928955
setp: 600, Loss: 0.4538378119468689
setp: 700, Loss: 0.4161302447319031
setp: 800, Loss: 0.4052204191684723
setp: 900, Loss: 0.40409183502197266
setp: 1000, Loss: 0.37890955805778503
setp: 1100, Loss: 0.3363786041736603
setp: 1200, Loss: 0.32186195254325867
setp: 1300, Loss: 0.32360920310020447
setp: 1400, Loss: 0.3628668785095215
setp: 1500, Loss: 0.3289869427680969
setp: 1600, Loss: 0.3197835087776184
setp: 1700, Loss: 0.34818291664123535
setp: 1800, Loss: 0.31652945280075073
setp: 1900, Loss: 0.3167582154273987
setp: 2000, Loss: 0.3193528950214386
setp: 2100, Loss: 0.31636926531791687
setp: 2200, Loss: 0.3557986617088318
setp: 2300, Loss: 0.31503400206565857
setp: 2400, Loss: 0.31952589750289917
setp: 2500, Loss: 0.31777843832969666
setp: 2600, Loss: 0.3202369511127472
setp: 2700, Loss: 0.32858097553253174
setp: 2800, Loss: 0.3182694613933563
setp: 2900, Loss: 0.31717634201049805
setp: 3000, Loss: 0.3351374864578247
setp: 3100, Loss: 0.31608861684799194
setp: 3200, Loss: 0.31623634696006775
setp: 3300, Loss: 0.3167230486869812
setp: 3400, Loss: 0.31654226779937744
setp: 3500, Loss: 0.316298246383667
setp: 3600, Loss: 0.3181626796722412
setp: 3700, Loss: 0.3182929754257202
setp: 3800, Loss: 0.31951117515563965
setp: 3900, Loss: 0.318792462348938
setp: 4000, Loss: 0.31733599305152893
setp: 4100, Loss: 0.3171074092388153
setp: 4200, Loss: 0.31710878014564514
setp: 4300, Loss: 0.3160546123981476
setp: 4400, Loss: 0.318164587020874
setp: 4500, Loss: 0.31859585642814636
setp: 4600, Loss: 0.3191191256046295
setp: 4700, Loss: 0.36066582798957825
setp: 4800, Loss: 0.3344905972480774
setp: 4900, Loss: 0.3783631920814514
training successfully ended.
validating...
acc: 0.9969262295081968
precision: 0.9938900203665988
recall: 1.0
F_score: 0.9969356486210419
validating...
acc: 0.9868421052631579
precision: 0.96875
recall: 0.96875
F_score: 0.96875
******fold 2******
[129, 479]
training...
setp: 0, Loss: 0.7111741304397583
setp: 100, Loss: 0.6843096017837524
setp: 200, Loss: 0.5484641194343567
setp: 300, Loss: 0.3910382390022278
setp: 400, Loss: 0.41674479842185974
setp: 500, Loss: 0.38171517848968506
setp: 600, Loss: 0.3612992465496063
setp: 700, Loss: 0.3454079031944275
setp: 800, Loss: 0.3320348858833313
setp: 900, Loss: 0.3257202208042145
setp: 1000, Loss: 0.3485063314437866
setp: 1100, Loss: 0.31899720430374146
setp: 1200, Loss: 0.3184674084186554
setp: 1300, Loss: 0.3190106153488159
setp: 1400, Loss: 0.31672728061676025
setp: 1500, Loss: 0.3194611966609955
setp: 1600, Loss: 0.31800583004951477
setp: 1700, Loss: 0.31948161125183105
setp: 1800, Loss: 0.3589015305042267
setp: 1900, Loss: 0.31659412384033203
setp: 2000, Loss: 0.3167407810688019
setp: 2100, Loss: 0.31712210178375244
setp: 2200, Loss: 0.3172929883003235
setp: 2300, Loss: 0.3165663778781891
setp: 2400, Loss: 0.3175891041755676
setp: 2500, Loss: 0.3167206943035126
setp: 2600, Loss: 0.31684115529060364
setp: 2700, Loss: 0.317806601524353
setp: 2800, Loss: 0.31699106097221375
setp: 2900, Loss: 0.3169221878051758
setp: 3000, Loss: 0.3777786195278168
setp: 3100, Loss: 0.31716570258140564
setp: 3200, Loss: 0.31663277745246887
setp: 3300, Loss: 0.3172943592071533
setp: 3400, Loss: 0.3166457712650299
setp: 3500, Loss: 0.31662359833717346
setp: 3600, Loss: 0.3178957998752594
setp: 3700, Loss: 0.3166346549987793
setp: 3800, Loss: 0.3171197772026062
setp: 3900, Loss: 0.31781792640686035
setp: 4000, Loss: 0.3168564736843109
setp: 4100, Loss: 0.3168610632419586
setp: 4200, Loss: 0.36846408247947693
setp: 4300, Loss: 0.31614360213279724
setp: 4400, Loss: 0.3161357343196869
setp: 4500, Loss: 0.3190242648124695
setp: 4600, Loss: 0.31746140122413635
setp: 4700, Loss: 0.31720083951950073
setp: 4800, Loss: 0.31803804636001587
setp: 4900, Loss: 0.31751397252082825
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9539473684210527
precision: 0.7666666666666667
recall: 1.0
F_score: 0.8679245283018869
******fold 3******
[113, 495]
training...
setp: 0, Loss: 0.718820333480835
setp: 100, Loss: 0.647900402545929
setp: 200, Loss: 0.5415497422218323
setp: 300, Loss: 0.40663909912109375
setp: 400, Loss: 0.3664422333240509
setp: 500, Loss: 0.3861878216266632
setp: 600, Loss: 0.35499876737594604
setp: 700, Loss: 0.32255351543426514
setp: 800, Loss: 0.32192274928092957
setp: 900, Loss: 0.32049456238746643
setp: 1000, Loss: 0.31869032979011536
setp: 1100, Loss: 0.31816399097442627
setp: 1200, Loss: 0.3223898708820343
setp: 1300, Loss: 0.3183034360408783
setp: 1400, Loss: 0.33779457211494446
setp: 1500, Loss: 0.31634020805358887
setp: 1600, Loss: 0.31815433502197266
setp: 1700, Loss: 0.31656578183174133
setp: 1800, Loss: 0.3161965608596802
setp: 1900, Loss: 0.31630682945251465
setp: 2000, Loss: 0.31837016344070435
setp: 2100, Loss: 0.31632688641548157
setp: 2200, Loss: 0.31732410192489624
setp: 2300, Loss: 0.31726154685020447
setp: 2400, Loss: 0.31732505559921265
setp: 2500, Loss: 0.3264818489551544
setp: 2600, Loss: 0.31684544682502747
setp: 2700, Loss: 0.3180527985095978
setp: 2800, Loss: 0.32343006134033203
setp: 2900, Loss: 0.31627050042152405
setp: 3000, Loss: 0.31597140431404114
setp: 3100, Loss: 0.31880417466163635
setp: 3200, Loss: 0.3160982131958008
setp: 3300, Loss: 0.31649690866470337
setp: 3400, Loss: 0.31811076402664185
setp: 3500, Loss: 0.3167266249656677
setp: 3600, Loss: 0.3171730637550354
setp: 3700, Loss: 0.31799638271331787
setp: 3800, Loss: 0.3188798427581787
setp: 3900, Loss: 0.31662505865097046
setp: 4000, Loss: 0.3174990713596344
setp: 4100, Loss: 0.3189534842967987
setp: 4200, Loss: 0.31810662150382996
setp: 4300, Loss: 0.3158719837665558
setp: 4400, Loss: 0.318983256816864
setp: 4500, Loss: 0.3180827796459198
setp: 4600, Loss: 0.3161967396736145
setp: 4700, Loss: 0.3170997202396393
setp: 4800, Loss: 0.31669673323631287
setp: 4900, Loss: 0.3163982033729553
training successfully ended.
validating...
acc: 0.998989898989899
precision: 0.9979838709677419
recall: 1.0
F_score: 0.9989909182643794
validating...
acc: 0.9802631578947368
precision: 0.95
recall: 0.9743589743589743
F_score: 0.9620253164556962
******fold 4******
[128, 480]
training...
setp: 0, Loss: 0.7015687227249146
setp: 100, Loss: 0.6105740666389465
setp: 200, Loss: 0.48431411385536194
setp: 300, Loss: 0.5041961669921875
setp: 400, Loss: 0.5043935179710388
setp: 500, Loss: 0.41787776350975037
setp: 600, Loss: 0.4085875153541565
setp: 700, Loss: 0.41114309430122375
setp: 800, Loss: 0.32413825392723083
setp: 900, Loss: 0.32665666937828064
setp: 1000, Loss: 0.3242323100566864
setp: 1100, Loss: 0.31902050971984863
setp: 1200, Loss: 0.38009610772132874
setp: 1300, Loss: 0.31975817680358887
setp: 1400, Loss: 0.3173145651817322
setp: 1500, Loss: 0.3194095194339752
setp: 1600, Loss: 0.31857621669769287
setp: 1700, Loss: 0.317290723323822
setp: 1800, Loss: 0.3171885013580322
setp: 1900, Loss: 0.3194348216056824
setp: 2000, Loss: 0.31648802757263184
setp: 2100, Loss: 0.31784680485725403
setp: 2200, Loss: 0.31856396794319153
setp: 2300, Loss: 0.31674641370773315
setp: 2400, Loss: 0.3824549913406372
setp: 2500, Loss: 0.32509738206863403
setp: 2600, Loss: 0.316459596157074
setp: 2700, Loss: 0.3165202736854553
setp: 2800, Loss: 0.3174566626548767
setp: 2900, Loss: 0.316417396068573
setp: 3000, Loss: 0.31664276123046875
setp: 3100, Loss: 0.3172905743122101
setp: 3200, Loss: 0.31666409969329834
setp: 3300, Loss: 0.31700336933135986
setp: 3400, Loss: 0.3339099884033203
setp: 3500, Loss: 0.31753578782081604
setp: 3600, Loss: 0.3165396451950073
setp: 3700, Loss: 0.3169998228549957
setp: 3800, Loss: 0.3165379762649536
setp: 3900, Loss: 0.3161914050579071
setp: 4000, Loss: 0.31676679849624634
setp: 4100, Loss: 0.31648460030555725
setp: 4200, Loss: 0.3166424334049225
setp: 4300, Loss: 0.3171261250972748
setp: 4400, Loss: 0.3163689076900482
setp: 4500, Loss: 0.3165813088417053
setp: 4600, Loss: 0.3777118921279907
setp: 4700, Loss: 0.3339307904243469
setp: 4800, Loss: 0.36615175008773804
setp: 4900, Loss: 0.31621700525283813
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9736842105263158
precision: 0.9545454545454546
recall: 0.875
F_score: 0.9130434782608695
model saved.
avg_acc: 0.9723684210526315, avg_f_score: 0.9274232914693622
-------------subject: 22-------------
==========valence==========
******fold 0******
[309, 299]
training...
setp: 0, Loss: 0.6921518445014954
setp: 100, Loss: 0.6865509152412415
setp: 200, Loss: 0.5458990931510925
setp: 300, Loss: 0.47240149974823
setp: 400, Loss: 0.4694792628288269
setp: 500, Loss: 0.37157467007637024
setp: 600, Loss: 0.336716890335083
setp: 700, Loss: 0.36019736528396606
setp: 800, Loss: 0.35299357771873474
setp: 900, Loss: 0.3279217779636383
setp: 1000, Loss: 0.3273552656173706
setp: 1100, Loss: 0.3235298991203308
setp: 1200, Loss: 0.3225879967212677
setp: 1300, Loss: 0.33717769384384155
setp: 1400, Loss: 0.35341113805770874
setp: 1500, Loss: 0.31966081261634827
setp: 1600, Loss: 0.36497995257377625
setp: 1700, Loss: 0.3220372498035431
setp: 1800, Loss: 0.3212890028953552
setp: 1900, Loss: 0.3515446186065674
setp: 2000, Loss: 0.3203067481517792
setp: 2100, Loss: 0.3316045105457306
setp: 2200, Loss: 0.3215961754322052
setp: 2300, Loss: 0.3208361566066742
setp: 2400, Loss: 0.3286794424057007
setp: 2500, Loss: 0.33034905791282654
setp: 2600, Loss: 0.3223218321800232
setp: 2700, Loss: 0.3249058425426483
setp: 2800, Loss: 0.3199112117290497
setp: 2900, Loss: 0.3201563358306885
setp: 3000, Loss: 0.31925225257873535
setp: 3100, Loss: 0.3198623061180115
setp: 3200, Loss: 0.3204861581325531
setp: 3300, Loss: 0.3514411449432373
setp: 3400, Loss: 0.31921541690826416
setp: 3500, Loss: 0.3202463686466217
setp: 3600, Loss: 0.319766640663147
setp: 3700, Loss: 0.3208359479904175
setp: 3800, Loss: 0.3496739864349365
setp: 3900, Loss: 0.3199915885925293
setp: 4000, Loss: 0.37474048137664795
setp: 4100, Loss: 0.3409011960029602
setp: 4200, Loss: 0.3408152759075165
setp: 4300, Loss: 0.32220250368118286
setp: 4400, Loss: 0.3233685791492462
setp: 4500, Loss: 0.3301266133785248
setp: 4600, Loss: 0.39635613560676575
setp: 4700, Loss: 0.32719695568084717
setp: 4800, Loss: 0.3360779881477356
setp: 4900, Loss: 0.32794275879859924
training successfully ended.
validating...
acc: 0.975328947368421
precision: 0.9966216216216216
recall: 0.9546925566343042
F_score: 0.975206611570248
validating...
acc: 0.9473684210526315
precision: 0.9565217391304348
recall: 0.9295774647887324
F_score: 0.9428571428571428
******fold 1******
[298, 310]
training...
setp: 0, Loss: 0.6820092797279358
setp: 100, Loss: 0.6327188014984131
setp: 200, Loss: 0.5143615007400513
setp: 300, Loss: 0.5016284584999084
setp: 400, Loss: 0.5230517387390137
setp: 500, Loss: 0.44140303134918213
setp: 600, Loss: 0.47171562910079956
setp: 700, Loss: 0.3743959069252014
setp: 800, Loss: 0.3935657739639282
setp: 900, Loss: 0.3682011663913727
setp: 1000, Loss: 0.4071879982948303
setp: 1100, Loss: 0.36236080527305603
setp: 1200, Loss: 0.4141903519630432
setp: 1300, Loss: 0.34141379594802856
setp: 1400, Loss: 0.36476677656173706
setp: 1500, Loss: 0.3623546063899994
setp: 1600, Loss: 0.33899635076522827
setp: 1700, Loss: 0.3526994585990906
setp: 1800, Loss: 0.3482798635959625
setp: 1900, Loss: 0.35225582122802734
setp: 2000, Loss: 0.3276522159576416
setp: 2100, Loss: 0.352588951587677
setp: 2200, Loss: 0.348139226436615
setp: 2300, Loss: 0.3212376832962036
setp: 2400, Loss: 0.31654462218284607
setp: 2500, Loss: 0.32043856382369995
setp: 2600, Loss: 0.3563360571861267
setp: 2700, Loss: 0.33153900504112244
setp: 2800, Loss: 0.321601003408432
setp: 2900, Loss: 0.35017383098602295
setp: 3000, Loss: 0.31754758954048157
setp: 3100, Loss: 0.32008275389671326
setp: 3200, Loss: 0.3157469928264618
setp: 3300, Loss: 0.3542513847351074
setp: 3400, Loss: 0.3747882843017578
setp: 3500, Loss: 0.32505282759666443
setp: 3600, Loss: 0.32129964232444763
setp: 3700, Loss: 0.34821850061416626
setp: 3800, Loss: 0.37735047936439514
setp: 3900, Loss: 0.4698481857776642
setp: 4000, Loss: 0.35214290022850037
setp: 4100, Loss: 0.34908539056777954
setp: 4200, Loss: 0.3231285810470581
setp: 4300, Loss: 0.3163571059703827
setp: 4400, Loss: 0.3178180754184723
setp: 4500, Loss: 0.34871745109558105
setp: 4600, Loss: 0.3462114632129669
setp: 4700, Loss: 0.3201138973236084
setp: 4800, Loss: 0.343778133392334
setp: 4900, Loss: 0.34294334053993225
training successfully ended.
validating...
acc: 0.9901315789473685
precision: 1.0
recall: 0.9798657718120806
F_score: 0.9898305084745762
validating...
acc: 0.9539473684210527
precision: 1.0
recall: 0.9146341463414634
F_score: 0.9554140127388536
******fold 2******
[313, 295]
training...
setp: 0, Loss: 0.6913671493530273
setp: 100, Loss: 0.7078580856323242
setp: 200, Loss: 0.5941770076751709
setp: 300, Loss: 0.4785262644290924
setp: 400, Loss: 0.46161139011383057
setp: 500, Loss: 0.3696480691432953
setp: 600, Loss: 0.3647913336753845
setp: 700, Loss: 0.37881577014923096
setp: 800, Loss: 0.3310803771018982
setp: 900, Loss: 0.3245493769645691
setp: 1000, Loss: 0.34053489565849304
setp: 1100, Loss: 0.32427114248275757
setp: 1200, Loss: 0.3225826621055603
setp: 1300, Loss: 0.31946486234664917
setp: 1400, Loss: 0.3204106092453003
setp: 1500, Loss: 0.337534636259079
setp: 1600, Loss: 0.34436044096946716
setp: 1700, Loss: 0.31962740421295166
setp: 1800, Loss: 0.3251248300075531
setp: 1900, Loss: 0.324317067861557
setp: 2000, Loss: 0.32038745284080505
setp: 2100, Loss: 0.3200603425502777
setp: 2200, Loss: 0.31870853900909424
setp: 2300, Loss: 0.32518890500068665
setp: 2400, Loss: 0.3181792199611664
setp: 2500, Loss: 0.34948572516441345
setp: 2600, Loss: 0.32646286487579346
setp: 2700, Loss: 0.32069918513298035
setp: 2800, Loss: 0.3187287449836731
setp: 2900, Loss: 0.33090266585350037
setp: 3000, Loss: 0.33683672547340393
setp: 3100, Loss: 0.3202475905418396
setp: 3200, Loss: 0.32058969140052795
setp: 3300, Loss: 0.319235235452652
setp: 3400, Loss: 0.3194165825843811
setp: 3500, Loss: 0.3190349340438843
setp: 3600, Loss: 0.31835973262786865
setp: 3700, Loss: 0.3213949501514435
setp: 3800, Loss: 0.3208081126213074
setp: 3900, Loss: 0.3209002614021301
setp: 4000, Loss: 0.3186165392398834
setp: 4100, Loss: 0.3188786506652832
setp: 4200, Loss: 0.32054340839385986
setp: 4300, Loss: 0.3182690143585205
setp: 4400, Loss: 0.34951820969581604
setp: 4500, Loss: 0.48519206047058105
setp: 4600, Loss: 0.32293611764907837
setp: 4700, Loss: 0.3291635513305664
setp: 4800, Loss: 0.32327479124069214
setp: 4900, Loss: 0.32014262676239014
training successfully ended.
validating...
acc: 0.9868421052631579
precision: 0.9841269841269841
recall: 0.9904153354632588
F_score: 0.9872611464968153
validating...
acc: 0.9276315789473685
precision: 0.9
recall: 0.9402985074626866
F_score: 0.9197080291970803
******fold 3******
[301, 307]
training...
setp: 0, Loss: 0.6885913014411926
setp: 100, Loss: 0.6916442513465881
setp: 200, Loss: 0.6187556982040405
setp: 300, Loss: 0.43914538621902466
setp: 400, Loss: 0.396772176027298
setp: 500, Loss: 0.37294802069664
setp: 600, Loss: 0.38028934597969055
setp: 700, Loss: 0.35415396094322205
setp: 800, Loss: 0.3380677103996277
setp: 900, Loss: 0.3263246715068817
setp: 1000, Loss: 0.33859071135520935
setp: 1100, Loss: 0.331426739692688
setp: 1200, Loss: 0.3199784755706787
setp: 1300, Loss: 0.3200418949127197
setp: 1400, Loss: 0.3235071003437042
setp: 1500, Loss: 0.325458288192749
setp: 1600, Loss: 0.3444848358631134
setp: 1700, Loss: 0.32213249802589417
setp: 1800, Loss: 0.32040783762931824
setp: 1900, Loss: 0.32175490260124207
setp: 2000, Loss: 0.3242918848991394
setp: 2100, Loss: 0.31990084052085876
setp: 2200, Loss: 0.3199382722377777
setp: 2300, Loss: 0.35051050782203674
setp: 2400, Loss: 0.32305842638015747
setp: 2500, Loss: 0.3378530740737915
setp: 2600, Loss: 0.3559744954109192
setp: 2700, Loss: 0.32138320803642273
setp: 2800, Loss: 0.33193981647491455
setp: 2900, Loss: 0.32067957520484924
setp: 3000, Loss: 0.31925806403160095
setp: 3100, Loss: 0.31837359070777893
setp: 3200, Loss: 0.3248530924320221
setp: 3300, Loss: 0.32107457518577576
setp: 3400, Loss: 0.3185458183288574
setp: 3500, Loss: 0.31964483857154846
setp: 3600, Loss: 0.3193935751914978
setp: 3700, Loss: 0.32037094235420227
setp: 3800, Loss: 0.32080087065696716
setp: 3900, Loss: 0.3216959536075592
setp: 4000, Loss: 0.32526513934135437
setp: 4100, Loss: 0.3289782404899597
setp: 4200, Loss: 0.35791918635368347
setp: 4300, Loss: 0.32249799370765686
setp: 4400, Loss: 0.32895204424858093
setp: 4500, Loss: 0.3208915591239929
setp: 4600, Loss: 0.318652480840683
setp: 4700, Loss: 0.3183560073375702
setp: 4800, Loss: 0.32050052285194397
setp: 4900, Loss: 0.31982821226119995
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9966777408637874
F_score: 0.9983361064891847
validating...
acc: 0.9407894736842105
precision: 0.9487179487179487
recall: 0.9367088607594937
F_score: 0.9426751592356688
******fold 4******
[299, 309]
training...
setp: 0, Loss: 0.7000101208686829
setp: 100, Loss: 0.6589160561561584
setp: 200, Loss: 0.5669916868209839
setp: 300, Loss: 0.5585506558418274
setp: 400, Loss: 0.5784674882888794
setp: 500, Loss: 0.4921480119228363
setp: 600, Loss: 0.5168002843856812
setp: 700, Loss: 0.4158964157104492
setp: 800, Loss: 0.46251359581947327
setp: 900, Loss: 0.3814121186733246
setp: 1000, Loss: 0.36001014709472656
setp: 1100, Loss: 0.353074312210083
setp: 1200, Loss: 0.40531036257743835
setp: 1300, Loss: 0.34002685546875
setp: 1400, Loss: 0.3651643395423889
setp: 1500, Loss: 0.35456934571266174
setp: 1600, Loss: 0.32391130924224854
setp: 1700, Loss: 0.3998929262161255
setp: 1800, Loss: 0.3309311270713806
setp: 1900, Loss: 0.3289059102535248
setp: 2000, Loss: 0.33142542839050293
setp: 2100, Loss: 0.32845592498779297
setp: 2200, Loss: 0.3386727571487427
setp: 2300, Loss: 0.31781676411628723
setp: 2400, Loss: 0.3221622705459595
setp: 2500, Loss: 0.33382153511047363
setp: 2600, Loss: 0.31665846705436707
setp: 2700, Loss: 0.33359646797180176
setp: 2800, Loss: 0.31840571761131287
setp: 2900, Loss: 0.3176221251487732
setp: 3000, Loss: 0.318337619304657
setp: 3100, Loss: 0.3230833113193512
setp: 3200, Loss: 0.318667471408844
setp: 3300, Loss: 0.3220263719558716
setp: 3400, Loss: 0.3176400065422058
setp: 3500, Loss: 0.31724268198013306
setp: 3600, Loss: 0.32070788741111755
setp: 3700, Loss: 0.31954777240753174
setp: 3800, Loss: 0.3222847282886505
setp: 3900, Loss: 0.3197600841522217
setp: 4000, Loss: 0.4091677665710449
setp: 4100, Loss: 0.3421739339828491
setp: 4200, Loss: 0.3342001736164093
setp: 4300, Loss: 0.31769856810569763
setp: 4400, Loss: 0.3208041191101074
setp: 4500, Loss: 0.3183431327342987
setp: 4600, Loss: 0.3188858926296234
setp: 4700, Loss: 0.3349979519844055
setp: 4800, Loss: 0.3656107485294342
setp: 4900, Loss: 0.31789058446884155
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.9493670886075949
recall: 0.9259259259259259
F_score: 0.9375000000000001
model saved.
avg_acc: 0.9407894736842106, avg_f_score: 0.9396308688057491
==========arousal==========
******fold 0******
[226, 382]
training...
setp: 0, Loss: 0.7688372731208801
setp: 100, Loss: 0.6614286303520203
setp: 200, Loss: 0.6511850953102112
setp: 300, Loss: 0.5688033103942871
setp: 400, Loss: 0.48526275157928467
setp: 500, Loss: 0.4780188798904419
setp: 600, Loss: 0.4577575922012329
setp: 700, Loss: 0.4312804639339447
setp: 800, Loss: 0.5202663540840149
setp: 900, Loss: 0.476387083530426
setp: 1000, Loss: 0.3748963177204132
setp: 1100, Loss: 0.3560750186443329
setp: 1200, Loss: 0.4314033091068268
setp: 1300, Loss: 0.4742875099182129
setp: 1400, Loss: 0.4712485671043396
setp: 1500, Loss: 0.3870159089565277
setp: 1600, Loss: 0.4384397566318512
setp: 1700, Loss: 0.3824138641357422
setp: 1800, Loss: 0.42118003964424133
setp: 1900, Loss: 0.4191283881664276
setp: 2000, Loss: 0.41363009810447693
setp: 2100, Loss: 0.3827519118785858
setp: 2200, Loss: 0.41154417395591736
setp: 2300, Loss: 0.3829365670681
setp: 2400, Loss: 0.38059234619140625
setp: 2500, Loss: 0.4238288700580597
setp: 2600, Loss: 0.38022762537002563
setp: 2700, Loss: 0.3820066750049591
setp: 2800, Loss: 0.4186469316482544
setp: 2900, Loss: 0.3490285873413086
setp: 3000, Loss: 0.3464355170726776
setp: 3100, Loss: 0.38783103227615356
setp: 3200, Loss: 0.3497825562953949
setp: 3300, Loss: 0.38139721751213074
setp: 3400, Loss: 0.3490197956562042
setp: 3500, Loss: 0.4143737852573395
setp: 3600, Loss: 0.35311147570610046
setp: 3700, Loss: 0.38539114594459534
setp: 3800, Loss: 0.4081380367279053
setp: 3900, Loss: 0.3910526633262634
setp: 4000, Loss: 0.4166017472743988
setp: 4100, Loss: 0.4104926586151123
setp: 4200, Loss: 0.38740628957748413
setp: 4300, Loss: 0.3533361852169037
setp: 4400, Loss: 0.3800961971282959
setp: 4500, Loss: 0.3796149492263794
setp: 4600, Loss: 0.38346433639526367
setp: 4700, Loss: 0.4120011031627655
setp: 4800, Loss: 0.34800970554351807
setp: 4900, Loss: 0.35946616530418396
training successfully ended.
validating...
acc: 0.9391447368421053
precision: 0.9748743718592965
recall: 0.8584070796460177
F_score: 0.9129411764705883
validating...
acc: 0.881578947368421
precision: 0.9019607843137255
recall: 0.7796610169491526
F_score: 0.8363636363636364
******fold 1******
[227, 381]
training...
setp: 0, Loss: 0.6848580837249756
setp: 100, Loss: 0.6604499220848083
setp: 200, Loss: 0.6118236780166626
setp: 300, Loss: 0.5206776261329651
setp: 400, Loss: 0.40319183468818665
setp: 500, Loss: 0.3842889666557312
setp: 600, Loss: 0.36088624596595764
setp: 700, Loss: 0.33806905150413513
setp: 800, Loss: 0.3628599941730499
setp: 900, Loss: 0.32495537400245667
setp: 1000, Loss: 0.332328200340271
setp: 1100, Loss: 0.3601153790950775
setp: 1200, Loss: 0.3291793167591095
setp: 1300, Loss: 0.32561057806015015
setp: 1400, Loss: 0.3224483132362366
setp: 1500, Loss: 0.31947198510169983
setp: 1600, Loss: 0.3210573196411133
setp: 1700, Loss: 0.32081347703933716
setp: 1800, Loss: 0.3201235234737396
setp: 1900, Loss: 0.32157814502716064
setp: 2000, Loss: 0.31990882754325867
setp: 2100, Loss: 0.3218085765838623
setp: 2200, Loss: 0.3534078598022461
setp: 2300, Loss: 0.3568294048309326
setp: 2400, Loss: 0.31976982951164246
setp: 2500, Loss: 0.3197464942932129
setp: 2600, Loss: 0.3212946653366089
setp: 2700, Loss: 0.3201325535774231
setp: 2800, Loss: 0.3203100860118866
setp: 2900, Loss: 0.32101190090179443
setp: 3000, Loss: 0.32278260588645935
setp: 3100, Loss: 0.32077667117118835
setp: 3200, Loss: 0.3651888966560364
setp: 3300, Loss: 0.32295820116996765
setp: 3400, Loss: 0.31922975182533264
setp: 3500, Loss: 0.3186367154121399
setp: 3600, Loss: 0.3207147419452667
setp: 3700, Loss: 0.32721319794654846
setp: 3800, Loss: 0.3220675587654114
setp: 3900, Loss: 0.3229671120643616
setp: 4000, Loss: 0.3328371047973633
setp: 4100, Loss: 0.3195422291755676
setp: 4200, Loss: 0.3187839388847351
setp: 4300, Loss: 0.3191538453102112
setp: 4400, Loss: 0.3192801773548126
setp: 4500, Loss: 0.3206484913825989
setp: 4600, Loss: 0.3201143443584442
setp: 4700, Loss: 0.3206651508808136
setp: 4800, Loss: 0.3208143711090088
setp: 4900, Loss: 0.4291589558124542
training successfully ended.
validating...
acc: 0.9835526315789473
precision: 1.0
recall: 0.9559471365638766
F_score: 0.9774774774774774
validating...
acc: 0.9407894736842105
precision: 1.0
recall: 0.8448275862068966
F_score: 0.9158878504672897
******fold 2******
[238, 370]
training...
setp: 0, Loss: 0.6755083799362183
setp: 100, Loss: 0.6355462074279785
setp: 200, Loss: 0.6121056079864502
setp: 300, Loss: 0.5258274674415588
setp: 400, Loss: 0.5062510967254639
setp: 500, Loss: 0.3781043291091919
setp: 600, Loss: 0.4013032913208008
setp: 700, Loss: 0.41293540596961975
setp: 800, Loss: 0.3825583755970001
setp: 900, Loss: 0.36943572759628296
setp: 1000, Loss: 0.33754733204841614
setp: 1100, Loss: 0.32477977871894836
setp: 1200, Loss: 0.3233598470687866
setp: 1300, Loss: 0.3232511878013611
setp: 1400, Loss: 0.3240223228931427
setp: 1500, Loss: 0.32299989461898804
setp: 1600, Loss: 0.3378716707229614
setp: 1700, Loss: 0.32737359404563904
setp: 1800, Loss: 0.34362050890922546
setp: 1900, Loss: 0.32455629110336304
setp: 2000, Loss: 0.3199527859687805
setp: 2100, Loss: 0.32188645005226135
setp: 2200, Loss: 0.32072898745536804
setp: 2300, Loss: 0.32030990719795227
setp: 2400, Loss: 0.32074737548828125
setp: 2500, Loss: 0.34918639063835144
setp: 2600, Loss: 0.32030487060546875
setp: 2700, Loss: 0.32152044773101807
setp: 2800, Loss: 0.3213210999965668
setp: 2900, Loss: 0.7185416221618652
setp: 3000, Loss: 0.405510276556015
setp: 3100, Loss: 0.3361903131008148
setp: 3200, Loss: 0.32282471656799316
setp: 3300, Loss: 0.3601827323436737
setp: 3400, Loss: 0.3244800567626953
setp: 3500, Loss: 0.32096266746520996
setp: 3600, Loss: 0.3191626965999603
setp: 3700, Loss: 0.31739097833633423
setp: 3800, Loss: 0.319338321685791
setp: 3900, Loss: 0.31964898109436035
setp: 4000, Loss: 0.32079416513442993
setp: 4100, Loss: 0.31921812891960144
setp: 4200, Loss: 0.31894877552986145
setp: 4300, Loss: 0.3190123736858368
setp: 4400, Loss: 0.34814453125
setp: 4500, Loss: 0.3617280125617981
setp: 4600, Loss: 0.3229016661643982
setp: 4700, Loss: 0.35206013917922974
setp: 4800, Loss: 0.322503000497818
setp: 4900, Loss: 0.3181982934474945
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9957983193277311
F_score: 0.9978947368421053
validating...
acc: 0.9671052631578947
precision: 0.9375
recall: 0.9574468085106383
F_score: 0.9473684210526315
******fold 3******
[217, 391]
training...
setp: 0, Loss: 0.629203736782074
setp: 100, Loss: 0.6623334288597107
setp: 200, Loss: 0.6925749182701111
setp: 300, Loss: 0.6098241209983826
setp: 400, Loss: 0.5714970231056213
setp: 500, Loss: 0.4774826765060425
setp: 600, Loss: 0.4225488603115082
setp: 700, Loss: 0.34763598442077637
setp: 800, Loss: 0.41205716133117676
setp: 900, Loss: 0.3500644862651825
setp: 1000, Loss: 0.374303936958313
setp: 1100, Loss: 0.35745134949684143
setp: 1200, Loss: 0.37728187441825867
setp: 1300, Loss: 0.32061171531677246
setp: 1400, Loss: 0.3272194564342499
setp: 1500, Loss: 0.360326886177063
setp: 1600, Loss: 0.3482823073863983
setp: 1700, Loss: 0.31874707341194153
setp: 1800, Loss: 0.3405474126338959
setp: 1900, Loss: 0.3516886830329895
setp: 2000, Loss: 0.3202921152114868
setp: 2100, Loss: 0.34354260563850403
setp: 2200, Loss: 0.31904053688049316
setp: 2300, Loss: 0.3190065026283264
setp: 2400, Loss: 0.32290440797805786
setp: 2500, Loss: 0.32074981927871704
setp: 2600, Loss: 0.31954678893089294
setp: 2700, Loss: 0.3184775114059448
setp: 2800, Loss: 0.31951266527175903
setp: 2900, Loss: 0.3236894905567169
setp: 3000, Loss: 0.32691967487335205
setp: 3100, Loss: 0.3276568055152893
setp: 3200, Loss: 0.3210172951221466
setp: 3300, Loss: 0.32437580823898315
setp: 3400, Loss: 0.3194454312324524
setp: 3500, Loss: 0.3182019591331482
setp: 3600, Loss: 0.3180411756038666
setp: 3700, Loss: 0.32139894366264343
setp: 3800, Loss: 0.3201930522918701
setp: 3900, Loss: 0.3204349875450134
setp: 4000, Loss: 0.320845365524292
setp: 4100, Loss: 0.3199487626552582
setp: 4200, Loss: 0.32014772295951843
setp: 4300, Loss: 0.32019203901290894
setp: 4400, Loss: 0.3195591866970062
setp: 4500, Loss: 0.3187665641307831
setp: 4600, Loss: 0.32030075788497925
setp: 4700, Loss: 0.32073915004730225
setp: 4800, Loss: 0.33522728085517883
setp: 4900, Loss: 0.32666924595832825
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9953917050691244
F_score: 0.997690531177829
validating...
acc: 0.9144736842105263
precision: 0.9661016949152542
recall: 0.8382352941176471
F_score: 0.8976377952755905
******fold 4******
[232, 376]
training...
setp: 0, Loss: 0.7808879017829895
setp: 100, Loss: 0.6763588786125183
setp: 200, Loss: 0.6513491272926331
setp: 300, Loss: 0.5244026184082031
setp: 400, Loss: 0.42431679368019104
setp: 500, Loss: 0.3789553940296173
setp: 600, Loss: 0.37235864996910095
setp: 700, Loss: 0.37691980600357056
setp: 800, Loss: 0.3831653296947479
setp: 900, Loss: 0.39304280281066895
setp: 1000, Loss: 0.3910718262195587
setp: 1100, Loss: 0.3313630521297455
setp: 1200, Loss: 0.32558444142341614
setp: 1300, Loss: 0.3554057776927948
setp: 1400, Loss: 0.35029298067092896
setp: 1500, Loss: 0.33185335993766785
setp: 1600, Loss: 0.32415929436683655
setp: 1700, Loss: 0.3387400209903717
setp: 1800, Loss: 0.32207709550857544
setp: 1900, Loss: 0.322774201631546
setp: 2000, Loss: 0.3202868103981018
setp: 2100, Loss: 0.321367472410202
setp: 2200, Loss: 0.32144695520401
setp: 2300, Loss: 0.342637836933136
setp: 2400, Loss: 0.32003575563430786
setp: 2500, Loss: 0.3203202486038208
setp: 2600, Loss: 0.32105597853660583
setp: 2700, Loss: 0.3212492763996124
setp: 2800, Loss: 0.3223351836204529
setp: 2900, Loss: 0.3236713409423828
setp: 3000, Loss: 0.32227635383605957
setp: 3100, Loss: 0.3227808177471161
setp: 3200, Loss: 0.32093676924705505
setp: 3300, Loss: 0.3202575743198395
setp: 3400, Loss: 0.3250334858894348
setp: 3500, Loss: 0.33542948961257935
setp: 3600, Loss: 0.3208259344100952
setp: 3700, Loss: 0.3208676278591156
setp: 3800, Loss: 0.3205496072769165
setp: 3900, Loss: 0.3196631669998169
setp: 4000, Loss: 0.32107308506965637
setp: 4100, Loss: 0.3205718994140625
setp: 4200, Loss: 0.3196016550064087
setp: 4300, Loss: 0.32027295231819153
setp: 4400, Loss: 0.3204505741596222
setp: 4500, Loss: 0.3290024697780609
setp: 4600, Loss: 0.39972543716430664
setp: 4700, Loss: 0.32037481665611267
setp: 4800, Loss: 0.32124635577201843
setp: 4900, Loss: 0.32177168130874634
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9473684210526315
precision: 0.9591836734693877
recall: 0.8867924528301887
F_score: 0.9215686274509803
model saved.
avg_acc: 0.9302631578947368, avg_f_score: 0.9037652661220257
-------------subject: 23-------------
==========valence==========
******fold 0******
[193, 415]
training...
setp: 0, Loss: 0.6864372491836548
setp: 100, Loss: 0.6455323696136475
setp: 200, Loss: 0.6578445434570312
setp: 300, Loss: 0.5903921127319336
setp: 400, Loss: 0.5453834533691406
setp: 500, Loss: 0.49834564328193665
setp: 600, Loss: 0.5052990913391113
setp: 700, Loss: 0.45269671082496643
setp: 800, Loss: 0.48766109347343445
setp: 900, Loss: 0.5012747049331665
setp: 1000, Loss: 0.4126608967781067
setp: 1100, Loss: 0.503864049911499
setp: 1200, Loss: 0.4209994673728943
setp: 1300, Loss: 0.440658837556839
setp: 1400, Loss: 0.5186944603919983
setp: 1500, Loss: 0.4739779829978943
setp: 1600, Loss: 0.4531315863132477
setp: 1700, Loss: 0.41533422470092773
setp: 1800, Loss: 0.4428611099720001
setp: 1900, Loss: 0.4366004168987274
setp: 2000, Loss: 0.4710259437561035
setp: 2100, Loss: 0.5021673440933228
setp: 2200, Loss: 0.4759809672832489
setp: 2300, Loss: 0.4407563805580139
setp: 2400, Loss: 0.41007348895072937
setp: 2500, Loss: 0.426332026720047
setp: 2600, Loss: 0.34116727113723755
setp: 2700, Loss: 0.3868728578090668
setp: 2800, Loss: 0.38863956928253174
setp: 2900, Loss: 0.35040444135665894
setp: 3000, Loss: 0.36335432529449463
setp: 3100, Loss: 0.3296927213668823
setp: 3200, Loss: 0.3173382580280304
setp: 3300, Loss: 0.3159749507904053
setp: 3400, Loss: 0.3168378174304962
setp: 3500, Loss: 0.31714487075805664
setp: 3600, Loss: 0.31943315267562866
setp: 3700, Loss: 0.31681954860687256
setp: 3800, Loss: 0.3166165351867676
setp: 3900, Loss: 0.3159130811691284
setp: 4000, Loss: 0.31743699312210083
setp: 4100, Loss: 0.3250158131122589
setp: 4200, Loss: 0.3475881516933441
setp: 4300, Loss: 0.31670334935188293
setp: 4400, Loss: 0.31697791814804077
setp: 4500, Loss: 0.3167387843132019
setp: 4600, Loss: 0.31702885031700134
setp: 4700, Loss: 0.3169054388999939
setp: 4800, Loss: 0.3173139691352844
setp: 4900, Loss: 0.32327088713645935
training successfully ended.
validating...
acc: 0.9901315789473685
precision: 1.0
recall: 0.9689119170984456
F_score: 0.9842105263157895
validating...
acc: 0.9671052631578947
precision: 1.0
recall: 0.9074074074074074
F_score: 0.9514563106796117
******fold 1******
[207, 401]
training...
setp: 0, Loss: 0.6625934839248657
setp: 100, Loss: 0.5554414391517639
setp: 200, Loss: 0.5549673438072205
setp: 300, Loss: 0.5298285484313965
setp: 400, Loss: 0.4518001079559326
setp: 500, Loss: 0.4231165051460266
setp: 600, Loss: 0.47884461283683777
setp: 700, Loss: 0.4800316393375397
setp: 800, Loss: 0.40963947772979736
setp: 900, Loss: 0.43392908573150635
setp: 1000, Loss: 0.375766783952713
setp: 1100, Loss: 0.41317448019981384
setp: 1200, Loss: 0.4122254550457001
setp: 1300, Loss: 0.41082221269607544
setp: 1400, Loss: 0.4426491856575012
setp: 1500, Loss: 0.41381704807281494
setp: 1600, Loss: 0.40999212861061096
setp: 1700, Loss: 0.416823148727417
setp: 1800, Loss: 0.4018726050853729
setp: 1900, Loss: 0.383685439825058
setp: 2000, Loss: 0.3778442144393921
setp: 2100, Loss: 0.40292468667030334
setp: 2200, Loss: 0.3970646858215332
setp: 2300, Loss: 0.3868391513824463
setp: 2400, Loss: 0.38032618165016174
setp: 2500, Loss: 0.4113844037055969
setp: 2600, Loss: 0.3871098756790161
setp: 2700, Loss: 0.3820245862007141
setp: 2800, Loss: 0.4114978313446045
setp: 2900, Loss: 0.3477831482887268
setp: 3000, Loss: 0.4087940752506256
setp: 3100, Loss: 0.4096907377243042
setp: 3200, Loss: 0.41058704257011414
setp: 3300, Loss: 0.43995416164398193
setp: 3400, Loss: 0.4101352095603943
setp: 3500, Loss: 0.4105747640132904
setp: 3600, Loss: 0.4102456867694855
setp: 3700, Loss: 0.378894567489624
setp: 3800, Loss: 0.3824745714664459
setp: 3900, Loss: 0.37757188081741333
setp: 4000, Loss: 0.37767788767814636
setp: 4100, Loss: 0.3834723234176636
setp: 4200, Loss: 0.37994384765625
setp: 4300, Loss: 0.40070194005966187
setp: 4400, Loss: 0.4112781286239624
setp: 4500, Loss: 0.3800418972969055
setp: 4600, Loss: 0.37908706068992615
setp: 4700, Loss: 0.4103778898715973
setp: 4800, Loss: 0.35167327523231506
setp: 4900, Loss: 0.37791168689727783
training successfully ended.
validating...
acc: 0.9358552631578947
precision: 1.0
recall: 0.8115942028985508
F_score: 0.896
validating...
acc: 0.9210526315789473
precision: 0.9666666666666667
recall: 0.725
F_score: 0.8285714285714285
******fold 2******
[206, 402]
training...
setp: 0, Loss: 0.7025594711303711
setp: 100, Loss: 0.581136167049408
setp: 200, Loss: 0.5750270485877991
setp: 300, Loss: 0.5454820990562439
setp: 400, Loss: 0.4838954508304596
setp: 500, Loss: 0.513052225112915
setp: 600, Loss: 0.45954492688179016
setp: 700, Loss: 0.511378824710846
setp: 800, Loss: 0.4167371690273285
setp: 900, Loss: 0.3985743820667267
setp: 1000, Loss: 0.48041823506355286
setp: 1100, Loss: 0.49032464623451233
setp: 1200, Loss: 0.3961597681045532
setp: 1300, Loss: 0.38039204478263855
setp: 1400, Loss: 0.38059672713279724
setp: 1500, Loss: 0.32605093717575073
setp: 1600, Loss: 0.31900912523269653
setp: 1700, Loss: 0.321089506149292
setp: 1800, Loss: 0.31711503863334656
setp: 1900, Loss: 0.31629136204719543
setp: 2000, Loss: 0.31661221385002136
setp: 2100, Loss: 0.316152423620224
setp: 2200, Loss: 0.3167256712913513
setp: 2300, Loss: 0.31784769892692566
setp: 2400, Loss: 0.32595500349998474
setp: 2500, Loss: 0.3191511631011963
setp: 2600, Loss: 0.3171408474445343
setp: 2700, Loss: 0.3207799792289734
setp: 2800, Loss: 0.3169313669204712
setp: 2900, Loss: 0.3201402425765991
setp: 3000, Loss: 0.3168342411518097
setp: 3100, Loss: 0.31698188185691833
setp: 3200, Loss: 0.3167242109775543
setp: 3300, Loss: 0.316898912191391
setp: 3400, Loss: 0.3175813555717468
setp: 3500, Loss: 0.31763777136802673
setp: 3600, Loss: 0.3162271976470947
setp: 3700, Loss: 0.31634506583213806
setp: 3800, Loss: 0.3170071244239807
setp: 3900, Loss: 0.3165246248245239
setp: 4000, Loss: 0.6163461804389954
setp: 4100, Loss: 0.32646694779396057
setp: 4200, Loss: 0.31688392162323
setp: 4300, Loss: 0.31702926754951477
setp: 4400, Loss: 0.31702473759651184
setp: 4500, Loss: 0.3162333369255066
setp: 4600, Loss: 0.3180083930492401
setp: 4700, Loss: 0.3163789212703705
setp: 4800, Loss: 0.3189545273780823
setp: 4900, Loss: 0.31721630692481995
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9
recall: 0.8780487804878049
F_score: 0.888888888888889
******fold 3******
[192, 416]
training...
setp: 0, Loss: 0.7057584524154663
setp: 100, Loss: 0.5928244590759277
setp: 200, Loss: 0.5985211133956909
setp: 300, Loss: 0.4997873604297638
setp: 400, Loss: 0.4784834682941437
setp: 500, Loss: 0.4912431240081787
setp: 600, Loss: 0.4174313247203827
setp: 700, Loss: 0.4110707938671112
setp: 800, Loss: 0.45381152629852295
setp: 900, Loss: 0.46302682161331177
setp: 1000, Loss: 0.4925709366798401
setp: 1100, Loss: 0.41239672899246216
setp: 1200, Loss: 0.44243428111076355
setp: 1300, Loss: 0.38034045696258545
setp: 1400, Loss: 0.4410180449485779
setp: 1500, Loss: 0.3853189945220947
setp: 1600, Loss: 0.38122132420539856
setp: 1700, Loss: 0.4429602324962616
setp: 1800, Loss: 0.41355809569358826
setp: 1900, Loss: 0.4440780580043793
setp: 2000, Loss: 0.37975722551345825
setp: 2100, Loss: 0.4411289691925049
setp: 2200, Loss: 0.4094666838645935
setp: 2300, Loss: 0.37982800602912903
setp: 2400, Loss: 0.4865206182003021
setp: 2500, Loss: 0.38047224283218384
setp: 2600, Loss: 0.40897223353385925
setp: 2700, Loss: 0.40966492891311646
setp: 2800, Loss: 0.41323697566986084
setp: 2900, Loss: 0.44224321842193604
setp: 3000, Loss: 0.4091317653656006
setp: 3100, Loss: 0.4406605362892151
setp: 3200, Loss: 0.3786740005016327
setp: 3300, Loss: 0.4403654932975769
setp: 3400, Loss: 0.3790757656097412
setp: 3500, Loss: 0.38318148255348206
setp: 3600, Loss: 0.468849778175354
setp: 3700, Loss: 0.44565320014953613
setp: 3800, Loss: 0.4415469169616699
setp: 3900, Loss: 0.3975832164287567
setp: 4000, Loss: 0.41222304105758667
setp: 4100, Loss: 0.3814332187175751
setp: 4200, Loss: 0.35679903626441956
setp: 4300, Loss: 0.38565608859062195
setp: 4400, Loss: 0.3658692240715027
setp: 4500, Loss: 0.3463454246520996
setp: 4600, Loss: 0.38352110981941223
setp: 4700, Loss: 0.39651134610176086
setp: 4800, Loss: 0.38298386335372925
setp: 4900, Loss: 0.3176655173301697
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.9895833333333334
F_score: 0.9947643979057591
validating...
acc: 0.9078947368421053
precision: 0.8867924528301887
recall: 0.8545454545454545
F_score: 0.8703703703703703
******fold 4******
[190, 418]
training...
setp: 0, Loss: 0.6895425319671631
setp: 100, Loss: 0.6714365482330322
setp: 200, Loss: 0.6330421566963196
setp: 300, Loss: 0.5096734762191772
setp: 400, Loss: 0.505058228969574
setp: 500, Loss: 0.46156221628189087
setp: 600, Loss: 0.44824421405792236
setp: 700, Loss: 0.5054739713668823
setp: 800, Loss: 0.47258260846138
setp: 900, Loss: 0.4449544847011566
setp: 1000, Loss: 0.48450717329978943
setp: 1100, Loss: 0.4146573543548584
setp: 1200, Loss: 0.4241825044155121
setp: 1300, Loss: 0.43985676765441895
setp: 1400, Loss: 0.41993775963783264
setp: 1500, Loss: 0.4209555685520172
setp: 1600, Loss: 0.44309452176094055
setp: 1700, Loss: 0.4001893401145935
setp: 1800, Loss: 0.37855127453804016
setp: 1900, Loss: 0.36002615094184875
setp: 2000, Loss: 0.3276221752166748
setp: 2100, Loss: 0.3475140333175659
setp: 2200, Loss: 0.33775946497917175
setp: 2300, Loss: 0.3175872266292572
setp: 2400, Loss: 0.3372732996940613
setp: 2500, Loss: 0.3167734444141388
setp: 2600, Loss: 0.3181439936161041
setp: 2700, Loss: 0.3777669072151184
setp: 2800, Loss: 0.3231201767921448
setp: 2900, Loss: 0.36756840348243713
setp: 3000, Loss: 0.3378303050994873
setp: 3100, Loss: 0.3213554918766022
setp: 3200, Loss: 0.318278431892395
setp: 3300, Loss: 0.3911958932876587
setp: 3400, Loss: 0.31593263149261475
setp: 3500, Loss: 0.31700947880744934
setp: 3600, Loss: 0.31727561354637146
setp: 3700, Loss: 0.3273155689239502
setp: 3800, Loss: 0.3470805883407593
setp: 3900, Loss: 0.3164965808391571
setp: 4000, Loss: 0.34693148732185364
setp: 4100, Loss: 0.3179793655872345
setp: 4200, Loss: 0.3165138065814972
setp: 4300, Loss: 0.3166213035583496
setp: 4400, Loss: 0.3162575364112854
setp: 4500, Loss: 0.35949501395225525
setp: 4600, Loss: 0.37244153022766113
setp: 4700, Loss: 0.31688812375068665
setp: 4800, Loss: 0.3169476091861725
setp: 4900, Loss: 0.315518856048584
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.9894736842105263
F_score: 0.9947089947089947
validating...
acc: 0.9671052631578947
precision: 0.9333333333333333
recall: 0.9824561403508771
F_score: 0.9572649572649572
model saved.
avg_acc: 0.9407894736842106, avg_f_score: 0.8993103911550513
==========arousal==========
******fold 0******
[404, 204]
training...
setp: 0, Loss: 0.6556854844093323
setp: 100, Loss: 0.6221871972084045
setp: 200, Loss: 0.644372284412384
setp: 300, Loss: 0.6878626346588135
setp: 400, Loss: 0.6636054515838623
setp: 500, Loss: 0.643698513507843
setp: 600, Loss: 0.667403519153595
setp: 700, Loss: 0.6366350650787354
setp: 800, Loss: 0.4606940746307373
setp: 900, Loss: 0.4027957320213318
setp: 1000, Loss: 0.3629657030105591
setp: 1100, Loss: 0.35268110036849976
setp: 1200, Loss: 0.3219379782676697
setp: 1300, Loss: 0.3223721385002136
setp: 1400, Loss: 0.35836830735206604
setp: 1500, Loss: 0.3197169005870819
setp: 1600, Loss: 0.32725846767425537
setp: 1700, Loss: 0.33004847168922424
setp: 1800, Loss: 0.33056673407554626
setp: 1900, Loss: 0.32437047362327576
setp: 2000, Loss: 0.3192877769470215
setp: 2100, Loss: 0.3212251663208008
setp: 2200, Loss: 0.31942805647850037
setp: 2300, Loss: 0.3188079595565796
setp: 2400, Loss: 0.31871670484542847
setp: 2500, Loss: 0.3199259042739868
setp: 2600, Loss: 0.3202933967113495
setp: 2700, Loss: 0.31874749064445496
setp: 2800, Loss: 0.3190882205963135
setp: 2900, Loss: 0.3192131519317627
setp: 3000, Loss: 0.3196287453174591
setp: 3100, Loss: 0.3185223937034607
setp: 3200, Loss: 0.31744277477264404
setp: 3300, Loss: 0.35014525055885315
setp: 3400, Loss: 0.3793236315250397
setp: 3500, Loss: 0.31859269738197327
setp: 3600, Loss: 0.3179067075252533
setp: 3700, Loss: 0.31939107179641724
setp: 3800, Loss: 0.31866514682769775
setp: 3900, Loss: 0.3194170296192169
setp: 4000, Loss: 0.3199962377548218
setp: 4100, Loss: 0.31920796632766724
setp: 4200, Loss: 0.3185621500015259
setp: 4300, Loss: 0.31876417994499207
setp: 4400, Loss: 0.3199559450149536
setp: 4500, Loss: 0.32015860080718994
setp: 4600, Loss: 0.31815749406814575
setp: 4700, Loss: 0.4214426279067993
setp: 4800, Loss: 0.37829363346099854
setp: 4900, Loss: 0.31763187050819397
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9975247524752475
F_score: 0.9987608426270136
validating...
acc: 0.9605263157894737
precision: 0.9904761904761905
recall: 0.9541284403669725
F_score: 0.9719626168224299
******fold 1******
[412, 196]
training...
setp: 0, Loss: 0.6965566873550415
setp: 100, Loss: 0.6447066068649292
setp: 200, Loss: 0.6413518786430359
setp: 300, Loss: 0.5735396146774292
setp: 400, Loss: 0.4324212074279785
setp: 500, Loss: 0.3571319282054901
setp: 600, Loss: 0.3417980968952179
setp: 700, Loss: 0.3264676332473755
setp: 800, Loss: 0.32034194469451904
setp: 900, Loss: 0.3216153085231781
setp: 1000, Loss: 0.32372021675109863
setp: 1100, Loss: 0.31926611065864563
setp: 1200, Loss: 0.3179158568382263
setp: 1300, Loss: 0.3173447549343109
setp: 1400, Loss: 0.31918418407440186
setp: 1500, Loss: 0.3229132890701294
setp: 1600, Loss: 0.3186177611351013
setp: 1700, Loss: 0.31960225105285645
setp: 1800, Loss: 0.3197154402732849
setp: 1900, Loss: 0.3178933262825012
setp: 2000, Loss: 0.31822794675827026
setp: 2100, Loss: 0.31849566102027893
setp: 2200, Loss: 0.33227843046188354
setp: 2300, Loss: 0.3557606637477875
setp: 2400, Loss: 0.31751957535743713
setp: 2500, Loss: 0.31700727343559265
setp: 2600, Loss: 0.31877601146698
setp: 2700, Loss: 0.3178039491176605
setp: 2800, Loss: 0.31859278678894043
setp: 2900, Loss: 0.3187088966369629
setp: 3000, Loss: 0.3180466890335083
setp: 3100, Loss: 0.3468046486377716
setp: 3200, Loss: 0.3178245723247528
setp: 3300, Loss: 0.3179563879966736
setp: 3400, Loss: 0.31802448630332947
setp: 3500, Loss: 0.3181990683078766
setp: 3600, Loss: 0.3193667232990265
setp: 3700, Loss: 0.31906211376190186
setp: 3800, Loss: 0.3223188519477844
setp: 3900, Loss: 0.3191487789154053
setp: 4000, Loss: 0.3178488612174988
setp: 4100, Loss: 0.3174617886543274
setp: 4200, Loss: 0.31777843832969666
setp: 4300, Loss: 0.31860780715942383
setp: 4400, Loss: 0.31788936257362366
setp: 4500, Loss: 0.31903329491615295
setp: 4600, Loss: 0.31750157475471497
setp: 4700, Loss: 0.31822267174720764
setp: 4800, Loss: 0.31859496235847473
setp: 4900, Loss: 0.3185376524925232
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9473684210526315
precision: 0.926605504587156
recall: 1.0
F_score: 0.9619047619047619
******fold 2******
[422, 186]
training...
setp: 0, Loss: 0.6123624444007874
setp: 100, Loss: 0.6464826464653015
setp: 200, Loss: 0.5882275700569153
setp: 300, Loss: 0.5936650037765503
setp: 400, Loss: 0.5062727332115173
setp: 500, Loss: 0.4129015803337097
setp: 600, Loss: 0.42433416843414307
setp: 700, Loss: 0.4045827388763428
setp: 800, Loss: 0.3470565676689148
setp: 900, Loss: 0.3226517140865326
setp: 1000, Loss: 0.3224022686481476
setp: 1100, Loss: 0.32217729091644287
setp: 1200, Loss: 0.3197670578956604
setp: 1300, Loss: 0.3209078013896942
setp: 1400, Loss: 0.31932005286216736
setp: 1500, Loss: 0.31760692596435547
setp: 1600, Loss: 0.318522572517395
setp: 1700, Loss: 0.3186620771884918
setp: 1800, Loss: 0.31776291131973267
setp: 1900, Loss: 0.3175579607486725
setp: 2000, Loss: 0.34858468174934387
setp: 2100, Loss: 0.32491883635520935
setp: 2200, Loss: 0.33896684646606445
setp: 2300, Loss: 0.46642547845840454
setp: 2400, Loss: 0.3183341920375824
setp: 2500, Loss: 0.31798774003982544
setp: 2600, Loss: 0.3181840181350708
setp: 2700, Loss: 0.3161960542201996
setp: 2800, Loss: 0.3158721625804901
setp: 2900, Loss: 0.3173961341381073
setp: 3000, Loss: 0.3171166777610779
setp: 3100, Loss: 0.31704267859458923
setp: 3200, Loss: 0.31637507677078247
setp: 3300, Loss: 0.3189251720905304
setp: 3400, Loss: 0.3167550563812256
setp: 3500, Loss: 0.6213018298149109
setp: 3600, Loss: 0.5729702711105347
setp: 3700, Loss: 0.36920589208602905
setp: 3800, Loss: 0.36058804392814636
setp: 3900, Loss: 0.3359566628932953
setp: 4000, Loss: 0.33132436871528625
setp: 4100, Loss: 0.3283774256706238
setp: 4200, Loss: 0.3267501890659332
setp: 4300, Loss: 0.3308633267879486
setp: 4400, Loss: 0.3238719403743744
setp: 4500, Loss: 0.3229026794433594
setp: 4600, Loss: 0.318847119808197
setp: 4700, Loss: 0.6565336585044861
setp: 4800, Loss: 0.3562696874141693
setp: 4900, Loss: 0.3220962882041931
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9976303317535545
F_score: 0.9988137603795968
validating...
acc: 0.9276315789473685
precision: 0.9651162790697675
recall: 0.9120879120879121
F_score: 0.9378531073446328
******fold 3******
[406, 202]
training...
setp: 0, Loss: 0.702649712562561
setp: 100, Loss: 0.6680477857589722
setp: 200, Loss: 0.6622374653816223
setp: 300, Loss: 0.5726022720336914
setp: 400, Loss: 0.44324493408203125
setp: 500, Loss: 0.3849651515483856
setp: 600, Loss: 0.3550738990306854
setp: 700, Loss: 0.33054643869400024
setp: 800, Loss: 0.321018785238266
setp: 900, Loss: 0.3194800913333893
setp: 1000, Loss: 0.3204670250415802
setp: 1100, Loss: 0.3201465606689453
setp: 1200, Loss: 0.31929248571395874
setp: 1300, Loss: 0.318742036819458
setp: 1400, Loss: 0.3194846510887146
setp: 1500, Loss: 0.31890541315078735
setp: 1600, Loss: 0.31988072395324707
setp: 1700, Loss: 0.31872308254241943
setp: 1800, Loss: 0.31873801350593567
setp: 1900, Loss: 0.319716215133667
setp: 2000, Loss: 0.3195231258869171
setp: 2100, Loss: 0.4632568657398224
setp: 2200, Loss: 0.3231293559074402
setp: 2300, Loss: 0.3212653696537018
setp: 2400, Loss: 0.3207704722881317
setp: 2500, Loss: 0.320338636636734
setp: 2600, Loss: 0.32204386591911316
setp: 2700, Loss: 0.3212989270687103
setp: 2800, Loss: 0.3202393054962158
setp: 2900, Loss: 0.3210398554801941
setp: 3000, Loss: 0.32130539417266846
setp: 3100, Loss: 0.3209923505783081
setp: 3200, Loss: 0.32040584087371826
setp: 3300, Loss: 0.3214825391769409
setp: 3400, Loss: 0.3212517201900482
setp: 3500, Loss: 0.32256701588630676
setp: 3600, Loss: 0.39040032029151917
setp: 3700, Loss: 0.3322440981864929
setp: 3800, Loss: 0.33385494351387024
setp: 3900, Loss: 0.3241385817527771
setp: 4000, Loss: 0.3568727374076843
setp: 4100, Loss: 0.3209976851940155
setp: 4200, Loss: 0.3365439474582672
setp: 4300, Loss: 0.3204997479915619
setp: 4400, Loss: 0.3208152949810028
setp: 4500, Loss: 0.32184162735939026
setp: 4600, Loss: 0.3214047849178314
setp: 4700, Loss: 0.3200472891330719
setp: 4800, Loss: 0.32064288854599
setp: 4900, Loss: 0.3207116723060608
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9975429975429976
recall: 1.0
F_score: 0.998769987699877
validating...
acc: 0.868421052631579
precision: 0.9223300970873787
recall: 0.8878504672897196
F_score: 0.9047619047619048
******fold 4******
[408, 200]
training...
setp: 0, Loss: 0.6475465297698975
setp: 100, Loss: 0.6013206243515015
setp: 200, Loss: 0.5474270582199097
setp: 300, Loss: 0.41798436641693115
setp: 400, Loss: 0.361811101436615
setp: 500, Loss: 0.3264239430427551
setp: 600, Loss: 0.33470258116722107
setp: 700, Loss: 0.32091304659843445
setp: 800, Loss: 0.31933414936065674
setp: 900, Loss: 0.3192831575870514
setp: 1000, Loss: 0.3187932074069977
setp: 1100, Loss: 0.3194769620895386
setp: 1200, Loss: 0.3176520764827728
setp: 1300, Loss: 0.31919577717781067
setp: 1400, Loss: 0.31962713599205017
setp: 1500, Loss: 0.318901002407074
setp: 1600, Loss: 0.3190939426422119
setp: 1700, Loss: 0.3195302188396454
setp: 1800, Loss: 0.4562479853630066
setp: 1900, Loss: 0.35054120421409607
setp: 2000, Loss: 0.37686648964881897
setp: 2100, Loss: 0.35597532987594604
setp: 2200, Loss: 0.32844802737236023
setp: 2300, Loss: 0.32331955432891846
setp: 2400, Loss: 0.3276814818382263
setp: 2500, Loss: 0.32043781876564026
setp: 2600, Loss: 0.3213541507720947
setp: 2700, Loss: 0.3206879198551178
setp: 2800, Loss: 0.3206709623336792
setp: 2900, Loss: 0.32144904136657715
setp: 3000, Loss: 0.33108699321746826
setp: 3100, Loss: 0.32018518447875977
setp: 3200, Loss: 0.320820689201355
setp: 3300, Loss: 0.32235634326934814
setp: 3400, Loss: 0.3212292194366455
setp: 3500, Loss: 0.35201385617256165
setp: 3600, Loss: 0.32397398352622986
setp: 3700, Loss: 0.3222874402999878
setp: 3800, Loss: 0.3219858407974243
setp: 3900, Loss: 0.3203015625476837
setp: 4000, Loss: 0.32209742069244385
setp: 4100, Loss: 0.32038256525993347
setp: 4200, Loss: 0.32123640179634094
setp: 4300, Loss: 0.3209156394004822
setp: 4400, Loss: 0.32036715745925903
setp: 4500, Loss: 0.3218805193901062
setp: 4600, Loss: 0.32059141993522644
setp: 4700, Loss: 0.3204931318759918
setp: 4800, Loss: 0.3209500312805176
setp: 4900, Loss: 0.3217293620109558
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8881578947368421
precision: 0.9150943396226415
recall: 0.9238095238095239
F_score: 0.9194312796208531
model saved.
avg_acc: 0.9184210526315791, avg_f_score: 0.9391827340909165
-------------subject: 24-------------
==========valence==========
******fold 0******
[318, 290]
training...
setp: 0, Loss: 0.6917121410369873
setp: 100, Loss: 0.6657236218452454
setp: 200, Loss: 0.5757535696029663
setp: 300, Loss: 0.5285329222679138
setp: 400, Loss: 0.40753769874572754
setp: 500, Loss: 0.4054293930530548
setp: 600, Loss: 0.3589121103286743
setp: 700, Loss: 0.3272385895252228
setp: 800, Loss: 0.3284316062927246
setp: 900, Loss: 0.32365548610687256
setp: 1000, Loss: 0.3292195796966553
setp: 1100, Loss: 0.32271480560302734
setp: 1200, Loss: 0.33406388759613037
setp: 1300, Loss: 0.3332706689834595
setp: 1400, Loss: 0.3225872218608856
setp: 1500, Loss: 0.3194868862628937
setp: 1600, Loss: 0.31931188702583313
setp: 1700, Loss: 0.3210887610912323
setp: 1800, Loss: 0.3209790885448456
setp: 1900, Loss: 0.31951698660850525
setp: 2000, Loss: 0.31947872042655945
setp: 2100, Loss: 0.32261499762535095
setp: 2200, Loss: 0.32108455896377563
setp: 2300, Loss: 0.3197946846485138
setp: 2400, Loss: 0.3210206627845764
setp: 2500, Loss: 0.3205459415912628
setp: 2600, Loss: 0.32013964653015137
setp: 2700, Loss: 0.3196006417274475
setp: 2800, Loss: 0.34451526403427124
setp: 2900, Loss: 0.3493840992450714
setp: 3000, Loss: 0.32187020778656006
setp: 3100, Loss: 0.31915125250816345
setp: 3200, Loss: 0.31818804144859314
setp: 3300, Loss: 0.3187826871871948
setp: 3400, Loss: 0.31869810819625854
setp: 3500, Loss: 0.3181582987308502
setp: 3600, Loss: 0.3192105293273926
setp: 3700, Loss: 0.3195813000202179
setp: 3800, Loss: 0.3191218972206116
setp: 3900, Loss: 0.31867364048957825
setp: 4000, Loss: 0.3218821883201599
setp: 4100, Loss: 0.32038769125938416
setp: 4200, Loss: 0.3193559944629669
setp: 4300, Loss: 0.3206676244735718
setp: 4400, Loss: 0.3199065625667572
setp: 4500, Loss: 0.3197951316833496
setp: 4600, Loss: 0.33737102150917053
setp: 4700, Loss: 0.3396861255168915
setp: 4800, Loss: 0.3232795298099518
setp: 4900, Loss: 0.3237723708152771
training successfully ended.
validating...
acc: 0.9868421052631579
precision: 1.0
recall: 0.9748427672955975
F_score: 0.9872611464968153
validating...
acc: 0.868421052631579
precision: 0.9692307692307692
recall: 0.7777777777777778
F_score: 0.863013698630137
******fold 1******
[317, 291]
training...
setp: 0, Loss: 0.6918327808380127
setp: 100, Loss: 0.5985726118087769
setp: 200, Loss: 0.5837339758872986
setp: 300, Loss: 0.4639734625816345
setp: 400, Loss: 0.40941131114959717
setp: 500, Loss: 0.4154515266418457
setp: 600, Loss: 0.3344619870185852
setp: 700, Loss: 0.3443596065044403
setp: 800, Loss: 0.3550242781639099
setp: 900, Loss: 0.3214215338230133
setp: 1000, Loss: 0.31816840171813965
setp: 1100, Loss: 0.3522091209888458
setp: 1200, Loss: 0.319890558719635
setp: 1300, Loss: 0.3486780524253845
setp: 1400, Loss: 0.3246171176433563
setp: 1500, Loss: 0.42700138688087463
setp: 1600, Loss: 0.3423970639705658
setp: 1700, Loss: 0.3210750222206116
setp: 1800, Loss: 0.31961771845817566
setp: 1900, Loss: 0.3173931837081909
setp: 2000, Loss: 0.3216765224933624
setp: 2100, Loss: 0.3184865117073059
setp: 2200, Loss: 0.31994155049324036
setp: 2300, Loss: 0.3188987970352173
setp: 2400, Loss: 0.3184957206249237
setp: 2500, Loss: 0.3175044059753418
setp: 2600, Loss: 0.3168282210826874
setp: 2700, Loss: 0.48840615153312683
setp: 2800, Loss: 0.37858790159225464
setp: 2900, Loss: 0.34826186299324036
setp: 3000, Loss: 0.3475434482097626
setp: 3100, Loss: 0.31696534156799316
setp: 3200, Loss: 0.34789955615997314
setp: 3300, Loss: 0.31708189845085144
setp: 3400, Loss: 0.31759679317474365
setp: 3500, Loss: 0.3181498646736145
setp: 3600, Loss: 0.31923019886016846
setp: 3700, Loss: 0.31905969977378845
setp: 3800, Loss: 0.3252565860748291
setp: 3900, Loss: 0.3257887065410614
setp: 4000, Loss: 0.31708598136901855
setp: 4100, Loss: 0.31800612807273865
setp: 4200, Loss: 0.3239043653011322
setp: 4300, Loss: 0.3351389169692993
setp: 4400, Loss: 0.36926087737083435
setp: 4500, Loss: 0.32193419337272644
setp: 4600, Loss: 0.3169899582862854
setp: 4700, Loss: 0.3161311447620392
setp: 4800, Loss: 0.31668663024902344
setp: 4900, Loss: 0.31782662868499756
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9968454258675079
F_score: 0.9984202211690364
validating...
acc: 0.881578947368421
precision: 0.8809523809523809
recall: 0.9024390243902439
F_score: 0.8915662650602411
******fold 2******
[331, 277]
training...
setp: 0, Loss: 0.6919904351234436
setp: 100, Loss: 0.6527252197265625
setp: 200, Loss: 0.5993843674659729
setp: 300, Loss: 0.4980490207672119
setp: 400, Loss: 0.4367392659187317
setp: 500, Loss: 0.37598487734794617
setp: 600, Loss: 0.3710765540599823
setp: 700, Loss: 0.32962626218795776
setp: 800, Loss: 0.3286062479019165
setp: 900, Loss: 0.34567761421203613
setp: 1000, Loss: 0.328139066696167
setp: 1100, Loss: 0.32190653681755066
setp: 1200, Loss: 0.3211527466773987
setp: 1300, Loss: 0.32066428661346436
setp: 1400, Loss: 0.3191272020339966
setp: 1500, Loss: 0.319450706243515
setp: 1600, Loss: 0.32140710949897766
setp: 1700, Loss: 0.3230029344558716
setp: 1800, Loss: 0.3209720551967621
setp: 1900, Loss: 0.31851059198379517
setp: 2000, Loss: 0.3438311517238617
setp: 2100, Loss: 0.3377362787723541
setp: 2200, Loss: 0.3185727596282959
setp: 2300, Loss: 0.319653183221817
setp: 2400, Loss: 0.31980806589126587
setp: 2500, Loss: 0.3212248980998993
setp: 2600, Loss: 0.31825706362724304
setp: 2700, Loss: 0.3184100091457367
setp: 2800, Loss: 0.3188801109790802
setp: 2900, Loss: 0.31989070773124695
setp: 3000, Loss: 0.368972510099411
setp: 3100, Loss: 0.3590007424354553
setp: 3200, Loss: 0.3406558632850647
setp: 3300, Loss: 0.3208676278591156
setp: 3400, Loss: 0.32159632444381714
setp: 3500, Loss: 0.32026898860931396
setp: 3600, Loss: 0.3228466808795929
setp: 3700, Loss: 0.32036304473876953
setp: 3800, Loss: 0.31948983669281006
setp: 3900, Loss: 0.3194209635257721
setp: 4000, Loss: 0.32097142934799194
setp: 4100, Loss: 0.32115328311920166
setp: 4200, Loss: 0.3218538165092468
setp: 4300, Loss: 0.32189083099365234
setp: 4400, Loss: 0.32164841890335083
setp: 4500, Loss: 0.321167916059494
setp: 4600, Loss: 0.40459153056144714
setp: 4700, Loss: 0.3301006853580475
setp: 4800, Loss: 0.343051016330719
setp: 4900, Loss: 0.31980445981025696
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8947368421052632
precision: 0.8421052631578947
recall: 0.9411764705882353
F_score: 0.8888888888888888
******fold 3******
[320, 288]
training...
setp: 0, Loss: 0.7053831219673157
setp: 100, Loss: 0.6225691437721252
setp: 200, Loss: 0.5595029592514038
setp: 300, Loss: 0.4946901500225067
setp: 400, Loss: 0.40940091013908386
setp: 500, Loss: 0.41925039887428284
setp: 600, Loss: 0.3679639399051666
setp: 700, Loss: 0.3260194957256317
setp: 800, Loss: 0.39428457617759705
setp: 900, Loss: 0.35060200095176697
setp: 1000, Loss: 0.32309916615486145
setp: 1100, Loss: 0.3798750638961792
setp: 1200, Loss: 0.32068803906440735
setp: 1300, Loss: 0.34973371028900146
setp: 1400, Loss: 0.34332939982414246
setp: 1500, Loss: 0.32167699933052063
setp: 1600, Loss: 0.324636846780777
setp: 1700, Loss: 0.318795770406723
setp: 1800, Loss: 0.3214063048362732
setp: 1900, Loss: 0.3208634853363037
setp: 2000, Loss: 0.32000648975372314
setp: 2100, Loss: 0.3194599151611328
setp: 2200, Loss: 0.3629130423069
setp: 2300, Loss: 0.318325012922287
setp: 2400, Loss: 0.32501915097236633
setp: 2500, Loss: 0.34406352043151855
setp: 2600, Loss: 0.31903520226478577
setp: 2700, Loss: 0.3555830419063568
setp: 2800, Loss: 0.3179496228694916
setp: 2900, Loss: 0.31815457344055176
setp: 3000, Loss: 0.3197341561317444
setp: 3100, Loss: 0.31859999895095825
setp: 3200, Loss: 0.31937265396118164
setp: 3300, Loss: 0.31866616010665894
setp: 3400, Loss: 0.3183598518371582
setp: 3500, Loss: 0.32012617588043213
setp: 3600, Loss: 0.3193839490413666
setp: 3700, Loss: 0.31929081678390503
setp: 3800, Loss: 0.3203396797180176
setp: 3900, Loss: 0.6096274852752686
setp: 4000, Loss: 0.4121924340724945
setp: 4100, Loss: 0.3576904833316803
setp: 4200, Loss: 0.3579270541667938
setp: 4300, Loss: 0.3303644359111786
setp: 4400, Loss: 0.32802411913871765
setp: 4500, Loss: 0.3348466753959656
setp: 4600, Loss: 0.32148632407188416
setp: 4700, Loss: 0.3194248676300049
setp: 4800, Loss: 0.32106807827949524
setp: 4900, Loss: 0.32194045186042786
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9605263157894737
recall: 0.9240506329113924
F_score: 0.9419354838709677
******fold 4******
[310, 298]
training...
setp: 0, Loss: 0.7128154039382935
setp: 100, Loss: 0.6644374132156372
setp: 200, Loss: 0.604280948638916
setp: 300, Loss: 0.5128023028373718
setp: 400, Loss: 0.4494974613189697
setp: 500, Loss: 0.401912122964859
setp: 600, Loss: 0.34902000427246094
setp: 700, Loss: 0.33681145310401917
setp: 800, Loss: 0.33332252502441406
setp: 900, Loss: 0.32706210017204285
setp: 1000, Loss: 0.32885828614234924
setp: 1100, Loss: 0.3237653970718384
setp: 1200, Loss: 0.32019543647766113
setp: 1300, Loss: 0.3526504933834076
setp: 1400, Loss: 0.3323774039745331
setp: 1500, Loss: 0.32526442408561707
setp: 1600, Loss: 0.3244708776473999
setp: 1700, Loss: 0.3211880624294281
setp: 1800, Loss: 0.3203452527523041
setp: 1900, Loss: 0.32016584277153015
setp: 2000, Loss: 0.34962382912635803
setp: 2100, Loss: 0.32284244894981384
setp: 2200, Loss: 0.3213669955730438
setp: 2300, Loss: 0.32042229175567627
setp: 2400, Loss: 0.3211236298084259
setp: 2500, Loss: 0.320235937833786
setp: 2600, Loss: 0.31999391317367554
setp: 2700, Loss: 0.33047693967819214
setp: 2800, Loss: 0.3595614731311798
setp: 2900, Loss: 0.32029736042022705
setp: 3000, Loss: 0.31833699345588684
setp: 3100, Loss: 0.31901636719703674
setp: 3200, Loss: 0.3190338909626007
setp: 3300, Loss: 0.31999680399894714
setp: 3400, Loss: 0.31930452585220337
setp: 3500, Loss: 0.320252001285553
setp: 3600, Loss: 0.3214116096496582
setp: 3700, Loss: 0.3207724988460541
setp: 3800, Loss: 0.3191502094268799
setp: 3900, Loss: 0.3189731538295746
setp: 4000, Loss: 0.3218749165534973
setp: 4100, Loss: 0.3201112151145935
setp: 4200, Loss: 0.605358362197876
setp: 4300, Loss: 0.36311402916908264
setp: 4400, Loss: 0.37307286262512207
setp: 4500, Loss: 0.33137381076812744
setp: 4600, Loss: 0.3360584080219269
setp: 4700, Loss: 0.32381680607795715
setp: 4800, Loss: 0.3255379796028137
setp: 4900, Loss: 0.3229822516441345
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.9230769230769231
recall: 0.9438202247191011
F_score: 0.9333333333333335
model saved.
avg_acc: 0.9013157894736843, avg_f_score: 0.9037475339567136
==========arousal==========
******fold 0******
[102, 506]
training...
setp: 0, Loss: 0.6974056959152222
setp: 100, Loss: 0.5667276978492737
setp: 200, Loss: 0.4786021411418915
setp: 300, Loss: 0.41816189885139465
setp: 400, Loss: 0.3790316581726074
setp: 500, Loss: 0.3730902373790741
setp: 600, Loss: 0.335364431142807
setp: 700, Loss: 0.3753296732902527
setp: 800, Loss: 0.38707640767097473
setp: 900, Loss: 0.35867470502853394
setp: 1000, Loss: 0.35069984197616577
setp: 1100, Loss: 0.32456350326538086
setp: 1200, Loss: 0.36556896567344666
setp: 1300, Loss: 0.34659767150878906
setp: 1400, Loss: 0.3189046084880829
setp: 1500, Loss: 0.31691908836364746
setp: 1600, Loss: 0.3540799915790558
setp: 1700, Loss: 0.3425000011920929
setp: 1800, Loss: 0.3198898732662201
setp: 1900, Loss: 0.3156091570854187
setp: 2000, Loss: 0.3188902735710144
setp: 2100, Loss: 0.3183334171772003
setp: 2200, Loss: 0.31780797243118286
setp: 2300, Loss: 0.4339592456817627
setp: 2400, Loss: 0.34730008244514465
setp: 2500, Loss: 0.31771618127822876
setp: 2600, Loss: 0.3193565607070923
setp: 2700, Loss: 0.3154353201389313
setp: 2800, Loss: 0.33735182881355286
setp: 2900, Loss: 0.3483169674873352
setp: 3000, Loss: 0.3160266578197479
setp: 3100, Loss: 0.31875866651535034
setp: 3200, Loss: 0.3204863667488098
setp: 3300, Loss: 0.31783103942871094
setp: 3400, Loss: 0.3196493089199066
setp: 3500, Loss: 0.31559810042381287
setp: 3600, Loss: 0.3322207033634186
setp: 3700, Loss: 0.31693777441978455
setp: 3800, Loss: 0.31651678681373596
setp: 3900, Loss: 0.3160214126110077
setp: 4000, Loss: 0.3179246783256531
setp: 4100, Loss: 0.3170384466648102
setp: 4200, Loss: 0.31933659315109253
setp: 4300, Loss: 0.3154071271419525
setp: 4400, Loss: 0.31736552715301514
setp: 4500, Loss: 0.3306410312652588
setp: 4600, Loss: 0.31596317887306213
setp: 4700, Loss: 0.31610435247421265
setp: 4800, Loss: 0.31863147020339966
setp: 4900, Loss: 0.31819918751716614
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9802631578947368
precision: 0.9375
recall: 0.967741935483871
F_score: 0.9523809523809523
******fold 1******
[109, 499]
training...
setp: 0, Loss: 0.6927582025527954
setp: 100, Loss: 0.5032650232315063
setp: 200, Loss: 0.5007635951042175
setp: 300, Loss: 0.46491554379463196
setp: 400, Loss: 0.3972291052341461
setp: 500, Loss: 0.4279363453388214
setp: 600, Loss: 0.37567204236984253
setp: 700, Loss: 0.33228641748428345
setp: 800, Loss: 0.3714824616909027
setp: 900, Loss: 0.31758150458335876
setp: 1000, Loss: 0.3232637345790863
setp: 1100, Loss: 0.349361777305603
setp: 1200, Loss: 0.34815365076065063
setp: 1300, Loss: 0.35131868720054626
setp: 1400, Loss: 0.31721335649490356
setp: 1500, Loss: 0.31736162304878235
setp: 1600, Loss: 0.3183187246322632
setp: 1700, Loss: 0.3159126341342926
setp: 1800, Loss: 0.31846630573272705
setp: 1900, Loss: 0.3164088726043701
setp: 2000, Loss: 0.47998467087745667
setp: 2100, Loss: 0.42120254039764404
setp: 2200, Loss: 0.31682389974594116
setp: 2300, Loss: 0.31662416458129883
setp: 2400, Loss: 0.3167787790298462
setp: 2500, Loss: 0.31581881642341614
setp: 2600, Loss: 0.31791332364082336
setp: 2700, Loss: 0.3163966238498688
setp: 2800, Loss: 0.3184676468372345
setp: 2900, Loss: 0.34838855266571045
setp: 3000, Loss: 0.31693705916404724
setp: 3100, Loss: 0.423897922039032
setp: 3200, Loss: 0.33875203132629395
setp: 3300, Loss: 0.31619739532470703
setp: 3400, Loss: 0.3170351982116699
setp: 3500, Loss: 0.31629595160484314
setp: 3600, Loss: 0.31808164715766907
setp: 3700, Loss: 0.34797510504722595
setp: 3800, Loss: 0.316808819770813
setp: 3900, Loss: 0.31743842363357544
setp: 4000, Loss: 0.3178170919418335
setp: 4100, Loss: 0.3158574402332306
setp: 4200, Loss: 0.3200478255748749
setp: 4300, Loss: 0.31609487533569336
setp: 4400, Loss: 0.3809901177883148
setp: 4500, Loss: 0.3500462770462036
setp: 4600, Loss: 0.31620341539382935
setp: 4700, Loss: 0.32418587803840637
setp: 4800, Loss: 0.34743374586105347
setp: 4900, Loss: 0.32096046209335327
training successfully ended.
validating...
acc: 0.998997995991984
precision: 0.998
recall: 1.0
F_score: 0.998998998998999
validating...
acc: 0.9868421052631579
precision: 1.0
recall: 0.9166666666666666
F_score: 0.9565217391304348
******fold 2******
[114, 494]
training...
setp: 0, Loss: 0.7467761635780334
setp: 100, Loss: 0.5552220344543457
setp: 200, Loss: 0.4942222237586975
setp: 300, Loss: 0.5079444050788879
setp: 400, Loss: 0.4650407135486603
setp: 500, Loss: 0.3298604488372803
setp: 600, Loss: 0.3535681366920471
setp: 700, Loss: 0.34268882870674133
setp: 800, Loss: 0.32037025690078735
setp: 900, Loss: 0.3231132924556732
setp: 1000, Loss: 0.32714179158210754
setp: 1100, Loss: 0.32107779383659363
setp: 1200, Loss: 0.31981080770492554
setp: 1300, Loss: 0.3452642858028412
setp: 1400, Loss: 0.31967270374298096
setp: 1500, Loss: 0.318279504776001
setp: 1600, Loss: 0.3241053521633148
setp: 1700, Loss: 0.34602582454681396
setp: 1800, Loss: 0.3161105513572693
setp: 1900, Loss: 0.31758105754852295
setp: 2000, Loss: 0.31548774242401123
setp: 2100, Loss: 0.3160810172557831
setp: 2200, Loss: 0.31580767035484314
setp: 2300, Loss: 0.31822389364242554
setp: 2400, Loss: 0.3478063642978668
setp: 2500, Loss: 0.3166349530220032
setp: 2600, Loss: 0.31801167130470276
setp: 2700, Loss: 0.47668448090553284
setp: 2800, Loss: 0.3460932672023773
setp: 2900, Loss: 0.3168866038322449
setp: 3000, Loss: 0.3160237669944763
setp: 3100, Loss: 0.315934419631958
setp: 3200, Loss: 0.32023125886917114
setp: 3300, Loss: 0.33666253089904785
setp: 3400, Loss: 0.3489924967288971
setp: 3500, Loss: 0.32003986835479736
setp: 3600, Loss: 0.31613078713417053
setp: 3700, Loss: 0.31627142429351807
setp: 3800, Loss: 0.3166724443435669
setp: 3900, Loss: 0.3176301121711731
setp: 4000, Loss: 0.3166384696960449
setp: 4100, Loss: 0.3170187771320343
setp: 4200, Loss: 0.3169715106487274
setp: 4300, Loss: 0.3171962797641754
setp: 4400, Loss: 0.3160140812397003
setp: 4500, Loss: 0.42891305685043335
setp: 4600, Loss: 0.32706648111343384
setp: 4700, Loss: 0.32334277033805847
setp: 4800, Loss: 0.3164094090461731
setp: 4900, Loss: 0.31568643450737
training successfully ended.
validating...
acc: 0.9979757085020243
precision: 0.9959677419354839
recall: 1.0
F_score: 0.997979797979798
validating...
acc: 0.9802631578947368
precision: 0.9
recall: 0.9473684210526315
F_score: 0.9230769230769231
******fold 3******
[100, 508]
training...
setp: 0, Loss: 0.7235445380210876
setp: 100, Loss: 0.4967785179615021
setp: 200, Loss: 0.6010908484458923
setp: 300, Loss: 0.4558180868625641
setp: 400, Loss: 0.4169270098209381
setp: 500, Loss: 0.35957974195480347
setp: 600, Loss: 0.35434165596961975
setp: 700, Loss: 0.4398190975189209
setp: 800, Loss: 0.39406490325927734
setp: 900, Loss: 0.3201712965965271
setp: 1000, Loss: 0.3646075129508972
setp: 1100, Loss: 0.3270096480846405
setp: 1200, Loss: 0.3816789984703064
setp: 1300, Loss: 0.3475436568260193
setp: 1400, Loss: 0.3180459141731262
setp: 1500, Loss: 0.31661665439605713
setp: 1600, Loss: 0.3173180818557739
setp: 1700, Loss: 0.3156210482120514
setp: 1800, Loss: 0.32046473026275635
setp: 1900, Loss: 0.31644144654273987
setp: 2000, Loss: 0.32072120904922485
setp: 2100, Loss: 0.3482252359390259
setp: 2200, Loss: 0.31747686862945557
setp: 2300, Loss: 0.3167751133441925
setp: 2400, Loss: 0.31746605038642883
setp: 2500, Loss: 0.3154487907886505
setp: 2600, Loss: 0.3188009560108185
setp: 2700, Loss: 0.3163345754146576
setp: 2800, Loss: 0.3174287974834442
setp: 2900, Loss: 0.3176853060722351
setp: 3000, Loss: 0.31684526801109314
setp: 3100, Loss: 0.31883418560028076
setp: 3200, Loss: 0.5407496094703674
setp: 3300, Loss: 0.403225302696228
setp: 3400, Loss: 0.3991662859916687
setp: 3500, Loss: 0.3349449038505554
setp: 3600, Loss: 0.3238162696361542
setp: 3700, Loss: 0.35114914178848267
setp: 3800, Loss: 0.40212738513946533
setp: 3900, Loss: 0.3391980528831482
setp: 4000, Loss: 0.32357603311538696
setp: 4100, Loss: 0.31718286871910095
setp: 4200, Loss: 0.32061636447906494
setp: 4300, Loss: 0.31875577569007874
setp: 4400, Loss: 0.32183629274368286
setp: 4500, Loss: 0.363994836807251
setp: 4600, Loss: 0.3251678943634033
setp: 4700, Loss: 0.3174189627170563
setp: 4800, Loss: 0.3186579942703247
setp: 4900, Loss: 0.3167944848537445
training successfully ended.
validating...
acc: 0.9990157480314961
precision: 0.9980353634577603
recall: 1.0
F_score: 0.9990167158308751
validating...
acc: 0.9539473684210527
precision: 0.90625
recall: 0.8787878787878788
F_score: 0.8923076923076922
******fold 4******
[107, 501]
training...
setp: 0, Loss: 0.6936604380607605
setp: 100, Loss: 0.5597267746925354
setp: 200, Loss: 0.5100546479225159
setp: 300, Loss: 0.40418899059295654
setp: 400, Loss: 0.46062716841697693
setp: 500, Loss: 0.37155771255493164
setp: 600, Loss: 0.3699396252632141
setp: 700, Loss: 0.349820613861084
setp: 800, Loss: 0.3456573784351349
setp: 900, Loss: 0.3240402340888977
setp: 1000, Loss: 0.31846413016319275
setp: 1100, Loss: 0.3454553186893463
setp: 1200, Loss: 0.410931795835495
setp: 1300, Loss: 0.3245994746685028
setp: 1400, Loss: 0.3206459581851959
setp: 1500, Loss: 0.33043956756591797
setp: 1600, Loss: 0.320792019367218
setp: 1700, Loss: 0.33303219079971313
setp: 1800, Loss: 0.3172834515571594
setp: 1900, Loss: 0.3167235851287842
setp: 2000, Loss: 0.3811086118221283
setp: 2100, Loss: 0.31676384806632996
setp: 2200, Loss: 0.3161907494068146
setp: 2300, Loss: 0.32158806920051575
setp: 2400, Loss: 0.32728493213653564
setp: 2500, Loss: 0.318547785282135
setp: 2600, Loss: 0.31601855158805847
setp: 2700, Loss: 0.3170863389968872
setp: 2800, Loss: 0.34856563806533813
setp: 2900, Loss: 0.3170796036720276
setp: 3000, Loss: 0.31650155782699585
setp: 3100, Loss: 0.3198554813861847
setp: 3200, Loss: 0.31747785210609436
setp: 3300, Loss: 0.31818461418151855
setp: 3400, Loss: 0.3169390857219696
setp: 3500, Loss: 0.31649795174598694
setp: 3600, Loss: 0.4237682819366455
setp: 3700, Loss: 0.31600895524024963
setp: 3800, Loss: 0.31766077876091003
setp: 3900, Loss: 0.31671494245529175
setp: 4000, Loss: 0.31792527437210083
setp: 4100, Loss: 0.3177018165588379
setp: 4200, Loss: 0.31708887219429016
setp: 4300, Loss: 0.3163551390171051
setp: 4400, Loss: 0.5194774270057678
setp: 4500, Loss: 0.4639759361743927
setp: 4600, Loss: 0.319029301404953
setp: 4700, Loss: 0.31612205505371094
setp: 4800, Loss: 0.3178485333919525
setp: 4900, Loss: 0.3168303370475769
training successfully ended.
validating...
acc: 0.9920159680638723
precision: 0.9939879759519038
recall: 0.9900199600798403
F_score: 0.9919999999999999
validating...
acc: 0.9473684210526315
precision: 0.8461538461538461
recall: 0.8461538461538461
F_score: 0.8461538461538461
model saved.
avg_acc: 0.9697368421052632, avg_f_score: 0.9140882306099698
-------------subject: 25-------------
==========valence==========
******fold 0******
[289, 319]
training...
setp: 0, Loss: 0.6930539608001709
setp: 100, Loss: 0.6749281883239746
setp: 200, Loss: 0.6012144088745117
setp: 300, Loss: 0.5198133587837219
setp: 400, Loss: 0.519128680229187
setp: 500, Loss: 0.6046245098114014
setp: 600, Loss: 0.5109863877296448
setp: 700, Loss: 0.442148894071579
setp: 800, Loss: 0.4281415343284607
setp: 900, Loss: 0.45926743745803833
setp: 1000, Loss: 0.3681415617465973
setp: 1100, Loss: 0.3556743860244751
setp: 1200, Loss: 0.3884773850440979
setp: 1300, Loss: 0.40744227170944214
setp: 1400, Loss: 0.35238367319107056
setp: 1500, Loss: 0.33196353912353516
setp: 1600, Loss: 0.32589295506477356
setp: 1700, Loss: 0.4037870764732361
setp: 1800, Loss: 0.3237941861152649
setp: 1900, Loss: 0.354543000459671
setp: 2000, Loss: 0.35156533122062683
setp: 2100, Loss: 0.42117759585380554
setp: 2200, Loss: 0.3505183756351471
setp: 2300, Loss: 0.3522576093673706
setp: 2400, Loss: 0.3514113426208496
setp: 2500, Loss: 0.3787764310836792
setp: 2600, Loss: 0.38318660855293274
setp: 2700, Loss: 0.3512957692146301
setp: 2800, Loss: 0.34958547353744507
setp: 2900, Loss: 0.3237967789173126
setp: 3000, Loss: 0.35597407817840576
setp: 3100, Loss: 0.3539687693119049
setp: 3200, Loss: 0.3484043478965759
setp: 3300, Loss: 0.3188447952270508
setp: 3400, Loss: 0.31753891706466675
setp: 3500, Loss: 0.31823834776878357
setp: 3600, Loss: 0.5965892672538757
setp: 3700, Loss: 0.3556652069091797
setp: 3800, Loss: 0.3601171374320984
setp: 3900, Loss: 0.35391440987586975
setp: 4000, Loss: 0.3378046452999115
setp: 4100, Loss: 0.319478839635849
setp: 4200, Loss: 0.35028213262557983
setp: 4300, Loss: 0.3495835065841675
setp: 4400, Loss: 0.3490121364593506
setp: 4500, Loss: 0.3492678701877594
setp: 4600, Loss: 0.35050106048583984
setp: 4700, Loss: 0.3565993309020996
setp: 4800, Loss: 0.3197895586490631
setp: 4900, Loss: 0.3482193350791931
training successfully ended.
validating...
acc: 0.9786184210526315
precision: 0.956953642384106
recall: 1.0
F_score: 0.9780033840947547
validating...
acc: 0.9276315789473685
precision: 0.9295774647887324
recall: 0.9166666666666666
F_score: 0.9230769230769231
******fold 1******
[287, 321]
training...
setp: 0, Loss: 0.7079125642776489
setp: 100, Loss: 0.6623462438583374
setp: 200, Loss: 0.5375482439994812
setp: 300, Loss: 0.44100311398506165
setp: 400, Loss: 0.4440588057041168
setp: 500, Loss: 0.3791464865207672
setp: 600, Loss: 0.33577945828437805
setp: 700, Loss: 0.33632737398147583
setp: 800, Loss: 0.35754337906837463
setp: 900, Loss: 0.3231590688228607
setp: 1000, Loss: 0.3408508896827698
setp: 1100, Loss: 0.32297495007514954
setp: 1200, Loss: 0.345746248960495
setp: 1300, Loss: 0.32761135697364807
setp: 1400, Loss: 0.3505834937095642
setp: 1500, Loss: 0.32086071372032166
setp: 1600, Loss: 0.3239593207836151
setp: 1700, Loss: 0.35365936160087585
setp: 1800, Loss: 0.351666659116745
setp: 1900, Loss: 0.33880385756492615
setp: 2000, Loss: 0.34519436955451965
setp: 2100, Loss: 0.32770994305610657
setp: 2200, Loss: 0.3270346522331238
setp: 2300, Loss: 0.31820163130760193
setp: 2400, Loss: 0.319528728723526
setp: 2500, Loss: 0.3197712004184723
setp: 2600, Loss: 0.32082992792129517
setp: 2700, Loss: 0.32108554244041443
setp: 2800, Loss: 0.32042646408081055
setp: 2900, Loss: 0.31913140416145325
setp: 3000, Loss: 0.3191492259502411
setp: 3100, Loss: 0.32018160820007324
setp: 3200, Loss: 0.32069265842437744
setp: 3300, Loss: 0.31901705265045166
setp: 3400, Loss: 0.32030805945396423
setp: 3500, Loss: 0.3191508650779724
setp: 3600, Loss: 0.333810031414032
setp: 3700, Loss: 0.34926891326904297
setp: 3800, Loss: 0.32522332668304443
setp: 3900, Loss: 0.3217222988605499
setp: 4000, Loss: 0.3241230249404907
setp: 4100, Loss: 0.31783613562583923
setp: 4200, Loss: 0.3186091184616089
setp: 4300, Loss: 0.31949523091316223
setp: 4400, Loss: 0.31948038935661316
setp: 4500, Loss: 0.32032594084739685
setp: 4600, Loss: 0.32074350118637085
setp: 4700, Loss: 0.320307195186615
setp: 4800, Loss: 0.3190394341945648
setp: 4900, Loss: 0.31891772150993347
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.9315068493150684
recall: 0.918918918918919
F_score: 0.9251700680272108
******fold 2******
[293, 315]
training...
setp: 0, Loss: 0.6955549120903015
setp: 100, Loss: 0.69058758020401
setp: 200, Loss: 0.6686059236526489
setp: 300, Loss: 0.5500075817108154
setp: 400, Loss: 0.4361553192138672
setp: 500, Loss: 0.38163140416145325
setp: 600, Loss: 0.5146608352661133
setp: 700, Loss: 0.42803284525871277
setp: 800, Loss: 0.3980443477630615
setp: 900, Loss: 0.33865073323249817
setp: 1000, Loss: 0.3686966001987457
setp: 1100, Loss: 0.3261154592037201
setp: 1200, Loss: 0.33895426988601685
setp: 1300, Loss: 0.33431145548820496
setp: 1400, Loss: 0.3336964249610901
setp: 1500, Loss: 0.3191978633403778
setp: 1600, Loss: 0.3189956545829773
setp: 1700, Loss: 0.32093995809555054
setp: 1800, Loss: 0.32240039110183716
setp: 1900, Loss: 0.34925076365470886
setp: 2000, Loss: 0.3181164860725403
setp: 2100, Loss: 0.3201286792755127
setp: 2200, Loss: 0.3191737234592438
setp: 2300, Loss: 0.3644409477710724
setp: 2400, Loss: 0.35067319869995117
setp: 2500, Loss: 0.319187730550766
setp: 2600, Loss: 0.317732036113739
setp: 2700, Loss: 0.3183274269104004
setp: 2800, Loss: 0.3174229860305786
setp: 2900, Loss: 0.3202155828475952
setp: 3000, Loss: 0.32516393065452576
setp: 3100, Loss: 0.31782612204551697
setp: 3200, Loss: 0.3179357349872589
setp: 3300, Loss: 0.3163968324661255
setp: 3400, Loss: 0.31742823123931885
setp: 3500, Loss: 0.31725406646728516
setp: 3600, Loss: 0.3194725215435028
setp: 3700, Loss: 0.31962817907333374
setp: 3800, Loss: 0.3500972092151642
setp: 3900, Loss: 0.3175523579120636
setp: 4000, Loss: 0.3169141411781311
setp: 4100, Loss: 0.3170524835586548
setp: 4200, Loss: 0.31806808710098267
setp: 4300, Loss: 0.31726744771003723
setp: 4400, Loss: 0.3175356984138489
setp: 4500, Loss: 0.3198081851005554
setp: 4600, Loss: 0.36493614315986633
setp: 4700, Loss: 0.3166477084159851
setp: 4800, Loss: 0.3177133798599243
setp: 4900, Loss: 0.34265005588531494
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9605263157894737
precision: 0.984375
recall: 0.9264705882352942
F_score: 0.9545454545454545
******fold 3******
[296, 312]
training...
setp: 0, Loss: 0.6866177320480347
setp: 100, Loss: 0.7117943167686462
setp: 200, Loss: 0.5496344566345215
setp: 300, Loss: 0.47745150327682495
setp: 400, Loss: 0.4039592444896698
setp: 500, Loss: 0.3787396252155304
setp: 600, Loss: 0.3996562063694
setp: 700, Loss: 0.33155158162117004
setp: 800, Loss: 0.3260098099708557
setp: 900, Loss: 0.32110974192619324
setp: 1000, Loss: 0.32226651906967163
setp: 1100, Loss: 0.3189828395843506
setp: 1200, Loss: 0.351474791765213
setp: 1300, Loss: 0.3623560070991516
setp: 1400, Loss: 0.3201889991760254
setp: 1500, Loss: 0.31990841031074524
setp: 1600, Loss: 0.31893596053123474
setp: 1700, Loss: 0.3196100890636444
setp: 1800, Loss: 0.32161492109298706
setp: 1900, Loss: 0.32043230533599854
setp: 2000, Loss: 0.31924694776535034
setp: 2100, Loss: 0.3185502588748932
setp: 2200, Loss: 0.3195044696331024
setp: 2300, Loss: 0.36054152250289917
setp: 2400, Loss: 0.3183315396308899
setp: 2500, Loss: 0.3193683624267578
setp: 2600, Loss: 0.3185389041900635
setp: 2700, Loss: 0.3186561167240143
setp: 2800, Loss: 0.3187165856361389
setp: 2900, Loss: 0.3189465403556824
setp: 3000, Loss: 0.3186085820198059
setp: 3100, Loss: 0.34190312027931213
setp: 3200, Loss: 0.31920671463012695
setp: 3300, Loss: 0.328506737947464
setp: 3400, Loss: 0.3198806345462799
setp: 3500, Loss: 0.3185552656650543
setp: 3600, Loss: 0.3186638355255127
setp: 3700, Loss: 0.31902140378952026
setp: 3800, Loss: 0.3194419741630554
setp: 3900, Loss: 0.31843695044517517
setp: 4000, Loss: 0.3180723488330841
setp: 4100, Loss: 0.31841665506362915
setp: 4200, Loss: 0.31948888301849365
setp: 4300, Loss: 0.33473458886146545
setp: 4400, Loss: 0.3222828209400177
setp: 4500, Loss: 0.31798869371414185
setp: 4600, Loss: 0.31840530037879944
setp: 4700, Loss: 0.3184085190296173
setp: 4800, Loss: 0.3183229863643646
setp: 4900, Loss: 0.31866469979286194
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9117647058823529
recall: 0.9538461538461539
F_score: 0.9323308270676691
******fold 4******
[279, 329]
training...
setp: 0, Loss: 0.6975024342536926
setp: 100, Loss: 0.669788122177124
setp: 200, Loss: 0.5345094203948975
setp: 300, Loss: 0.48817968368530273
setp: 400, Loss: 0.40407127141952515
setp: 500, Loss: 0.3609500527381897
setp: 600, Loss: 0.3565257787704468
setp: 700, Loss: 0.3318135142326355
setp: 800, Loss: 0.3523782789707184
setp: 900, Loss: 0.3329509496688843
setp: 1000, Loss: 0.32182079553604126
setp: 1100, Loss: 0.3226878345012665
setp: 1200, Loss: 0.3606564402580261
setp: 1300, Loss: 0.3275095522403717
setp: 1400, Loss: 0.3230046033859253
setp: 1500, Loss: 0.35023602843284607
setp: 1600, Loss: 0.3190581202507019
setp: 1700, Loss: 0.3207870423793793
setp: 1800, Loss: 0.31958767771720886
setp: 1900, Loss: 0.31968992948532104
setp: 2000, Loss: 0.3196808993816376
setp: 2100, Loss: 0.3192177712917328
setp: 2200, Loss: 0.3198424279689789
setp: 2300, Loss: 0.32062864303588867
setp: 2400, Loss: 0.32169800996780396
setp: 2500, Loss: 0.31793349981307983
setp: 2600, Loss: 0.3183775842189789
setp: 2700, Loss: 0.3210291862487793
setp: 2800, Loss: 0.32045653462409973
setp: 2900, Loss: 0.3190802335739136
setp: 3000, Loss: 0.31918203830718994
setp: 3100, Loss: 0.3546437621116638
setp: 3200, Loss: 0.32075151801109314
setp: 3300, Loss: 0.33093398809432983
setp: 3400, Loss: 0.32890433073043823
setp: 3500, Loss: 0.31779029965400696
setp: 3600, Loss: 0.31971704959869385
setp: 3700, Loss: 0.32007163763046265
setp: 3800, Loss: 0.3192075788974762
setp: 3900, Loss: 0.31905999779701233
setp: 4000, Loss: 0.3187163174152374
setp: 4100, Loss: 0.3191916048526764
setp: 4200, Loss: 0.3196631073951721
setp: 4300, Loss: 0.4000968337059021
setp: 4400, Loss: 0.32462266087532043
setp: 4500, Loss: 0.3231278657913208
setp: 4600, Loss: 0.3254353702068329
setp: 4700, Loss: 0.32475045323371887
setp: 4800, Loss: 0.35632941126823425
setp: 4900, Loss: 0.3213277757167816
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.95
recall: 0.926829268292683
F_score: 0.9382716049382716
model saved.
avg_acc: 0.9381578947368421, avg_f_score: 0.9346789755311058
==========arousal==========
******fold 0******
[149, 459]
training...
setp: 0, Loss: 0.6956384181976318
setp: 100, Loss: 0.6609941720962524
setp: 200, Loss: 0.5777187347412109
setp: 300, Loss: 0.6489734649658203
setp: 400, Loss: 0.4609082341194153
setp: 500, Loss: 0.40814870595932007
setp: 600, Loss: 0.4176153540611267
setp: 700, Loss: 0.4934450089931488
setp: 800, Loss: 0.4814666211605072
setp: 900, Loss: 0.467489093542099
setp: 1000, Loss: 0.3839372396469116
setp: 1100, Loss: 0.3626418709754944
setp: 1200, Loss: 0.3690358102321625
setp: 1300, Loss: 0.33673474192619324
setp: 1400, Loss: 0.36456194519996643
setp: 1500, Loss: 0.3461126983165741
setp: 1600, Loss: 0.33770960569381714
setp: 1700, Loss: 0.3298506438732147
setp: 1800, Loss: 0.3197312653064728
setp: 1900, Loss: 0.31750932335853577
setp: 2000, Loss: 0.3181949555873871
setp: 2100, Loss: 0.32101547718048096
setp: 2200, Loss: 0.32565414905548096
setp: 2300, Loss: 0.32007110118865967
setp: 2400, Loss: 0.31896403431892395
setp: 2500, Loss: 0.31656286120414734
setp: 2600, Loss: 0.3177046477794647
setp: 2700, Loss: 0.3511722981929779
setp: 2800, Loss: 0.31937333941459656
setp: 2900, Loss: 0.31769970059394836
setp: 3000, Loss: 0.3194805085659027
setp: 3100, Loss: 0.31868746876716614
setp: 3200, Loss: 0.3192268908023834
setp: 3300, Loss: 0.3863610625267029
setp: 3400, Loss: 0.3182527422904968
setp: 3500, Loss: 0.3185736835002899
setp: 3600, Loss: 0.3192455470561981
setp: 3700, Loss: 0.31683364510536194
setp: 3800, Loss: 0.31739702820777893
setp: 3900, Loss: 0.318865567445755
setp: 4000, Loss: 0.31881868839263916
setp: 4100, Loss: 0.32496413588523865
setp: 4200, Loss: 0.328978955745697
setp: 4300, Loss: 0.3172793984413147
setp: 4400, Loss: 0.31654343008995056
setp: 4500, Loss: 0.3172415494918823
setp: 4600, Loss: 0.3164174556732178
setp: 4700, Loss: 0.31626421213150024
setp: 4800, Loss: 0.31582608819007874
setp: 4900, Loss: 0.4090433120727539
training successfully ended.
validating...
acc: 0.9139433551198257
precision: 0.8531598513011153
recall: 1.0
F_score: 0.9207622868605818
validating...
acc: 0.8486842105263158
precision: 0.6451612903225806
recall: 0.975609756097561
F_score: 0.7766990291262136
******fold 1******
[152, 456]
training...
setp: 0, Loss: 0.6928894519805908
setp: 100, Loss: 0.639630913734436
setp: 200, Loss: 0.5307488441467285
setp: 300, Loss: 0.4512411952018738
setp: 400, Loss: 0.4527687132358551
setp: 500, Loss: 0.45612064003944397
setp: 600, Loss: 0.37125661969184875
setp: 700, Loss: 0.39474794268608093
setp: 800, Loss: 0.4251423478126526
setp: 900, Loss: 0.3787153363227844
setp: 1000, Loss: 0.37109696865081787
setp: 1100, Loss: 0.3682641386985779
setp: 1200, Loss: 0.35566121339797974
setp: 1300, Loss: 0.3358059823513031
setp: 1400, Loss: 0.3195212483406067
setp: 1500, Loss: 0.3169788420200348
setp: 1600, Loss: 0.3283308446407318
setp: 1700, Loss: 0.3179311752319336
setp: 1800, Loss: 0.31910690665245056
setp: 1900, Loss: 0.3268458843231201
setp: 2000, Loss: 0.3218304514884949
setp: 2100, Loss: 0.31951218843460083
setp: 2200, Loss: 0.3170069754123688
setp: 2300, Loss: 0.3447093367576599
setp: 2400, Loss: 0.3533869683742523
setp: 2500, Loss: 0.3173772990703583
setp: 2600, Loss: 0.318511426448822
setp: 2700, Loss: 0.371364563703537
setp: 2800, Loss: 0.34214693307876587
setp: 2900, Loss: 0.3180464804172516
setp: 3000, Loss: 0.3182593882083893
setp: 3100, Loss: 0.31944718956947327
setp: 3200, Loss: 0.31789249181747437
setp: 3300, Loss: 0.3256015181541443
setp: 3400, Loss: 0.34797006845474243
setp: 3500, Loss: 0.3325009047985077
setp: 3600, Loss: 0.3638688623905182
setp: 3700, Loss: 0.3176426589488983
setp: 3800, Loss: 0.32347792387008667
setp: 3900, Loss: 0.33289310336112976
setp: 4000, Loss: 0.32721349596977234
setp: 4100, Loss: 0.31812629103660583
setp: 4200, Loss: 0.31833964586257935
setp: 4300, Loss: 0.3172266483306885
setp: 4400, Loss: 0.3166969418525696
setp: 4500, Loss: 0.3171755075454712
setp: 4600, Loss: 0.32213038206100464
setp: 4700, Loss: 0.31860700249671936
setp: 4800, Loss: 0.31574100255966187
setp: 4900, Loss: 0.3525069057941437
training successfully ended.
validating...
acc: 0.9989035087719298
precision: 0.9978118161925602
recall: 1.0
F_score: 0.9989047097480832
validating...
acc: 0.9605263157894737
precision: 0.9210526315789473
recall: 0.9210526315789473
F_score: 0.9210526315789473
******fold 2******
[149, 459]
training...
setp: 0, Loss: 0.6999129056930542
setp: 100, Loss: 0.58766108751297
setp: 200, Loss: 0.5054726600646973
setp: 300, Loss: 0.41691887378692627
setp: 400, Loss: 0.43152952194213867
setp: 500, Loss: 0.3634551763534546
setp: 600, Loss: 0.3629082143306732
setp: 700, Loss: 0.42046815156936646
setp: 800, Loss: 0.35738658905029297
setp: 900, Loss: 0.3652231991291046
setp: 1000, Loss: 0.35184699296951294
setp: 1100, Loss: 0.3538130819797516
setp: 1200, Loss: 0.3812520205974579
setp: 1300, Loss: 0.3184971809387207
setp: 1400, Loss: 0.3460541069507599
setp: 1500, Loss: 0.32048746943473816
setp: 1600, Loss: 0.31689903140068054
setp: 1700, Loss: 0.3162170648574829
setp: 1800, Loss: 0.31803688406944275
setp: 1900, Loss: 0.315799355506897
setp: 2000, Loss: 0.3200330436229706
setp: 2100, Loss: 0.37349769473075867
setp: 2200, Loss: 0.324136346578598
setp: 2300, Loss: 0.31961381435394287
setp: 2400, Loss: 0.350848525762558
setp: 2500, Loss: 0.34537273645401
setp: 2600, Loss: 0.3171319365501404
setp: 2700, Loss: 0.34791019558906555
setp: 2800, Loss: 0.3177221715450287
setp: 2900, Loss: 0.3194739818572998
setp: 3000, Loss: 0.3453789949417114
setp: 3100, Loss: 0.32047897577285767
setp: 3200, Loss: 0.3190946578979492
setp: 3300, Loss: 0.358160138130188
setp: 3400, Loss: 0.31689271330833435
setp: 3500, Loss: 0.3228473961353302
setp: 3600, Loss: 0.3486999571323395
setp: 3700, Loss: 0.31669971346855164
setp: 3800, Loss: 0.32193806767463684
setp: 3900, Loss: 0.3582857549190521
setp: 4000, Loss: 0.31697389483451843
setp: 4100, Loss: 0.3169715106487274
setp: 4200, Loss: 0.3161157965660095
setp: 4300, Loss: 0.34088096022605896
setp: 4400, Loss: 0.31764259934425354
setp: 4500, Loss: 0.31654107570648193
setp: 4600, Loss: 0.31613078713417053
setp: 4700, Loss: 0.317108154296875
setp: 4800, Loss: 0.3153479993343353
setp: 4900, Loss: 0.3177133798599243
training successfully ended.
validating...
acc: 0.9956427015250545
precision: 0.9978118161925602
recall: 0.9934640522875817
F_score: 0.9956331877729258
validating...
acc: 0.9473684210526315
precision: 0.9230769230769231
recall: 0.8780487804878049
F_score: 0.9
******fold 3******
[152, 456]
training...
setp: 0, Loss: 0.7397521734237671
setp: 100, Loss: 0.6757229566574097
setp: 200, Loss: 0.5857580304145813
setp: 300, Loss: 0.48228177428245544
setp: 400, Loss: 0.5457401275634766
setp: 500, Loss: 0.539806604385376
setp: 600, Loss: 0.514674723148346
setp: 700, Loss: 0.4542456865310669
setp: 800, Loss: 0.38664931058883667
setp: 900, Loss: 0.38717421889305115
setp: 1000, Loss: 0.3644411265850067
setp: 1100, Loss: 0.37275323271751404
setp: 1200, Loss: 0.3570934236049652
setp: 1300, Loss: 0.3394636809825897
setp: 1400, Loss: 0.32251429557800293
setp: 1500, Loss: 0.3223665952682495
setp: 1600, Loss: 0.318451464176178
setp: 1700, Loss: 0.333891361951828
setp: 1800, Loss: 0.3511340022087097
setp: 1900, Loss: 0.3194543123245239
setp: 2000, Loss: 0.31873807311058044
setp: 2100, Loss: 0.31765425205230713
setp: 2200, Loss: 0.3209834098815918
setp: 2300, Loss: 0.3237348794937134
setp: 2400, Loss: 0.3183736205101013
setp: 2500, Loss: 0.316641241312027
setp: 2600, Loss: 0.316063791513443
setp: 2700, Loss: 0.32428833842277527
setp: 2800, Loss: 0.32788699865341187
setp: 2900, Loss: 0.3224848210811615
setp: 3000, Loss: 0.31925004720687866
setp: 3100, Loss: 0.320755273103714
setp: 3200, Loss: 0.31898003816604614
setp: 3300, Loss: 0.3180193603038788
setp: 3400, Loss: 0.3196620047092438
setp: 3500, Loss: 0.4657013714313507
setp: 3600, Loss: 0.3641016483306885
setp: 3700, Loss: 0.3178544044494629
setp: 3800, Loss: 0.318067342042923
setp: 3900, Loss: 0.31588584184646606
setp: 4000, Loss: 0.3167230486869812
setp: 4100, Loss: 0.3169344365596771
setp: 4200, Loss: 0.31680968403816223
setp: 4300, Loss: 0.5116671323776245
setp: 4400, Loss: 0.3208964765071869
setp: 4500, Loss: 0.35093259811401367
setp: 4600, Loss: 0.31670352816581726
setp: 4700, Loss: 0.31727689504623413
setp: 4800, Loss: 0.3171742558479309
setp: 4900, Loss: 0.3194553256034851
training successfully ended.
validating...
acc: 0.9616228070175439
precision: 0.9287169042769857
recall: 1.0
F_score: 0.9630411826821541
validating...
acc: 0.8486842105263158
precision: 0.631578947368421
recall: 0.9473684210526315
F_score: 0.7578947368421052
******fold 4******
[158, 450]
training...
setp: 0, Loss: 0.7322806119918823
setp: 100, Loss: 0.6406947374343872
setp: 200, Loss: 0.5559455752372742
setp: 300, Loss: 0.4865094721317291
setp: 400, Loss: 0.3698748052120209
setp: 500, Loss: 0.4823954403400421
setp: 600, Loss: 0.387511670589447
setp: 700, Loss: 0.39338454604148865
setp: 800, Loss: 0.35577842593193054
setp: 900, Loss: 0.36740630865097046
setp: 1000, Loss: 0.3579259514808655
setp: 1100, Loss: 0.3267732858657837
setp: 1200, Loss: 0.32143285870552063
setp: 1300, Loss: 0.42898353934288025
setp: 1400, Loss: 0.3486187756061554
setp: 1500, Loss: 0.356455534696579
setp: 1600, Loss: 0.34337925910949707
setp: 1700, Loss: 0.3491571843624115
setp: 1800, Loss: 0.321609765291214
setp: 1900, Loss: 0.32436272501945496
setp: 2000, Loss: 0.31649234890937805
setp: 2100, Loss: 0.3184531331062317
setp: 2200, Loss: 0.3215859532356262
setp: 2300, Loss: 0.32120585441589355
setp: 2400, Loss: 0.31679800152778625
setp: 2500, Loss: 0.3166104853153229
setp: 2600, Loss: 0.3181036412715912
setp: 2700, Loss: 0.35106590390205383
setp: 2800, Loss: 0.31752461194992065
setp: 2900, Loss: 0.3168603777885437
setp: 3000, Loss: 0.32934075593948364
setp: 3100, Loss: 0.3312658667564392
setp: 3200, Loss: 0.32032421231269836
setp: 3300, Loss: 0.3190734088420868
setp: 3400, Loss: 0.3173295557498932
setp: 3500, Loss: 0.3169914782047272
setp: 3600, Loss: 0.32039836049079895
setp: 3700, Loss: 0.3178856074810028
setp: 3800, Loss: 0.3181561231613159
setp: 3900, Loss: 0.31699830293655396
setp: 4000, Loss: 0.327244371175766
setp: 4100, Loss: 0.31884950399398804
setp: 4200, Loss: 0.3685767650604248
setp: 4300, Loss: 0.31801286339759827
setp: 4400, Loss: 0.31640660762786865
setp: 4500, Loss: 0.316957026720047
setp: 4600, Loss: 0.33289650082588196
setp: 4700, Loss: 0.32579678297042847
setp: 4800, Loss: 0.3169004023075104
setp: 4900, Loss: 0.31561392545700073
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9736842105263158
precision: 0.9117647058823529
recall: 0.96875
F_score: 0.9393939393939394
model saved.
avg_acc: 0.9157894736842105, avg_f_score: 0.8590080673882412
-------------subject: 26-------------
==========valence==========
******fold 0******
[221, 387]
training...
setp: 0, Loss: 0.6751103401184082
setp: 100, Loss: 0.6949901580810547
setp: 200, Loss: 0.659880518913269
setp: 300, Loss: 0.5694847702980042
setp: 400, Loss: 0.40151065587997437
setp: 500, Loss: 0.37141504883766174
setp: 600, Loss: 0.3893199563026428
setp: 700, Loss: 0.3267100155353546
setp: 800, Loss: 0.32414510846138
setp: 900, Loss: 0.3237597942352295
setp: 1000, Loss: 0.3521648645401001
setp: 1100, Loss: 0.338365763425827
setp: 1200, Loss: 0.3239190876483917
setp: 1300, Loss: 0.3203425407409668
setp: 1400, Loss: 0.3259575664997101
setp: 1500, Loss: 0.3220995366573334
setp: 1600, Loss: 0.320719838142395
setp: 1700, Loss: 0.3209153413772583
setp: 1800, Loss: 0.318057656288147
setp: 1900, Loss: 0.3517981469631195
setp: 2000, Loss: 0.31810620427131653
setp: 2100, Loss: 0.3207242786884308
setp: 2200, Loss: 0.32207196950912476
setp: 2300, Loss: 0.38437268137931824
setp: 2400, Loss: 0.3217112421989441
setp: 2500, Loss: 0.350085973739624
setp: 2600, Loss: 0.3188685476779938
setp: 2700, Loss: 0.3194129168987274
setp: 2800, Loss: 0.319610059261322
setp: 2900, Loss: 0.3508404791355133
setp: 3000, Loss: 0.32002344727516174
setp: 3100, Loss: 0.3186313211917877
setp: 3200, Loss: 0.3188903033733368
setp: 3300, Loss: 0.3206484615802765
setp: 3400, Loss: 0.3195228576660156
setp: 3500, Loss: 0.31899338960647583
setp: 3600, Loss: 0.34890949726104736
setp: 3700, Loss: 0.3263549506664276
setp: 3800, Loss: 0.35185176134109497
setp: 3900, Loss: 0.3180697560310364
setp: 4000, Loss: 0.31913068890571594
setp: 4100, Loss: 0.31920763850212097
setp: 4200, Loss: 0.3192952871322632
setp: 4300, Loss: 0.3211735785007477
setp: 4400, Loss: 0.3207564949989319
setp: 4500, Loss: 0.3187761902809143
setp: 4600, Loss: 0.31923991441726685
setp: 4700, Loss: 0.31934526562690735
setp: 4800, Loss: 0.351533979177475
setp: 4900, Loss: 0.32042407989501953
training successfully ended.
validating...
acc: 0.8174342105263158
precision: 0.9910714285714286
recall: 0.502262443438914
F_score: 0.6666666666666666
validating...
acc: 0.7894736842105263
precision: 0.8823529411764706
recall: 0.3333333333333333
F_score: 0.48387096774193544
******fold 1******
[209, 399]
training...
setp: 0, Loss: 0.6419174075126648
setp: 100, Loss: 0.5815147757530212
setp: 200, Loss: 0.5804956555366516
setp: 300, Loss: 0.4858267307281494
setp: 400, Loss: 0.4168242812156677
setp: 500, Loss: 0.3515768051147461
setp: 600, Loss: 0.32962581515312195
setp: 700, Loss: 0.3625655174255371
setp: 800, Loss: 0.3314861059188843
setp: 900, Loss: 0.32417649030685425
setp: 1000, Loss: 0.36309751868247986
setp: 1100, Loss: 0.3366721272468567
setp: 1200, Loss: 0.3200641870498657
setp: 1300, Loss: 0.3216313421726227
setp: 1400, Loss: 0.33344393968582153
setp: 1500, Loss: 0.32037463784217834
setp: 1600, Loss: 0.31891337037086487
setp: 1700, Loss: 0.31900423765182495
setp: 1800, Loss: 0.3188742995262146
setp: 1900, Loss: 0.3183726668357849
setp: 2000, Loss: 0.31825530529022217
setp: 2100, Loss: 0.3502756953239441
setp: 2200, Loss: 0.33412250876426697
setp: 2300, Loss: 0.3377368450164795
setp: 2400, Loss: 0.3198845088481903
setp: 2500, Loss: 0.3182253837585449
setp: 2600, Loss: 0.31986334919929504
setp: 2700, Loss: 0.32005101442337036
setp: 2800, Loss: 0.3185631036758423
setp: 2900, Loss: 0.31887730956077576
setp: 3000, Loss: 0.3189820647239685
setp: 3100, Loss: 0.3189261853694916
setp: 3200, Loss: 0.3238658905029297
setp: 3300, Loss: 0.33323386311531067
setp: 3400, Loss: 0.3212435841560364
setp: 3500, Loss: 0.331692636013031
setp: 3600, Loss: 0.319673627614975
setp: 3700, Loss: 0.3197181522846222
setp: 3800, Loss: 0.31747475266456604
setp: 3900, Loss: 0.31796637177467346
setp: 4000, Loss: 0.31953299045562744
setp: 4100, Loss: 0.31993383169174194
setp: 4200, Loss: 0.3202283978462219
setp: 4300, Loss: 0.3206078112125397
setp: 4400, Loss: 0.31943637132644653
setp: 4500, Loss: 0.3200024962425232
setp: 4600, Loss: 0.32016095519065857
setp: 4700, Loss: 0.3195924460887909
setp: 4800, Loss: 0.319074422121048
setp: 4900, Loss: 0.32001355290412903
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8486842105263158
precision: 0.8035714285714286
recall: 0.7894736842105263
F_score: 0.7964601769911505
******fold 2******
[210, 398]
training...
setp: 0, Loss: 0.6246269345283508
setp: 100, Loss: 0.6428019404411316
setp: 200, Loss: 0.6818708777427673
setp: 300, Loss: 0.5450864434242249
setp: 400, Loss: 0.45126229524612427
setp: 500, Loss: 0.3955954611301422
setp: 600, Loss: 0.393586665391922
setp: 700, Loss: 0.34511157870292664
setp: 800, Loss: 0.37239399552345276
setp: 900, Loss: 0.3638019859790802
setp: 1000, Loss: 0.33421292901039124
setp: 1100, Loss: 0.3205638527870178
setp: 1200, Loss: 0.3260427415370941
setp: 1300, Loss: 0.3632495403289795
setp: 1400, Loss: 0.3301967680454254
setp: 1500, Loss: 0.3381401598453522
setp: 1600, Loss: 0.32613497972488403
setp: 1700, Loss: 0.31990334391593933
setp: 1800, Loss: 0.31930387020111084
setp: 1900, Loss: 0.3716786205768585
setp: 2000, Loss: 0.3182350993156433
setp: 2100, Loss: 0.35085567831993103
setp: 2200, Loss: 0.31877461075782776
setp: 2300, Loss: 0.3490317761898041
setp: 2400, Loss: 0.3209543526172638
setp: 2500, Loss: 0.3211299777030945
setp: 2600, Loss: 0.3193373680114746
setp: 2700, Loss: 0.31964924931526184
setp: 2800, Loss: 0.3206241726875305
setp: 2900, Loss: 0.38966092467308044
setp: 3000, Loss: 0.32532772421836853
setp: 3100, Loss: 0.31779810786247253
setp: 3200, Loss: 0.3362918794155121
setp: 3300, Loss: 0.3206765353679657
setp: 3400, Loss: 0.3274405598640442
setp: 3500, Loss: 0.3186783790588379
setp: 3600, Loss: 0.3258182406425476
setp: 3700, Loss: 0.31972190737724304
setp: 3800, Loss: 0.3303436040878296
setp: 3900, Loss: 0.31934410333633423
setp: 4000, Loss: 0.35012301802635193
setp: 4100, Loss: 0.3188973367214203
setp: 4200, Loss: 0.34915891289711
setp: 4300, Loss: 0.32038629055023193
setp: 4400, Loss: 0.31943872570991516
setp: 4500, Loss: 0.31935209035873413
setp: 4600, Loss: 0.3193563222885132
setp: 4700, Loss: 0.32039374113082886
setp: 4800, Loss: 0.3199288547039032
setp: 4900, Loss: 0.35653021931648254
training successfully ended.
validating...
acc: 0.9802631578947368
precision: 0.9759615384615384
recall: 0.9666666666666667
F_score: 0.9712918660287081
validating...
acc: 0.9144736842105263
precision: 0.9215686274509803
recall: 0.8392857142857143
F_score: 0.8785046728971961
******fold 3******
[217, 391]
training...
setp: 0, Loss: 0.6704910397529602
setp: 100, Loss: 0.6250042915344238
setp: 200, Loss: 0.631869375705719
setp: 300, Loss: 0.6067953109741211
setp: 400, Loss: 0.5507843494415283
setp: 500, Loss: 0.5224416255950928
setp: 600, Loss: 0.3823670446872711
setp: 700, Loss: 0.36904478073120117
setp: 800, Loss: 0.35072943568229675
setp: 900, Loss: 0.39273810386657715
setp: 1000, Loss: 0.3349653482437134
setp: 1100, Loss: 0.35517439246177673
setp: 1200, Loss: 0.3216710388660431
setp: 1300, Loss: 0.32152292132377625
setp: 1400, Loss: 0.3193374574184418
setp: 1500, Loss: 0.3504618704319
setp: 1600, Loss: 0.3184417486190796
setp: 1700, Loss: 0.3190619647502899
setp: 1800, Loss: 0.3212631046772003
setp: 1900, Loss: 0.31782764196395874
setp: 2000, Loss: 0.3160170316696167
setp: 2100, Loss: 0.3633471727371216
setp: 2200, Loss: 0.4111819267272949
setp: 2300, Loss: 0.40844839811325073
setp: 2400, Loss: 0.3860431909561157
setp: 2500, Loss: 0.3159245252609253
setp: 2600, Loss: 0.3182486295700073
setp: 2700, Loss: 0.3169268071651459
setp: 2800, Loss: 0.31878504157066345
setp: 2900, Loss: 0.3187200129032135
setp: 3000, Loss: 0.317649781703949
setp: 3100, Loss: 0.31895920634269714
setp: 3200, Loss: 0.3193471431732178
setp: 3300, Loss: 0.3174923062324524
setp: 3400, Loss: 0.31918826699256897
setp: 3500, Loss: 0.31757280230522156
setp: 3600, Loss: 0.5365241169929504
setp: 3700, Loss: 0.3361246883869171
setp: 3800, Loss: 0.3634311258792877
setp: 3900, Loss: 0.3165183663368225
setp: 4000, Loss: 0.3472607433795929
setp: 4100, Loss: 0.31787800788879395
setp: 4200, Loss: 0.3169693946838379
setp: 4300, Loss: 0.31784382462501526
setp: 4400, Loss: 0.3162888288497925
setp: 4500, Loss: 0.3177735209465027
setp: 4600, Loss: 0.3172180652618408
setp: 4700, Loss: 0.3176879584789276
setp: 4800, Loss: 0.3179096579551697
setp: 4900, Loss: 0.31726858019828796
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9347826086956522
recall: 0.8775510204081632
F_score: 0.9052631578947369
******fold 4******
[207, 401]
training...
setp: 0, Loss: 0.6824142932891846
setp: 100, Loss: 0.6019998788833618
setp: 200, Loss: 0.6217372417449951
setp: 300, Loss: 0.6561580896377563
setp: 400, Loss: 0.51888507604599
setp: 500, Loss: 0.4675956964492798
setp: 600, Loss: 0.4066011309623718
setp: 700, Loss: 0.35323676466941833
setp: 800, Loss: 0.38403746485710144
setp: 900, Loss: 0.3562646210193634
setp: 1000, Loss: 0.33622005581855774
setp: 1100, Loss: 0.32680585980415344
setp: 1200, Loss: 0.3399199843406677
setp: 1300, Loss: 0.3401230275630951
setp: 1400, Loss: 0.3922356963157654
setp: 1500, Loss: 0.3281472623348236
setp: 1600, Loss: 0.3240543007850647
setp: 1700, Loss: 0.3210884630680084
setp: 1800, Loss: 0.319158673286438
setp: 1900, Loss: 0.3514680564403534
setp: 2000, Loss: 0.3191589117050171
setp: 2100, Loss: 0.32086312770843506
setp: 2200, Loss: 0.35094040632247925
setp: 2300, Loss: 0.35160234570503235
setp: 2400, Loss: 0.32047557830810547
setp: 2500, Loss: 0.319033145904541
setp: 2600, Loss: 0.33114492893218994
setp: 2700, Loss: 0.32579943537712097
setp: 2800, Loss: 0.31986114382743835
setp: 2900, Loss: 0.31914278864860535
setp: 3000, Loss: 0.3187873661518097
setp: 3100, Loss: 0.3183257579803467
setp: 3200, Loss: 0.3206948935985565
setp: 3300, Loss: 0.3197025954723358
setp: 3400, Loss: 0.31886881589889526
setp: 3500, Loss: 0.32033297419548035
setp: 3600, Loss: 0.31985723972320557
setp: 3700, Loss: 0.31963294744491577
setp: 3800, Loss: 0.3502643406391144
setp: 3900, Loss: 0.5439797043800354
setp: 4000, Loss: 0.370168000459671
setp: 4100, Loss: 0.33560648560523987
setp: 4200, Loss: 0.3774733543395996
setp: 4300, Loss: 0.3408854305744171
setp: 4400, Loss: 0.34609177708625793
setp: 4500, Loss: 0.3270387649536133
setp: 4600, Loss: 0.3275212049484253
setp: 4700, Loss: 0.3316584825515747
setp: 4800, Loss: 0.32803940773010254
setp: 4900, Loss: 0.32730546593666077
training successfully ended.
validating...
acc: 0.9786184210526315
precision: 0.985
recall: 0.9516908212560387
F_score: 0.968058968058968
validating...
acc: 0.9013157894736842
precision: 0.84375
recall: 0.9152542372881356
F_score: 0.8780487804878049
model saved.
avg_acc: 0.8789473684210526, avg_f_score: 0.7884295512025647
==========arousal==========
******fold 0******
[354, 254]
training...
setp: 0, Loss: 0.7658699750900269
setp: 100, Loss: 0.6197694540023804
setp: 200, Loss: 0.5858530402183533
setp: 300, Loss: 0.6704728603363037
setp: 400, Loss: 0.5474288463592529
setp: 500, Loss: 0.4576985836029053
setp: 600, Loss: 0.43518516421318054
setp: 700, Loss: 0.3838631212711334
setp: 800, Loss: 0.35130491852760315
setp: 900, Loss: 0.3302123546600342
setp: 1000, Loss: 0.3264189660549164
setp: 1100, Loss: 0.33923786878585815
setp: 1200, Loss: 0.35850635170936584
setp: 1300, Loss: 0.3283391296863556
setp: 1400, Loss: 0.3270251750946045
setp: 1500, Loss: 0.3218354284763336
setp: 1600, Loss: 0.3218027651309967
setp: 1700, Loss: 0.3196239769458771
setp: 1800, Loss: 0.3205852508544922
setp: 1900, Loss: 0.3203069865703583
setp: 2000, Loss: 0.3183485269546509
setp: 2100, Loss: 0.3176940083503723
setp: 2200, Loss: 0.4101998805999756
setp: 2300, Loss: 0.3186075985431671
setp: 2400, Loss: 0.32075226306915283
setp: 2500, Loss: 0.3187207877635956
setp: 2600, Loss: 0.31809306144714355
setp: 2700, Loss: 0.31801649928092957
setp: 2800, Loss: 0.3190338611602783
setp: 2900, Loss: 0.31910040974617004
setp: 3000, Loss: 0.31892120838165283
setp: 3100, Loss: 0.31850239634513855
setp: 3200, Loss: 0.31931132078170776
setp: 3300, Loss: 0.3194764256477356
setp: 3400, Loss: 0.31885644793510437
setp: 3500, Loss: 0.32986804842948914
setp: 3600, Loss: 0.3227856159210205
setp: 3700, Loss: 0.32546380162239075
setp: 3800, Loss: 0.3218827545642853
setp: 3900, Loss: 0.3167063593864441
setp: 4000, Loss: 0.3170793950557709
setp: 4100, Loss: 0.3181793689727783
setp: 4200, Loss: 0.3181256651878357
setp: 4300, Loss: 0.3192685842514038
setp: 4400, Loss: 0.31869807839393616
setp: 4500, Loss: 0.31776323914527893
setp: 4600, Loss: 0.3176562190055847
setp: 4700, Loss: 0.31846973299980164
setp: 4800, Loss: 0.31821587681770325
setp: 4900, Loss: 0.317710816860199
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9078947368421053
precision: 0.8791208791208791
recall: 0.963855421686747
F_score: 0.9195402298850575
******fold 1******
[350, 258]
training...
setp: 0, Loss: 0.7155274748802185
setp: 100, Loss: 0.620821475982666
setp: 200, Loss: 0.6542333960533142
setp: 300, Loss: 0.661359965801239
setp: 400, Loss: 0.5716234445571899
setp: 500, Loss: 0.5645582675933838
setp: 600, Loss: 0.5658646821975708
setp: 700, Loss: 0.6207365989685059
setp: 800, Loss: 0.4824678599834442
setp: 900, Loss: 0.5114074945449829
setp: 1000, Loss: 0.4516220688819885
setp: 1100, Loss: 0.3646946847438812
setp: 1200, Loss: 0.42073991894721985
setp: 1300, Loss: 0.40760257840156555
setp: 1400, Loss: 0.3664633631706238
setp: 1500, Loss: 0.3211086094379425
setp: 1600, Loss: 0.3903943598270416
setp: 1700, Loss: 0.3196789622306824
setp: 1800, Loss: 0.3530310392379761
setp: 1900, Loss: 0.3526402711868286
setp: 2000, Loss: 0.3191048502922058
setp: 2100, Loss: 0.3212461769580841
setp: 2200, Loss: 0.32368186116218567
setp: 2300, Loss: 0.31704509258270264
setp: 2400, Loss: 0.3242853581905365
setp: 2500, Loss: 0.3322461247444153
setp: 2600, Loss: 0.37081485986709595
setp: 2700, Loss: 0.3160199522972107
setp: 2800, Loss: 0.395842045545578
setp: 2900, Loss: 0.32505595684051514
setp: 3000, Loss: 0.32845762372016907
setp: 3100, Loss: 0.3194665312767029
setp: 3200, Loss: 0.35006648302078247
setp: 3300, Loss: 0.31582382321357727
setp: 3400, Loss: 0.3190939724445343
setp: 3500, Loss: 0.34789761900901794
setp: 3600, Loss: 0.3163955509662628
setp: 3700, Loss: 0.31756410002708435
setp: 3800, Loss: 0.34805968403816223
setp: 3900, Loss: 0.32064756751060486
setp: 4000, Loss: 0.3183853328227997
setp: 4100, Loss: 0.3165690302848816
setp: 4200, Loss: 0.31617382168769836
setp: 4300, Loss: 0.3398064076900482
setp: 4400, Loss: 0.319242924451828
setp: 4500, Loss: 0.3235405385494232
setp: 4600, Loss: 0.31796538829803467
setp: 4700, Loss: 0.32672253251075745
setp: 4800, Loss: 0.37730827927589417
setp: 4900, Loss: 0.3177696764469147
training successfully ended.
validating...
acc: 0.9375
precision: 1.0
recall: 0.8914285714285715
F_score: 0.9425981873111783
validating...
acc: 0.8421052631578947
precision: 0.9565217391304348
recall: 0.7586206896551724
F_score: 0.8461538461538461
******fold 2******
[347, 261]
training...
setp: 0, Loss: 0.6809203028678894
setp: 100, Loss: 0.6527665853500366
setp: 200, Loss: 0.6143758893013
setp: 300, Loss: 0.6479852795600891
setp: 400, Loss: 0.5520086288452148
setp: 500, Loss: 0.4510299265384674
setp: 600, Loss: 0.42109358310699463
setp: 700, Loss: 0.38385337591171265
setp: 800, Loss: 0.3392399549484253
setp: 900, Loss: 0.3564552366733551
setp: 1000, Loss: 0.33268505334854126
setp: 1100, Loss: 0.32424718141555786
setp: 1200, Loss: 0.3323439061641693
setp: 1300, Loss: 0.34038642048835754
setp: 1400, Loss: 0.32974445819854736
setp: 1500, Loss: 0.3309019207954407
setp: 1600, Loss: 0.32068556547164917
setp: 1700, Loss: 0.3208760619163513
setp: 1800, Loss: 0.3217352330684662
setp: 1900, Loss: 0.32260116934776306
setp: 2000, Loss: 0.32189440727233887
setp: 2100, Loss: 0.3204064667224884
setp: 2200, Loss: 0.3232996165752411
setp: 2300, Loss: 0.3214966356754303
setp: 2400, Loss: 0.3215857446193695
setp: 2500, Loss: 0.32163679599761963
setp: 2600, Loss: 0.32214513421058655
setp: 2700, Loss: 0.3206799626350403
setp: 2800, Loss: 0.3773097097873688
setp: 2900, Loss: 0.3271976113319397
setp: 3000, Loss: 0.32008373737335205
setp: 3100, Loss: 0.31992775201797485
setp: 3200, Loss: 0.31959810853004456
setp: 3300, Loss: 0.3208763599395752
setp: 3400, Loss: 0.3213070034980774
setp: 3500, Loss: 0.32009708881378174
setp: 3600, Loss: 0.3202204704284668
setp: 3700, Loss: 0.3212486803531647
setp: 3800, Loss: 0.32150930166244507
setp: 3900, Loss: 0.3208431601524353
setp: 4000, Loss: 0.31983765959739685
setp: 4100, Loss: 0.32226133346557617
setp: 4200, Loss: 0.32065215706825256
setp: 4300, Loss: 0.3204292356967926
setp: 4400, Loss: 0.3209127187728882
setp: 4500, Loss: 0.32103675603866577
setp: 4600, Loss: 0.3389972150325775
setp: 4700, Loss: 0.33652329444885254
setp: 4800, Loss: 0.3305457830429077
setp: 4900, Loss: 0.3299713432788849
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9971181556195965
F_score: 0.9985569985569985
validating...
acc: 0.8881578947368421
precision: 0.9506172839506173
recall: 0.8555555555555555
F_score: 0.9005847953216374
******fold 3******
[355, 253]
training...
setp: 0, Loss: 0.6973185539245605
setp: 100, Loss: 0.6752811670303345
setp: 200, Loss: 0.6030305027961731
setp: 300, Loss: 0.6487812399864197
setp: 400, Loss: 0.5671703815460205
setp: 500, Loss: 0.5852099657058716
setp: 600, Loss: 0.5952950716018677
setp: 700, Loss: 0.552985429763794
setp: 800, Loss: 0.4511828124523163
setp: 900, Loss: 0.4677380919456482
setp: 1000, Loss: 0.5213411450386047
setp: 1100, Loss: 0.4534722864627838
setp: 1200, Loss: 0.44341444969177246
setp: 1300, Loss: 0.3952857553958893
setp: 1400, Loss: 0.3561443090438843
setp: 1500, Loss: 0.36183249950408936
setp: 1600, Loss: 0.34363871812820435
setp: 1700, Loss: 0.3388318717479706
setp: 1800, Loss: 0.3222373425960541
setp: 1900, Loss: 0.3210449516773224
setp: 2000, Loss: 0.3351510465145111
setp: 2100, Loss: 0.3513934016227722
setp: 2200, Loss: 0.3951479196548462
setp: 2300, Loss: 0.3214181065559387
setp: 2400, Loss: 0.32013818621635437
setp: 2500, Loss: 0.36815670132637024
setp: 2600, Loss: 0.3170349895954132
setp: 2700, Loss: 0.32207611203193665
setp: 2800, Loss: 0.35158398747444153
setp: 2900, Loss: 0.35475271940231323
setp: 3000, Loss: 0.34659716486930847
setp: 3100, Loss: 0.38753730058670044
setp: 3200, Loss: 0.33825525641441345
setp: 3300, Loss: 0.35019105672836304
setp: 3400, Loss: 0.322187215089798
setp: 3500, Loss: 0.369353711605072
setp: 3600, Loss: 0.5550538301467896
setp: 3700, Loss: 0.32997554540634155
setp: 3800, Loss: 0.3169640004634857
setp: 3900, Loss: 0.31869736313819885
setp: 4000, Loss: 0.3180200159549713
setp: 4100, Loss: 0.3478294312953949
setp: 4200, Loss: 0.31633639335632324
setp: 4300, Loss: 0.31815457344055176
setp: 4400, Loss: 0.31741976737976074
setp: 4500, Loss: 0.31736499071121216
setp: 4600, Loss: 0.31777989864349365
setp: 4700, Loss: 0.31902894377708435
setp: 4800, Loss: 0.31749340891838074
setp: 4900, Loss: 0.32057833671569824
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9971830985915493
recall: 0.9971830985915493
F_score: 0.9971830985915493
validating...
acc: 0.881578947368421
precision: 0.872093023255814
recall: 0.9146341463414634
F_score: 0.8928571428571429
******fold 4******
[342, 266]
training...
setp: 0, Loss: 0.6929495930671692
setp: 100, Loss: 0.6345816254615784
setp: 200, Loss: 0.6297283172607422
setp: 300, Loss: 0.5702950358390808
setp: 400, Loss: 0.48670122027397156
setp: 500, Loss: 0.4196009337902069
setp: 600, Loss: 0.36157193779945374
setp: 700, Loss: 0.36530041694641113
setp: 800, Loss: 0.336326003074646
setp: 900, Loss: 0.3301961123943329
setp: 1000, Loss: 0.3280368447303772
setp: 1100, Loss: 0.32310113310813904
setp: 1200, Loss: 0.3216167986392975
setp: 1300, Loss: 0.3216458261013031
setp: 1400, Loss: 0.3233174681663513
setp: 1500, Loss: 0.32297536730766296
setp: 1600, Loss: 0.32026734948158264
setp: 1700, Loss: 0.36049139499664307
setp: 1800, Loss: 0.32319706678390503
setp: 1900, Loss: 0.32448211312294006
setp: 2000, Loss: 0.3278884291648865
setp: 2100, Loss: 0.3236168324947357
setp: 2200, Loss: 0.32114458084106445
setp: 2300, Loss: 0.32041609287261963
setp: 2400, Loss: 0.3192272186279297
setp: 2500, Loss: 0.32222557067871094
setp: 2600, Loss: 0.31957346200942993
setp: 2700, Loss: 0.3194544017314911
setp: 2800, Loss: 0.3215128183364868
setp: 2900, Loss: 0.32133546471595764
setp: 3000, Loss: 0.3191820979118347
setp: 3100, Loss: 0.31894025206565857
setp: 3200, Loss: 0.3198159635066986
setp: 3300, Loss: 0.32064276933670044
setp: 3400, Loss: 0.3208794593811035
setp: 3500, Loss: 0.3570552170276642
setp: 3600, Loss: 0.3204643428325653
setp: 3700, Loss: 0.3822416067123413
setp: 3800, Loss: 0.32021990418434143
setp: 3900, Loss: 0.3190303146839142
setp: 4000, Loss: 0.3198796510696411
setp: 4100, Loss: 0.3197171986103058
setp: 4200, Loss: 0.31976717710494995
setp: 4300, Loss: 0.31957879662513733
setp: 4400, Loss: 0.32132983207702637
setp: 4500, Loss: 0.31981396675109863
setp: 4600, Loss: 0.31926900148391724
setp: 4700, Loss: 0.3207744061946869
setp: 4800, Loss: 0.3207182288169861
setp: 4900, Loss: 0.3190178871154785
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.881578947368421
precision: 0.8888888888888888
recall: 0.9263157894736842
F_score: 0.9072164948453608
model saved.
avg_acc: 0.8802631578947369, avg_f_score: 0.8932705018126089
-------------subject: 27-------------
==========valence==========
******fold 0******
[160, 448]
training...
setp: 0, Loss: 0.7029027342796326
setp: 100, Loss: 0.584385097026825
setp: 200, Loss: 0.6271020174026489
setp: 300, Loss: 0.459379106760025
setp: 400, Loss: 0.49645841121673584
setp: 500, Loss: 0.4284425675868988
setp: 600, Loss: 0.4181244969367981
setp: 700, Loss: 0.4003885090351105
setp: 800, Loss: 0.39566418528556824
setp: 900, Loss: 0.4911806881427765
setp: 1000, Loss: 0.3570447862148285
setp: 1100, Loss: 0.4073738753795624
setp: 1200, Loss: 0.4263485372066498
setp: 1300, Loss: 0.353535920381546
setp: 1400, Loss: 0.3224446475505829
setp: 1500, Loss: 0.3223668336868286
setp: 1600, Loss: 0.3196282982826233
setp: 1700, Loss: 0.3470780849456787
setp: 1800, Loss: 0.32844266295433044
setp: 1900, Loss: 0.33965033292770386
setp: 2000, Loss: 0.3293551206588745
setp: 2100, Loss: 0.3297514319419861
setp: 2200, Loss: 0.3166479468345642
setp: 2300, Loss: 0.31726452708244324
setp: 2400, Loss: 0.368095338344574
setp: 2500, Loss: 0.3166619837284088
setp: 2600, Loss: 0.31600168347358704
setp: 2700, Loss: 0.31628555059432983
setp: 2800, Loss: 0.32394346594810486
setp: 2900, Loss: 0.3158426284790039
setp: 3000, Loss: 0.3158085346221924
setp: 3100, Loss: 0.3464025855064392
setp: 3200, Loss: 0.3192792534828186
setp: 3300, Loss: 0.3171716034412384
setp: 3400, Loss: 0.3159424960613251
setp: 3500, Loss: 0.3175123929977417
setp: 3600, Loss: 0.31604161858558655
setp: 3700, Loss: 0.31612372398376465
setp: 3800, Loss: 0.3468175232410431
setp: 3900, Loss: 0.5828078985214233
setp: 4000, Loss: 0.35067564249038696
setp: 4100, Loss: 0.3158600628376007
setp: 4200, Loss: 0.3168639540672302
setp: 4300, Loss: 0.3159661591053009
setp: 4400, Loss: 0.3158435523509979
setp: 4500, Loss: 0.34661394357681274
setp: 4600, Loss: 0.3186820149421692
setp: 4700, Loss: 0.3256911337375641
setp: 4800, Loss: 0.31620728969573975
setp: 4900, Loss: 0.31691426038742065
training successfully ended.
validating...
acc: 0.9966517857142857
precision: 0.9933481152993349
recall: 1.0
F_score: 0.9966629588431591
validating...
acc: 0.9144736842105263
precision: 0.7575757575757576
recall: 0.8333333333333334
F_score: 0.7936507936507938
******fold 1******
[145, 463]
training...
setp: 0, Loss: 0.7040017247200012
setp: 100, Loss: 0.6230863332748413
setp: 200, Loss: 0.4439508020877838
setp: 300, Loss: 0.3440225422382355
setp: 400, Loss: 0.34032249450683594
setp: 500, Loss: 0.332035630941391
setp: 600, Loss: 0.32066354155540466
setp: 700, Loss: 0.33738937973976135
setp: 800, Loss: 0.32034704089164734
setp: 900, Loss: 0.31827518343925476
setp: 1000, Loss: 0.31980395317077637
setp: 1100, Loss: 0.3178485929965973
setp: 1200, Loss: 0.3196292221546173
setp: 1300, Loss: 0.3175399601459503
setp: 1400, Loss: 0.3194832503795624
setp: 1500, Loss: 0.3193364143371582
setp: 1600, Loss: 0.3208380341529846
setp: 1700, Loss: 0.31819191575050354
setp: 1800, Loss: 0.31615790724754333
setp: 1900, Loss: 0.31796300411224365
setp: 2000, Loss: 0.316438227891922
setp: 2100, Loss: 0.31797856092453003
setp: 2200, Loss: 0.3184553384780884
setp: 2300, Loss: 0.31724148988723755
setp: 2400, Loss: 0.34581679105758667
setp: 2500, Loss: 0.31791990995407104
setp: 2600, Loss: 0.3166082799434662
setp: 2700, Loss: 0.3162843883037567
setp: 2800, Loss: 0.3162086009979248
setp: 2900, Loss: 0.3162083029747009
setp: 3000, Loss: 0.3184088468551636
setp: 3100, Loss: 0.3179217576980591
setp: 3200, Loss: 0.3216295838356018
setp: 3300, Loss: 0.316451758146286
setp: 3400, Loss: 0.3159864544868469
setp: 3500, Loss: 0.3156175911426544
setp: 3600, Loss: 0.31708815693855286
setp: 3700, Loss: 0.31587231159210205
setp: 3800, Loss: 0.31642234325408936
setp: 3900, Loss: 0.31877556443214417
setp: 4000, Loss: 0.3164770007133484
setp: 4100, Loss: 0.317302942276001
setp: 4200, Loss: 0.31662437319755554
setp: 4300, Loss: 0.3169637620449066
setp: 4400, Loss: 0.3264974355697632
setp: 4500, Loss: 0.3160395622253418
setp: 4600, Loss: 0.31720295548439026
setp: 4700, Loss: 0.3162032961845398
setp: 4800, Loss: 0.3182688355445862
setp: 4900, Loss: 0.31632503867149353
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9539473684210527
precision: 0.9523809523809523
recall: 0.8888888888888888
F_score: 0.9195402298850575
******fold 2******
[141, 467]
training...
setp: 0, Loss: 0.693325400352478
setp: 100, Loss: 0.519238293170929
setp: 200, Loss: 0.4604641795158386
setp: 300, Loss: 0.48825281858444214
setp: 400, Loss: 0.4302586615085602
setp: 500, Loss: 0.3395705819129944
setp: 600, Loss: 0.3573110103607178
setp: 700, Loss: 0.340005487203598
setp: 800, Loss: 0.33737823367118835
setp: 900, Loss: 0.32493671774864197
setp: 1000, Loss: 0.31822702288627625
setp: 1100, Loss: 0.31655487418174744
setp: 1200, Loss: 0.3214515149593353
setp: 1300, Loss: 0.3188760280609131
setp: 1400, Loss: 0.3490152060985565
setp: 1500, Loss: 0.31804001331329346
setp: 1600, Loss: 0.31596580147743225
setp: 1700, Loss: 0.31592875719070435
setp: 1800, Loss: 0.31719326972961426
setp: 1900, Loss: 0.3162868916988373
setp: 2000, Loss: 0.3163653612136841
setp: 2100, Loss: 0.3170403838157654
setp: 2200, Loss: 0.3163408041000366
setp: 2300, Loss: 0.31575384736061096
setp: 2400, Loss: 0.3169461190700531
setp: 2500, Loss: 0.3166906237602234
setp: 2600, Loss: 0.3227475583553314
setp: 2700, Loss: 0.34425899386405945
setp: 2800, Loss: 0.3422308564186096
setp: 2900, Loss: 0.3163975179195404
setp: 3000, Loss: 0.3165934383869171
setp: 3100, Loss: 0.31618908047676086
setp: 3200, Loss: 0.3153928220272064
setp: 3300, Loss: 0.3165440261363983
setp: 3400, Loss: 0.3159044086933136
setp: 3500, Loss: 0.3153892755508423
setp: 3600, Loss: 0.316537469625473
setp: 3700, Loss: 0.31636708974838257
setp: 3800, Loss: 0.3154601454734802
setp: 3900, Loss: 0.3168151080608368
setp: 4000, Loss: 0.3161374032497406
setp: 4100, Loss: 0.3605693578720093
setp: 4200, Loss: 0.3178078234195709
setp: 4300, Loss: 0.31592515110969543
setp: 4400, Loss: 0.31548556685447693
setp: 4500, Loss: 0.31604453921318054
setp: 4600, Loss: 0.31617406010627747
setp: 4700, Loss: 0.31548750400543213
setp: 4800, Loss: 0.31801408529281616
setp: 4900, Loss: 0.34530919790267944
training successfully ended.
validating...
acc: 0.9796573875802997
precision: 1.0
recall: 0.9593147751605996
F_score: 0.9792349726775956
validating...
acc: 0.8947368421052632
precision: 0.9714285714285714
recall: 0.6938775510204082
F_score: 0.8095238095238094
******fold 3******
[160, 448]
training...
setp: 0, Loss: 0.7252097129821777
setp: 100, Loss: 0.6376453638076782
setp: 200, Loss: 0.45582157373428345
setp: 300, Loss: 0.35485804080963135
setp: 400, Loss: 0.3376480042934418
setp: 500, Loss: 0.3924832046031952
setp: 600, Loss: 0.3258731961250305
setp: 700, Loss: 0.32063668966293335
setp: 800, Loss: 0.32059431076049805
setp: 900, Loss: 0.3205913007259369
setp: 1000, Loss: 0.31966909766197205
setp: 1100, Loss: 0.31904441118240356
setp: 1200, Loss: 0.32021504640579224
setp: 1300, Loss: 0.31877341866493225
setp: 1400, Loss: 0.3191227316856384
setp: 1500, Loss: 0.31839489936828613
setp: 1600, Loss: 0.31857192516326904
setp: 1700, Loss: 0.3189277648925781
setp: 1800, Loss: 0.31892454624176025
setp: 1900, Loss: 0.3190038502216339
setp: 2000, Loss: 0.3904195725917816
setp: 2100, Loss: 0.3262237012386322
setp: 2200, Loss: 0.3173905611038208
setp: 2300, Loss: 0.3179648816585541
setp: 2400, Loss: 0.3180886209011078
setp: 2500, Loss: 0.31836947798728943
setp: 2600, Loss: 0.31851503252983093
setp: 2700, Loss: 0.3174532353878021
setp: 2800, Loss: 0.31772664189338684
setp: 2900, Loss: 0.3925973176956177
setp: 3000, Loss: 0.3173529803752899
setp: 3100, Loss: 0.32970091700553894
setp: 3200, Loss: 0.31857365369796753
setp: 3300, Loss: 0.31767383217811584
setp: 3400, Loss: 0.31734946370124817
setp: 3500, Loss: 0.31799396872520447
setp: 3600, Loss: 0.31752610206604004
setp: 3700, Loss: 0.3183756172657013
setp: 3800, Loss: 0.31903979182243347
setp: 3900, Loss: 0.31937289237976074
setp: 4000, Loss: 0.3178435266017914
setp: 4100, Loss: 0.3171175420284271
setp: 4200, Loss: 0.3181416392326355
setp: 4300, Loss: 0.3182603716850281
setp: 4400, Loss: 0.31739863753318787
setp: 4500, Loss: 0.31818312406539917
setp: 4600, Loss: 0.3183842599391937
setp: 4700, Loss: 0.31737542152404785
setp: 4800, Loss: 0.3458382785320282
setp: 4900, Loss: 0.3404366970062256
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.8
recall: 0.9333333333333333
F_score: 0.8615384615384616
******fold 4******
[154, 454]
training...
setp: 0, Loss: 0.7208334803581238
setp: 100, Loss: 0.6740699410438538
setp: 200, Loss: 0.5849875211715698
setp: 300, Loss: 0.5438762903213501
setp: 400, Loss: 0.4963725507259369
setp: 500, Loss: 0.4552166163921356
setp: 600, Loss: 0.4330120086669922
setp: 700, Loss: 0.4951665997505188
setp: 800, Loss: 0.4259289503097534
setp: 900, Loss: 0.41407322883605957
setp: 1000, Loss: 0.39843887090682983
setp: 1100, Loss: 0.3773308992385864
setp: 1200, Loss: 0.36912041902542114
setp: 1300, Loss: 0.3488326668739319
setp: 1400, Loss: 0.31759095191955566
setp: 1500, Loss: 0.3230472207069397
setp: 1600, Loss: 0.38548144698143005
setp: 1700, Loss: 0.3273988962173462
setp: 1800, Loss: 0.3378757834434509
setp: 1900, Loss: 0.3483213782310486
setp: 2000, Loss: 0.3153165876865387
setp: 2100, Loss: 0.3846452534198761
setp: 2200, Loss: 0.3489573895931244
setp: 2300, Loss: 0.3189428150653839
setp: 2400, Loss: 0.408966988325119
setp: 2500, Loss: 0.3482511639595032
setp: 2600, Loss: 0.31787732243537903
setp: 2700, Loss: 0.4114920496940613
setp: 2800, Loss: 0.3171152174472809
setp: 2900, Loss: 0.32103219628334045
setp: 3000, Loss: 0.44307804107666016
setp: 3100, Loss: 0.319396048784256
setp: 3200, Loss: 0.31670960783958435
setp: 3300, Loss: 0.3169950544834137
setp: 3400, Loss: 0.3169569969177246
setp: 3500, Loss: 0.31760987639427185
setp: 3600, Loss: 0.31686708331108093
setp: 3700, Loss: 0.31696876883506775
setp: 3800, Loss: 0.31708917021751404
setp: 3900, Loss: 0.3173272907733917
setp: 4000, Loss: 0.3162660002708435
setp: 4100, Loss: 0.34858009219169617
setp: 4200, Loss: 0.3164525628089905
setp: 4300, Loss: 0.31689688563346863
setp: 4400, Loss: 0.316044420003891
setp: 4500, Loss: 0.39943116903305054
setp: 4600, Loss: 0.34476426243782043
setp: 4700, Loss: 0.31716686487197876
setp: 4800, Loss: 0.3475704789161682
setp: 4900, Loss: 0.3149094879627228
training successfully ended.
validating...
acc: 0.9845814977973568
precision: 0.9954954954954955
recall: 0.973568281938326
F_score: 0.9844097995545658
validating...
acc: 0.9407894736842105
precision: 0.8857142857142857
recall: 0.8611111111111112
F_score: 0.8732394366197184
model saved.
avg_acc: 0.9289473684210525, avg_f_score: 0.851498546243568
==========arousal==========
******fold 0******
[196, 412]
training...
setp: 0, Loss: 0.8003032207489014
setp: 100, Loss: 0.6436144113540649
setp: 200, Loss: 0.5829353332519531
setp: 300, Loss: 0.5684278011322021
setp: 400, Loss: 0.4820900857448578
setp: 500, Loss: 0.42816901206970215
setp: 600, Loss: 0.46046802401542664
setp: 700, Loss: 0.5065218806266785
setp: 800, Loss: 0.4584248960018158
setp: 900, Loss: 0.41852518916130066
setp: 1000, Loss: 0.3820488154888153
setp: 1100, Loss: 0.4454610049724579
setp: 1200, Loss: 0.44193559885025024
setp: 1300, Loss: 0.36073118448257446
setp: 1400, Loss: 0.49369245767593384
setp: 1500, Loss: 0.4745068848133087
setp: 1600, Loss: 0.4177960753440857
setp: 1700, Loss: 0.3513883352279663
setp: 1800, Loss: 0.3485304117202759
setp: 1900, Loss: 0.4324139952659607
setp: 2000, Loss: 0.3794543743133545
setp: 2100, Loss: 0.3849511742591858
setp: 2200, Loss: 0.35539722442626953
setp: 2300, Loss: 0.41143539547920227
setp: 2400, Loss: 0.31640541553497314
setp: 2500, Loss: 0.35060685873031616
setp: 2600, Loss: 0.39303597807884216
setp: 2700, Loss: 0.3786182105541229
setp: 2800, Loss: 0.3777913451194763
setp: 2900, Loss: 0.3786713778972626
setp: 3000, Loss: 0.34846732020378113
setp: 3100, Loss: 0.37981563806533813
setp: 3200, Loss: 0.3486788868904114
setp: 3300, Loss: 0.4419015049934387
setp: 3400, Loss: 0.4407486319541931
setp: 3500, Loss: 0.3795635998249054
setp: 3600, Loss: 0.34809333086013794
setp: 3700, Loss: 0.31688588857650757
setp: 3800, Loss: 0.3804570138454437
setp: 3900, Loss: 0.37995365262031555
setp: 4000, Loss: 0.3801760971546173
setp: 4100, Loss: 0.35793447494506836
setp: 4200, Loss: 0.41758227348327637
setp: 4300, Loss: 0.3180130422115326
setp: 4400, Loss: 0.3476155400276184
setp: 4500, Loss: 0.3852490782737732
setp: 4600, Loss: 0.3778235614299774
setp: 4700, Loss: 0.38799434900283813
setp: 4800, Loss: 0.37906262278556824
setp: 4900, Loss: 0.3478011190891266
training successfully ended.
validating...
acc: 0.944078947368421
precision: 1.0
recall: 0.826530612244898
F_score: 0.9050279329608939
validating...
acc: 0.881578947368421
precision: 0.9459459459459459
recall: 0.6862745098039216
F_score: 0.7954545454545455
******fold 1******
[195, 413]
training...
setp: 0, Loss: 0.8380492329597473
setp: 100, Loss: 0.5741490125656128
setp: 200, Loss: 0.5372503995895386
setp: 300, Loss: 0.5881649255752563
setp: 400, Loss: 0.5532174110412598
setp: 500, Loss: 0.532019317150116
setp: 600, Loss: 0.4801211357116699
setp: 700, Loss: 0.5138100981712341
setp: 800, Loss: 0.4482758641242981
setp: 900, Loss: 0.39775654673576355
setp: 1000, Loss: 0.36667129397392273
setp: 1100, Loss: 0.4517245590686798
setp: 1200, Loss: 0.45045775175094604
setp: 1300, Loss: 0.40600383281707764
setp: 1400, Loss: 0.4142303466796875
setp: 1500, Loss: 0.41063547134399414
setp: 1600, Loss: 0.38770484924316406
setp: 1700, Loss: 0.4491344094276428
setp: 1800, Loss: 0.3489561378955841
setp: 1900, Loss: 0.34958675503730774
setp: 2000, Loss: 0.3344899117946625
setp: 2100, Loss: 0.3929876983165741
setp: 2200, Loss: 0.4100116491317749
setp: 2300, Loss: 0.4079320430755615
setp: 2400, Loss: 0.3794153928756714
setp: 2500, Loss: 0.3963414132595062
setp: 2600, Loss: 0.3537411689758301
setp: 2700, Loss: 0.34798935055732727
setp: 2800, Loss: 0.3507561683654785
setp: 2900, Loss: 0.3209598958492279
setp: 3000, Loss: 0.39675477147102356
setp: 3100, Loss: 0.3979063332080841
setp: 3200, Loss: 0.37897154688835144
setp: 3300, Loss: 0.3816435933113098
setp: 3400, Loss: 0.40951865911483765
setp: 3500, Loss: 0.39305129647254944
setp: 3600, Loss: 0.4575265347957611
setp: 3700, Loss: 0.3751852810382843
setp: 3800, Loss: 0.3477013111114502
setp: 3900, Loss: 0.31571218371391296
setp: 4000, Loss: 0.37772393226623535
setp: 4100, Loss: 0.41062137484550476
setp: 4200, Loss: 0.37934616208076477
setp: 4300, Loss: 0.37838298082351685
setp: 4400, Loss: 0.37868958711624146
setp: 4500, Loss: 0.347930908203125
setp: 4600, Loss: 0.3468557596206665
setp: 4700, Loss: 0.3485381305217743
setp: 4800, Loss: 0.31557849049568176
setp: 4900, Loss: 0.379668265581131
training successfully ended.
validating...
acc: 0.9457236842105263
precision: 1.0
recall: 0.8307692307692308
F_score: 0.907563025210084
validating...
acc: 0.8421052631578947
precision: 0.9117647058823529
recall: 0.5961538461538461
F_score: 0.7209302325581395
******fold 2******
[189, 419]
training...
setp: 0, Loss: 0.7166486978530884
setp: 100, Loss: 0.5970156192779541
setp: 200, Loss: 0.6661391854286194
setp: 300, Loss: 0.5381456613540649
setp: 400, Loss: 0.5614127516746521
setp: 500, Loss: 0.5677565336227417
setp: 600, Loss: 0.48660823702812195
setp: 700, Loss: 0.5228621959686279
setp: 800, Loss: 0.44998684525489807
setp: 900, Loss: 0.5169786214828491
setp: 1000, Loss: 0.45076021552085876
setp: 1100, Loss: 0.4356832802295685
setp: 1200, Loss: 0.42394396662712097
setp: 1300, Loss: 0.4023492634296417
setp: 1400, Loss: 0.43943798542022705
setp: 1500, Loss: 0.4401811361312866
setp: 1600, Loss: 0.4112951457500458
setp: 1700, Loss: 0.43650850653648376
setp: 1800, Loss: 0.4486892819404602
setp: 1900, Loss: 0.41598594188690186
setp: 2000, Loss: 0.4110785126686096
setp: 2100, Loss: 0.4226651191711426
setp: 2200, Loss: 0.379284143447876
setp: 2300, Loss: 0.44346311688423157
setp: 2400, Loss: 0.3926514685153961
setp: 2500, Loss: 0.41173863410949707
setp: 2600, Loss: 0.38031792640686035
setp: 2700, Loss: 0.40984219312667847
setp: 2800, Loss: 0.38016626238822937
setp: 2900, Loss: 0.37772271037101746
setp: 3000, Loss: 0.41164201498031616
setp: 3100, Loss: 0.409553587436676
setp: 3200, Loss: 0.3810786008834839
setp: 3300, Loss: 0.4057129919528961
setp: 3400, Loss: 0.42474088072776794
setp: 3500, Loss: 0.3775738775730133
setp: 3600, Loss: 0.3501484990119934
setp: 3700, Loss: 0.408952534198761
setp: 3800, Loss: 0.39308488368988037
setp: 3900, Loss: 0.3800618350505829
setp: 4000, Loss: 0.38331830501556396
setp: 4100, Loss: 0.35051360726356506
setp: 4200, Loss: 0.38104507327079773
setp: 4300, Loss: 0.3881523013114929
setp: 4400, Loss: 0.4382646679878235
setp: 4500, Loss: 0.3789461553096771
setp: 4600, Loss: 0.410510778427124
setp: 4700, Loss: 0.37935513257980347
setp: 4800, Loss: 0.3467634618282318
setp: 4900, Loss: 0.3489159643650055
training successfully ended.
validating...
acc: 0.9276315789473685
precision: 0.9931972789115646
recall: 0.7724867724867724
F_score: 0.8690476190476191
validating...
acc: 0.8355263157894737
precision: 0.9459459459459459
recall: 0.603448275862069
F_score: 0.7368421052631577
******fold 3******
[201, 407]
training...
setp: 0, Loss: 0.6582015752792358
setp: 100, Loss: 0.6106153130531311
setp: 200, Loss: 0.5391653776168823
setp: 300, Loss: 0.5628343820571899
setp: 400, Loss: 0.5220034122467041
setp: 500, Loss: 0.484435498714447
setp: 600, Loss: 0.3981218934059143
setp: 700, Loss: 0.40872296690940857
setp: 800, Loss: 0.39569416642189026
setp: 900, Loss: 0.39501839876174927
setp: 1000, Loss: 0.38668110966682434
setp: 1100, Loss: 0.37658435106277466
setp: 1200, Loss: 0.38368698954582214
setp: 1300, Loss: 0.3967784345149994
setp: 1400, Loss: 0.3476914167404175
setp: 1500, Loss: 0.4615532159805298
setp: 1600, Loss: 0.3791038990020752
setp: 1700, Loss: 0.37979573011398315
setp: 1800, Loss: 0.380680650472641
setp: 1900, Loss: 0.41041216254234314
setp: 2000, Loss: 0.34784188866615295
setp: 2100, Loss: 0.36132553219795227
setp: 2200, Loss: 0.34833067655563354
setp: 2300, Loss: 0.37928974628448486
setp: 2400, Loss: 0.37962090969085693
setp: 2500, Loss: 0.367109090089798
setp: 2600, Loss: 0.38195541501045227
setp: 2700, Loss: 0.38422590494155884
setp: 2800, Loss: 0.38181009888648987
setp: 2900, Loss: 0.39839208126068115
setp: 3000, Loss: 0.3738398551940918
setp: 3100, Loss: 0.4918263554573059
setp: 3200, Loss: 0.4401172697544098
setp: 3300, Loss: 0.3464527726173401
setp: 3400, Loss: 0.4415828585624695
setp: 3500, Loss: 0.38013285398483276
setp: 3600, Loss: 0.37859588861465454
setp: 3700, Loss: 0.37964099645614624
setp: 3800, Loss: 0.3800117075443268
setp: 3900, Loss: 0.3474084436893463
setp: 4000, Loss: 0.3492424488067627
setp: 4100, Loss: 0.35057199001312256
setp: 4200, Loss: 0.37860751152038574
setp: 4300, Loss: 0.379159152507782
setp: 4400, Loss: 0.3476414382457733
setp: 4500, Loss: 0.3781616985797882
setp: 4600, Loss: 0.3782603144645691
setp: 4700, Loss: 0.38054341077804565
setp: 4800, Loss: 0.38326895236968994
setp: 4900, Loss: 0.32636502385139465
training successfully ended.
validating...
acc: 0.9473684210526315
precision: 1.0
recall: 0.8407960199004975
F_score: 0.9135135135135135
validating...
acc: 0.9078947368421053
precision: 0.9444444444444444
recall: 0.7391304347826086
F_score: 0.8292682926829269
******fold 4******
[207, 401]
training...
setp: 0, Loss: 0.7142955660820007
setp: 100, Loss: 0.5933785438537598
setp: 200, Loss: 0.5499862432479858
setp: 300, Loss: 0.5131005048751831
setp: 400, Loss: 0.46531862020492554
setp: 500, Loss: 0.46282652020454407
setp: 600, Loss: 0.4553465247154236
setp: 700, Loss: 0.4090215265750885
setp: 800, Loss: 0.405020147562027
setp: 900, Loss: 0.3533537983894348
setp: 1000, Loss: 0.43443018198013306
setp: 1100, Loss: 0.35161682963371277
setp: 1200, Loss: 0.3847969174385071
setp: 1300, Loss: 0.34978237748146057
setp: 1400, Loss: 0.3507485091686249
setp: 1500, Loss: 0.43877464532852173
setp: 1600, Loss: 0.44484734535217285
setp: 1700, Loss: 0.3525124192237854
setp: 1800, Loss: 0.4452022910118103
setp: 1900, Loss: 0.3564811944961548
setp: 2000, Loss: 0.348627507686615
setp: 2100, Loss: 0.37930142879486084
setp: 2200, Loss: 0.3793853521347046
setp: 2300, Loss: 0.3473433554172516
setp: 2400, Loss: 0.37836599349975586
setp: 2500, Loss: 0.38437697291374207
setp: 2600, Loss: 0.3847930431365967
setp: 2700, Loss: 0.35773515701293945
setp: 2800, Loss: 0.34905460476875305
setp: 2900, Loss: 0.4107864201068878
setp: 3000, Loss: 0.3577306568622589
setp: 3100, Loss: 0.38038498163223267
setp: 3200, Loss: 0.3539201021194458
setp: 3300, Loss: 0.34537479281425476
setp: 3400, Loss: 0.38158324360847473
setp: 3500, Loss: 0.3801383674144745
setp: 3600, Loss: 0.34747835993766785
setp: 3700, Loss: 0.4408692717552185
setp: 3800, Loss: 0.3508583605289459
setp: 3900, Loss: 0.34804031252861023
setp: 4000, Loss: 0.34902507066726685
setp: 4100, Loss: 0.3795759677886963
setp: 4200, Loss: 0.3187887370586395
setp: 4300, Loss: 0.37916049361228943
setp: 4400, Loss: 0.38047704100608826
setp: 4500, Loss: 0.3786122500896454
setp: 4600, Loss: 0.3474092185497284
setp: 4700, Loss: 0.34881919622421265
setp: 4800, Loss: 0.40906935930252075
setp: 4900, Loss: 0.34750548005104065
training successfully ended.
validating...
acc: 0.8519736842105263
precision: 0.7746478873239436
recall: 0.7971014492753623
F_score: 0.7857142857142857
validating...
acc: 0.8223684210526315
precision: 0.6585365853658537
recall: 0.675
F_score: 0.6666666666666667
model saved.
avg_acc: 0.8578947368421053, avg_f_score: 0.7498323685250872
-------------subject: 28-------------
==========valence==========
******fold 0******
[227, 381]
training...
setp: 0, Loss: 0.6775137782096863
setp: 100, Loss: 0.6619378924369812
setp: 200, Loss: 0.5314947366714478
setp: 300, Loss: 0.5189436674118042
setp: 400, Loss: 0.4262872040271759
setp: 500, Loss: 0.3703725039958954
setp: 600, Loss: 0.3651289939880371
setp: 700, Loss: 0.35424014925956726
setp: 800, Loss: 0.32689473032951355
setp: 900, Loss: 0.321611225605011
setp: 1000, Loss: 0.34254780411720276
setp: 1100, Loss: 0.3345131278038025
setp: 1200, Loss: 0.32055923342704773
setp: 1300, Loss: 0.3272210955619812
setp: 1400, Loss: 0.32255029678344727
setp: 1500, Loss: 0.32566332817077637
setp: 1600, Loss: 0.3185238838195801
setp: 1700, Loss: 0.31972941756248474
setp: 1800, Loss: 0.3199661672115326
setp: 1900, Loss: 0.3192310035228729
setp: 2000, Loss: 0.3200187385082245
setp: 2100, Loss: 0.3207409679889679
setp: 2200, Loss: 0.3301754295825958
setp: 2300, Loss: 0.32008498907089233
setp: 2400, Loss: 0.3193179965019226
setp: 2500, Loss: 0.31830841302871704
setp: 2600, Loss: 0.3200780749320984
setp: 2700, Loss: 0.3227316439151764
setp: 2800, Loss: 0.31846871972084045
setp: 2900, Loss: 0.3224913775920868
setp: 3000, Loss: 0.31938987970352173
setp: 3100, Loss: 0.31849855184555054
setp: 3200, Loss: 0.3190263509750366
setp: 3300, Loss: 0.32050132751464844
setp: 3400, Loss: 0.3185978829860687
setp: 3500, Loss: 0.3184027075767517
setp: 3600, Loss: 0.3186597526073456
setp: 3700, Loss: 0.4092749357223511
setp: 3800, Loss: 0.32997795939445496
setp: 3900, Loss: 0.31996095180511475
setp: 4000, Loss: 0.31871700286865234
setp: 4100, Loss: 0.31847167015075684
setp: 4200, Loss: 0.31895923614501953
setp: 4300, Loss: 0.3190242052078247
setp: 4400, Loss: 0.3188963532447815
setp: 4500, Loss: 0.31895318627357483
setp: 4600, Loss: 0.32076409459114075
setp: 4700, Loss: 0.3181212842464447
setp: 4800, Loss: 0.3188506066799164
setp: 4900, Loss: 0.32555580139160156
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 1.0
recall: 0.986784140969163
F_score: 0.9933481152993348
validating...
acc: 0.9276315789473685
precision: 0.9607843137254902
recall: 0.8448275862068966
F_score: 0.8990825688073395
******fold 1******
[226, 382]
training...
setp: 0, Loss: 0.6516939401626587
setp: 100, Loss: 0.6176618337631226
setp: 200, Loss: 0.5141676068305969
setp: 300, Loss: 0.44764599204063416
setp: 400, Loss: 0.42527666687965393
setp: 500, Loss: 0.34449848532676697
setp: 600, Loss: 0.3478747010231018
setp: 700, Loss: 0.3867465555667877
setp: 800, Loss: 0.32803693413734436
setp: 900, Loss: 0.3234057128429413
setp: 1000, Loss: 0.3224794864654541
setp: 1100, Loss: 0.31924936175346375
setp: 1200, Loss: 0.3237477242946625
setp: 1300, Loss: 0.322731614112854
setp: 1400, Loss: 0.3208010494709015
setp: 1500, Loss: 0.3244422972202301
setp: 1600, Loss: 0.32104915380477905
setp: 1700, Loss: 0.31894224882125854
setp: 1800, Loss: 0.31906217336654663
setp: 1900, Loss: 0.32439059019088745
setp: 2000, Loss: 0.3194127678871155
setp: 2100, Loss: 0.31788384914398193
setp: 2200, Loss: 0.36575832962989807
setp: 2300, Loss: 0.31814953684806824
setp: 2400, Loss: 0.33338674902915955
setp: 2500, Loss: 0.3495158553123474
setp: 2600, Loss: 0.32211947441101074
setp: 2700, Loss: 0.3187731206417084
setp: 2800, Loss: 0.31919145584106445
setp: 2900, Loss: 0.3183881938457489
setp: 3000, Loss: 0.31790921092033386
setp: 3100, Loss: 0.3194396197795868
setp: 3200, Loss: 0.31986579298973083
setp: 3300, Loss: 0.31878164410591125
setp: 3400, Loss: 0.32750266790390015
setp: 3500, Loss: 0.31746774911880493
setp: 3600, Loss: 0.3161490857601166
setp: 3700, Loss: 0.3185569941997528
setp: 3800, Loss: 0.32127419114112854
setp: 3900, Loss: 0.31762707233428955
setp: 4000, Loss: 0.31663766503334045
setp: 4100, Loss: 0.31766319274902344
setp: 4200, Loss: 0.32601746916770935
setp: 4300, Loss: 0.44054651260375977
setp: 4400, Loss: 0.41352298855781555
setp: 4500, Loss: 0.33906155824661255
setp: 4600, Loss: 0.3396621644496918
setp: 4700, Loss: 0.33382201194763184
setp: 4800, Loss: 0.41053342819213867
setp: 4900, Loss: 0.3232394754886627
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9473684210526315
precision: 0.9473684210526315
recall: 0.9152542372881356
F_score: 0.9310344827586206
******fold 2******
[235, 373]
training...
setp: 0, Loss: 0.674846351146698
setp: 100, Loss: 0.630175769329071
setp: 200, Loss: 0.5240998268127441
setp: 300, Loss: 0.5177595615386963
setp: 400, Loss: 0.37831613421440125
setp: 500, Loss: 0.37191569805145264
setp: 600, Loss: 0.33418211340904236
setp: 700, Loss: 0.33852288126945496
setp: 800, Loss: 0.33070653676986694
setp: 900, Loss: 0.32281821966171265
setp: 1000, Loss: 0.32272323966026306
setp: 1100, Loss: 0.3215685486793518
setp: 1200, Loss: 0.3201615810394287
setp: 1300, Loss: 0.31883564591407776
setp: 1400, Loss: 0.31770312786102295
setp: 1500, Loss: 0.3195783197879791
setp: 1600, Loss: 0.33881810307502747
setp: 1700, Loss: 0.3167757987976074
setp: 1800, Loss: 0.3199734091758728
setp: 1900, Loss: 0.3195369243621826
setp: 2000, Loss: 0.3194813132286072
setp: 2100, Loss: 0.3174523413181305
setp: 2200, Loss: 0.32261645793914795
setp: 2300, Loss: 0.3255653381347656
setp: 2400, Loss: 0.32414481043815613
setp: 2500, Loss: 0.3183439373970032
setp: 2600, Loss: 0.31807032227516174
setp: 2700, Loss: 0.3184024691581726
setp: 2800, Loss: 0.3185073137283325
setp: 2900, Loss: 0.3183820843696594
setp: 3000, Loss: 0.31843018531799316
setp: 3100, Loss: 0.3180948495864868
setp: 3200, Loss: 0.3179172873497009
setp: 3300, Loss: 0.31708982586860657
setp: 3400, Loss: 0.3174985349178314
setp: 3500, Loss: 0.3177982568740845
setp: 3600, Loss: 0.3165735900402069
setp: 3700, Loss: 0.5708783268928528
setp: 3800, Loss: 0.4486592411994934
setp: 3900, Loss: 0.346086710691452
setp: 4000, Loss: 0.3256060481071472
setp: 4100, Loss: 0.37185463309288025
setp: 4200, Loss: 0.32339638471603394
setp: 4300, Loss: 0.327043354511261
setp: 4400, Loss: 0.32287153601646423
setp: 4500, Loss: 0.3197996914386749
setp: 4600, Loss: 0.32244277000427246
setp: 4700, Loss: 0.32020947337150574
setp: 4800, Loss: 0.35294124484062195
setp: 4900, Loss: 0.3445969223976135
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.9347826086956522
recall: 0.86
F_score: 0.8958333333333334
******fold 3******
[232, 376]
training...
setp: 0, Loss: 0.6440984606742859
setp: 100, Loss: 0.6495791673660278
setp: 200, Loss: 0.5305226445198059
setp: 300, Loss: 0.5436874628067017
setp: 400, Loss: 0.5651111602783203
setp: 500, Loss: 0.4024446904659271
setp: 600, Loss: 0.34097495675086975
setp: 700, Loss: 0.3464004099369049
setp: 800, Loss: 0.35072457790374756
setp: 900, Loss: 0.42133620381355286
setp: 1000, Loss: 0.32651287317276
setp: 1100, Loss: 0.32053521275520325
setp: 1200, Loss: 0.32171157002449036
setp: 1300, Loss: 0.32569029927253723
setp: 1400, Loss: 0.34653952717781067
setp: 1500, Loss: 0.31602904200553894
setp: 1600, Loss: 0.31626126170158386
setp: 1700, Loss: 0.31989726424217224
setp: 1800, Loss: 0.32183554768562317
setp: 1900, Loss: 0.3528444766998291
setp: 2000, Loss: 0.32579806447029114
setp: 2100, Loss: 0.31515833735466003
setp: 2200, Loss: 0.321577250957489
setp: 2300, Loss: 0.317669540643692
setp: 2400, Loss: 0.31930017471313477
setp: 2500, Loss: 0.331763356924057
setp: 2600, Loss: 0.3160386085510254
setp: 2700, Loss: 0.3242367208003998
setp: 2800, Loss: 0.32413947582244873
setp: 2900, Loss: 0.31887051463127136
setp: 3000, Loss: 0.3181435763835907
setp: 3100, Loss: 0.3196720480918884
setp: 3200, Loss: 0.32324734330177307
setp: 3300, Loss: 0.3364051878452301
setp: 3400, Loss: 0.3169906437397003
setp: 3500, Loss: 0.3173257112503052
setp: 3600, Loss: 0.32057395577430725
setp: 3700, Loss: 0.3645455539226532
setp: 3800, Loss: 0.3551970422267914
setp: 3900, Loss: 0.31764230132102966
setp: 4000, Loss: 0.316102534532547
setp: 4100, Loss: 0.31858834624290466
setp: 4200, Loss: 0.31709587574005127
setp: 4300, Loss: 0.35120657086372375
setp: 4400, Loss: 0.3191562592983246
setp: 4500, Loss: 0.31648221611976624
setp: 4600, Loss: 0.31771790981292725
setp: 4700, Loss: 0.3366429805755615
setp: 4800, Loss: 0.3202565014362335
setp: 4900, Loss: 0.32345500588417053
training successfully ended.
validating...
acc: 0.9802631578947368
precision: 1.0
recall: 0.9482758620689655
F_score: 0.9734513274336283
validating...
acc: 0.9210526315789473
precision: 0.9767441860465116
recall: 0.7924528301886793
F_score: 0.875
******fold 4******
[220, 388]
training...
setp: 0, Loss: 0.685282289981842
setp: 100, Loss: 0.6201913356781006
setp: 200, Loss: 0.44457000494003296
setp: 300, Loss: 0.5022560358047485
setp: 400, Loss: 0.42224133014678955
setp: 500, Loss: 0.3720150291919708
setp: 600, Loss: 0.32845303416252136
setp: 700, Loss: 0.35846638679504395
setp: 800, Loss: 0.3461261987686157
setp: 900, Loss: 0.3186616897583008
setp: 1000, Loss: 0.3190394937992096
setp: 1100, Loss: 0.3179437518119812
setp: 1200, Loss: 0.32674717903137207
setp: 1300, Loss: 0.31923332810401917
setp: 1400, Loss: 0.31852835416793823
setp: 1500, Loss: 0.3188589811325073
setp: 1600, Loss: 0.32110172510147095
setp: 1700, Loss: 0.3183775842189789
setp: 1800, Loss: 0.3347788453102112
setp: 1900, Loss: 0.31944313645362854
setp: 2000, Loss: 0.3191326856613159
setp: 2100, Loss: 0.31655654311180115
setp: 2200, Loss: 0.3182874619960785
setp: 2300, Loss: 0.31790339946746826
setp: 2400, Loss: 0.31911322474479675
setp: 2500, Loss: 0.31739023327827454
setp: 2600, Loss: 0.3170471489429474
setp: 2700, Loss: 0.3182682693004608
setp: 2800, Loss: 0.3166804611682892
setp: 2900, Loss: 0.33669012784957886
setp: 3000, Loss: 0.324995219707489
setp: 3100, Loss: 0.35261550545692444
setp: 3200, Loss: 0.3196873068809509
setp: 3300, Loss: 0.3186528980731964
setp: 3400, Loss: 0.316344290971756
setp: 3500, Loss: 0.3175068795681
setp: 3600, Loss: 0.3165532946586609
setp: 3700, Loss: 0.32006072998046875
setp: 3800, Loss: 0.319143146276474
setp: 3900, Loss: 0.317738801240921
setp: 4000, Loss: 0.31640323996543884
setp: 4100, Loss: 0.3186187744140625
setp: 4200, Loss: 0.31713101267814636
setp: 4300, Loss: 0.3182395100593567
setp: 4400, Loss: 0.31644803285598755
setp: 4500, Loss: 0.31679731607437134
setp: 4600, Loss: 0.4217534363269806
setp: 4700, Loss: 0.3516494333744049
setp: 4800, Loss: 0.3885193169116974
setp: 4900, Loss: 0.31665661931037903
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.95
recall: 0.8769230769230769
F_score: 0.912
model saved.
avg_acc: 0.9315789473684211, avg_f_score: 0.9025900769798587
==========arousal==========
******fold 0******
[300, 308]
training...
setp: 0, Loss: 0.7087048292160034
setp: 100, Loss: 0.6941874623298645
setp: 200, Loss: 0.5811667442321777
setp: 300, Loss: 0.5343486666679382
setp: 400, Loss: 0.5161271691322327
setp: 500, Loss: 0.5284319519996643
setp: 600, Loss: 0.47681474685668945
setp: 700, Loss: 0.46638914942741394
setp: 800, Loss: 0.397006630897522
setp: 900, Loss: 0.382184773683548
setp: 1000, Loss: 0.5061161518096924
setp: 1100, Loss: 0.4415758550167084
setp: 1200, Loss: 0.42114129662513733
setp: 1300, Loss: 0.4307195544242859
setp: 1400, Loss: 0.41160380840301514
setp: 1500, Loss: 0.40924862027168274
setp: 1600, Loss: 0.44685253500938416
setp: 1700, Loss: 0.35193318128585815
setp: 1800, Loss: 0.4115298390388489
setp: 1900, Loss: 0.3866088390350342
setp: 2000, Loss: 0.4494348168373108
setp: 2100, Loss: 0.3497166931629181
setp: 2200, Loss: 0.35910019278526306
setp: 2300, Loss: 0.38304850459098816
setp: 2400, Loss: 0.4185856878757477
setp: 2500, Loss: 0.41685229539871216
setp: 2600, Loss: 0.38138842582702637
setp: 2700, Loss: 0.3801400065422058
setp: 2800, Loss: 0.34923166036605835
setp: 2900, Loss: 0.44352102279663086
setp: 3000, Loss: 0.3887169361114502
setp: 3100, Loss: 0.38241785764694214
setp: 3200, Loss: 0.3491463363170624
setp: 3300, Loss: 0.3506009876728058
setp: 3400, Loss: 0.318671315908432
setp: 3500, Loss: 0.34860119223594666
setp: 3600, Loss: 0.3209852874279022
setp: 3700, Loss: 0.33412304520606995
setp: 3800, Loss: 0.35707640647888184
setp: 3900, Loss: 0.3502369523048401
setp: 4000, Loss: 0.31706133484840393
setp: 4100, Loss: 0.3175424635410309
setp: 4200, Loss: 0.3493193984031677
setp: 4300, Loss: 0.34000736474990845
setp: 4400, Loss: 0.35409167408943176
setp: 4500, Loss: 0.31783926486968994
setp: 4600, Loss: 0.3179142475128174
setp: 4700, Loss: 0.3185511529445648
setp: 4800, Loss: 0.31918686628341675
setp: 4900, Loss: 0.3187386393547058
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.8863636363636364
recall: 0.975
F_score: 0.9285714285714285
******fold 1******
[314, 294]
training...
setp: 0, Loss: 0.6930635571479797
setp: 100, Loss: 0.6913533806800842
setp: 200, Loss: 0.6934078931808472
setp: 300, Loss: 0.6889856457710266
setp: 400, Loss: 0.6938283443450928
setp: 500, Loss: 0.6911060214042664
setp: 600, Loss: 0.6975882649421692
setp: 700, Loss: 0.6189192533493042
setp: 800, Loss: 0.5749859809875488
setp: 900, Loss: 0.4523051381111145
setp: 1000, Loss: 0.38607072830200195
setp: 1100, Loss: 0.33385607600212097
setp: 1200, Loss: 0.43308091163635254
setp: 1300, Loss: 0.32374250888824463
setp: 1400, Loss: 0.32680442929267883
setp: 1500, Loss: 0.3236158490180969
setp: 1600, Loss: 0.31956854462623596
setp: 1700, Loss: 0.3259866237640381
setp: 1800, Loss: 0.32011881470680237
setp: 1900, Loss: 0.32889488339424133
setp: 2000, Loss: 0.3229003846645355
setp: 2100, Loss: 0.3194531500339508
setp: 2200, Loss: 0.31864628195762634
setp: 2300, Loss: 0.3224177360534668
setp: 2400, Loss: 0.32375821471214294
setp: 2500, Loss: 0.3227633833885193
setp: 2600, Loss: 0.3253718316555023
setp: 2700, Loss: 0.3281501829624176
setp: 2800, Loss: 0.32013678550720215
setp: 2900, Loss: 0.32147321105003357
setp: 3000, Loss: 0.31957581639289856
setp: 3100, Loss: 0.31959906220436096
setp: 3200, Loss: 0.3179495334625244
setp: 3300, Loss: 0.3219267427921295
setp: 3400, Loss: 0.3196694850921631
setp: 3500, Loss: 0.3171411156654358
setp: 3600, Loss: 0.34557777643203735
setp: 3700, Loss: 0.31800955533981323
setp: 3800, Loss: 0.31763404607772827
setp: 3900, Loss: 0.3190012574195862
setp: 4000, Loss: 0.31885191798210144
setp: 4100, Loss: 0.3172979950904846
setp: 4200, Loss: 0.4218093454837799
setp: 4300, Loss: 0.32030242681503296
setp: 4400, Loss: 0.32117119431495667
setp: 4500, Loss: 0.31801390647888184
setp: 4600, Loss: 0.31964975595474243
setp: 4700, Loss: 0.31921741366386414
setp: 4800, Loss: 0.3211001455783844
setp: 4900, Loss: 0.3196086287498474
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.9516129032258065
recall: 0.8939393939393939
F_score: 0.921875
******fold 2******
[317, 291]
training...
setp: 0, Loss: 0.692563533782959
setp: 100, Loss: 0.66001296043396
setp: 200, Loss: 0.5878224968910217
setp: 300, Loss: 0.6470040678977966
setp: 400, Loss: 0.5162733793258667
setp: 500, Loss: 0.44362008571624756
setp: 600, Loss: 0.4089146554470062
setp: 700, Loss: 0.45670247077941895
setp: 800, Loss: 0.4438110589981079
setp: 900, Loss: 0.38440531492233276
setp: 1000, Loss: 0.5110179781913757
setp: 1100, Loss: 0.4639398157596588
setp: 1200, Loss: 0.4369896352291107
setp: 1300, Loss: 0.4463270902633667
setp: 1400, Loss: 0.36236488819122314
setp: 1500, Loss: 0.41294461488723755
setp: 1600, Loss: 0.3824460208415985
setp: 1700, Loss: 0.42123943567276
setp: 1800, Loss: 0.3883219063282013
setp: 1900, Loss: 0.3278903365135193
setp: 2000, Loss: 0.4063136875629425
setp: 2100, Loss: 0.3462124466896057
setp: 2200, Loss: 0.3186943531036377
setp: 2300, Loss: 0.36707764863967896
setp: 2400, Loss: 0.3533632159233093
setp: 2500, Loss: 0.3202759921550751
setp: 2600, Loss: 0.3179035782814026
setp: 2700, Loss: 0.3185681700706482
setp: 2800, Loss: 0.3180101215839386
setp: 2900, Loss: 0.3793987035751343
setp: 3000, Loss: 0.3197413384914398
setp: 3100, Loss: 0.3183492124080658
setp: 3200, Loss: 0.34967315196990967
setp: 3300, Loss: 0.32128241658210754
setp: 3400, Loss: 0.3221978545188904
setp: 3500, Loss: 0.3209487795829773
setp: 3600, Loss: 0.3167124390602112
setp: 3700, Loss: 0.32599589228630066
setp: 3800, Loss: 0.31825828552246094
setp: 3900, Loss: 0.31663113832473755
setp: 4000, Loss: 0.3461047112941742
setp: 4100, Loss: 0.31741777062416077
setp: 4200, Loss: 0.3172229826450348
setp: 4300, Loss: 0.31712132692337036
setp: 4400, Loss: 0.31805354356765747
setp: 4500, Loss: 0.31822195649147034
setp: 4600, Loss: 0.3181007504463196
setp: 4700, Loss: 0.3181700110435486
setp: 4800, Loss: 0.3197009563446045
setp: 4900, Loss: 0.31980100274086
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9968553459119497
recall: 1.0
F_score: 0.9984251968503937
validating...
acc: 0.881578947368421
precision: 0.835820895522388
recall: 0.8888888888888888
F_score: 0.8615384615384615
******fold 3******
[291, 317]
training...
setp: 0, Loss: 0.7006587386131287
setp: 100, Loss: 0.6546236872673035
setp: 200, Loss: 0.5964115262031555
setp: 300, Loss: 0.5448641777038574
setp: 400, Loss: 0.49117910861968994
setp: 500, Loss: 0.42833301424980164
setp: 600, Loss: 0.41835635900497437
setp: 700, Loss: 0.3416960537433624
setp: 800, Loss: 0.36289453506469727
setp: 900, Loss: 0.35966166853904724
setp: 1000, Loss: 0.35419371724128723
setp: 1100, Loss: 0.3481406271457672
setp: 1200, Loss: 0.3204778730869293
setp: 1300, Loss: 0.318808376789093
setp: 1400, Loss: 0.3195899426937103
setp: 1500, Loss: 0.3188181519508362
setp: 1600, Loss: 0.31818047165870667
setp: 1700, Loss: 0.31744685769081116
setp: 1800, Loss: 0.31951141357421875
setp: 1900, Loss: 0.3178781270980835
setp: 2000, Loss: 0.31907641887664795
setp: 2100, Loss: 0.31775984168052673
setp: 2200, Loss: 0.3231120705604553
setp: 2300, Loss: 0.3520148694515228
setp: 2400, Loss: 0.32161030173301697
setp: 2500, Loss: 0.33041495084762573
setp: 2600, Loss: 0.31873100996017456
setp: 2700, Loss: 0.31973710656166077
setp: 2800, Loss: 0.320015549659729
setp: 2900, Loss: 0.32079485058784485
setp: 3000, Loss: 0.31651410460472107
setp: 3100, Loss: 0.31877800822257996
setp: 3200, Loss: 0.3166316747665405
setp: 3300, Loss: 0.4111688435077667
setp: 3400, Loss: 0.31673064827919006
setp: 3500, Loss: 0.316506564617157
setp: 3600, Loss: 0.3165279030799866
setp: 3700, Loss: 0.3178991973400116
setp: 3800, Loss: 0.31753021478652954
setp: 3900, Loss: 0.3184930086135864
setp: 4000, Loss: 0.31713223457336426
setp: 4100, Loss: 0.3163830041885376
setp: 4200, Loss: 0.31871655583381653
setp: 4300, Loss: 0.3165549337863922
setp: 4400, Loss: 0.3198259770870209
setp: 4500, Loss: 0.4387233555316925
setp: 4600, Loss: 0.32734835147857666
setp: 4700, Loss: 0.316596657037735
setp: 4800, Loss: 0.3185994625091553
setp: 4900, Loss: 0.31792986392974854
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9144736842105263
precision: 0.9418604651162791
recall: 0.9101123595505618
F_score: 0.9257142857142857
******fold 4******
[298, 310]
training...
setp: 0, Loss: 0.7282300591468811
setp: 100, Loss: 0.6757059097290039
setp: 200, Loss: 0.6689582467079163
setp: 300, Loss: 0.549858808517456
setp: 400, Loss: 0.49808260798454285
setp: 500, Loss: 0.5846884250640869
setp: 600, Loss: 0.629146158695221
setp: 700, Loss: 0.4680819809436798
setp: 800, Loss: 0.5039032697677612
setp: 900, Loss: 0.4522583484649658
setp: 1000, Loss: 0.4353306293487549
setp: 1100, Loss: 0.41355353593826294
setp: 1200, Loss: 0.38908296823501587
setp: 1300, Loss: 0.44660502672195435
setp: 1400, Loss: 0.4434772729873657
setp: 1500, Loss: 0.4564001262187958
setp: 1600, Loss: 0.41839903593063354
setp: 1700, Loss: 0.46454185247421265
setp: 1800, Loss: 0.4456520974636078
setp: 1900, Loss: 0.410300076007843
setp: 2000, Loss: 0.415211945772171
setp: 2100, Loss: 0.428191214799881
setp: 2200, Loss: 0.4106000065803528
setp: 2300, Loss: 0.40977081656455994
setp: 2400, Loss: 0.44363945722579956
setp: 2500, Loss: 0.4415442645549774
setp: 2600, Loss: 0.42985594272613525
setp: 2700, Loss: 0.44162771105766296
setp: 2800, Loss: 0.4441449046134949
setp: 2900, Loss: 0.35240867733955383
setp: 3000, Loss: 0.38181471824645996
setp: 3100, Loss: 0.3819192349910736
setp: 3200, Loss: 0.44592735171318054
setp: 3300, Loss: 0.44313958287239075
setp: 3400, Loss: 0.4240553677082062
setp: 3500, Loss: 0.37650066614151
setp: 3600, Loss: 0.3951304852962494
setp: 3700, Loss: 0.3715069591999054
setp: 3800, Loss: 0.32034116983413696
setp: 3900, Loss: 0.329245001077652
setp: 4000, Loss: 0.3601556420326233
setp: 4100, Loss: 0.3214132487773895
setp: 4200, Loss: 0.3466888666152954
setp: 4300, Loss: 0.3186565637588501
setp: 4400, Loss: 0.32660973072052
setp: 4500, Loss: 0.3186577558517456
setp: 4600, Loss: 0.3192240297794342
setp: 4700, Loss: 0.32077205181121826
setp: 4800, Loss: 0.31879809498786926
setp: 4900, Loss: 0.33302146196365356
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9966555183946488
recall: 1.0
F_score: 0.998324958123953
validating...
acc: 0.9407894736842105
precision: 0.974025974025974
recall: 0.9146341463414634
F_score: 0.9433962264150942
model saved.
avg_acc: 0.9184210526315789, avg_f_score: 0.9162190804478539
-------------subject: 29-------------
==========valence==========
******fold 0******
[255, 353]
training...
setp: 0, Loss: 0.7468291521072388
setp: 100, Loss: 0.6735498905181885
setp: 200, Loss: 0.4678693413734436
setp: 300, Loss: 0.4938688576221466
setp: 400, Loss: 0.5764540433883667
setp: 500, Loss: 0.3742132782936096
setp: 600, Loss: 0.38759467005729675
setp: 700, Loss: 0.386086642742157
setp: 800, Loss: 0.3602524399757385
setp: 900, Loss: 0.3381420969963074
setp: 1000, Loss: 0.32064560055732727
setp: 1100, Loss: 0.31751778721809387
setp: 1200, Loss: 0.31659573316574097
setp: 1300, Loss: 0.31823423504829407
setp: 1400, Loss: 0.3486695885658264
setp: 1500, Loss: 0.31801626086235046
setp: 1600, Loss: 0.34159693121910095
setp: 1700, Loss: 0.31645411252975464
setp: 1800, Loss: 0.3472493886947632
setp: 1900, Loss: 0.31955456733703613
setp: 2000, Loss: 0.3188806176185608
setp: 2100, Loss: 0.31614595651626587
setp: 2200, Loss: 0.3214512765407562
setp: 2300, Loss: 0.3410303592681885
setp: 2400, Loss: 0.3265385031700134
setp: 2500, Loss: 0.34826216101646423
setp: 2600, Loss: 0.3173116445541382
setp: 2700, Loss: 0.3229934573173523
setp: 2800, Loss: 0.3160008490085602
setp: 2900, Loss: 0.3172152638435364
setp: 3000, Loss: 0.31597718596458435
setp: 3100, Loss: 0.31562983989715576
setp: 3200, Loss: 0.33277377486228943
setp: 3300, Loss: 0.31553465127944946
setp: 3400, Loss: 0.31547316908836365
setp: 3500, Loss: 0.3165743052959442
setp: 3600, Loss: 0.3164195716381073
setp: 3700, Loss: 0.31632763147354126
setp: 3800, Loss: 0.3206663429737091
setp: 3900, Loss: 0.3178790509700775
setp: 4000, Loss: 0.31604042649269104
setp: 4100, Loss: 0.3173258602619171
setp: 4200, Loss: 0.3200378715991974
setp: 4300, Loss: 0.31759533286094666
setp: 4400, Loss: 0.31628280878067017
setp: 4500, Loss: 0.3171047568321228
setp: 4600, Loss: 0.318537175655365
setp: 4700, Loss: 0.31567248702049255
setp: 4800, Loss: 0.3168073892593384
setp: 4900, Loss: 0.3178407549858093
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9802631578947368
precision: 0.9850746268656716
recall: 0.9705882352941176
F_score: 0.9777777777777777
******fold 1******
[253, 355]
training...
setp: 0, Loss: 0.6861876249313354
setp: 100, Loss: 0.6630315780639648
setp: 200, Loss: 0.49911922216415405
setp: 300, Loss: 0.44440191984176636
setp: 400, Loss: 0.4155021607875824
setp: 500, Loss: 0.360833078622818
setp: 600, Loss: 0.4138604998588562
setp: 700, Loss: 0.34870991110801697
setp: 800, Loss: 0.4285120666027069
setp: 900, Loss: 0.3198373317718506
setp: 1000, Loss: 0.3185311555862427
setp: 1100, Loss: 0.32763639092445374
setp: 1200, Loss: 0.32933342456817627
setp: 1300, Loss: 0.3159313499927521
setp: 1400, Loss: 0.31756556034088135
setp: 1500, Loss: 0.31585100293159485
setp: 1600, Loss: 0.337289959192276
setp: 1700, Loss: 0.3175312578678131
setp: 1800, Loss: 0.3249056041240692
setp: 1900, Loss: 0.33797672390937805
setp: 2000, Loss: 0.3657272756099701
setp: 2100, Loss: 0.31582531332969666
setp: 2200, Loss: 0.3161992132663727
setp: 2300, Loss: 0.31568267941474915
setp: 2400, Loss: 0.3162520229816437
setp: 2500, Loss: 0.31834375858306885
setp: 2600, Loss: 0.31619763374328613
setp: 2700, Loss: 0.31627073884010315
setp: 2800, Loss: 0.31634777784347534
setp: 2900, Loss: 0.31918755173683167
setp: 3000, Loss: 0.31634414196014404
setp: 3100, Loss: 0.3162876069545746
setp: 3200, Loss: 0.31602931022644043
setp: 3300, Loss: 0.31521478295326233
setp: 3400, Loss: 0.31600287556648254
setp: 3500, Loss: 0.325371116399765
setp: 3600, Loss: 0.3164222538471222
setp: 3700, Loss: 0.3157702684402466
setp: 3800, Loss: 0.3180703818798065
setp: 3900, Loss: 0.3183455169200897
setp: 4000, Loss: 0.3158113360404968
setp: 4100, Loss: 0.31652146577835083
setp: 4200, Loss: 0.3164118528366089
setp: 4300, Loss: 0.3174963593482971
setp: 4400, Loss: 0.3196408152580261
setp: 4500, Loss: 0.31579193472862244
setp: 4600, Loss: 0.3179675340652466
setp: 4700, Loss: 0.31627577543258667
setp: 4800, Loss: 0.3210214674472809
setp: 4900, Loss: 0.31570911407470703
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9960474308300395
F_score: 0.998019801980198
validating...
acc: 0.9342105263157895
precision: 0.9838709677419355
recall: 0.8714285714285714
F_score: 0.9242424242424242
******fold 2******
[271, 337]
training...
setp: 0, Loss: 0.7065808176994324
setp: 100, Loss: 0.643338143825531
setp: 200, Loss: 0.5287604331970215
setp: 300, Loss: 0.5289592146873474
setp: 400, Loss: 0.38861289620399475
setp: 500, Loss: 0.33215296268463135
setp: 600, Loss: 0.4337416887283325
setp: 700, Loss: 0.31938982009887695
setp: 800, Loss: 0.32430291175842285
setp: 900, Loss: 0.32357388734817505
setp: 1000, Loss: 0.32904067635536194
setp: 1100, Loss: 0.32012203335762024
setp: 1200, Loss: 0.3170987069606781
setp: 1300, Loss: 0.3174366056919098
setp: 1400, Loss: 0.32427069544792175
setp: 1500, Loss: 0.3173127770423889
setp: 1600, Loss: 0.3177248239517212
setp: 1700, Loss: 0.31689882278442383
setp: 1800, Loss: 0.3156271278858185
setp: 1900, Loss: 0.3202952742576599
setp: 2000, Loss: 0.3170289099216461
setp: 2100, Loss: 0.3176467716693878
setp: 2200, Loss: 0.3156689405441284
setp: 2300, Loss: 0.3166325092315674
setp: 2400, Loss: 0.3173536956310272
setp: 2500, Loss: 0.3169071078300476
setp: 2600, Loss: 0.3194819390773773
setp: 2700, Loss: 0.3172267973423004
setp: 2800, Loss: 0.3160586655139923
setp: 2900, Loss: 0.31917694211006165
setp: 3000, Loss: 0.3175745904445648
setp: 3100, Loss: 0.31835418939590454
setp: 3200, Loss: 0.325268030166626
setp: 3300, Loss: 0.3358752429485321
setp: 3400, Loss: 0.3158697187900543
setp: 3500, Loss: 0.317375123500824
setp: 3600, Loss: 0.31725016236305237
setp: 3700, Loss: 0.31647688150405884
setp: 3800, Loss: 0.3172786831855774
setp: 3900, Loss: 0.3154519200325012
setp: 4000, Loss: 0.3159736692905426
setp: 4100, Loss: 0.45589321851730347
setp: 4200, Loss: 0.3165997862815857
setp: 4300, Loss: 0.31823644042015076
setp: 4400, Loss: 0.3157173991203308
setp: 4500, Loss: 0.316496878862381
setp: 4600, Loss: 0.31632906198501587
setp: 4700, Loss: 0.3168060779571533
setp: 4800, Loss: 0.316455602645874
setp: 4900, Loss: 0.3196665048599243
training successfully ended.
validating...
acc: 0.9868421052631579
precision: 0.9713261648745519
recall: 1.0
F_score: 0.9854545454545455
validating...
acc: 0.9802631578947368
precision: 0.9454545454545454
recall: 1.0
F_score: 0.9719626168224299
******fold 3******
[257, 351]
training...
setp: 0, Loss: 0.627483606338501
setp: 100, Loss: 0.6417157649993896
setp: 200, Loss: 0.4618598520755768
setp: 300, Loss: 0.47226986289024353
setp: 400, Loss: 0.393331915140152
setp: 500, Loss: 0.34901779890060425
setp: 600, Loss: 0.35620614886283875
setp: 700, Loss: 0.35008060932159424
setp: 800, Loss: 0.3770429790019989
setp: 900, Loss: 0.3189235329627991
setp: 1000, Loss: 0.31661200523376465
setp: 1100, Loss: 0.31610167026519775
setp: 1200, Loss: 0.3170877695083618
setp: 1300, Loss: 0.31643420457839966
setp: 1400, Loss: 0.3169560730457306
setp: 1500, Loss: 0.31668469309806824
setp: 1600, Loss: 0.31840094923973083
setp: 1700, Loss: 0.3164154291152954
setp: 1800, Loss: 0.3165328800678253
setp: 1900, Loss: 0.3620458245277405
setp: 2000, Loss: 0.33075016736984253
setp: 2100, Loss: 0.32018935680389404
setp: 2200, Loss: 0.317097544670105
setp: 2300, Loss: 0.31717759370803833
setp: 2400, Loss: 0.3164544701576233
setp: 2500, Loss: 0.3460255265235901
setp: 2600, Loss: 0.32384932041168213
setp: 2700, Loss: 0.326199471950531
setp: 2800, Loss: 0.31565290689468384
setp: 2900, Loss: 0.3153960406780243
setp: 3000, Loss: 0.31560373306274414
setp: 3100, Loss: 0.3155212998390198
setp: 3200, Loss: 0.3162674903869629
setp: 3300, Loss: 0.3299301564693451
setp: 3400, Loss: 0.31599023938179016
setp: 3500, Loss: 0.3168948292732239
setp: 3600, Loss: 0.31615790724754333
setp: 3700, Loss: 0.3175885081291199
setp: 3800, Loss: 0.32708480954170227
setp: 3900, Loss: 0.3165101110935211
setp: 4000, Loss: 0.3154177665710449
setp: 4100, Loss: 0.31765779852867126
setp: 4200, Loss: 0.3175719380378723
setp: 4300, Loss: 0.31619033217430115
setp: 4400, Loss: 0.3162314295768738
setp: 4500, Loss: 0.3163735270500183
setp: 4600, Loss: 0.3166882395744324
setp: 4700, Loss: 0.3173408508300781
setp: 4800, Loss: 0.3157106339931488
setp: 4900, Loss: 0.31547796726226807
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.9375
recall: 0.9090909090909091
F_score: 0.923076923076923
******fold 4******
[256, 352]
training...
setp: 0, Loss: 0.6773207187652588
setp: 100, Loss: 0.5909818410873413
setp: 200, Loss: 0.4758031964302063
setp: 300, Loss: 0.47177043557167053
setp: 400, Loss: 0.3732262849807739
setp: 500, Loss: 0.3478882908821106
setp: 600, Loss: 0.39639055728912354
setp: 700, Loss: 0.3511774241924286
setp: 800, Loss: 0.40868231654167175
setp: 900, Loss: 0.3460364043712616
setp: 1000, Loss: 0.3185770809650421
setp: 1100, Loss: 0.31989428400993347
setp: 1200, Loss: 0.31725382804870605
setp: 1300, Loss: 0.316586971282959
setp: 1400, Loss: 0.3517882227897644
setp: 1500, Loss: 0.3435508608818054
setp: 1600, Loss: 0.3843325078487396
setp: 1700, Loss: 0.31986385583877563
setp: 1800, Loss: 0.32068687677383423
setp: 1900, Loss: 0.3172619938850403
setp: 2000, Loss: 0.32886722683906555
setp: 2100, Loss: 0.3159855306148529
setp: 2200, Loss: 0.317228227853775
setp: 2300, Loss: 0.31700223684310913
setp: 2400, Loss: 0.3157290518283844
setp: 2500, Loss: 0.3173769414424896
setp: 2600, Loss: 0.3171427547931671
setp: 2700, Loss: 0.3172945976257324
setp: 2800, Loss: 0.31599316000938416
setp: 2900, Loss: 0.31619957089424133
setp: 3000, Loss: 0.3159615695476532
setp: 3100, Loss: 0.3160102367401123
setp: 3200, Loss: 0.31667962670326233
setp: 3300, Loss: 0.31584692001342773
setp: 3400, Loss: 0.3155365288257599
setp: 3500, Loss: 0.3166511058807373
setp: 3600, Loss: 0.3162991404533386
setp: 3700, Loss: 0.3164736032485962
setp: 3800, Loss: 0.3168548345565796
setp: 3900, Loss: 0.6848552823066711
setp: 4000, Loss: 0.51805579662323
setp: 4100, Loss: 0.34368252754211426
setp: 4200, Loss: 0.3383690416812897
setp: 4300, Loss: 0.32578611373901367
setp: 4400, Loss: 0.36722004413604736
setp: 4500, Loss: 0.32405996322631836
setp: 4600, Loss: 0.42158079147338867
setp: 4700, Loss: 0.3212967813014984
setp: 4800, Loss: 0.3186177909374237
setp: 4900, Loss: 0.319760799407959
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.9384615384615385
recall: 0.9104477611940298
F_score: 0.9242424242424243
model saved.
avg_acc: 0.9526315789473685, avg_f_score: 0.9442604332323958
==========arousal==========
******fold 0******
[194, 414]
training...
setp: 0, Loss: 0.6888498663902283
setp: 100, Loss: 0.6458368897438049
setp: 200, Loss: 0.5811077952384949
setp: 300, Loss: 0.49466830492019653
setp: 400, Loss: 0.44042548537254333
setp: 500, Loss: 0.444372296333313
setp: 600, Loss: 0.4817591905593872
setp: 700, Loss: 0.36469602584838867
setp: 800, Loss: 0.3624745011329651
setp: 900, Loss: 0.4404711425304413
setp: 1000, Loss: 0.38237184286117554
setp: 1100, Loss: 0.36022722721099854
setp: 1200, Loss: 0.37874680757522583
setp: 1300, Loss: 0.410530149936676
setp: 1400, Loss: 0.35277384519577026
setp: 1500, Loss: 0.3503028452396393
setp: 1600, Loss: 0.35126686096191406
setp: 1700, Loss: 0.4472965896129608
setp: 1800, Loss: 0.3860361576080322
setp: 1900, Loss: 0.32420822978019714
setp: 2000, Loss: 0.3174251317977905
setp: 2100, Loss: 0.31611087918281555
setp: 2200, Loss: 0.3168793320655823
setp: 2300, Loss: 0.3480338454246521
setp: 2400, Loss: 0.3366539478302002
setp: 2500, Loss: 0.32037487626075745
setp: 2600, Loss: 0.36671024560928345
setp: 2700, Loss: 0.3213942050933838
setp: 2800, Loss: 0.3364413380622864
setp: 2900, Loss: 0.31915438175201416
setp: 3000, Loss: 0.31771957874298096
setp: 3100, Loss: 0.3172498047351837
setp: 3200, Loss: 0.3176591694355011
setp: 3300, Loss: 0.34662556648254395
setp: 3400, Loss: 0.319354772567749
setp: 3500, Loss: 0.3162640929222107
setp: 3600, Loss: 0.31628766655921936
setp: 3700, Loss: 0.31750768423080444
setp: 3800, Loss: 0.3166934847831726
setp: 3900, Loss: 0.31560689210891724
setp: 4000, Loss: 0.3158019781112671
setp: 4100, Loss: 0.3351483643054962
setp: 4200, Loss: 0.3170391619205475
setp: 4300, Loss: 0.31965264678001404
setp: 4400, Loss: 0.3158605098724365
setp: 4500, Loss: 0.3160861134529114
setp: 4600, Loss: 0.3167831301689148
setp: 4700, Loss: 0.31715771555900574
setp: 4800, Loss: 0.31690409779548645
setp: 4900, Loss: 0.31720638275146484
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9605263157894737
precision: 0.9607843137254902
recall: 0.9245283018867925
F_score: 0.9423076923076923
******fold 1******
[198, 410]
training...
setp: 0, Loss: 0.6760557293891907
setp: 100, Loss: 0.6423479914665222
setp: 200, Loss: 0.5403581857681274
setp: 300, Loss: 0.5671256184577942
setp: 400, Loss: 0.499493807554245
setp: 500, Loss: 0.4379691183567047
setp: 600, Loss: 0.48125171661376953
setp: 700, Loss: 0.45308640599250793
setp: 800, Loss: 0.3867577314376831
setp: 900, Loss: 0.33341607451438904
setp: 1000, Loss: 0.3447128236293793
setp: 1100, Loss: 0.3592875897884369
setp: 1200, Loss: 0.32702845335006714
setp: 1300, Loss: 0.3280283808708191
setp: 1400, Loss: 0.33010202646255493
setp: 1500, Loss: 0.3184277415275574
setp: 1600, Loss: 0.32208746671676636
setp: 1700, Loss: 0.3343231976032257
setp: 1800, Loss: 0.37758782505989075
setp: 1900, Loss: 0.32005441188812256
setp: 2000, Loss: 0.31844043731689453
setp: 2100, Loss: 0.31657329201698303
setp: 2200, Loss: 0.31912553310394287
setp: 2300, Loss: 0.3195773959159851
setp: 2400, Loss: 0.3171345889568329
setp: 2500, Loss: 0.33778026700019836
setp: 2600, Loss: 0.31618866324424744
setp: 2700, Loss: 0.3211079239845276
setp: 2800, Loss: 0.31621605157852173
setp: 2900, Loss: 0.3152707815170288
setp: 3000, Loss: 0.31799155473709106
setp: 3100, Loss: 0.31783396005630493
setp: 3200, Loss: 0.3157077133655548
setp: 3300, Loss: 0.31805282831192017
setp: 3400, Loss: 0.31867000460624695
setp: 3500, Loss: 0.35575246810913086
setp: 3600, Loss: 0.3178462088108063
setp: 3700, Loss: 0.35823336243629456
setp: 3800, Loss: 0.3167146146297455
setp: 3900, Loss: 0.32396477460861206
setp: 4000, Loss: 0.3179297149181366
setp: 4100, Loss: 0.31617826223373413
setp: 4200, Loss: 0.3155865967273712
setp: 4300, Loss: 0.31705403327941895
setp: 4400, Loss: 0.31702181696891785
setp: 4500, Loss: 0.3169267177581787
setp: 4600, Loss: 0.31643950939178467
setp: 4700, Loss: 0.31719130277633667
setp: 4800, Loss: 0.3171035051345825
setp: 4900, Loss: 0.31863388419151306
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9949494949494949
F_score: 0.9974683544303797
validating...
acc: 0.9539473684210527
precision: 0.9375
recall: 0.9183673469387755
F_score: 0.9278350515463918
******fold 2******
[210, 398]
training...
setp: 0, Loss: 0.6787596344947815
setp: 100, Loss: 0.5564345717430115
setp: 200, Loss: 0.5958974361419678
setp: 300, Loss: 0.4473097324371338
setp: 400, Loss: 0.5436816215515137
setp: 500, Loss: 0.4583454430103302
setp: 600, Loss: 0.4817603528499603
setp: 700, Loss: 0.4736127257347107
setp: 800, Loss: 0.44270622730255127
setp: 900, Loss: 0.38386690616607666
setp: 1000, Loss: 0.38624897599220276
setp: 1100, Loss: 0.36778637766838074
setp: 1200, Loss: 0.3255162835121155
setp: 1300, Loss: 0.3436177670955658
setp: 1400, Loss: 0.35395547747612
setp: 1500, Loss: 0.35258784890174866
setp: 1600, Loss: 0.35192662477493286
setp: 1700, Loss: 0.3544834852218628
setp: 1800, Loss: 0.31636446714401245
setp: 1900, Loss: 0.32616111636161804
setp: 2000, Loss: 0.3158821761608124
setp: 2100, Loss: 0.32333335280418396
setp: 2200, Loss: 0.3214285373687744
setp: 2300, Loss: 0.3277174234390259
setp: 2400, Loss: 0.32537388801574707
setp: 2500, Loss: 0.3424766957759857
setp: 2600, Loss: 0.32214587926864624
setp: 2700, Loss: 0.315792053937912
setp: 2800, Loss: 0.3193219304084778
setp: 2900, Loss: 0.3187037408351898
setp: 3000, Loss: 0.31982743740081787
setp: 3100, Loss: 0.3161037862300873
setp: 3200, Loss: 0.3157827854156494
setp: 3300, Loss: 0.31677669286727905
setp: 3400, Loss: 0.31592079997062683
setp: 3500, Loss: 0.3158527612686157
setp: 3600, Loss: 0.3478504717350006
setp: 3700, Loss: 0.31485140323638916
setp: 3800, Loss: 0.31911906599998474
setp: 3900, Loss: 0.3751293122768402
setp: 4000, Loss: 0.31658002734184265
setp: 4100, Loss: 0.3170170187950134
setp: 4200, Loss: 0.3162597417831421
setp: 4300, Loss: 0.33255261182785034
setp: 4400, Loss: 0.3189581334590912
setp: 4500, Loss: 0.3312566578388214
setp: 4600, Loss: 0.32086506485939026
setp: 4700, Loss: 0.32422152161598206
setp: 4800, Loss: 0.3160586357116699
setp: 4900, Loss: 0.3816937506198883
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9952380952380953
recall: 0.9952380952380953
F_score: 0.9952380952380953
validating...
acc: 0.9473684210526315
precision: 0.8918918918918919
recall: 0.8918918918918919
F_score: 0.8918918918918919
******fold 3******
[192, 416]
training...
setp: 0, Loss: 0.7419809699058533
setp: 100, Loss: 0.6419777274131775
setp: 200, Loss: 0.5811984539031982
setp: 300, Loss: 0.5343570113182068
setp: 400, Loss: 0.4187716245651245
setp: 500, Loss: 0.479483425617218
setp: 600, Loss: 0.3978085517883301
setp: 700, Loss: 0.4262332022190094
setp: 800, Loss: 0.3745841681957245
setp: 900, Loss: 0.36431872844696045
setp: 1000, Loss: 0.36307549476623535
setp: 1100, Loss: 0.3863944709300995
setp: 1200, Loss: 0.35657960176467896
setp: 1300, Loss: 0.3198271095752716
setp: 1400, Loss: 0.346362441778183
setp: 1500, Loss: 0.3166595995426178
setp: 1600, Loss: 0.3168034255504608
setp: 1700, Loss: 0.3173244893550873
setp: 1800, Loss: 0.377625048160553
setp: 1900, Loss: 0.31712597608566284
setp: 2000, Loss: 0.31615790724754333
setp: 2100, Loss: 0.3190186023712158
setp: 2200, Loss: 0.3173116147518158
setp: 2300, Loss: 0.3715636730194092
setp: 2400, Loss: 0.3285822570323944
setp: 2500, Loss: 0.34828439354896545
setp: 2600, Loss: 0.3169845938682556
setp: 2700, Loss: 0.31687474250793457
setp: 2800, Loss: 0.3165484070777893
setp: 2900, Loss: 0.3183792233467102
setp: 3000, Loss: 0.34932368993759155
setp: 3100, Loss: 0.31632059812545776
setp: 3200, Loss: 0.31846973299980164
setp: 3300, Loss: 0.31822624802589417
setp: 3400, Loss: 0.3157850503921509
setp: 3500, Loss: 0.35901984572410583
setp: 3600, Loss: 0.32402974367141724
setp: 3700, Loss: 0.37790313363075256
setp: 3800, Loss: 0.3162207007408142
setp: 3900, Loss: 0.3156418800354004
setp: 4000, Loss: 0.37207916378974915
setp: 4100, Loss: 0.3161030411720276
setp: 4200, Loss: 0.31627342104911804
setp: 4300, Loss: 0.31663569808006287
setp: 4400, Loss: 0.3167280852794647
setp: 4500, Loss: 0.31788498163223267
setp: 4600, Loss: 0.31688353419303894
setp: 4700, Loss: 0.3167339563369751
setp: 4800, Loss: 0.31783294677734375
setp: 4900, Loss: 0.3486529588699341
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 1.0
recall: 0.984375
F_score: 0.9921259842519685
validating...
acc: 0.9473684210526315
precision: 0.9795918367346939
recall: 0.8727272727272727
F_score: 0.923076923076923
******fold 4******
[194, 414]
training...
setp: 0, Loss: 0.7340301275253296
setp: 100, Loss: 0.64527428150177
setp: 200, Loss: 0.6182004809379578
setp: 300, Loss: 0.4931199252605438
setp: 400, Loss: 0.46671998500823975
setp: 500, Loss: 0.4556824564933777
setp: 600, Loss: 0.40540027618408203
setp: 700, Loss: 0.3854718804359436
setp: 800, Loss: 0.3268211781978607
setp: 900, Loss: 0.3670191764831543
setp: 1000, Loss: 0.32006174325942993
setp: 1100, Loss: 0.32140782475471497
setp: 1200, Loss: 0.35613980889320374
setp: 1300, Loss: 0.3213503956794739
setp: 1400, Loss: 0.32021352648735046
setp: 1500, Loss: 0.3172886073589325
setp: 1600, Loss: 0.3163072168827057
setp: 1700, Loss: 0.31728801131248474
setp: 1800, Loss: 0.3780081868171692
setp: 1900, Loss: 0.34005048871040344
setp: 2000, Loss: 0.31732138991355896
setp: 2100, Loss: 0.3477882742881775
setp: 2200, Loss: 0.316657692193985
setp: 2300, Loss: 0.34594547748565674
setp: 2400, Loss: 0.3257235884666443
setp: 2500, Loss: 0.3176482915878296
setp: 2600, Loss: 0.3239596486091614
setp: 2700, Loss: 0.3255363404750824
setp: 2800, Loss: 0.3175603747367859
setp: 2900, Loss: 0.31597188115119934
setp: 3000, Loss: 0.33235734701156616
setp: 3100, Loss: 0.317485511302948
setp: 3200, Loss: 0.31592828035354614
setp: 3300, Loss: 0.31645333766937256
setp: 3400, Loss: 0.3163925111293793
setp: 3500, Loss: 0.31722331047058105
setp: 3600, Loss: 0.31654807925224304
setp: 3700, Loss: 0.36414751410484314
setp: 3800, Loss: 0.325639933347702
setp: 3900, Loss: 0.3154803216457367
setp: 4000, Loss: 0.31910470128059387
setp: 4100, Loss: 0.31557899713516235
setp: 4200, Loss: 0.31822600960731506
setp: 4300, Loss: 0.31815105676651
setp: 4400, Loss: 0.3158547282218933
setp: 4500, Loss: 0.31845298409461975
setp: 4600, Loss: 0.31912681460380554
setp: 4700, Loss: 0.31574511528015137
setp: 4800, Loss: 0.31540805101394653
setp: 4900, Loss: 0.3166259825229645
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9948453608247423
F_score: 0.9974160206718347
validating...
acc: 0.9144736842105263
precision: 0.8846153846153846
recall: 0.8679245283018868
F_score: 0.8761904761904762
model saved.
avg_acc: 0.9447368421052632, avg_f_score: 0.912260407002675
-------------subject: 30-------------
==========valence==========
******fold 0******
[203, 405]
training...
setp: 0, Loss: 0.6639827489852905
setp: 100, Loss: 0.6678441762924194
setp: 200, Loss: 0.6436513662338257
setp: 300, Loss: 0.6217445135116577
setp: 400, Loss: 0.614675760269165
setp: 500, Loss: 0.6089246273040771
setp: 600, Loss: 0.5580617785453796
setp: 700, Loss: 0.5482655167579651
setp: 800, Loss: 0.4589254558086395
setp: 900, Loss: 0.376139372587204
setp: 1000, Loss: 0.4196726977825165
setp: 1100, Loss: 0.3384939134120941
setp: 1200, Loss: 0.3274421989917755
setp: 1300, Loss: 0.3251534700393677
setp: 1400, Loss: 0.32305261492729187
setp: 1500, Loss: 0.3279015123844147
setp: 1600, Loss: 0.3217751681804657
setp: 1700, Loss: 0.3256184756755829
setp: 1800, Loss: 0.3207906484603882
setp: 1900, Loss: 0.3277348279953003
setp: 2000, Loss: 0.32220861315727234
setp: 2100, Loss: 0.3200887441635132
setp: 2200, Loss: 0.3202126920223236
setp: 2300, Loss: 0.324922114610672
setp: 2400, Loss: 0.321907639503479
setp: 2500, Loss: 0.3250389099121094
setp: 2600, Loss: 0.3334495425224304
setp: 2700, Loss: 0.32441428303718567
setp: 2800, Loss: 0.31806862354278564
setp: 2900, Loss: 0.3224153518676758
setp: 3000, Loss: 0.32004058361053467
setp: 3100, Loss: 0.3206031918525696
setp: 3200, Loss: 0.3197307288646698
setp: 3300, Loss: 0.31881681084632874
setp: 3400, Loss: 0.31948956847190857
setp: 3500, Loss: 0.3192674219608307
setp: 3600, Loss: 0.3206867277622223
setp: 3700, Loss: 0.3183360695838928
setp: 3800, Loss: 0.31988027691841125
setp: 3900, Loss: 0.31908851861953735
setp: 4000, Loss: 0.32313400506973267
setp: 4100, Loss: 0.3301697075366974
setp: 4200, Loss: 0.32774707674980164
setp: 4300, Loss: 0.3190133571624756
setp: 4400, Loss: 0.31874820590019226
setp: 4500, Loss: 0.31820544600486755
setp: 4600, Loss: 0.3222430646419525
setp: 4700, Loss: 0.31764036417007446
setp: 4800, Loss: 0.3209287226200104
setp: 4900, Loss: 0.3191818296909332
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9605263157894737
precision: 0.975
recall: 0.8863636363636364
F_score: 0.9285714285714285
******fold 1******
[196, 412]
training...
setp: 0, Loss: 0.6759974956512451
setp: 100, Loss: 0.5973770022392273
setp: 200, Loss: 0.6001174449920654
setp: 300, Loss: 0.6449436545372009
setp: 400, Loss: 0.6211463809013367
setp: 500, Loss: 0.6260034441947937
setp: 600, Loss: 0.5071508884429932
setp: 700, Loss: 0.5313484072685242
setp: 800, Loss: 0.5720387697219849
setp: 900, Loss: 0.435779333114624
setp: 1000, Loss: 0.391142874956131
setp: 1100, Loss: 0.39835676550865173
setp: 1200, Loss: 0.4479673504829407
setp: 1300, Loss: 0.42721179127693176
setp: 1400, Loss: 0.3374214172363281
setp: 1500, Loss: 0.3852372169494629
setp: 1600, Loss: 0.37944337725639343
setp: 1700, Loss: 0.32786187529563904
setp: 1800, Loss: 0.31809118390083313
setp: 1900, Loss: 0.33743560314178467
setp: 2000, Loss: 0.3304827809333801
setp: 2100, Loss: 0.34845900535583496
setp: 2200, Loss: 0.3629973828792572
setp: 2300, Loss: 0.31883180141448975
setp: 2400, Loss: 0.3464011549949646
setp: 2500, Loss: 0.3166276812553406
setp: 2600, Loss: 0.3225950598716736
setp: 2700, Loss: 0.3229571282863617
setp: 2800, Loss: 0.31727349758148193
setp: 2900, Loss: 0.31802114844322205
setp: 3000, Loss: 0.33435702323913574
setp: 3100, Loss: 0.3251490890979767
setp: 3200, Loss: 0.3229716122150421
setp: 3300, Loss: 0.3324888348579407
setp: 3400, Loss: 0.3208359479904175
setp: 3500, Loss: 0.33851414918899536
setp: 3600, Loss: 0.32099828124046326
setp: 3700, Loss: 0.31550633907318115
setp: 3800, Loss: 0.3198843002319336
setp: 3900, Loss: 0.3172323703765869
setp: 4000, Loss: 0.3176935315132141
setp: 4100, Loss: 0.3192267119884491
setp: 4200, Loss: 0.31903156638145447
setp: 4300, Loss: 0.31655365228652954
setp: 4400, Loss: 0.31714457273483276
setp: 4500, Loss: 0.31667277216911316
setp: 4600, Loss: 0.3191395699977875
setp: 4700, Loss: 0.31656476855278015
setp: 4800, Loss: 0.3173666298389435
setp: 4900, Loss: 0.321792334318161
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.98989898989899
recall: 1.0
F_score: 0.9949238578680203
validating...
acc: 0.9605263157894737
precision: 0.9245283018867925
recall: 0.9607843137254902
F_score: 0.9423076923076923
******fold 2******
[201, 407]
training...
setp: 0, Loss: 0.6715203523635864
setp: 100, Loss: 0.6178805232048035
setp: 200, Loss: 0.5877273678779602
setp: 300, Loss: 0.583924412727356
setp: 400, Loss: 0.4944343864917755
setp: 500, Loss: 0.44687193632125854
setp: 600, Loss: 0.3890119791030884
setp: 700, Loss: 0.4263441860675812
setp: 800, Loss: 0.3560347557067871
setp: 900, Loss: 0.3290967047214508
setp: 1000, Loss: 0.31788280606269836
setp: 1100, Loss: 0.31794023513793945
setp: 1200, Loss: 0.31999558210372925
setp: 1300, Loss: 0.31713396310806274
setp: 1400, Loss: 0.3167625367641449
setp: 1500, Loss: 0.31735506653785706
setp: 1600, Loss: 0.31743893027305603
setp: 1700, Loss: 0.37581926584243774
setp: 1800, Loss: 0.3389044404029846
setp: 1900, Loss: 0.4174041748046875
setp: 2000, Loss: 0.3171462416648865
setp: 2100, Loss: 0.3176991641521454
setp: 2200, Loss: 0.3483578860759735
setp: 2300, Loss: 0.31798437237739563
setp: 2400, Loss: 0.3170202076435089
setp: 2500, Loss: 0.3176698088645935
setp: 2600, Loss: 0.3175640106201172
setp: 2700, Loss: 0.31818732619285583
setp: 2800, Loss: 0.31683939695358276
setp: 2900, Loss: 0.31612542271614075
setp: 3000, Loss: 0.317246675491333
setp: 3100, Loss: 0.5463082790374756
setp: 3200, Loss: 0.322531133890152
setp: 3300, Loss: 0.4184827208518982
setp: 3400, Loss: 0.3225979804992676
setp: 3500, Loss: 0.3155652582645416
setp: 3600, Loss: 0.3167991638183594
setp: 3700, Loss: 0.31602659821510315
setp: 3800, Loss: 0.31688743829727173
setp: 3900, Loss: 0.31667250394821167
setp: 4000, Loss: 0.3169623613357544
setp: 4100, Loss: 0.3481394946575165
setp: 4200, Loss: 0.31702449917793274
setp: 4300, Loss: 0.31726905703544617
setp: 4400, Loss: 0.3172306716442108
setp: 4500, Loss: 0.3172861337661743
setp: 4600, Loss: 0.318381667137146
setp: 4700, Loss: 0.5933946967124939
setp: 4800, Loss: 0.33494308590888977
setp: 4900, Loss: 0.3418174982070923
training successfully ended.
validating...
acc: 0.9786184210526315
precision: 1.0
recall: 0.9353233830845771
F_score: 0.9665809768637532
validating...
acc: 0.9078947368421053
precision: 0.8636363636363636
recall: 0.8260869565217391
F_score: 0.8444444444444444
******fold 3******
[200, 408]
training...
setp: 0, Loss: 0.6299625635147095
setp: 100, Loss: 0.6002618670463562
setp: 200, Loss: 0.6248066425323486
setp: 300, Loss: 0.589326024055481
setp: 400, Loss: 0.5228258371353149
setp: 500, Loss: 0.3760499060153961
setp: 600, Loss: 0.34384533762931824
setp: 700, Loss: 0.3280875086784363
setp: 800, Loss: 0.35362401604652405
setp: 900, Loss: 0.31915393471717834
setp: 1000, Loss: 0.31877267360687256
setp: 1100, Loss: 0.3223671317100525
setp: 1200, Loss: 0.3192053437232971
setp: 1300, Loss: 0.3530387878417969
setp: 1400, Loss: 0.33658039569854736
setp: 1500, Loss: 0.3189997971057892
setp: 1600, Loss: 0.31736740469932556
setp: 1700, Loss: 0.3182874619960785
setp: 1800, Loss: 0.3172403872013092
setp: 1900, Loss: 0.31789258122444153
setp: 2000, Loss: 0.31603866815567017
setp: 2100, Loss: 0.3186459243297577
setp: 2200, Loss: 0.350375771522522
setp: 2300, Loss: 0.31934675574302673
setp: 2400, Loss: 0.31601789593696594
setp: 2500, Loss: 0.3162733018398285
setp: 2600, Loss: 0.31647348403930664
setp: 2700, Loss: 0.318917840719223
setp: 2800, Loss: 0.31683337688446045
setp: 2900, Loss: 0.31646233797073364
setp: 3000, Loss: 0.623926043510437
setp: 3100, Loss: 0.5255312919616699
setp: 3200, Loss: 0.3527251183986664
setp: 3300, Loss: 0.35000497102737427
setp: 3400, Loss: 0.32288193702697754
setp: 3500, Loss: 0.3179565668106079
setp: 3600, Loss: 0.318063348531723
setp: 3700, Loss: 0.31760174036026
setp: 3800, Loss: 0.31824982166290283
setp: 3900, Loss: 0.31687238812446594
setp: 4000, Loss: 0.32093745470046997
setp: 4100, Loss: 0.3178233206272125
setp: 4200, Loss: 0.31871530413627625
setp: 4300, Loss: 0.3163098096847534
setp: 4400, Loss: 0.31656700372695923
setp: 4500, Loss: 0.3188898265361786
setp: 4600, Loss: 0.31754496693611145
setp: 4700, Loss: 0.31719163060188293
setp: 4800, Loss: 0.31683000922203064
setp: 4900, Loss: 0.319373220205307
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9539473684210527
precision: 0.9545454545454546
recall: 0.8936170212765957
F_score: 0.9230769230769231
******fold 4******
[188, 420]
training...
setp: 0, Loss: 0.699735164642334
setp: 100, Loss: 0.5959920287132263
setp: 200, Loss: 0.6183468699455261
setp: 300, Loss: 0.5682165622711182
setp: 400, Loss: 0.5550282597541809
setp: 500, Loss: 0.45139017701148987
setp: 600, Loss: 0.4666491150856018
setp: 700, Loss: 0.44074732065200806
setp: 800, Loss: 0.398945689201355
setp: 900, Loss: 0.44456857442855835
setp: 1000, Loss: 0.41841432452201843
setp: 1100, Loss: 0.32635512948036194
setp: 1200, Loss: 0.38615119457244873
setp: 1300, Loss: 0.31814005970954895
setp: 1400, Loss: 0.3217739462852478
setp: 1500, Loss: 0.3497430980205536
setp: 1600, Loss: 0.3676275610923767
setp: 1700, Loss: 0.31955620646476746
setp: 1800, Loss: 0.3188090920448303
setp: 1900, Loss: 0.3314989507198334
setp: 2000, Loss: 0.32633766531944275
setp: 2100, Loss: 0.34623420238494873
setp: 2200, Loss: 0.3372442126274109
setp: 2300, Loss: 0.324958473443985
setp: 2400, Loss: 0.32484447956085205
setp: 2500, Loss: 0.3373121917247772
setp: 2600, Loss: 0.32226014137268066
setp: 2700, Loss: 0.3164443075656891
setp: 2800, Loss: 0.34214627742767334
setp: 2900, Loss: 0.31527796387672424
setp: 3000, Loss: 0.31648769974708557
setp: 3100, Loss: 0.3254823386669159
setp: 3200, Loss: 0.3159126937389374
setp: 3300, Loss: 0.31853294372558594
setp: 3400, Loss: 0.347685307264328
setp: 3500, Loss: 0.31913724541664124
setp: 3600, Loss: 0.316423624753952
setp: 3700, Loss: 0.31615573167800903
setp: 3800, Loss: 0.3168644607067108
setp: 3900, Loss: 0.31835222244262695
setp: 4000, Loss: 0.3172856867313385
setp: 4100, Loss: 0.3169427514076233
setp: 4200, Loss: 0.31878799200057983
setp: 4300, Loss: 0.3173370063304901
setp: 4400, Loss: 0.3169979155063629
setp: 4500, Loss: 0.31652745604515076
setp: 4600, Loss: 0.31679585576057434
setp: 4700, Loss: 0.31689247488975525
setp: 4800, Loss: 0.31690213084220886
setp: 4900, Loss: 0.3161216974258423
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.9137931034482759
recall: 0.8983050847457628
F_score: 0.9059829059829059
model saved.
avg_acc: 0.9421052631578947, avg_f_score: 0.908876678876679
==========arousal==========
******fold 0******
[308, 300]
training...
setp: 0, Loss: 0.7032296657562256
setp: 100, Loss: 0.693757176399231
setp: 200, Loss: 0.6049252152442932
setp: 300, Loss: 0.5036523342132568
setp: 400, Loss: 0.4008667767047882
setp: 500, Loss: 0.3560107350349426
setp: 600, Loss: 0.34692251682281494
setp: 700, Loss: 0.3274233937263489
setp: 800, Loss: 0.32716691493988037
setp: 900, Loss: 0.32284119725227356
setp: 1000, Loss: 0.3197247087955475
setp: 1100, Loss: 0.3301413655281067
setp: 1200, Loss: 0.32644304633140564
setp: 1300, Loss: 0.3239738941192627
setp: 1400, Loss: 0.32207944989204407
setp: 1500, Loss: 0.3209991157054901
setp: 1600, Loss: 0.3201366066932678
setp: 1700, Loss: 0.3199421465396881
setp: 1800, Loss: 0.31892791390419006
setp: 1900, Loss: 0.3210820257663727
setp: 2000, Loss: 0.31996551156044006
setp: 2100, Loss: 0.3193800449371338
setp: 2200, Loss: 0.3882597088813782
setp: 2300, Loss: 0.34045806527137756
setp: 2400, Loss: 0.31787237524986267
setp: 2500, Loss: 0.31793177127838135
setp: 2600, Loss: 0.3178573548793793
setp: 2700, Loss: 0.3196272552013397
setp: 2800, Loss: 0.31893306970596313
setp: 2900, Loss: 0.31837132573127747
setp: 3000, Loss: 0.32021385431289673
setp: 3100, Loss: 0.3194020390510559
setp: 3200, Loss: 0.31971246004104614
setp: 3300, Loss: 0.3511185050010681
setp: 3400, Loss: 0.32183343172073364
setp: 3500, Loss: 0.31936532258987427
setp: 3600, Loss: 0.3189847767353058
setp: 3700, Loss: 0.3188793659210205
setp: 3800, Loss: 0.3192547559738159
setp: 3900, Loss: 0.32040879130363464
setp: 4000, Loss: 0.31820565462112427
setp: 4100, Loss: 0.31936192512512207
setp: 4200, Loss: 0.34326303005218506
setp: 4300, Loss: 0.3297808766365051
setp: 4400, Loss: 0.320065438747406
setp: 4500, Loss: 0.31721383333206177
setp: 4600, Loss: 0.31792280077934265
setp: 4700, Loss: 0.31810086965560913
setp: 4800, Loss: 0.3179240822792053
setp: 4900, Loss: 0.3191494047641754
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.8961038961038961
recall: 0.9583333333333334
F_score: 0.9261744966442952
******fold 1******
[298, 310]
training...
setp: 0, Loss: 0.7062883377075195
setp: 100, Loss: 0.6669551730155945
setp: 200, Loss: 0.5583828687667847
setp: 300, Loss: 0.5299739241600037
setp: 400, Loss: 0.44078394770622253
setp: 500, Loss: 0.3608597218990326
setp: 600, Loss: 0.3672389090061188
setp: 700, Loss: 0.3383713960647583
setp: 800, Loss: 0.36515742540359497
setp: 900, Loss: 0.3216450810432434
setp: 1000, Loss: 0.3207155168056488
setp: 1100, Loss: 0.35219454765319824
setp: 1200, Loss: 0.32089847326278687
setp: 1300, Loss: 0.32282811403274536
setp: 1400, Loss: 0.32259735465049744
setp: 1500, Loss: 0.3200947940349579
setp: 1600, Loss: 0.3184150755405426
setp: 1700, Loss: 0.37189850211143494
setp: 1800, Loss: 0.3198279142379761
setp: 1900, Loss: 0.32138538360595703
setp: 2000, Loss: 0.3276306986808777
setp: 2100, Loss: 0.318437397480011
setp: 2200, Loss: 0.3193221986293793
setp: 2300, Loss: 0.32096871733665466
setp: 2400, Loss: 0.3182827830314636
setp: 2500, Loss: 0.31779569387435913
setp: 2600, Loss: 0.31930920481681824
setp: 2700, Loss: 0.31832846999168396
setp: 2800, Loss: 0.3194248080253601
setp: 2900, Loss: 0.3177171051502228
setp: 3000, Loss: 0.4959769546985626
setp: 3100, Loss: 0.33518868684768677
setp: 3200, Loss: 0.33726754784584045
setp: 3300, Loss: 0.3444836139678955
setp: 3400, Loss: 0.3203686475753784
setp: 3500, Loss: 0.3182649612426758
setp: 3600, Loss: 0.3202453553676605
setp: 3700, Loss: 0.31914448738098145
setp: 3800, Loss: 0.319430410861969
setp: 3900, Loss: 0.31857532262802124
setp: 4000, Loss: 0.3176663815975189
setp: 4100, Loss: 0.3500254452228546
setp: 4200, Loss: 0.32052841782569885
setp: 4300, Loss: 0.3185107409954071
setp: 4400, Loss: 0.3878900706768036
setp: 4500, Loss: 0.3231399655342102
setp: 4600, Loss: 0.3190995156764984
setp: 4700, Loss: 0.3182865083217621
setp: 4800, Loss: 0.3174409568309784
setp: 4900, Loss: 0.32426023483276367
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.9
recall: 0.9878048780487805
F_score: 0.9418604651162791
******fold 2******
[311, 297]
training...
setp: 0, Loss: 0.696821391582489
setp: 100, Loss: 0.629463791847229
setp: 200, Loss: 0.5360144972801208
setp: 300, Loss: 0.500149130821228
setp: 400, Loss: 0.5856397151947021
setp: 500, Loss: 0.4852084517478943
setp: 600, Loss: 0.3929511308670044
setp: 700, Loss: 0.3892415761947632
setp: 800, Loss: 0.4190787672996521
setp: 900, Loss: 0.341549813747406
setp: 1000, Loss: 0.32284680008888245
setp: 1100, Loss: 0.3263888955116272
setp: 1200, Loss: 0.3280513882637024
setp: 1300, Loss: 0.327839732170105
setp: 1400, Loss: 0.318920373916626
setp: 1500, Loss: 0.3224237859249115
setp: 1600, Loss: 0.31728947162628174
setp: 1700, Loss: 0.31999796628952026
setp: 1800, Loss: 0.3196581304073334
setp: 1900, Loss: 0.31907737255096436
setp: 2000, Loss: 0.3175096809864044
setp: 2100, Loss: 0.517092227935791
setp: 2200, Loss: 0.3514610826969147
setp: 2300, Loss: 0.3201475143432617
setp: 2400, Loss: 0.31838011741638184
setp: 2500, Loss: 0.31698694825172424
setp: 2600, Loss: 0.3178868591785431
setp: 2700, Loss: 0.3186827003955841
setp: 2800, Loss: 0.3180219531059265
setp: 2900, Loss: 0.31893548369407654
setp: 3000, Loss: 0.3188610374927521
setp: 3100, Loss: 0.3181678056716919
setp: 3200, Loss: 0.3190052807331085
setp: 3300, Loss: 0.42877838015556335
setp: 3400, Loss: 0.3311194181442261
setp: 3500, Loss: 0.3174610435962677
setp: 3600, Loss: 0.3188210129737854
setp: 3700, Loss: 0.317891389131546
setp: 3800, Loss: 0.31735748052597046
setp: 3900, Loss: 0.31674066185951233
setp: 4000, Loss: 0.31716540455818176
setp: 4100, Loss: 0.34768834710121155
setp: 4200, Loss: 0.3177211880683899
setp: 4300, Loss: 0.3180089592933655
setp: 4400, Loss: 0.31651413440704346
setp: 4500, Loss: 0.3185350000858307
setp: 4600, Loss: 0.3184468746185303
setp: 4700, Loss: 0.31733790040016174
setp: 4800, Loss: 0.31730371713638306
setp: 4900, Loss: 0.32628369331359863
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9967845659163987
F_score: 0.998389694041868
validating...
acc: 0.9144736842105263
precision: 0.8888888888888888
recall: 0.927536231884058
F_score: 0.9078014184397163
******fold 3******
[302, 306]
training...
setp: 0, Loss: 0.7552236318588257
setp: 100, Loss: 0.6944503784179688
setp: 200, Loss: 0.6950815916061401
setp: 300, Loss: 0.6826508641242981
setp: 400, Loss: 0.569366991519928
setp: 500, Loss: 0.44581082463264465
setp: 600, Loss: 0.35218697786331177
setp: 700, Loss: 0.3742055892944336
setp: 800, Loss: 0.3300897479057312
setp: 900, Loss: 0.32182660698890686
setp: 1000, Loss: 0.32035261392593384
setp: 1100, Loss: 0.3242938220500946
setp: 1200, Loss: 0.3274264335632324
setp: 1300, Loss: 0.3286583423614502
setp: 1400, Loss: 0.31814900040626526
setp: 1500, Loss: 0.31936734914779663
setp: 1600, Loss: 0.3191021680831909
setp: 1700, Loss: 0.3195277750492096
setp: 1800, Loss: 0.3174878656864166
setp: 1900, Loss: 0.31842041015625
setp: 2000, Loss: 0.3184652030467987
setp: 2100, Loss: 0.318091481924057
setp: 2200, Loss: 0.4057338833808899
setp: 2300, Loss: 0.33713725209236145
setp: 2400, Loss: 0.3194076716899872
setp: 2500, Loss: 0.316690057516098
setp: 2600, Loss: 0.3170085847377777
setp: 2700, Loss: 0.31704720854759216
setp: 2800, Loss: 0.3175337016582489
setp: 2900, Loss: 0.3176019787788391
setp: 3000, Loss: 0.31830233335494995
setp: 3100, Loss: 0.31859612464904785
setp: 3200, Loss: 0.31734347343444824
setp: 3300, Loss: 0.46258729696273804
setp: 3400, Loss: 0.41730445623397827
setp: 3500, Loss: 0.3723926544189453
setp: 3600, Loss: 0.3370343744754791
setp: 3700, Loss: 0.3270666003227234
setp: 3800, Loss: 0.32951536774635315
setp: 3900, Loss: 0.3276331424713135
setp: 4000, Loss: 0.32580143213272095
setp: 4100, Loss: 0.3255749046802521
setp: 4200, Loss: 0.3258179724216461
setp: 4300, Loss: 0.32641270756721497
setp: 4400, Loss: 0.3252003490924835
setp: 4500, Loss: 0.3308229148387909
setp: 4600, Loss: 0.324041485786438
setp: 4700, Loss: 0.32242828607559204
setp: 4800, Loss: 0.3232920467853546
setp: 4900, Loss: 0.32256266474723816
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9144736842105263
precision: 0.891566265060241
recall: 0.9487179487179487
F_score: 0.9192546583850931
******fold 4******
[301, 307]
training...
setp: 0, Loss: 0.7451107501983643
setp: 100, Loss: 0.6659978628158569
setp: 200, Loss: 0.6328375339508057
setp: 300, Loss: 0.5389882922172546
setp: 400, Loss: 0.4357936382293701
setp: 500, Loss: 0.40119612216949463
setp: 600, Loss: 0.3589929938316345
setp: 700, Loss: 0.35918229818344116
setp: 800, Loss: 0.39402052760124207
setp: 900, Loss: 0.3663223087787628
setp: 1000, Loss: 0.32043975591659546
setp: 1100, Loss: 0.32505863904953003
setp: 1200, Loss: 0.3231137990951538
setp: 1300, Loss: 0.31898215413093567
setp: 1400, Loss: 0.32009220123291016
setp: 1500, Loss: 0.3233146369457245
setp: 1600, Loss: 0.32021504640579224
setp: 1700, Loss: 0.31961533427238464
setp: 1800, Loss: 0.31840455532073975
setp: 1900, Loss: 0.3218538463115692
setp: 2000, Loss: 0.3186388611793518
setp: 2100, Loss: 0.31837111711502075
setp: 2200, Loss: 0.3188469409942627
setp: 2300, Loss: 0.32027989625930786
setp: 2400, Loss: 0.3176693618297577
setp: 2500, Loss: 0.31686049699783325
setp: 2600, Loss: 0.31837862730026245
setp: 2700, Loss: 0.5167195200920105
setp: 2800, Loss: 0.35335618257522583
setp: 2900, Loss: 0.3570001721382141
setp: 3000, Loss: 0.3313218057155609
setp: 3100, Loss: 0.3288864493370056
setp: 3200, Loss: 0.32754966616630554
setp: 3300, Loss: 0.32572224736213684
setp: 3400, Loss: 0.3565603196620941
setp: 3500, Loss: 0.32355403900146484
setp: 3600, Loss: 0.3796256184577942
setp: 3700, Loss: 0.33954039216041565
setp: 3800, Loss: 0.3260262906551361
setp: 3900, Loss: 0.3232290744781494
setp: 4000, Loss: 0.32429367303848267
setp: 4100, Loss: 0.3247346580028534
setp: 4200, Loss: 0.32535746693611145
setp: 4300, Loss: 0.32315370440483093
setp: 4400, Loss: 0.3223966062068939
setp: 4500, Loss: 0.32243242859840393
setp: 4600, Loss: 0.3225323259830475
setp: 4700, Loss: 0.3219843804836273
setp: 4800, Loss: 0.32249826192855835
setp: 4900, Loss: 0.3220851421356201
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8947368421052632
precision: 0.9090909090909091
recall: 0.8860759493670886
F_score: 0.8974358974358974
model saved.
avg_acc: 0.9171052631578946, avg_f_score: 0.9185053872042562
-------------subject: 31-------------
==========valence==========
******fold 0******
[216, 392]
training...
setp: 0, Loss: 0.6610888838768005
setp: 100, Loss: 0.679268479347229
setp: 200, Loss: 0.6605666875839233
setp: 300, Loss: 0.5686954855918884
setp: 400, Loss: 0.45125073194503784
setp: 500, Loss: 0.3571738004684448
setp: 600, Loss: 0.3304826021194458
setp: 700, Loss: 0.3278313875198364
setp: 800, Loss: 0.3527868390083313
setp: 900, Loss: 0.37881094217300415
setp: 1000, Loss: 0.3253217935562134
setp: 1100, Loss: 0.32189735770225525
setp: 1200, Loss: 0.3217671513557434
setp: 1300, Loss: 0.3203554153442383
setp: 1400, Loss: 0.3201906979084015
setp: 1500, Loss: 0.3198876678943634
setp: 1600, Loss: 0.32019129395484924
setp: 1700, Loss: 0.3385486304759979
setp: 1800, Loss: 0.32049164175987244
setp: 1900, Loss: 0.32134777307510376
setp: 2000, Loss: 0.31819993257522583
setp: 2100, Loss: 0.3192209303379059
setp: 2200, Loss: 0.3196350336074829
setp: 2300, Loss: 0.32062169909477234
setp: 2400, Loss: 0.31954365968704224
setp: 2500, Loss: 0.31880277395248413
setp: 2600, Loss: 0.31942227482795715
setp: 2700, Loss: 0.3195154368877411
setp: 2800, Loss: 0.3185999095439911
setp: 2900, Loss: 0.3568359613418579
setp: 3000, Loss: 0.35430222749710083
setp: 3100, Loss: 0.3319578468799591
setp: 3200, Loss: 0.31964588165283203
setp: 3300, Loss: 0.3194465935230255
setp: 3400, Loss: 0.31989187002182007
setp: 3500, Loss: 0.3196626305580139
setp: 3600, Loss: 0.321641743183136
setp: 3700, Loss: 0.3222956657409668
setp: 3800, Loss: 0.3463534116744995
setp: 3900, Loss: 0.3205083906650543
setp: 4000, Loss: 0.31938230991363525
setp: 4100, Loss: 0.32018792629241943
setp: 4200, Loss: 0.3207552433013916
setp: 4300, Loss: 0.3205607831478119
setp: 4400, Loss: 0.319309264421463
setp: 4500, Loss: 0.3199976682662964
setp: 4600, Loss: 0.3200981318950653
setp: 4700, Loss: 0.31902939081192017
setp: 4800, Loss: 0.32090094685554504
setp: 4900, Loss: 0.34238043427467346
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.9148936170212766
recall: 0.86
F_score: 0.8865979381443299
******fold 1******
[209, 399]
training...
setp: 0, Loss: 0.6298663020133972
setp: 100, Loss: 0.581232488155365
setp: 200, Loss: 0.6213001012802124
setp: 300, Loss: 0.5631122589111328
setp: 400, Loss: 0.4359341859817505
setp: 500, Loss: 0.35248425602912903
setp: 600, Loss: 0.33705341815948486
setp: 700, Loss: 0.3317958414554596
setp: 800, Loss: 0.3240705728530884
setp: 900, Loss: 0.38031628727912903
setp: 1000, Loss: 0.3251083791255951
setp: 1100, Loss: 0.319890558719635
setp: 1200, Loss: 0.3396698236465454
setp: 1300, Loss: 0.3280538320541382
setp: 1400, Loss: 0.3273383378982544
setp: 1500, Loss: 0.31804126501083374
setp: 1600, Loss: 0.3182479739189148
setp: 1700, Loss: 0.31941235065460205
setp: 1800, Loss: 0.31749263405799866
setp: 1900, Loss: 0.31967198848724365
setp: 2000, Loss: 0.31843316555023193
setp: 2100, Loss: 0.3189931809902191
setp: 2200, Loss: 0.3206096291542053
setp: 2300, Loss: 0.31939610838890076
setp: 2400, Loss: 0.3186948001384735
setp: 2500, Loss: 0.3192541301250458
setp: 2600, Loss: 0.3192139267921448
setp: 2700, Loss: 0.39745667576789856
setp: 2800, Loss: 0.32019269466400146
setp: 2900, Loss: 0.4485762119293213
setp: 3000, Loss: 0.32314956188201904
setp: 3100, Loss: 0.3180916905403137
setp: 3200, Loss: 0.32633712887763977
setp: 3300, Loss: 0.3189466595649719
setp: 3400, Loss: 0.31803280115127563
setp: 3500, Loss: 0.3182908296585083
setp: 3600, Loss: 0.3191649615764618
setp: 3700, Loss: 0.3173479437828064
setp: 3800, Loss: 0.3191065788269043
setp: 3900, Loss: 0.3183254301548004
setp: 4000, Loss: 0.31841951608657837
setp: 4100, Loss: 0.32041338086128235
setp: 4200, Loss: 0.3190538287162781
setp: 4300, Loss: 0.31824225187301636
setp: 4400, Loss: 0.3189966082572937
setp: 4500, Loss: 0.3192133903503418
setp: 4600, Loss: 0.40642645955085754
setp: 4700, Loss: 0.40082600712776184
setp: 4800, Loss: 0.32480502128601074
setp: 4900, Loss: 0.31906452775001526
training successfully ended.
validating...
acc: 0.9835526315789473
precision: 1.0
recall: 0.9521531100478469
F_score: 0.9754901960784313
validating...
acc: 0.8881578947368421
precision: 1.0
recall: 0.7017543859649122
F_score: 0.8247422680412371
******fold 2******
[220, 388]
training...
setp: 0, Loss: 0.7917753458023071
setp: 100, Loss: 0.6605977416038513
setp: 200, Loss: 0.5661989450454712
setp: 300, Loss: 0.6004047393798828
setp: 400, Loss: 0.5434077978134155
setp: 500, Loss: 0.46962815523147583
setp: 600, Loss: 0.3420144319534302
setp: 700, Loss: 0.386284202337265
setp: 800, Loss: 0.39481431245803833
setp: 900, Loss: 0.4001902937889099
setp: 1000, Loss: 0.34244799613952637
setp: 1100, Loss: 0.3239421844482422
setp: 1200, Loss: 0.3211304843425751
setp: 1300, Loss: 0.3235231935977936
setp: 1400, Loss: 0.317351758480072
setp: 1500, Loss: 0.3184663653373718
setp: 1600, Loss: 0.31764277815818787
setp: 1700, Loss: 0.32083943486213684
setp: 1800, Loss: 0.32505515217781067
setp: 1900, Loss: 0.3776363730430603
setp: 2000, Loss: 0.31871405243873596
setp: 2100, Loss: 0.3180130124092102
setp: 2200, Loss: 0.3231268525123596
setp: 2300, Loss: 0.3523884415626526
setp: 2400, Loss: 0.3243735134601593
setp: 2500, Loss: 0.350923627614975
setp: 2600, Loss: 0.3355365991592407
setp: 2700, Loss: 0.34966397285461426
setp: 2800, Loss: 0.3174583315849304
setp: 2900, Loss: 0.32068997621536255
setp: 3000, Loss: 0.3182925283908844
setp: 3100, Loss: 0.32014769315719604
setp: 3200, Loss: 0.3176286220550537
setp: 3300, Loss: 0.32904180884361267
setp: 3400, Loss: 0.31864938139915466
setp: 3500, Loss: 0.32182255387306213
setp: 3600, Loss: 0.3171747028827667
setp: 3700, Loss: 0.323559433221817
setp: 3800, Loss: 0.3277863562107086
setp: 3900, Loss: 0.3173180818557739
setp: 4000, Loss: 0.31647562980651855
setp: 4100, Loss: 0.31702205538749695
setp: 4200, Loss: 0.31806302070617676
setp: 4300, Loss: 0.31760379672050476
setp: 4400, Loss: 0.31770145893096924
setp: 4500, Loss: 0.3184201419353485
setp: 4600, Loss: 0.3181566298007965
setp: 4700, Loss: 0.3173752725124359
setp: 4800, Loss: 0.3762880861759186
setp: 4900, Loss: 0.3267707824707031
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9302325581395349
recall: 0.8695652173913043
F_score: 0.898876404494382
******fold 3******
[215, 393]
training...
setp: 0, Loss: 0.6953266263008118
setp: 100, Loss: 0.6261112093925476
setp: 200, Loss: 0.680643618106842
setp: 300, Loss: 0.61411052942276
setp: 400, Loss: 0.4080502986907959
setp: 500, Loss: 0.3373640477657318
setp: 600, Loss: 0.342169851064682
setp: 700, Loss: 0.3390483856201172
setp: 800, Loss: 0.3508969247341156
setp: 900, Loss: 0.32039332389831543
setp: 1000, Loss: 0.3199002146720886
setp: 1100, Loss: 0.3203430771827698
setp: 1200, Loss: 0.31874531507492065
setp: 1300, Loss: 0.32021597027778625
setp: 1400, Loss: 0.31812140345573425
setp: 1500, Loss: 0.31838464736938477
setp: 1600, Loss: 0.31884312629699707
setp: 1700, Loss: 0.31932005286216736
setp: 1800, Loss: 0.31770777702331543
setp: 1900, Loss: 0.3198288381099701
setp: 2000, Loss: 0.31904149055480957
setp: 2100, Loss: 0.32688018679618835
setp: 2200, Loss: 0.350263774394989
setp: 2300, Loss: 0.36135444045066833
setp: 2400, Loss: 0.3186459243297577
setp: 2500, Loss: 0.31834056973457336
setp: 2600, Loss: 0.3178000748157501
setp: 2700, Loss: 0.3485318124294281
setp: 2800, Loss: 0.31822553277015686
setp: 2900, Loss: 0.31870853900909424
setp: 3000, Loss: 0.3194689452648163
setp: 3100, Loss: 0.31859350204467773
setp: 3200, Loss: 0.3192974925041199
setp: 3300, Loss: 0.31887367367744446
setp: 3400, Loss: 0.31859979033470154
setp: 3500, Loss: 0.31845733523368835
setp: 3600, Loss: 0.31939613819122314
setp: 3700, Loss: 0.31781288981437683
setp: 3800, Loss: 0.3199210464954376
setp: 3900, Loss: 0.3469647169113159
setp: 4000, Loss: 0.35719043016433716
setp: 4100, Loss: 0.3664153516292572
setp: 4200, Loss: 0.3299352526664734
setp: 4300, Loss: 0.3236676752567291
setp: 4400, Loss: 0.3211427330970764
setp: 4500, Loss: 0.32098186016082764
setp: 4600, Loss: 0.3512711524963379
setp: 4700, Loss: 0.32091033458709717
setp: 4800, Loss: 0.32144585251808167
setp: 4900, Loss: 0.32158932089805603
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9953488372093023
recall: 0.9953488372093023
F_score: 0.9953488372093023
validating...
acc: 0.875
precision: 0.7857142857142857
recall: 0.8627450980392157
F_score: 0.8224299065420562
******fold 4******
[204, 404]
training...
setp: 0, Loss: 0.6694689989089966
setp: 100, Loss: 0.6015341281890869
setp: 200, Loss: 0.5782364010810852
setp: 300, Loss: 0.644133448600769
setp: 400, Loss: 0.5823688507080078
setp: 500, Loss: 0.6861298084259033
setp: 600, Loss: 0.6439238786697388
setp: 700, Loss: 0.6884854435920715
setp: 800, Loss: 0.582663357257843
setp: 900, Loss: 0.6223138570785522
setp: 1000, Loss: 0.6664884090423584
setp: 1100, Loss: 0.5787308812141418
setp: 1200, Loss: 0.6435015797615051
setp: 1300, Loss: 0.6436625719070435
setp: 1400, Loss: 0.5990337133407593
setp: 1500, Loss: 0.6865372657775879
setp: 1600, Loss: 0.6435205936431885
setp: 1700, Loss: 0.6221871376037598
setp: 1800, Loss: 0.7584257125854492
setp: 1900, Loss: 0.6645235419273376
setp: 2000, Loss: 0.6018151044845581
setp: 2100, Loss: 0.578793466091156
setp: 2200, Loss: 0.6440872550010681
setp: 2300, Loss: 0.5822083353996277
setp: 2400, Loss: 0.6860065460205078
setp: 2500, Loss: 0.6438790559768677
setp: 2600, Loss: 0.6882673501968384
setp: 2700, Loss: 0.5826664566993713
setp: 2800, Loss: 0.6223699450492859
setp: 2900, Loss: 0.6663901805877686
setp: 3000, Loss: 0.5788742899894714
setp: 3100, Loss: 0.6435023546218872
setp: 3200, Loss: 0.6436505317687988
setp: 3300, Loss: 0.5990967154502869
setp: 3400, Loss: 0.6864361763000488
setp: 3500, Loss: 0.6435214877128601
setp: 3600, Loss: 0.622194766998291
setp: 3700, Loss: 0.7580059170722961
setp: 3800, Loss: 0.6644951105117798
setp: 3900, Loss: 0.6017937660217285
setp: 4000, Loss: 0.5788291096687317
setp: 4100, Loss: 0.6440683603286743
setp: 4200, Loss: 0.5822967886924744
setp: 4300, Loss: 0.6860191822052002
setp: 4400, Loss: 0.6438637971878052
setp: 4500, Loss: 0.6881544589996338
setp: 4600, Loss: 0.5826500654220581
setp: 4700, Loss: 0.6223704218864441
setp: 4800, Loss: 0.6663320660591125
setp: 4900, Loss: 0.5789541602134705
training successfully ended.
validating...
acc: 0.6644736842105263
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.5921052631578947
precision: 0
recall: 0.0
F_score: 0
model saved.
avg_acc: 0.8447368421052632, avg_f_score: 0.686529303444401
==========arousal==========
******fold 0******
[298, 310]
training...
setp: 0, Loss: 0.695284903049469
setp: 100, Loss: 0.6902278065681458
setp: 200, Loss: 0.5746498107910156
setp: 300, Loss: 0.4298289120197296
setp: 400, Loss: 0.4097890853881836
setp: 500, Loss: 0.3591728210449219
setp: 600, Loss: 0.32816049456596375
setp: 700, Loss: 0.33158543705940247
setp: 800, Loss: 0.3518657088279724
setp: 900, Loss: 0.3237174153327942
setp: 1000, Loss: 0.3359893262386322
setp: 1100, Loss: 0.32043594121932983
setp: 1200, Loss: 0.31870099902153015
setp: 1300, Loss: 0.319324791431427
setp: 1400, Loss: 0.31787678599357605
setp: 1500, Loss: 0.32069727778434753
setp: 1600, Loss: 0.31884288787841797
setp: 1700, Loss: 0.31876498460769653
setp: 1800, Loss: 0.3184950649738312
setp: 1900, Loss: 0.32019832730293274
setp: 2000, Loss: 0.3187761604785919
setp: 2100, Loss: 0.3192432224750519
setp: 2200, Loss: 0.32018914818763733
setp: 2300, Loss: 0.36082029342651367
setp: 2400, Loss: 0.34347429871559143
setp: 2500, Loss: 0.342404305934906
setp: 2600, Loss: 0.31876906752586365
setp: 2700, Loss: 0.34861743450164795
setp: 2800, Loss: 0.3172641694545746
setp: 2900, Loss: 0.31911206245422363
setp: 3000, Loss: 0.3184957504272461
setp: 3100, Loss: 0.31874892115592957
setp: 3200, Loss: 0.31889504194259644
setp: 3300, Loss: 0.3180318772792816
setp: 3400, Loss: 0.31875601410865784
setp: 3500, Loss: 0.3188735544681549
setp: 3600, Loss: 0.31881335377693176
setp: 3700, Loss: 0.31888553500175476
setp: 3800, Loss: 0.4378645122051239
setp: 3900, Loss: 0.33319735527038574
setp: 4000, Loss: 0.32132625579833984
setp: 4100, Loss: 0.33494099974632263
setp: 4200, Loss: 0.3345002830028534
setp: 4300, Loss: 0.3250812888145447
setp: 4400, Loss: 0.3195638060569763
setp: 4500, Loss: 0.31959882378578186
setp: 4600, Loss: 0.31976690888404846
setp: 4700, Loss: 0.318134605884552
setp: 4800, Loss: 0.31945791840553284
setp: 4900, Loss: 0.31952354311943054
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9620253164556962
recall: 0.926829268292683
F_score: 0.9440993788819876
******fold 1******
[309, 299]
training...
setp: 0, Loss: 0.7086514234542847
setp: 100, Loss: 0.6903749704360962
setp: 200, Loss: 0.6925913095474243
setp: 300, Loss: 0.6913295388221741
setp: 400, Loss: 0.6932694315910339
setp: 500, Loss: 0.695034921169281
setp: 600, Loss: 0.6922063827514648
setp: 700, Loss: 0.6932144165039062
setp: 800, Loss: 0.6932709217071533
setp: 900, Loss: 0.6932347416877747
setp: 1000, Loss: 0.691905677318573
setp: 1100, Loss: 0.6940665245056152
setp: 1200, Loss: 0.6942433714866638
setp: 1300, Loss: 0.6947246789932251
setp: 1400, Loss: 0.6927438378334045
setp: 1500, Loss: 0.6941494941711426
setp: 1600, Loss: 0.694172203540802
setp: 1700, Loss: 0.6938159465789795
setp: 1800, Loss: 0.6927159428596497
setp: 1900, Loss: 0.6932563185691833
setp: 2000, Loss: 0.6907762885093689
setp: 2100, Loss: 0.6926997303962708
setp: 2200, Loss: 0.6916342377662659
setp: 2300, Loss: 0.693260133266449
setp: 2400, Loss: 0.6948940753936768
setp: 2500, Loss: 0.6924089193344116
setp: 2600, Loss: 0.6932018399238586
setp: 2700, Loss: 0.6932616829872131
setp: 2800, Loss: 0.6932213306427002
setp: 2900, Loss: 0.6921482682228088
setp: 3000, Loss: 0.6940088272094727
setp: 3100, Loss: 0.6942050457000732
setp: 3200, Loss: 0.694606602191925
setp: 3300, Loss: 0.6927955746650696
setp: 3400, Loss: 0.6941124200820923
setp: 3500, Loss: 0.6941373944282532
setp: 3600, Loss: 0.693767786026001
setp: 3700, Loss: 0.6927482485771179
setp: 3800, Loss: 0.6932516694068909
setp: 3900, Loss: 0.6908504962921143
setp: 4000, Loss: 0.692725419998169
setp: 4100, Loss: 0.6916959285736084
setp: 4200, Loss: 0.6932575702667236
setp: 4300, Loss: 0.6948544979095459
setp: 4400, Loss: 0.6924453973770142
setp: 4500, Loss: 0.6931997537612915
setp: 4600, Loss: 0.6932604312896729
setp: 4700, Loss: 0.6932190656661987
setp: 4800, Loss: 0.6921862363815308
setp: 4900, Loss: 0.6940000653266907
training successfully ended.
validating...
acc: 0.5082236842105263
precision: 0.5082236842105263
recall: 1.0
F_score: 0.6739367502726281
validating...
acc: 0.46710526315789475
precision: 0.46710526315789475
recall: 1.0
F_score: 0.6367713004484306
******fold 2******
[308, 300]
training...
setp: 0, Loss: 0.6639844179153442
setp: 100, Loss: 0.6954522728919983
setp: 200, Loss: 0.6963671445846558
setp: 300, Loss: 0.6960927844047546
setp: 400, Loss: 0.6102858781814575
setp: 500, Loss: 0.4856366217136383
setp: 600, Loss: 0.49304166436195374
setp: 700, Loss: 0.5080218315124512
setp: 800, Loss: 0.6078513264656067
setp: 900, Loss: 0.4718848764896393
setp: 1000, Loss: 0.4919055104255676
setp: 1100, Loss: 0.46183446049690247
setp: 1200, Loss: 0.4359755218029022
setp: 1300, Loss: 0.394267201423645
setp: 1400, Loss: 0.3677126467227936
setp: 1500, Loss: 0.3647333085536957
setp: 1600, Loss: 0.3614688515663147
setp: 1700, Loss: 0.36227840185165405
setp: 1800, Loss: 0.3605446219444275
setp: 1900, Loss: 0.38850054144859314
setp: 2000, Loss: 0.3193036913871765
setp: 2100, Loss: 0.36252206563949585
setp: 2200, Loss: 0.3590849041938782
setp: 2300, Loss: 0.35047224164009094
setp: 2400, Loss: 0.3202146887779236
setp: 2500, Loss: 0.3555145561695099
setp: 2600, Loss: 0.32041311264038086
setp: 2700, Loss: 0.3578467071056366
setp: 2800, Loss: 0.3165944516658783
setp: 2900, Loss: 0.3815671503543854
setp: 3000, Loss: 0.31712716817855835
setp: 3100, Loss: 0.3394879102706909
setp: 3200, Loss: 0.3238806128501892
setp: 3300, Loss: 0.3265734910964966
setp: 3400, Loss: 0.40108972787857056
setp: 3500, Loss: 0.3307555019855499
setp: 3600, Loss: 0.3304479718208313
setp: 3700, Loss: 0.3801422715187073
setp: 3800, Loss: 0.47521916031837463
setp: 3900, Loss: 0.32044798135757446
setp: 4000, Loss: 0.3478163778781891
setp: 4100, Loss: 0.3200911283493042
setp: 4200, Loss: 0.31776177883148193
setp: 4300, Loss: 0.3191778361797333
setp: 4400, Loss: 0.3178395926952362
setp: 4500, Loss: 0.3188021183013916
setp: 4600, Loss: 0.3497432470321655
setp: 4700, Loss: 0.31628867983818054
setp: 4800, Loss: 0.32306379079818726
setp: 4900, Loss: 0.31944528222084045
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9935483870967742
recall: 1.0
F_score: 0.9967637540453074
validating...
acc: 0.868421052631579
precision: 0.8421052631578947
recall: 0.8888888888888888
F_score: 0.8648648648648649
******fold 3******
[299, 309]
training...
setp: 0, Loss: 0.686576247215271
setp: 100, Loss: 0.685319185256958
setp: 200, Loss: 0.6156600117683411
setp: 300, Loss: 0.5723147392272949
setp: 400, Loss: 0.43595606088638306
setp: 500, Loss: 0.4442192316055298
setp: 600, Loss: 0.4286140203475952
setp: 700, Loss: 0.34631505608558655
setp: 800, Loss: 0.45559826493263245
setp: 900, Loss: 0.36162856221199036
setp: 1000, Loss: 0.324950248003006
setp: 1100, Loss: 0.43043339252471924
setp: 1200, Loss: 0.31822627782821655
setp: 1300, Loss: 0.32508546113967896
setp: 1400, Loss: 0.3331330418586731
setp: 1500, Loss: 0.3756774365901947
setp: 1600, Loss: 0.3174792528152466
setp: 1700, Loss: 0.33500435948371887
setp: 1800, Loss: 0.32418185472488403
setp: 1900, Loss: 0.3285704553127289
setp: 2000, Loss: 0.3507503867149353
setp: 2100, Loss: 0.35063135623931885
setp: 2200, Loss: 0.32127702236175537
setp: 2300, Loss: 0.31653836369514465
setp: 2400, Loss: 0.31850603222846985
setp: 2500, Loss: 0.3168551027774811
setp: 2600, Loss: 0.33356133103370667
setp: 2700, Loss: 0.36309194564819336
setp: 2800, Loss: 0.3166080415248871
setp: 2900, Loss: 0.3174838721752167
setp: 3000, Loss: 0.3539951741695404
setp: 3100, Loss: 0.3482639193534851
setp: 3200, Loss: 0.33561959862709045
setp: 3300, Loss: 0.3586941361427307
setp: 3400, Loss: 0.34731289744377136
setp: 3500, Loss: 0.3153671622276306
setp: 3600, Loss: 0.3167843222618103
setp: 3700, Loss: 0.3160880208015442
setp: 3800, Loss: 0.31903818249702454
setp: 3900, Loss: 0.3177757263183594
setp: 4000, Loss: 0.3178812563419342
setp: 4100, Loss: 0.31729674339294434
setp: 4200, Loss: 0.31578528881073
setp: 4300, Loss: 0.318022757768631
setp: 4400, Loss: 0.31680068373680115
setp: 4500, Loss: 0.3177769184112549
setp: 4600, Loss: 0.32023754715919495
setp: 4700, Loss: 0.31662115454673767
setp: 4800, Loss: 0.31730422377586365
setp: 4900, Loss: 0.3181625306606293
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8947368421052632
precision: 0.9333333333333333
recall: 0.8641975308641975
F_score: 0.8974358974358974
******fold 4******
[306, 302]
training...
setp: 0, Loss: 0.720909833908081
setp: 100, Loss: 0.6910060048103333
setp: 200, Loss: 0.6185997128486633
setp: 300, Loss: 0.48220178484916687
setp: 400, Loss: 0.40152859687805176
setp: 500, Loss: 0.38700827956199646
setp: 600, Loss: 0.34576430916786194
setp: 700, Loss: 0.35752883553504944
setp: 800, Loss: 0.32794448733329773
setp: 900, Loss: 0.35074618458747864
setp: 1000, Loss: 0.3218700885772705
setp: 1100, Loss: 0.3204703629016876
setp: 1200, Loss: 0.32660675048828125
setp: 1300, Loss: 0.32019108533859253
setp: 1400, Loss: 0.32011109590530396
setp: 1500, Loss: 0.32025590538978577
setp: 1600, Loss: 0.324781596660614
setp: 1700, Loss: 0.3190593719482422
setp: 1800, Loss: 0.3187377154827118
setp: 1900, Loss: 0.321529746055603
setp: 2000, Loss: 0.31964123249053955
setp: 2100, Loss: 0.31879714131355286
setp: 2200, Loss: 0.3194061815738678
setp: 2300, Loss: 0.3189712464809418
setp: 2400, Loss: 0.34982243180274963
setp: 2500, Loss: 0.31912028789520264
setp: 2600, Loss: 0.3199400007724762
setp: 2700, Loss: 0.3195312023162842
setp: 2800, Loss: 0.319316565990448
setp: 2900, Loss: 0.3201756477355957
setp: 3000, Loss: 0.3774856925010681
setp: 3100, Loss: 0.32303234934806824
setp: 3200, Loss: 0.3347064256668091
setp: 3300, Loss: 0.31822124123573303
setp: 3400, Loss: 0.3190166652202606
setp: 3500, Loss: 0.3189879059791565
setp: 3600, Loss: 0.31894993782043457
setp: 3700, Loss: 0.3185288608074188
setp: 3800, Loss: 0.31977590918540955
setp: 3900, Loss: 0.4139619469642639
setp: 4000, Loss: 0.32116538286209106
setp: 4100, Loss: 0.323130339384079
setp: 4200, Loss: 0.31838905811309814
setp: 4300, Loss: 0.3183288276195526
setp: 4400, Loss: 0.31829896569252014
setp: 4500, Loss: 0.3192256987094879
setp: 4600, Loss: 0.3194403052330017
setp: 4700, Loss: 0.3189668357372284
setp: 4800, Loss: 0.3197697699069977
setp: 4900, Loss: 0.31906232237815857
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8552631578947368
precision: 0.8170731707317073
recall: 0.9054054054054054
F_score: 0.8589743589743589
model saved.
avg_acc: 0.8052631578947368, avg_f_score: 0.8404291601211078
-------------subject: 32-------------
==========valence==========
******fold 0******
[309, 299]
training...
setp: 0, Loss: 0.6933594942092896
setp: 100, Loss: 0.579718291759491
setp: 200, Loss: 0.5423145294189453
setp: 300, Loss: 0.63614821434021
setp: 400, Loss: 0.6191152334213257
setp: 500, Loss: 0.5342361927032471
setp: 600, Loss: 0.5971666574478149
setp: 700, Loss: 0.602914035320282
setp: 800, Loss: 0.5053759813308716
setp: 900, Loss: 0.5136172771453857
setp: 1000, Loss: 0.5696457028388977
setp: 1100, Loss: 0.5159801840782166
setp: 1200, Loss: 0.4237954020500183
setp: 1300, Loss: 0.5112671852111816
setp: 1400, Loss: 0.40539804100990295
setp: 1500, Loss: 0.37303024530410767
setp: 1600, Loss: 0.4750617444515228
setp: 1700, Loss: 0.405466765165329
setp: 1800, Loss: 0.3824516832828522
setp: 1900, Loss: 0.3888537883758545
setp: 2000, Loss: 0.36493024230003357
setp: 2100, Loss: 0.3563601076602936
setp: 2200, Loss: 0.38057219982147217
setp: 2300, Loss: 0.3597978353500366
setp: 2400, Loss: 0.35144349932670593
setp: 2500, Loss: 0.35146787762641907
setp: 2600, Loss: 0.3899354338645935
setp: 2700, Loss: 0.3430211842060089
setp: 2800, Loss: 0.3331403136253357
setp: 2900, Loss: 0.37998801469802856
setp: 3000, Loss: 0.4129936397075653
setp: 3100, Loss: 0.4754934310913086
setp: 3200, Loss: 0.37891677021980286
setp: 3300, Loss: 0.3230578899383545
setp: 3400, Loss: 0.3211192786693573
setp: 3500, Loss: 0.31804484128952026
setp: 3600, Loss: 0.3198947608470917
setp: 3700, Loss: 0.3183920979499817
setp: 3800, Loss: 0.32171058654785156
setp: 3900, Loss: 0.31713876128196716
setp: 4000, Loss: 0.3171318769454956
setp: 4100, Loss: 0.34125420451164246
setp: 4200, Loss: 0.3614678382873535
setp: 4300, Loss: 0.3169289827346802
setp: 4400, Loss: 0.3182552456855774
setp: 4500, Loss: 0.31720712780952454
setp: 4600, Loss: 0.3173752427101135
setp: 4700, Loss: 0.3172590136528015
setp: 4800, Loss: 0.31997865438461304
setp: 4900, Loss: 0.3322611451148987
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.8961038961038961
recall: 0.971830985915493
F_score: 0.9324324324324325
******fold 1******
[296, 312]
training...
setp: 0, Loss: 0.6942808628082275
setp: 100, Loss: 0.6581642627716064
setp: 200, Loss: 0.6589177250862122
setp: 300, Loss: 0.6101068258285522
setp: 400, Loss: 0.6489886045455933
setp: 500, Loss: 0.5446361303329468
setp: 600, Loss: 0.5351897478103638
setp: 700, Loss: 0.6417677402496338
setp: 800, Loss: 0.573583722114563
setp: 900, Loss: 0.5332925915718079
setp: 1000, Loss: 0.533643901348114
setp: 1100, Loss: 0.5140852928161621
setp: 1200, Loss: 0.622865617275238
setp: 1300, Loss: 0.4932587742805481
setp: 1400, Loss: 0.5206013321876526
setp: 1500, Loss: 0.40573710203170776
setp: 1600, Loss: 0.4708372950553894
setp: 1700, Loss: 0.4499766528606415
setp: 1800, Loss: 0.38235196471214294
setp: 1900, Loss: 0.36723488569259644
setp: 2000, Loss: 0.3671654462814331
setp: 2100, Loss: 0.3537455201148987
setp: 2200, Loss: 0.38393092155456543
setp: 2300, Loss: 0.3715600371360779
setp: 2400, Loss: 0.3282278776168823
setp: 2500, Loss: 0.35255780816078186
setp: 2600, Loss: 0.38041234016418457
setp: 2700, Loss: 0.3495224714279175
setp: 2800, Loss: 0.34867703914642334
setp: 2900, Loss: 0.36324405670166016
setp: 3000, Loss: 0.39151516556739807
setp: 3100, Loss: 0.3540562391281128
setp: 3200, Loss: 0.3476172089576721
setp: 3300, Loss: 0.3501814007759094
setp: 3400, Loss: 0.3168860375881195
setp: 3500, Loss: 0.3192339241504669
setp: 3600, Loss: 0.34985846281051636
setp: 3700, Loss: 0.31742510199546814
setp: 3800, Loss: 0.3194286525249481
setp: 3900, Loss: 0.4893680512905121
setp: 4000, Loss: 0.36226406693458557
setp: 4100, Loss: 0.375970721244812
setp: 4200, Loss: 0.35816454887390137
setp: 4300, Loss: 0.3198205232620239
setp: 4400, Loss: 0.34878915548324585
setp: 4500, Loss: 0.3790602684020996
setp: 4600, Loss: 0.34812626242637634
setp: 4700, Loss: 0.3476312458515167
setp: 4800, Loss: 0.34991344809532166
setp: 4900, Loss: 0.34852829575538635
training successfully ended.
validating...
acc: 0.9769736842105263
precision: 0.9548387096774194
recall: 1.0
F_score: 0.9768976897689768
validating...
acc: 0.8421052631578947
precision: 0.8658536585365854
recall: 0.8452380952380952
F_score: 0.8554216867469879
******fold 2******
[314, 294]
training...
setp: 0, Loss: 0.7279199361801147
setp: 100, Loss: 0.6485983729362488
setp: 200, Loss: 0.5404648780822754
setp: 300, Loss: 0.6413338780403137
setp: 400, Loss: 0.6433501243591309
setp: 500, Loss: 0.6010586023330688
setp: 600, Loss: 0.5397320985794067
setp: 700, Loss: 0.5448201894760132
setp: 800, Loss: 0.5255712270736694
setp: 900, Loss: 0.4790225028991699
setp: 1000, Loss: 0.5102630257606506
setp: 1100, Loss: 0.39137542247772217
setp: 1200, Loss: 0.4388306438922882
setp: 1300, Loss: 0.4266332983970642
setp: 1400, Loss: 0.48178645968437195
setp: 1500, Loss: 0.3795561194419861
setp: 1600, Loss: 0.40201637148857117
setp: 1700, Loss: 0.4494982361793518
setp: 1800, Loss: 0.46976199746131897
setp: 1900, Loss: 0.500210702419281
setp: 2000, Loss: 0.46094200015068054
setp: 2100, Loss: 0.36119166016578674
setp: 2200, Loss: 0.4110643267631531
setp: 2300, Loss: 0.4477008283138275
setp: 2400, Loss: 0.38129568099975586
setp: 2500, Loss: 0.3483859598636627
setp: 2600, Loss: 0.31829336285591125
setp: 2700, Loss: 0.37928158044815063
setp: 2800, Loss: 0.3213542401790619
setp: 2900, Loss: 0.3507883548736572
setp: 3000, Loss: 0.34768757224082947
setp: 3100, Loss: 0.3790058195590973
setp: 3200, Loss: 0.3830854892730713
setp: 3300, Loss: 0.3187045156955719
setp: 3400, Loss: 0.38482508063316345
setp: 3500, Loss: 0.3703852891921997
setp: 3600, Loss: 0.37889358401298523
setp: 3700, Loss: 0.3792813718318939
setp: 3800, Loss: 0.34979474544525146
setp: 3900, Loss: 0.4106042683124542
setp: 4000, Loss: 0.31766876578330994
setp: 4100, Loss: 0.40938684344291687
setp: 4200, Loss: 0.40961188077926636
setp: 4300, Loss: 0.3799770176410675
setp: 4400, Loss: 0.37127169966697693
setp: 4500, Loss: 0.3204174339771271
setp: 4600, Loss: 0.361798495054245
setp: 4700, Loss: 0.31771260499954224
setp: 4800, Loss: 0.3574800193309784
setp: 4900, Loss: 0.34781089425086975
training successfully ended.
validating...
acc: 0.9572368421052632
precision: 0.9235294117647059
recall: 1.0
F_score: 0.9602446483180428
validating...
acc: 0.8157894736842105
precision: 0.7375
recall: 0.8939393939393939
F_score: 0.8082191780821918
******fold 3******
[312, 296]
training...
setp: 0, Loss: 0.6951780915260315
setp: 100, Loss: 0.6227561235427856
setp: 200, Loss: 0.6200310587882996
setp: 300, Loss: 0.5973697900772095
setp: 400, Loss: 0.6005186438560486
setp: 500, Loss: 0.4787116050720215
setp: 600, Loss: 0.45564261078834534
setp: 700, Loss: 0.4038742780685425
setp: 800, Loss: 0.41359350085258484
setp: 900, Loss: 0.35805609822273254
setp: 1000, Loss: 0.3319641947746277
setp: 1100, Loss: 0.3302857577800751
setp: 1200, Loss: 0.31921878457069397
setp: 1300, Loss: 0.3211200535297394
setp: 1400, Loss: 0.3481498062610626
setp: 1500, Loss: 0.3296891748905182
setp: 1600, Loss: 0.31822890043258667
setp: 1700, Loss: 0.320608526468277
setp: 1800, Loss: 0.3200649321079254
setp: 1900, Loss: 0.32335641980171204
setp: 2000, Loss: 0.3206039071083069
setp: 2100, Loss: 0.3186034858226776
setp: 2200, Loss: 0.31711432337760925
setp: 2300, Loss: 0.3195369243621826
setp: 2400, Loss: 0.31932660937309265
setp: 2500, Loss: 0.32424092292785645
setp: 2600, Loss: 0.31805381178855896
setp: 2700, Loss: 0.3201470673084259
setp: 2800, Loss: 0.32015755772590637
setp: 2900, Loss: 0.3185639977455139
setp: 3000, Loss: 0.3187952935695648
setp: 3100, Loss: 0.3169580101966858
setp: 3200, Loss: 0.8129625916481018
setp: 3300, Loss: 0.6322546601295471
setp: 3400, Loss: 0.5346603989601135
setp: 3500, Loss: 0.5472400784492493
setp: 3600, Loss: 0.5429307222366333
setp: 3700, Loss: 0.48767274618148804
setp: 3800, Loss: 0.4798874855041504
setp: 3900, Loss: 0.40465110540390015
setp: 4000, Loss: 0.4012938141822815
setp: 4100, Loss: 0.36082783341407776
setp: 4200, Loss: 0.3545583188533783
setp: 4300, Loss: 0.32536643743515015
setp: 4400, Loss: 0.3337835967540741
setp: 4500, Loss: 0.32714390754699707
setp: 4600, Loss: 0.32299894094467163
setp: 4700, Loss: 0.32582443952560425
setp: 4800, Loss: 0.43951302766799927
setp: 4900, Loss: 0.32347261905670166
training successfully ended.
validating...
acc: 0.993421052631579
precision: 0.9873417721518988
recall: 1.0
F_score: 0.9936305732484078
validating...
acc: 0.9210526315789473
precision: 0.868421052631579
recall: 0.9705882352941176
F_score: 0.9166666666666667
******fold 4******
[289, 319]
training...
setp: 0, Loss: 0.7071983814239502
setp: 100, Loss: 0.6599036455154419
setp: 200, Loss: 0.616179883480072
setp: 300, Loss: 0.5770415663719177
setp: 400, Loss: 0.6845490336418152
setp: 500, Loss: 0.6302568912506104
setp: 600, Loss: 0.5541113615036011
setp: 700, Loss: 0.5297938585281372
setp: 800, Loss: 0.6132648587226868
setp: 900, Loss: 0.5548377633094788
setp: 1000, Loss: 0.5288361310958862
setp: 1100, Loss: 0.45489785075187683
setp: 1200, Loss: 0.398397833108902
setp: 1300, Loss: 0.48180025815963745
setp: 1400, Loss: 0.41116276383399963
setp: 1500, Loss: 0.3751753866672516
setp: 1600, Loss: 0.4298947751522064
setp: 1700, Loss: 0.38948938250541687
setp: 1800, Loss: 0.35549208521842957
setp: 1900, Loss: 0.4017057418823242
setp: 2000, Loss: 0.3718529939651489
setp: 2100, Loss: 0.3320519030094147
setp: 2200, Loss: 0.3599647283554077
setp: 2300, Loss: 0.32174164056777954
setp: 2400, Loss: 0.42551594972610474
setp: 2500, Loss: 0.3792842626571655
setp: 2600, Loss: 0.3296245336532593
setp: 2700, Loss: 0.39567288756370544
setp: 2800, Loss: 0.39704906940460205
setp: 2900, Loss: 0.3766903877258301
setp: 3000, Loss: 0.34660229086875916
setp: 3100, Loss: 0.31744250655174255
setp: 3200, Loss: 0.37927788496017456
setp: 3300, Loss: 0.3177381157875061
setp: 3400, Loss: 0.3173777461051941
setp: 3500, Loss: 0.3199669420719147
setp: 3600, Loss: 0.3211229741573334
setp: 3700, Loss: 0.3174208402633667
setp: 3800, Loss: 0.3216084837913513
setp: 3900, Loss: 0.32002371549606323
setp: 4000, Loss: 0.3177308738231659
setp: 4100, Loss: 0.4150298535823822
setp: 4200, Loss: 0.3241887390613556
setp: 4300, Loss: 0.31688249111175537
setp: 4400, Loss: 0.31794652342796326
setp: 4500, Loss: 0.31802260875701904
setp: 4600, Loss: 0.3170035779476166
setp: 4700, Loss: 0.3187157213687897
setp: 4800, Loss: 0.3761345446109772
setp: 4900, Loss: 0.3770328760147095
training successfully ended.
validating...
acc: 0.9802631578947368
precision: 0.9928825622775801
recall: 0.9653979238754326
F_score: 0.9789473684210526
validating...
acc: 0.8881578947368421
precision: 0.9404761904761905
recall: 0.8681318681318682
F_score: 0.9028571428571429
model saved.
avg_acc: 0.8802631578947369, avg_f_score: 0.8831194213570844
==========arousal==========
******fold 0******
[180, 428]
training...
setp: 0, Loss: 0.692976713180542
setp: 100, Loss: 0.5546336770057678
setp: 200, Loss: 0.4703858196735382
setp: 300, Loss: 0.3741663098335266
setp: 400, Loss: 0.35696470737457275
setp: 500, Loss: 0.3688620924949646
setp: 600, Loss: 0.3413121700286865
setp: 700, Loss: 0.32731401920318604
setp: 800, Loss: 0.32210803031921387
setp: 900, Loss: 0.34719347953796387
setp: 1000, Loss: 0.3255883753299713
setp: 1100, Loss: 0.32119324803352356
setp: 1200, Loss: 0.3264903426170349
setp: 1300, Loss: 0.31765761971473694
setp: 1400, Loss: 0.3276668190956116
setp: 1500, Loss: 0.3268311321735382
setp: 1600, Loss: 0.31727632880210876
setp: 1700, Loss: 0.31684410572052
setp: 1800, Loss: 0.3176240921020508
setp: 1900, Loss: 0.31791064143180847
setp: 2000, Loss: 0.31765714287757874
setp: 2100, Loss: 0.31820136308670044
setp: 2200, Loss: 0.31705811619758606
setp: 2300, Loss: 0.31891149282455444
setp: 2400, Loss: 0.31957659125328064
setp: 2500, Loss: 0.3164787292480469
setp: 2600, Loss: 0.31758031249046326
setp: 2700, Loss: 0.3199414610862732
setp: 2800, Loss: 0.6473311185836792
setp: 2900, Loss: 0.3248039484024048
setp: 3000, Loss: 0.32302939891815186
setp: 3100, Loss: 0.31684643030166626
setp: 3200, Loss: 0.31845954060554504
setp: 3300, Loss: 0.31743550300598145
setp: 3400, Loss: 0.3171008825302124
setp: 3500, Loss: 0.3179403245449066
setp: 3600, Loss: 0.4059574604034424
setp: 3700, Loss: 0.3181121051311493
setp: 3800, Loss: 0.31875115633010864
setp: 3900, Loss: 0.3167259097099304
setp: 4000, Loss: 0.31647467613220215
setp: 4100, Loss: 0.3161487281322479
setp: 4200, Loss: 0.31882521510124207
setp: 4300, Loss: 0.3171026408672333
setp: 4400, Loss: 0.31681719422340393
setp: 4500, Loss: 0.812680184841156
setp: 4600, Loss: 0.5998420119285583
setp: 4700, Loss: 0.38560405373573303
setp: 4800, Loss: 0.3994982838630676
setp: 4900, Loss: 0.35986074805259705
training successfully ended.
validating...
acc: 0.9813084112149533
precision: 0.9681818181818181
recall: 0.9953271028037384
F_score: 0.9815668202764977
validating...
acc: 0.9210526315789473
precision: 0.8
recall: 1.0
F_score: 0.888888888888889
******fold 1******
[176, 432]
training...
setp: 0, Loss: 0.6936380863189697
setp: 100, Loss: 0.5550406575202942
setp: 200, Loss: 0.46987372636795044
setp: 300, Loss: 0.43929678201675415
setp: 400, Loss: 0.45489880442619324
setp: 500, Loss: 0.4121140241622925
setp: 600, Loss: 0.3303857147693634
setp: 700, Loss: 0.3295621871948242
setp: 800, Loss: 0.32500016689300537
setp: 900, Loss: 0.3228217363357544
setp: 1000, Loss: 0.3220006227493286
setp: 1100, Loss: 0.34800052642822266
setp: 1200, Loss: 0.31949561834335327
setp: 1300, Loss: 0.31983399391174316
setp: 1400, Loss: 0.344917356967926
setp: 1500, Loss: 0.326494425535202
setp: 1600, Loss: 0.32843711972236633
setp: 1700, Loss: 0.3171355128288269
setp: 1800, Loss: 0.31774404644966125
setp: 1900, Loss: 0.317560613155365
setp: 2000, Loss: 0.3172672390937805
setp: 2100, Loss: 0.31783196330070496
setp: 2200, Loss: 0.31704673171043396
setp: 2300, Loss: 0.32492202520370483
setp: 2400, Loss: 0.33840811252593994
setp: 2500, Loss: 0.3480123281478882
setp: 2600, Loss: 0.31815919280052185
setp: 2700, Loss: 0.31656572222709656
setp: 2800, Loss: 0.31700217723846436
setp: 2900, Loss: 0.3193303048610687
setp: 3000, Loss: 0.31889286637306213
setp: 3100, Loss: 0.31913819909095764
setp: 3200, Loss: 0.3640105128288269
setp: 3300, Loss: 0.31656864285469055
setp: 3400, Loss: 0.3186522424221039
setp: 3500, Loss: 0.3162800073623657
setp: 3600, Loss: 0.3158220648765564
setp: 3700, Loss: 0.3177357614040375
setp: 3800, Loss: 0.31818240880966187
setp: 3900, Loss: 0.31796666979789734
setp: 4000, Loss: 0.3168238699436188
setp: 4100, Loss: 0.31649476289749146
setp: 4200, Loss: 0.31712836027145386
setp: 4300, Loss: 0.3161259591579437
setp: 4400, Loss: 0.31628942489624023
setp: 4500, Loss: 0.3271043598651886
setp: 4600, Loss: 0.3608921766281128
setp: 4700, Loss: 0.3161775469779968
setp: 4800, Loss: 0.316976934671402
setp: 4900, Loss: 0.31616565585136414
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9473684210526315
precision: 0.94
recall: 0.9038461538461539
F_score: 0.9215686274509804
******fold 2******
[185, 423]
training...
setp: 0, Loss: 0.7940518260002136
setp: 100, Loss: 0.6463121175765991
setp: 200, Loss: 0.5720633268356323
setp: 300, Loss: 0.5141653418540955
setp: 400, Loss: 0.41772952675819397
setp: 500, Loss: 0.40241602063179016
setp: 600, Loss: 0.35427892208099365
setp: 700, Loss: 0.3665415942668915
setp: 800, Loss: 0.3328421413898468
setp: 900, Loss: 0.3289407789707184
setp: 1000, Loss: 0.39535024762153625
setp: 1100, Loss: 0.3472472131252289
setp: 1200, Loss: 0.33902567625045776
setp: 1300, Loss: 0.33759763836860657
setp: 1400, Loss: 0.3210752308368683
setp: 1500, Loss: 0.3213047981262207
setp: 1600, Loss: 0.31999659538269043
setp: 1700, Loss: 0.319594144821167
setp: 1800, Loss: 0.3206939995288849
setp: 1900, Loss: 0.3205365240573883
setp: 2000, Loss: 0.3227540850639343
setp: 2100, Loss: 0.32051247358322144
setp: 2200, Loss: 0.3497028946876526
setp: 2300, Loss: 0.3198243975639343
setp: 2400, Loss: 0.3210412561893463
setp: 2500, Loss: 0.319793701171875
setp: 2600, Loss: 0.35128727555274963
setp: 2700, Loss: 0.3202047646045685
setp: 2800, Loss: 0.3562636077404022
setp: 2900, Loss: 0.3356456458568573
setp: 3000, Loss: 0.3196062743663788
setp: 3100, Loss: 0.3272101581096649
setp: 3200, Loss: 0.3202971816062927
setp: 3300, Loss: 0.31981122493743896
setp: 3400, Loss: 0.31921419501304626
setp: 3500, Loss: 0.31922784447669983
setp: 3600, Loss: 0.3204183578491211
setp: 3700, Loss: 0.3196771740913391
setp: 3800, Loss: 0.3201587200164795
setp: 3900, Loss: 0.3210175335407257
setp: 4000, Loss: 0.31973907351493835
setp: 4100, Loss: 0.31984326243400574
setp: 4200, Loss: 0.3193570375442505
setp: 4300, Loss: 0.32139089703559875
setp: 4400, Loss: 0.318795382976532
setp: 4500, Loss: 0.3198539912700653
setp: 4600, Loss: 0.31962651014328003
setp: 4700, Loss: 0.31991371512413025
setp: 4800, Loss: 0.3196141719818115
setp: 4900, Loss: 0.35256198048591614
training successfully ended.
validating...
acc: 0.9917763157894737
precision: 0.9838709677419355
recall: 0.9891891891891892
F_score: 0.9865229110512129
validating...
acc: 0.8947368421052632
precision: 0.7454545454545455
recall: 0.9534883720930233
F_score: 0.8367346938775511
******fold 3******
[182, 426]
training...
setp: 0, Loss: 0.6979612708091736
setp: 100, Loss: 0.6537895798683167
setp: 200, Loss: 0.6401544809341431
setp: 300, Loss: 0.4642883837223053
setp: 400, Loss: 0.3936832547187805
setp: 500, Loss: 0.3896203935146332
setp: 600, Loss: 0.42470812797546387
setp: 700, Loss: 0.3753928244113922
setp: 800, Loss: 0.4428061842918396
setp: 900, Loss: 0.3212842047214508
setp: 1000, Loss: 0.3252923786640167
setp: 1100, Loss: 0.32304060459136963
setp: 1200, Loss: 0.3226718604564667
setp: 1300, Loss: 0.3342193365097046
setp: 1400, Loss: 0.31914713978767395
setp: 1500, Loss: 0.3173668086528778
setp: 1600, Loss: 0.31790298223495483
setp: 1700, Loss: 0.3177754580974579
setp: 1800, Loss: 0.3201698362827301
setp: 1900, Loss: 0.31817540526390076
setp: 2000, Loss: 0.3180828392505646
setp: 2100, Loss: 0.3189198672771454
setp: 2200, Loss: 0.3171434998512268
setp: 2300, Loss: 0.3180832862854004
setp: 2400, Loss: 0.31842494010925293
setp: 2500, Loss: 0.31892719864845276
setp: 2600, Loss: 0.31790128350257874
setp: 2700, Loss: 0.5052586793899536
setp: 2800, Loss: 0.3308526575565338
setp: 2900, Loss: 0.3188404440879822
setp: 3000, Loss: 0.3174867033958435
setp: 3100, Loss: 0.3175121545791626
setp: 3200, Loss: 0.3188234269618988
setp: 3300, Loss: 0.3173980712890625
setp: 3400, Loss: 0.31721898913383484
setp: 3500, Loss: 0.3366197347640991
setp: 3600, Loss: 0.3186416029930115
setp: 3700, Loss: 0.31696227192878723
setp: 3800, Loss: 0.31683212518692017
setp: 3900, Loss: 0.31698086857795715
setp: 4000, Loss: 0.3194127380847931
setp: 4100, Loss: 0.31669968366622925
setp: 4200, Loss: 0.3207460343837738
setp: 4300, Loss: 0.31757453083992004
setp: 4400, Loss: 0.32062050700187683
setp: 4500, Loss: 0.3270469605922699
setp: 4600, Loss: 0.316372811794281
setp: 4700, Loss: 0.3169225752353668
setp: 4800, Loss: 0.31703221797943115
setp: 4900, Loss: 0.31738242506980896
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9802631578947368
precision: 0.9777777777777777
recall: 0.9565217391304348
F_score: 0.967032967032967
******fold 4******
[189, 419]
training...
setp: 0, Loss: 0.6654645204544067
setp: 100, Loss: 0.6459073424339294
setp: 200, Loss: 0.5943736433982849
setp: 300, Loss: 0.49422961473464966
setp: 400, Loss: 0.4461286962032318
setp: 500, Loss: 0.412251740694046
setp: 600, Loss: 0.4239237606525421
setp: 700, Loss: 0.43901950120925903
setp: 800, Loss: 0.3802313208580017
setp: 900, Loss: 0.3754042983055115
setp: 1000, Loss: 0.3294190466403961
setp: 1100, Loss: 0.340696781873703
setp: 1200, Loss: 0.32439419627189636
setp: 1300, Loss: 0.32280856370925903
setp: 1400, Loss: 0.3345637917518616
setp: 1500, Loss: 0.3242323398590088
setp: 1600, Loss: 0.319898784160614
setp: 1700, Loss: 0.32154664397239685
setp: 1800, Loss: 0.31971845030784607
setp: 1900, Loss: 0.3422149121761322
setp: 2000, Loss: 0.3182840943336487
setp: 2100, Loss: 0.319974809885025
setp: 2200, Loss: 0.31938573718070984
setp: 2300, Loss: 0.3200448155403137
setp: 2400, Loss: 0.3187713921070099
setp: 2500, Loss: 0.31936201453208923
setp: 2600, Loss: 0.3191474676132202
setp: 2700, Loss: 0.3196257948875427
setp: 2800, Loss: 0.3196668326854706
setp: 2900, Loss: 0.3198729157447815
setp: 3000, Loss: 0.3223121762275696
setp: 3100, Loss: 0.3345847427845001
setp: 3200, Loss: 0.3505413830280304
setp: 3300, Loss: 0.31850114464759827
setp: 3400, Loss: 0.31898975372314453
setp: 3500, Loss: 0.320551335811615
setp: 3600, Loss: 0.3195580840110779
setp: 3700, Loss: 0.3190132677555084
setp: 3800, Loss: 0.3196784555912018
setp: 3900, Loss: 0.31934642791748047
setp: 4000, Loss: 0.3199704885482788
setp: 4100, Loss: 0.319799542427063
setp: 4200, Loss: 0.3197250962257385
setp: 4300, Loss: 0.31837064027786255
setp: 4400, Loss: 0.3198876678943634
setp: 4500, Loss: 0.32041752338409424
setp: 4600, Loss: 0.335263729095459
setp: 4700, Loss: 0.3170945346355438
setp: 4800, Loss: 0.31763410568237305
setp: 4900, Loss: 0.3189242482185364
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.8717948717948718
recall: 0.8717948717948718
F_score: 0.8717948717948718
model saved.
avg_acc: 0.9355263157894737, avg_f_score: 0.897204009809052
D:\学习\毕设\project\train.py:233: MatplotlibDeprecationWarning: Support for FigureCanvases without a required_interactive_framework attribute was deprecated in Matplotlib 3.6 and will be removed two minor releases later.
  fig1 = plt.figure(figsize=(10, 5))
D:\学习\毕设\project\train.py:246: MatplotlibDeprecationWarning: Support for FigureCanvases without a required_interactive_framework attribute was deprecated in Matplotlib 3.6 and will be removed two minor releases later.
  fig2 = plt.figure(figsize=(10, 5))
---------RESULT---------
valence
  acc: [0.93157895 0.88421053 0.95131579 0.9        0.92368421 0.95526316
 0.89342105 0.83552632 0.81447368 0.93157895 0.80921053 0.90263158
 0.88552632 0.91052632 0.93421053 0.91973684 0.89473684 0.87763158
 0.925      0.91184211 0.75131579 0.94078947 0.94078947 0.90131579
 0.93815789 0.87894737 0.92894737 0.93157895 0.95263158 0.94210526
 0.84473684 0.88026316]
  average acc: 0.9007401315789474
  f-score: [0.93512076 0.84081951 0.94371373 0.91987446 0.90701049 0.90965094
 0.80025129 0.71307256 0.69636137 0.92976884 0.78653074 0.88584349
 0.89439237 0.9069842  0.93243576 0.93732946 0.88145589 0.81800387
 0.91417265 0.89224197 0.54251381 0.93963087 0.89931039 0.90374753
 0.93467898 0.78842955 0.85149855 0.90259008 0.94426043 0.90887668
 0.6865293  0.88311942]
  average f-score: 0.863444373482782
arousal
  acc: [0.91578947 0.90131579 0.96447368 0.92631579 0.83289474 0.88157895
 0.89078947 0.88421053 0.91184211 0.91184211 0.86315789 0.92105263
 0.95131579 0.91184211 0.91052632 0.88947368 0.85789474 0.95263158
 0.91973684 0.95       0.97236842 0.93026316 0.91842105 0.96973684
 0.91578947 0.88026316 0.85789474 0.91842105 0.94473684 0.91710526
 0.80526316 0.93552632]
  average acc: 0.909827302631579
  f-score: [0.89533084 0.8753039  0.977664   0.93921037 0.86562732 0.89670313
 0.8497698  0.85821872 0.88078427 0.90361759 0.88581914 0.77697657
 0.83902148 0.86394839 0.91414794 0.89285319 0.79572986 0.93653459
 0.88245299 0.8839378  0.92742329 0.90376527 0.93918273 0.91408823
 0.85900807 0.8932705  0.74983237 0.91621908 0.91226041 0.91850539
 0.84042916 0.89720401]
  average f-score: 0.8839012621323373

Process finished with exit code 0
