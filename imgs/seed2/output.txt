D:\Anaconda\Anaconda\envs\EEG\python.exe D:/学习/毕设/project/train.py
-------------subject: 1-------------
==========valence==========
******fold 0******
[311, 297]
training...
setp: 0, Loss: 0.7134145498275757
setp: 100, Loss: 0.6434798240661621
setp: 200, Loss: 0.4243808090686798
setp: 300, Loss: 0.42284056544303894
setp: 400, Loss: 0.3453042805194855
setp: 500, Loss: 0.3467003107070923
setp: 600, Loss: 0.3215711712837219
setp: 700, Loss: 0.32147079706192017
setp: 800, Loss: 0.32065606117248535
setp: 900, Loss: 0.3194734752178192
setp: 1000, Loss: 0.3203917443752289
setp: 1100, Loss: 0.3198551833629608
setp: 1200, Loss: 0.31868448853492737
setp: 1300, Loss: 0.31939032673835754
setp: 1400, Loss: 0.31737759709358215
setp: 1500, Loss: 0.316463440656662
setp: 1600, Loss: 0.3190625011920929
setp: 1700, Loss: 0.3185165822505951
setp: 1800, Loss: 0.3193877935409546
setp: 1900, Loss: 0.31885597109794617
setp: 2000, Loss: 0.42129814624786377
setp: 2100, Loss: 0.3430355489253998
setp: 2200, Loss: 0.3275904655456543
setp: 2300, Loss: 0.3901618421077728
setp: 2400, Loss: 0.3306029140949249
setp: 2500, Loss: 0.3222982883453369
setp: 2600, Loss: 0.3224450349807739
setp: 2700, Loss: 0.3219543695449829
setp: 2800, Loss: 0.32111895084381104
setp: 2900, Loss: 0.3213759958744049
setp: 3000, Loss: 0.3220556974411011
setp: 3100, Loss: 0.3204564154148102
setp: 3200, Loss: 0.33733344078063965
setp: 3300, Loss: 0.3184899091720581
setp: 3400, Loss: 0.31799623370170593
setp: 3500, Loss: 0.3187648355960846
setp: 3600, Loss: 0.31864985823631287
setp: 3700, Loss: 0.3188135027885437
setp: 3800, Loss: 0.3186028301715851
setp: 3900, Loss: 0.31821951270103455
setp: 4000, Loss: 0.31798431277275085
setp: 4100, Loss: 0.317537784576416
setp: 4200, Loss: 0.3167016804218292
setp: 4300, Loss: 0.31870174407958984
setp: 4400, Loss: 0.3168255388736725
setp: 4500, Loss: 0.31728604435920715
setp: 4600, Loss: 0.317157506942749
setp: 4700, Loss: 0.31752148270606995
setp: 4800, Loss: 0.3172438442707062
setp: 4900, Loss: 0.31741413474082947
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.9230769230769231
recall: 0.9545454545454546
F_score: 0.9385474860335197
******fold 1******
[318, 290]
training...
setp: 0, Loss: 0.7971360683441162
setp: 100, Loss: 0.6902863383293152
setp: 200, Loss: 0.5860100984573364
setp: 300, Loss: 0.4157315790653229
setp: 400, Loss: 0.3547309339046478
setp: 500, Loss: 0.32543832063674927
setp: 600, Loss: 0.32538366317749023
setp: 700, Loss: 0.3198414444923401
setp: 800, Loss: 0.3232954144477844
setp: 900, Loss: 0.3620588481426239
setp: 1000, Loss: 0.3198561668395996
setp: 1100, Loss: 0.32130053639411926
setp: 1200, Loss: 0.3187580704689026
setp: 1300, Loss: 0.3190668523311615
setp: 1400, Loss: 0.3191879689693451
setp: 1500, Loss: 0.31873783469200134
setp: 1600, Loss: 0.31863805651664734
setp: 1700, Loss: 0.3214338719844818
setp: 1800, Loss: 0.3310752213001251
setp: 1900, Loss: 0.3208231031894684
setp: 2000, Loss: 0.31807857751846313
setp: 2100, Loss: 0.3176790177822113
setp: 2200, Loss: 0.3188762068748474
setp: 2300, Loss: 0.3181743025779724
setp: 2400, Loss: 0.31851738691329956
setp: 2500, Loss: 0.3190044164657593
setp: 2600, Loss: 0.3177720904350281
setp: 2700, Loss: 0.31877419352531433
setp: 2800, Loss: 0.31919020414352417
setp: 2900, Loss: 0.31846722960472107
setp: 3000, Loss: 0.31988024711608887
setp: 3100, Loss: 0.31901487708091736
setp: 3200, Loss: 0.31890562176704407
setp: 3300, Loss: 0.31941869854927063
setp: 3400, Loss: 0.3190784454345703
setp: 3500, Loss: 0.4733272194862366
setp: 3600, Loss: 0.3885231018066406
setp: 3700, Loss: 0.33457982540130615
setp: 3800, Loss: 0.36281678080558777
setp: 3900, Loss: 0.3279326558113098
setp: 4000, Loss: 0.32771605253219604
setp: 4100, Loss: 0.3379269242286682
setp: 4200, Loss: 0.3250104784965515
setp: 4300, Loss: 0.3253174424171448
setp: 4400, Loss: 0.326587975025177
setp: 4500, Loss: 0.3250614404678345
setp: 4600, Loss: 0.32586804032325745
setp: 4700, Loss: 0.32621267437934875
setp: 4800, Loss: 0.3699857294559479
setp: 4900, Loss: 0.3559572100639343
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9473684210526315
precision: 0.9506172839506173
recall: 0.9506172839506173
F_score: 0.9506172839506173
******fold 2******
[315, 293]
training...
setp: 0, Loss: 0.7251865267753601
setp: 100, Loss: 0.6503468751907349
setp: 200, Loss: 0.5078980922698975
setp: 300, Loss: 0.4463506042957306
setp: 400, Loss: 0.467319518327713
setp: 500, Loss: 0.5601660013198853
setp: 600, Loss: 0.4657413065433502
setp: 700, Loss: 0.49920764565467834
setp: 800, Loss: 0.3564773201942444
setp: 900, Loss: 0.3516513705253601
setp: 1000, Loss: 0.3186660706996918
setp: 1100, Loss: 0.3223639130592346
setp: 1200, Loss: 0.32524386048316956
setp: 1300, Loss: 0.31969010829925537
setp: 1400, Loss: 0.3184359669685364
setp: 1500, Loss: 0.31991299986839294
setp: 1600, Loss: 0.3185551166534424
setp: 1700, Loss: 0.3219863772392273
setp: 1800, Loss: 0.3325054347515106
setp: 1900, Loss: 0.33821606636047363
setp: 2000, Loss: 0.3218643069267273
setp: 2100, Loss: 0.32038766145706177
setp: 2200, Loss: 0.31764814257621765
setp: 2300, Loss: 0.31629815697669983
setp: 2400, Loss: 0.31558921933174133
setp: 2500, Loss: 0.31640705466270447
setp: 2600, Loss: 0.3159031569957733
setp: 2700, Loss: 0.31554996967315674
setp: 2800, Loss: 0.31606340408325195
setp: 2900, Loss: 0.31585609912872314
setp: 3000, Loss: 0.31773772835731506
setp: 3100, Loss: 0.3170076310634613
setp: 3200, Loss: 0.3173430562019348
setp: 3300, Loss: 0.31712260842323303
setp: 3400, Loss: 0.31563901901245117
setp: 3500, Loss: 0.31886008381843567
setp: 3600, Loss: 0.317928671836853
setp: 3700, Loss: 0.3176683485507965
setp: 3800, Loss: 0.31658312678337097
setp: 3900, Loss: 0.3168349862098694
setp: 4000, Loss: 0.31812605261802673
setp: 4100, Loss: 0.3169091045856476
setp: 4200, Loss: 0.4197485148906708
setp: 4300, Loss: 0.32373514771461487
setp: 4400, Loss: 0.3206779360771179
setp: 4500, Loss: 0.3188781440258026
setp: 4600, Loss: 0.3173465430736542
setp: 4700, Loss: 0.31685870885849
setp: 4800, Loss: 0.3167826235294342
setp: 4900, Loss: 0.32071352005004883
training successfully ended.
validating...
acc: 0.9884868421052632
precision: 0.9967741935483871
recall: 0.9809523809523809
F_score: 0.9888000000000001
validating...
acc: 0.9342105263157895
precision: 0.9512195121951219
recall: 0.9285714285714286
F_score: 0.9397590361445782
******fold 3******
[319, 289]
training...
setp: 0, Loss: 0.6916234493255615
setp: 100, Loss: 0.6949964165687561
setp: 200, Loss: 0.6942154765129089
setp: 300, Loss: 0.6915671229362488
setp: 400, Loss: 0.6866543292999268
setp: 500, Loss: 0.7049584984779358
setp: 600, Loss: 0.6888212561607361
setp: 700, Loss: 0.6960505247116089
setp: 800, Loss: 0.6863186359405518
setp: 900, Loss: 0.6912357211112976
setp: 1000, Loss: 0.6939184665679932
setp: 1100, Loss: 0.6871077418327332
setp: 1200, Loss: 0.6885654926300049
setp: 1300, Loss: 0.697795033454895
setp: 1400, Loss: 0.6892442107200623
setp: 1500, Loss: 0.6892741918563843
setp: 1600, Loss: 0.694362461566925
setp: 1700, Loss: 0.6974958181381226
setp: 1800, Loss: 0.6872556209564209
setp: 1900, Loss: 0.6962069272994995
setp: 2000, Loss: 0.6945676803588867
setp: 2100, Loss: 0.694181501865387
setp: 2200, Loss: 0.6915921568870544
setp: 2300, Loss: 0.6867607235908508
setp: 2400, Loss: 0.7052971124649048
setp: 2500, Loss: 0.6888716816902161
setp: 2600, Loss: 0.6958816647529602
setp: 2700, Loss: 0.6864383816719055
setp: 2800, Loss: 0.691223680973053
setp: 2900, Loss: 0.6938626170158386
setp: 3000, Loss: 0.687442421913147
setp: 3100, Loss: 0.688593864440918
setp: 3200, Loss: 0.6978838443756104
setp: 3300, Loss: 0.6894129514694214
setp: 3400, Loss: 0.6894479393959045
setp: 3500, Loss: 0.6943809390068054
setp: 3600, Loss: 0.6974918842315674
setp: 3700, Loss: 0.687516987323761
setp: 3800, Loss: 0.6960699558258057
setp: 3900, Loss: 0.6946074962615967
setp: 4000, Loss: 0.6941583752632141
setp: 4100, Loss: 0.69162917137146
setp: 4200, Loss: 0.686907947063446
setp: 4300, Loss: 0.705497682094574
setp: 4400, Loss: 0.6889325976371765
setp: 4500, Loss: 0.6957769989967346
setp: 4600, Loss: 0.6864632368087769
setp: 4700, Loss: 0.6912209987640381
setp: 4800, Loss: 0.6938355565071106
setp: 4900, Loss: 0.6875499486923218
training successfully ended.
validating...
acc: 0.524671052631579
precision: 0.524671052631579
recall: 1.0
F_score: 0.6882416396979504
validating...
acc: 0.5263157894736842
precision: 0.5263157894736842
recall: 1.0
F_score: 0.6896551724137931
******fold 4******
[333, 275]
training...
setp: 0, Loss: 0.6853799223899841
setp: 100, Loss: 0.6293432116508484
setp: 200, Loss: 0.4378529489040375
setp: 300, Loss: 0.454744428396225
setp: 400, Loss: 0.36709433794021606
setp: 500, Loss: 0.38620203733444214
setp: 600, Loss: 0.3498622179031372
setp: 700, Loss: 0.3443795144557953
setp: 800, Loss: 0.320666640996933
setp: 900, Loss: 0.38588595390319824
setp: 1000, Loss: 0.34890955686569214
setp: 1100, Loss: 0.31769371032714844
setp: 1200, Loss: 0.3494483232498169
setp: 1300, Loss: 0.3179772198200226
setp: 1400, Loss: 0.34850063920021057
setp: 1500, Loss: 0.3180290460586548
setp: 1600, Loss: 0.37908387184143066
setp: 1700, Loss: 0.3166116178035736
setp: 1800, Loss: 0.3493175208568573
setp: 1900, Loss: 0.34817251563072205
setp: 2000, Loss: 0.35007303953170776
setp: 2100, Loss: 0.31985682249069214
setp: 2200, Loss: 0.3664853274822235
setp: 2300, Loss: 0.3175254166126251
setp: 2400, Loss: 0.34978124499320984
setp: 2500, Loss: 0.3592561185359955
setp: 2600, Loss: 0.3224726617336273
setp: 2700, Loss: 0.3162243962287903
setp: 2800, Loss: 0.3166414797306061
setp: 2900, Loss: 0.31661438941955566
setp: 3000, Loss: 0.31599751114845276
setp: 3100, Loss: 0.31823503971099854
setp: 3200, Loss: 0.3185257911682129
setp: 3300, Loss: 0.31725117564201355
setp: 3400, Loss: 0.31622546911239624
setp: 3500, Loss: 0.3188587725162506
setp: 3600, Loss: 0.315956711769104
setp: 3700, Loss: 0.31768590211868286
setp: 3800, Loss: 0.3161075711250305
setp: 3900, Loss: 0.31845158338546753
setp: 4000, Loss: 0.3165878355503082
setp: 4100, Loss: 0.31653696298599243
setp: 4200, Loss: 0.7887297868728638
setp: 4300, Loss: 0.5021651983261108
setp: 4400, Loss: 0.3697163164615631
setp: 4500, Loss: 0.4474947452545166
setp: 4600, Loss: 0.3548259139060974
setp: 4700, Loss: 0.3642268776893616
setp: 4800, Loss: 0.35443753004074097
setp: 4900, Loss: 0.32014328241348267
training successfully ended.
validating...
acc: 0.9819078947368421
precision: 0.9680232558139535
recall: 1.0
F_score: 0.983751846381093
validating...
acc: 0.9078947368421053
precision: 0.8421052631578947
recall: 0.9696969696969697
F_score: 0.9014084507042254
model saved.
avg_acc: 0.8486842105263157, avg_f_score: 0.8839974858493468
==========arousal==========
******fold 0******
[240, 368]
training...
setp: 0, Loss: 0.6915579438209534
setp: 100, Loss: 0.6287139654159546
setp: 200, Loss: 0.42935073375701904
setp: 300, Loss: 0.4029764235019684
setp: 400, Loss: 0.3564935326576233
setp: 500, Loss: 0.34243395924568176
setp: 600, Loss: 0.37466180324554443
setp: 700, Loss: 0.40417611598968506
setp: 800, Loss: 0.31821900606155396
setp: 900, Loss: 0.3183925449848175
setp: 1000, Loss: 0.3199659585952759
setp: 1100, Loss: 0.3183192312717438
setp: 1200, Loss: 0.31824222207069397
setp: 1300, Loss: 0.31628182530403137
setp: 1400, Loss: 0.31753015518188477
setp: 1500, Loss: 0.3196655511856079
setp: 1600, Loss: 0.3939838707447052
setp: 1700, Loss: 0.3246609568595886
setp: 1800, Loss: 0.3221721947193146
setp: 1900, Loss: 0.31969186663627625
setp: 2000, Loss: 0.3194989860057831
setp: 2100, Loss: 0.3174745738506317
setp: 2200, Loss: 0.31855350732803345
setp: 2300, Loss: 0.3186582028865814
setp: 2400, Loss: 0.3191112279891968
setp: 2500, Loss: 0.31981080770492554
setp: 2600, Loss: 0.31855860352516174
setp: 2700, Loss: 0.3185635507106781
setp: 2800, Loss: 0.31806474924087524
setp: 2900, Loss: 0.31829553842544556
setp: 3000, Loss: 0.3597166836261749
setp: 3100, Loss: 0.32054439187049866
setp: 3200, Loss: 0.3173450827598572
setp: 3300, Loss: 0.3176477551460266
setp: 3400, Loss: 0.31856459379196167
setp: 3500, Loss: 0.3178642690181732
setp: 3600, Loss: 0.3188673257827759
setp: 3700, Loss: 0.38011452555656433
setp: 3800, Loss: 0.3185628652572632
setp: 3900, Loss: 0.3169521987438202
setp: 4000, Loss: 0.31608688831329346
setp: 4100, Loss: 0.3167285621166229
setp: 4200, Loss: 0.3171309232711792
setp: 4300, Loss: 0.3179859220981598
setp: 4400, Loss: 0.31852149963378906
setp: 4500, Loss: 0.43850791454315186
setp: 4600, Loss: 0.3191395699977875
setp: 4700, Loss: 0.31683939695358276
setp: 4800, Loss: 0.31812629103660583
setp: 4900, Loss: 0.3165736496448517
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9671052631578947
precision: 0.9402985074626866
recall: 0.984375
F_score: 0.9618320610687023
******fold 1******
[245, 363]
training...
setp: 0, Loss: 0.6438915729522705
setp: 100, Loss: 0.6640884280204773
setp: 200, Loss: 0.49596819281578064
setp: 300, Loss: 0.4026183784008026
setp: 400, Loss: 0.3665929436683655
setp: 500, Loss: 0.32942336797714233
setp: 600, Loss: 0.33195555210113525
setp: 700, Loss: 0.35902997851371765
setp: 800, Loss: 0.33191514015197754
setp: 900, Loss: 0.4388444423675537
setp: 1000, Loss: 0.3246936798095703
setp: 1100, Loss: 0.3179014027118683
setp: 1200, Loss: 0.3188709318637848
setp: 1300, Loss: 0.3171719014644623
setp: 1400, Loss: 0.3171616196632385
setp: 1500, Loss: 0.3173028528690338
setp: 1600, Loss: 0.3185122609138489
setp: 1700, Loss: 0.3172835111618042
setp: 1800, Loss: 0.317699134349823
setp: 1900, Loss: 0.3184284269809723
setp: 2000, Loss: 0.31668880581855774
setp: 2100, Loss: 0.31725430488586426
setp: 2200, Loss: 0.31722840666770935
setp: 2300, Loss: 0.4846757650375366
setp: 2400, Loss: 0.340953528881073
setp: 2500, Loss: 0.33558040857315063
setp: 2600, Loss: 0.32524216175079346
setp: 2700, Loss: 0.3229265511035919
setp: 2800, Loss: 0.32094743847846985
setp: 2900, Loss: 0.3219224214553833
setp: 3000, Loss: 0.32205575704574585
setp: 3100, Loss: 0.3203316330909729
setp: 3200, Loss: 0.3183295726776123
setp: 3300, Loss: 0.31883323192596436
setp: 3400, Loss: 0.31806066632270813
setp: 3500, Loss: 0.6209385395050049
setp: 3600, Loss: 0.3756174147129059
setp: 3700, Loss: 0.333440899848938
setp: 3800, Loss: 0.32646721601486206
setp: 3900, Loss: 0.3201708197593689
setp: 4000, Loss: 0.3209339678287506
setp: 4100, Loss: 0.3191532492637634
setp: 4200, Loss: 0.32036757469177246
setp: 4300, Loss: 0.3250565826892853
setp: 4400, Loss: 0.31884342432022095
setp: 4500, Loss: 0.3198011815547943
setp: 4600, Loss: 0.3212890923023224
setp: 4700, Loss: 0.3191784620285034
setp: 4800, Loss: 0.3189052939414978
setp: 4900, Loss: 0.3189487159252167
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9539473684210527
precision: 1.0
recall: 0.8813559322033898
F_score: 0.936936936936937
******fold 2******
[247, 361]
training...
setp: 0, Loss: 0.6679360866546631
setp: 100, Loss: 0.6794260740280151
setp: 200, Loss: 0.4708583354949951
setp: 300, Loss: 0.3878173530101776
setp: 400, Loss: 0.347526878118515
setp: 500, Loss: 0.3523377478122711
setp: 600, Loss: 0.3309050500392914
setp: 700, Loss: 0.35034364461898804
setp: 800, Loss: 0.31992894411087036
setp: 900, Loss: 0.3213515281677246
setp: 1000, Loss: 0.3209347426891327
setp: 1100, Loss: 0.3326542377471924
setp: 1200, Loss: 0.356354683637619
setp: 1300, Loss: 0.3181895911693573
setp: 1400, Loss: 0.31767985224723816
setp: 1500, Loss: 0.3173084557056427
setp: 1600, Loss: 0.3173801600933075
setp: 1700, Loss: 0.3174839913845062
setp: 1800, Loss: 0.317979097366333
setp: 1900, Loss: 0.3178091049194336
setp: 2000, Loss: 0.31663814187049866
setp: 2100, Loss: 0.31632986664772034
setp: 2200, Loss: 0.318318635225296
setp: 2300, Loss: 0.3175422251224518
setp: 2400, Loss: 0.3172284960746765
setp: 2500, Loss: 0.6297598481178284
setp: 2600, Loss: 0.3734186887741089
setp: 2700, Loss: 0.3379397392272949
setp: 2800, Loss: 0.3251258134841919
setp: 2900, Loss: 0.35689064860343933
setp: 3000, Loss: 0.32121458649635315
setp: 3100, Loss: 0.32142767310142517
setp: 3200, Loss: 0.32040858268737793
setp: 3300, Loss: 0.32146939635276794
setp: 3400, Loss: 0.31904512643814087
setp: 3500, Loss: 0.31940725445747375
setp: 3600, Loss: 0.32062476873397827
setp: 3700, Loss: 0.6412404775619507
setp: 3800, Loss: 0.484081506729126
setp: 3900, Loss: 0.36055466532707214
setp: 4000, Loss: 0.3351556658744812
setp: 4100, Loss: 0.35757240653038025
setp: 4200, Loss: 0.3294437825679779
setp: 4300, Loss: 0.34378790855407715
setp: 4400, Loss: 0.35220906138420105
setp: 4500, Loss: 0.3510974049568176
setp: 4600, Loss: 0.319497287273407
setp: 4700, Loss: 0.3371272683143616
setp: 4800, Loss: 0.31900137662887573
setp: 4900, Loss: 0.3186340034008026
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 1.0
recall: 0.9878542510121457
F_score: 0.9938900203665988
validating...
acc: 0.9013157894736842
precision: 0.85
recall: 0.8947368421052632
F_score: 0.8717948717948718
******fold 3******
[252, 356]
training...
setp: 0, Loss: 0.714778482913971
setp: 100, Loss: 0.673740565776825
setp: 200, Loss: 0.5089998245239258
setp: 300, Loss: 0.37974321842193604
setp: 400, Loss: 0.3492485582828522
setp: 500, Loss: 0.32652342319488525
setp: 600, Loss: 0.32750096917152405
setp: 700, Loss: 0.3217935562133789
setp: 800, Loss: 0.3212827146053314
setp: 900, Loss: 0.3191659450531006
setp: 1000, Loss: 0.331424355506897
setp: 1100, Loss: 0.32546475529670715
setp: 1200, Loss: 0.31881213188171387
setp: 1300, Loss: 0.3169326186180115
setp: 1400, Loss: 0.31758150458335876
setp: 1500, Loss: 0.3179633319377899
setp: 1600, Loss: 0.3174803853034973
setp: 1700, Loss: 0.3181307911872864
setp: 1800, Loss: 0.31789860129356384
setp: 1900, Loss: 0.3179800808429718
setp: 2000, Loss: 0.31780436635017395
setp: 2100, Loss: 0.3180987238883972
setp: 2200, Loss: 0.3180004060268402
setp: 2300, Loss: 0.3320032060146332
setp: 2400, Loss: 0.31789055466651917
setp: 2500, Loss: 0.31780120730400085
setp: 2600, Loss: 0.3185896873474121
setp: 2700, Loss: 0.3180229067802429
setp: 2800, Loss: 0.3174046277999878
setp: 2900, Loss: 0.3188462555408478
setp: 3000, Loss: 0.6067541241645813
setp: 3100, Loss: 0.414595365524292
setp: 3200, Loss: 0.3550114929676056
setp: 3300, Loss: 0.34472742676734924
setp: 3400, Loss: 0.3362189829349518
setp: 3500, Loss: 0.3345261514186859
setp: 3600, Loss: 0.333474725484848
setp: 3700, Loss: 0.35769373178482056
setp: 3800, Loss: 0.3828203082084656
setp: 3900, Loss: 0.3241773545742035
setp: 4000, Loss: 0.32189881801605225
setp: 4100, Loss: 0.3256382942199707
setp: 4200, Loss: 0.3239215612411499
setp: 4300, Loss: 0.3247462809085846
setp: 4400, Loss: 0.3546561002731323
setp: 4500, Loss: 0.32122814655303955
setp: 4600, Loss: 0.32033610343933105
setp: 4700, Loss: 0.320445716381073
setp: 4800, Loss: 0.32187867164611816
setp: 4900, Loss: 0.32001736760139465
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.996031746031746
F_score: 0.9980119284294234
validating...
acc: 0.9276315789473685
precision: 0.9361702127659575
recall: 0.8461538461538461
F_score: 0.8888888888888888
******fold 4******
[232, 376]
training...
setp: 0, Loss: 0.7092388868331909
setp: 100, Loss: 0.6966351270675659
setp: 200, Loss: 0.6440784931182861
setp: 300, Loss: 0.6272628307342529
setp: 400, Loss: 0.4915913939476013
setp: 500, Loss: 0.412993460893631
setp: 600, Loss: 0.39487510919570923
setp: 700, Loss: 0.3519781827926636
setp: 800, Loss: 0.327593594789505
setp: 900, Loss: 0.319173663854599
setp: 1000, Loss: 0.32391050457954407
setp: 1100, Loss: 0.3215167820453644
setp: 1200, Loss: 0.32359373569488525
setp: 1300, Loss: 0.32007691264152527
setp: 1400, Loss: 0.3177930414676666
setp: 1500, Loss: 0.32382214069366455
setp: 1600, Loss: 0.31778472661972046
setp: 1700, Loss: 0.31746622920036316
setp: 1800, Loss: 0.31714704632759094
setp: 1900, Loss: 0.3186575770378113
setp: 2000, Loss: 0.31734126806259155
setp: 2100, Loss: 0.31661126017570496
setp: 2200, Loss: 0.3170948326587677
setp: 2300, Loss: 0.31838831305503845
setp: 2400, Loss: 0.31844672560691833
setp: 2500, Loss: 0.3176411986351013
setp: 2600, Loss: 0.31698495149612427
setp: 2700, Loss: 0.31749168038368225
setp: 2800, Loss: 0.31647181510925293
setp: 2900, Loss: 0.3172511160373688
setp: 3000, Loss: 0.31795594096183777
setp: 3100, Loss: 0.3170512616634369
setp: 3200, Loss: 0.31794273853302
setp: 3300, Loss: 0.31720852851867676
setp: 3400, Loss: 0.5762174129486084
setp: 3500, Loss: 0.5008505582809448
setp: 3600, Loss: 0.4123491644859314
setp: 3700, Loss: 0.39145320653915405
setp: 3800, Loss: 0.34595978260040283
setp: 3900, Loss: 0.34235912561416626
setp: 4000, Loss: 0.3873394727706909
setp: 4100, Loss: 0.43606966733932495
setp: 4200, Loss: 0.33042117953300476
setp: 4300, Loss: 0.32386672496795654
setp: 4400, Loss: 0.3238641917705536
setp: 4500, Loss: 0.3194315731525421
setp: 4600, Loss: 0.3229815661907196
setp: 4700, Loss: 0.32244566082954407
setp: 4800, Loss: 0.32106074690818787
setp: 4900, Loss: 0.3261145055294037
training successfully ended.
validating...
acc: 0.9884868421052632
precision: 0.9746835443037974
recall: 0.9956896551724138
F_score: 0.9850746268656717
validating...
acc: 0.8947368421052632
precision: 0.8783783783783784
recall: 0.9027777777777778
F_score: 0.8904109589041096
model saved.
avg_acc: 0.9289473684210525, avg_f_score: 0.909972743518702
-------------subject: 2-------------
==========valence==========
******fold 0******
[213, 395]
training...
setp: 0, Loss: 0.7828327417373657
setp: 100, Loss: 0.6417374014854431
setp: 200, Loss: 0.6244775652885437
setp: 300, Loss: 0.4766848087310791
setp: 400, Loss: 0.510881245136261
setp: 500, Loss: 0.42661404609680176
setp: 600, Loss: 0.38944414258003235
setp: 700, Loss: 0.4357273578643799
setp: 800, Loss: 0.38981902599334717
setp: 900, Loss: 0.4180799126625061
setp: 1000, Loss: 0.43300777673721313
setp: 1100, Loss: 0.3548530340194702
setp: 1200, Loss: 0.3851919174194336
setp: 1300, Loss: 0.4120861887931824
setp: 1400, Loss: 0.42544084787368774
setp: 1500, Loss: 0.35109516978263855
setp: 1600, Loss: 0.38964226841926575
setp: 1700, Loss: 0.4456583559513092
setp: 1800, Loss: 0.4188860058784485
setp: 1900, Loss: 0.4120092988014221
setp: 2000, Loss: 0.37304866313934326
setp: 2100, Loss: 0.3847412168979645
setp: 2200, Loss: 0.3516959846019745
setp: 2300, Loss: 0.4103551208972931
setp: 2400, Loss: 0.31947559118270874
setp: 2500, Loss: 0.32631370425224304
setp: 2600, Loss: 0.31934404373168945
setp: 2700, Loss: 0.3480205535888672
setp: 2800, Loss: 0.351192444562912
setp: 2900, Loss: 0.35017600655555725
setp: 3000, Loss: 0.36375710368156433
setp: 3100, Loss: 0.3543335795402527
setp: 3200, Loss: 0.34776005148887634
setp: 3300, Loss: 0.34859219193458557
setp: 3400, Loss: 0.35962891578674316
setp: 3500, Loss: 0.36460360884666443
setp: 3600, Loss: 0.34781211614608765
setp: 3700, Loss: 0.3490627110004425
setp: 3800, Loss: 0.3472333252429962
setp: 3900, Loss: 0.35026073455810547
setp: 4000, Loss: 0.34927645325660706
setp: 4100, Loss: 0.3237035870552063
setp: 4200, Loss: 0.36350592970848083
setp: 4300, Loss: 0.31772637367248535
setp: 4400, Loss: 0.31801170110702515
setp: 4500, Loss: 0.3228814899921417
setp: 4600, Loss: 0.3474160134792328
setp: 4700, Loss: 0.3493640422821045
setp: 4800, Loss: 0.34876903891563416
setp: 4900, Loss: 0.32037991285324097
training successfully ended.
validating...
acc: 0.9868421052631579
precision: 0.9951690821256038
recall: 0.9671361502347418
F_score: 0.9809523809523809
validating...
acc: 0.8947368421052632
precision: 0.9117647058823529
recall: 0.8611111111111112
F_score: 0.8857142857142858
******fold 1******
[244, 364]
training...
setp: 0, Loss: 0.7060992121696472
setp: 100, Loss: 0.6402521133422852
setp: 200, Loss: 0.5717447400093079
setp: 300, Loss: 0.41779419779777527
setp: 400, Loss: 0.4652804434299469
setp: 500, Loss: 0.45077115297317505
setp: 600, Loss: 0.39097869396209717
setp: 700, Loss: 0.490434467792511
setp: 800, Loss: 0.3542247712612152
setp: 900, Loss: 0.3876817226409912
setp: 1000, Loss: 0.3512533903121948
setp: 1100, Loss: 0.3524667024612427
setp: 1200, Loss: 0.3483164608478546
setp: 1300, Loss: 0.41048213839530945
setp: 1400, Loss: 0.3882109522819519
setp: 1500, Loss: 0.38025084137916565
setp: 1600, Loss: 0.42129725217819214
setp: 1700, Loss: 0.3209499716758728
setp: 1800, Loss: 0.37974560260772705
setp: 1900, Loss: 0.3504558205604553
setp: 2000, Loss: 0.3821534216403961
setp: 2100, Loss: 0.35484299063682556
setp: 2200, Loss: 0.34977972507476807
setp: 2300, Loss: 0.37896957993507385
setp: 2400, Loss: 0.3802814781665802
setp: 2500, Loss: 0.34957078099250793
setp: 2600, Loss: 0.38087961077690125
setp: 2700, Loss: 0.34788674116134644
setp: 2800, Loss: 0.3802621364593506
setp: 2900, Loss: 0.40133005380630493
setp: 3000, Loss: 0.34854304790496826
setp: 3100, Loss: 0.34978288412094116
setp: 3200, Loss: 0.39478594064712524
setp: 3300, Loss: 0.3786427974700928
setp: 3400, Loss: 0.3814697861671448
setp: 3500, Loss: 0.3783171772956848
setp: 3600, Loss: 0.3203258812427521
setp: 3700, Loss: 0.37938639521598816
setp: 3800, Loss: 0.35426750779151917
setp: 3900, Loss: 0.34925398230552673
setp: 4000, Loss: 0.34864571690559387
setp: 4100, Loss: 0.35048770904541016
setp: 4200, Loss: 0.3498634994029999
setp: 4300, Loss: 0.35075804591178894
setp: 4400, Loss: 0.34950315952301025
setp: 4500, Loss: 0.34916621446609497
setp: 4600, Loss: 0.3179415762424469
setp: 4700, Loss: 0.34990930557250977
setp: 4800, Loss: 0.3603549599647522
setp: 4900, Loss: 0.31587424874305725
training successfully ended.
validating...
acc: 0.975328947368421
precision: 0.9956709956709957
recall: 0.9426229508196722
F_score: 0.9684210526315791
validating...
acc: 0.9210526315789473
precision: 0.8372093023255814
recall: 0.8780487804878049
F_score: 0.8571428571428572
******fold 2******
[224, 384]
training...
setp: 0, Loss: 0.7364326119422913
setp: 100, Loss: 0.645222544670105
setp: 200, Loss: 0.682191014289856
setp: 300, Loss: 0.6525933742523193
setp: 400, Loss: 0.5707371830940247
setp: 500, Loss: 0.473378449678421
setp: 600, Loss: 0.44499874114990234
setp: 700, Loss: 0.4866904616355896
setp: 800, Loss: 0.43206149339675903
setp: 900, Loss: 0.44523024559020996
setp: 1000, Loss: 0.4080239534378052
setp: 1100, Loss: 0.41643598675727844
setp: 1200, Loss: 0.4154303967952728
setp: 1300, Loss: 0.4116440713405609
setp: 1400, Loss: 0.45341727137565613
setp: 1500, Loss: 0.42485594749450684
setp: 1600, Loss: 0.3844112753868103
setp: 1700, Loss: 0.36756300926208496
setp: 1800, Loss: 0.3539028465747833
setp: 1900, Loss: 0.38920363783836365
setp: 2000, Loss: 0.35497161746025085
setp: 2100, Loss: 0.38262197375297546
setp: 2200, Loss: 0.38348138332366943
setp: 2300, Loss: 0.3506099283695221
setp: 2400, Loss: 0.35145512223243713
setp: 2500, Loss: 0.38479357957839966
setp: 2600, Loss: 0.39836615324020386
setp: 2700, Loss: 0.3878081142902374
setp: 2800, Loss: 0.4125036895275116
setp: 2900, Loss: 0.3589949309825897
setp: 3000, Loss: 0.3818068504333496
setp: 3100, Loss: 0.3874627351760864
setp: 3200, Loss: 0.3805312514305115
setp: 3300, Loss: 0.3788672089576721
setp: 3400, Loss: 0.382342129945755
setp: 3500, Loss: 0.32523658871650696
setp: 3600, Loss: 0.32373109459877014
setp: 3700, Loss: 0.3202488124370575
setp: 3800, Loss: 0.321597695350647
setp: 3900, Loss: 0.32044023275375366
setp: 4000, Loss: 0.32242250442504883
setp: 4100, Loss: 0.31959885358810425
setp: 4200, Loss: 0.32235318422317505
setp: 4300, Loss: 0.3225221633911133
setp: 4400, Loss: 0.3192158341407776
setp: 4500, Loss: 0.32030799984931946
setp: 4600, Loss: 0.31818506121635437
setp: 4700, Loss: 0.3198584318161011
setp: 4800, Loss: 0.31838458776474
setp: 4900, Loss: 0.32127702236175537
training successfully ended.
validating...
acc: 0.9490131578947368
precision: 1.0
recall: 0.8616071428571429
F_score: 0.9256594724220625
validating...
acc: 0.8486842105263158
precision: 1.0
recall: 0.6229508196721312
F_score: 0.7676767676767677
******fold 3******
[221, 387]
training...
setp: 0, Loss: 0.6845661997795105
setp: 100, Loss: 0.6445265412330627
setp: 200, Loss: 0.6461480855941772
setp: 300, Loss: 0.6208015084266663
setp: 400, Loss: 0.6218506097793579
setp: 500, Loss: 0.540550708770752
setp: 600, Loss: 0.498957097530365
setp: 700, Loss: 0.4621361196041107
setp: 800, Loss: 0.4507886469364166
setp: 900, Loss: 0.45186930894851685
setp: 1000, Loss: 0.42022910714149475
setp: 1100, Loss: 0.4201793074607849
setp: 1200, Loss: 0.3913053274154663
setp: 1300, Loss: 0.47138190269470215
setp: 1400, Loss: 0.412069171667099
setp: 1500, Loss: 0.46584272384643555
setp: 1600, Loss: 0.40934547781944275
setp: 1700, Loss: 0.37258395552635193
setp: 1800, Loss: 0.39376989006996155
setp: 1900, Loss: 0.356093168258667
setp: 2000, Loss: 0.3504794239997864
setp: 2100, Loss: 0.37936413288116455
setp: 2200, Loss: 0.3792695105075836
setp: 2300, Loss: 0.42452695965766907
setp: 2400, Loss: 0.38097044825553894
setp: 2500, Loss: 0.3819526135921478
setp: 2600, Loss: 0.3599487245082855
setp: 2700, Loss: 0.3802907466888428
setp: 2800, Loss: 0.3525371849536896
setp: 2900, Loss: 0.379321426153183
setp: 3000, Loss: 0.3867166340351105
setp: 3100, Loss: 0.3504088222980499
setp: 3200, Loss: 0.3941667675971985
setp: 3300, Loss: 0.37865176796913147
setp: 3400, Loss: 0.4184909760951996
setp: 3500, Loss: 0.3968508243560791
setp: 3600, Loss: 0.3676852881908417
setp: 3700, Loss: 0.3498304486274719
setp: 3800, Loss: 0.3513364791870117
setp: 3900, Loss: 0.349283903837204
setp: 4000, Loss: 0.3812650442123413
setp: 4100, Loss: 0.37793049216270447
setp: 4200, Loss: 0.37985336780548096
setp: 4300, Loss: 0.3805250823497772
setp: 4400, Loss: 0.3832768201828003
setp: 4500, Loss: 0.3489004373550415
setp: 4600, Loss: 0.3792535364627838
setp: 4700, Loss: 0.35542812943458557
setp: 4800, Loss: 0.3791576027870178
setp: 4900, Loss: 0.3807414174079895
training successfully ended.
validating...
acc: 0.9490131578947368
precision: 0.9166666666666666
recall: 0.9457013574660633
F_score: 0.930957683741648
validating...
acc: 0.8881578947368421
precision: 0.8852459016393442
recall: 0.84375
F_score: 0.864
******fold 4******
[238, 370]
training...
setp: 0, Loss: 0.8000232577323914
setp: 100, Loss: 0.6836621761322021
setp: 200, Loss: 0.643947422504425
setp: 300, Loss: 0.5088885426521301
setp: 400, Loss: 0.4359213709831238
setp: 500, Loss: 0.5220355987548828
setp: 600, Loss: 0.47441285848617554
setp: 700, Loss: 0.4280276298522949
setp: 800, Loss: 0.41647639870643616
setp: 900, Loss: 0.40379786491394043
setp: 1000, Loss: 0.39316049218177795
setp: 1100, Loss: 0.44831645488739014
setp: 1200, Loss: 0.42230936884880066
setp: 1300, Loss: 0.39730632305145264
setp: 1400, Loss: 0.4250001907348633
setp: 1500, Loss: 0.3542056977748871
setp: 1600, Loss: 0.39112547039985657
setp: 1700, Loss: 0.44449344277381897
setp: 1800, Loss: 0.38873669505119324
setp: 1900, Loss: 0.3853570222854614
setp: 2000, Loss: 0.42112094163894653
setp: 2100, Loss: 0.37762466073036194
setp: 2200, Loss: 0.3822631537914276
setp: 2300, Loss: 0.3505121171474457
setp: 2400, Loss: 0.34751859307289124
setp: 2500, Loss: 0.3524263799190521
setp: 2600, Loss: 0.3848709464073181
setp: 2700, Loss: 0.3497949242591858
setp: 2800, Loss: 0.35468244552612305
setp: 2900, Loss: 0.380781888961792
setp: 3000, Loss: 0.4162124693393707
setp: 3100, Loss: 0.3837454915046692
setp: 3200, Loss: 0.35024914145469666
setp: 3300, Loss: 0.41072723269462585
setp: 3400, Loss: 0.34859904646873474
setp: 3500, Loss: 0.3836117386817932
setp: 3600, Loss: 0.4180540442466736
setp: 3700, Loss: 0.37959519028663635
setp: 3800, Loss: 0.3805793523788452
setp: 3900, Loss: 0.3827010691165924
setp: 4000, Loss: 0.35437771677970886
setp: 4100, Loss: 0.3802241384983063
setp: 4200, Loss: 0.3546765148639679
setp: 4300, Loss: 0.34929564595222473
setp: 4400, Loss: 0.34975507855415344
setp: 4500, Loss: 0.37949037551879883
setp: 4600, Loss: 0.35007017850875854
setp: 4700, Loss: 0.34854528307914734
setp: 4800, Loss: 0.3954179584980011
setp: 4900, Loss: 0.4364786744117737
training successfully ended.
validating...
acc: 0.9375
precision: 0.9166666666666666
recall: 0.9243697478991597
F_score: 0.9205020920502092
validating...
acc: 0.9078947368421053
precision: 0.7894736842105263
recall: 0.9574468085106383
F_score: 0.8653846153846154
model saved.
avg_acc: 0.8921052631578948, avg_f_score: 0.8479837051837051
==========arousal==========
******fold 0******
[242, 366]
training...
setp: 0, Loss: 0.6860538125038147
setp: 100, Loss: 0.6662882566452026
setp: 200, Loss: 0.5795754790306091
setp: 300, Loss: 0.46528345346450806
setp: 400, Loss: 0.439849317073822
setp: 500, Loss: 0.37296062707901
setp: 600, Loss: 0.34247887134552
setp: 700, Loss: 0.3325003385543823
setp: 800, Loss: 0.36112990975379944
setp: 900, Loss: 0.3318837583065033
setp: 1000, Loss: 0.36176660656929016
setp: 1100, Loss: 0.34391218423843384
setp: 1200, Loss: 0.33153724670410156
setp: 1300, Loss: 0.3341349959373474
setp: 1400, Loss: 0.33065277338027954
setp: 1500, Loss: 0.3623427152633667
setp: 1600, Loss: 0.35860779881477356
setp: 1700, Loss: 0.32475510239601135
setp: 1800, Loss: 0.334152489900589
setp: 1900, Loss: 0.3369428515434265
setp: 2000, Loss: 0.32130980491638184
setp: 2100, Loss: 0.3207610845565796
setp: 2200, Loss: 0.31979289650917053
setp: 2300, Loss: 0.320259690284729
setp: 2400, Loss: 0.31950366497039795
setp: 2500, Loss: 0.31969597935676575
setp: 2600, Loss: 0.3201400637626648
setp: 2700, Loss: 0.3204084038734436
setp: 2800, Loss: 0.32025662064552307
setp: 2900, Loss: 0.32101964950561523
setp: 3000, Loss: 0.3200376033782959
setp: 3100, Loss: 0.33209526538848877
setp: 3200, Loss: 0.328278511762619
setp: 3300, Loss: 0.3360103368759155
setp: 3400, Loss: 0.32363662123680115
setp: 3500, Loss: 0.32082557678222656
setp: 3600, Loss: 0.3253866136074066
setp: 3700, Loss: 0.3228773772716522
setp: 3800, Loss: 0.3236205279827118
setp: 3900, Loss: 0.32547494769096375
setp: 4000, Loss: 0.3369728624820709
setp: 4100, Loss: 0.32434356212615967
setp: 4200, Loss: 0.3221457600593567
setp: 4300, Loss: 0.3190002739429474
setp: 4400, Loss: 0.31903326511383057
setp: 4500, Loss: 0.31978559494018555
setp: 4600, Loss: 0.31939035654067993
setp: 4700, Loss: 0.3198730945587158
setp: 4800, Loss: 0.3203250765800476
setp: 4900, Loss: 0.31945130228996277
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8618421052631579
precision: 0.9019607843137255
recall: 0.7419354838709677
F_score: 0.8141592920353982
******fold 1******
[237, 371]
training...
setp: 0, Loss: 0.673043966293335
setp: 100, Loss: 0.6480770707130432
setp: 200, Loss: 0.5812344551086426
setp: 300, Loss: 0.4704030156135559
setp: 400, Loss: 0.4518147110939026
setp: 500, Loss: 0.45433148741722107
setp: 600, Loss: 0.3408375382423401
setp: 700, Loss: 0.38533535599708557
setp: 800, Loss: 0.3923068940639496
setp: 900, Loss: 0.38810136914253235
setp: 1000, Loss: 0.40698832273483276
setp: 1100, Loss: 0.35358142852783203
setp: 1200, Loss: 0.3781755864620209
setp: 1300, Loss: 0.34312447905540466
setp: 1400, Loss: 0.33194732666015625
setp: 1500, Loss: 0.36955830454826355
setp: 1600, Loss: 0.4366895854473114
setp: 1700, Loss: 0.3325231671333313
setp: 1800, Loss: 0.35085269808769226
setp: 1900, Loss: 0.3517705798149109
setp: 2000, Loss: 0.3465515673160553
setp: 2100, Loss: 0.3239330053329468
setp: 2200, Loss: 0.32469308376312256
setp: 2300, Loss: 0.3680318593978882
setp: 2400, Loss: 0.32018575072288513
setp: 2500, Loss: 0.3187289834022522
setp: 2600, Loss: 0.3204624056816101
setp: 2700, Loss: 0.32503917813301086
setp: 2800, Loss: 0.3248918652534485
setp: 2900, Loss: 0.32960009574890137
setp: 3000, Loss: 0.3380385935306549
setp: 3100, Loss: 0.32003316283226013
setp: 3200, Loss: 0.3190459609031677
setp: 3300, Loss: 0.3191317617893219
setp: 3400, Loss: 0.35138627886772156
setp: 3500, Loss: 0.3190760314464569
setp: 3600, Loss: 0.3193551003932953
setp: 3700, Loss: 0.32095271348953247
setp: 3800, Loss: 0.31903260946273804
setp: 3900, Loss: 0.3207184374332428
setp: 4000, Loss: 0.31982189416885376
setp: 4100, Loss: 0.31895536184310913
setp: 4200, Loss: 0.3202114701271057
setp: 4300, Loss: 0.319139301776886
setp: 4400, Loss: 0.352051317691803
setp: 4500, Loss: 0.3553348481655121
setp: 4600, Loss: 0.3476701080799103
setp: 4700, Loss: 0.3456384539604187
setp: 4800, Loss: 0.340587854385376
setp: 4900, Loss: 0.32225826382637024
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9957983193277311
recall: 1.0
F_score: 0.9978947368421053
validating...
acc: 0.9144736842105263
precision: 0.875
recall: 0.9402985074626866
F_score: 0.9064748201438849
******fold 2******
[255, 353]
training...
setp: 0, Loss: 0.7197897434234619
setp: 100, Loss: 0.6609724760055542
setp: 200, Loss: 0.5848320126533508
setp: 300, Loss: 0.44725778698921204
setp: 400, Loss: 0.41311678290367126
setp: 500, Loss: 0.3987995982170105
setp: 600, Loss: 0.36296290159225464
setp: 700, Loss: 0.32787013053894043
setp: 800, Loss: 0.37907326221466064
setp: 900, Loss: 0.35948920249938965
setp: 1000, Loss: 0.40302637219429016
setp: 1100, Loss: 0.3363112807273865
setp: 1200, Loss: 0.330487459897995
setp: 1300, Loss: 0.41622233390808105
setp: 1400, Loss: 0.3454766273498535
setp: 1500, Loss: 0.32811442017555237
setp: 1600, Loss: 0.32036125659942627
setp: 1700, Loss: 0.3203161060810089
setp: 1800, Loss: 0.3203657269477844
setp: 1900, Loss: 0.34766650199890137
setp: 2000, Loss: 0.31954047083854675
setp: 2100, Loss: 0.3209463953971863
setp: 2200, Loss: 0.3193013072013855
setp: 2300, Loss: 0.3206193447113037
setp: 2400, Loss: 0.3403248190879822
setp: 2500, Loss: 0.33414560556411743
setp: 2600, Loss: 0.3205270767211914
setp: 2700, Loss: 0.32207340002059937
setp: 2800, Loss: 0.3219321668148041
setp: 2900, Loss: 0.324274480342865
setp: 3000, Loss: 0.320128470659256
setp: 3100, Loss: 0.31920650601387024
setp: 3200, Loss: 0.3200872242450714
setp: 3300, Loss: 0.31825560331344604
setp: 3400, Loss: 0.32121720910072327
setp: 3500, Loss: 0.31841886043548584
setp: 3600, Loss: 0.32046598196029663
setp: 3700, Loss: 0.3198607861995697
setp: 3800, Loss: 0.3197549283504486
setp: 3900, Loss: 0.3705412447452545
setp: 4000, Loss: 0.34329527616500854
setp: 4100, Loss: 0.31958433985710144
setp: 4200, Loss: 0.3245863616466522
setp: 4300, Loss: 0.31711477041244507
setp: 4400, Loss: 0.327070951461792
setp: 4500, Loss: 0.3217780292034149
setp: 4600, Loss: 0.3188267946243286
setp: 4700, Loss: 0.3212602734565735
setp: 4800, Loss: 0.3394416272640228
setp: 4900, Loss: 0.3225362300872803
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9539473684210527
precision: 0.9375
recall: 0.9183673469387755
F_score: 0.9278350515463918
******fold 3******
[242, 366]
training...
setp: 0, Loss: 0.7780262231826782
setp: 100, Loss: 0.6709500551223755
setp: 200, Loss: 0.5125688314437866
setp: 300, Loss: 0.48271650075912476
setp: 400, Loss: 0.4241188168525696
setp: 500, Loss: 0.39604029059410095
setp: 600, Loss: 0.4275086224079132
setp: 700, Loss: 0.3745075762271881
setp: 800, Loss: 0.3579601049423218
setp: 900, Loss: 0.421050488948822
setp: 1000, Loss: 0.3762292265892029
setp: 1100, Loss: 0.33161357045173645
setp: 1200, Loss: 0.3355444371700287
setp: 1300, Loss: 0.3306245505809784
setp: 1400, Loss: 0.3236367106437683
setp: 1500, Loss: 0.3406924307346344
setp: 1600, Loss: 0.3201594948768616
setp: 1700, Loss: 0.32170578837394714
setp: 1800, Loss: 0.3505139648914337
setp: 1900, Loss: 0.3554436266422272
setp: 2000, Loss: 0.3414444923400879
setp: 2100, Loss: 0.3274313807487488
setp: 2200, Loss: 0.3420984148979187
setp: 2300, Loss: 0.32007652521133423
setp: 2400, Loss: 0.32447701692581177
setp: 2500, Loss: 0.32023298740386963
setp: 2600, Loss: 0.31830015778541565
setp: 2700, Loss: 0.3191474974155426
setp: 2800, Loss: 0.32063552737236023
setp: 2900, Loss: 0.3201485574245453
setp: 3000, Loss: 0.31997328996658325
setp: 3100, Loss: 0.31961390376091003
setp: 3200, Loss: 0.3194308578968048
setp: 3300, Loss: 0.447530597448349
setp: 3400, Loss: 0.35621893405914307
setp: 3500, Loss: 0.33051756024360657
setp: 3600, Loss: 0.32161691784858704
setp: 3700, Loss: 0.35010260343551636
setp: 3800, Loss: 0.34893694519996643
setp: 3900, Loss: 0.3184173107147217
setp: 4000, Loss: 0.3181309998035431
setp: 4100, Loss: 0.318995863199234
setp: 4200, Loss: 0.319308876991272
setp: 4300, Loss: 0.3184948265552521
setp: 4400, Loss: 0.47382521629333496
setp: 4500, Loss: 0.3324999213218689
setp: 4600, Loss: 0.3494897186756134
setp: 4700, Loss: 0.32920750975608826
setp: 4800, Loss: 0.36283910274505615
setp: 4900, Loss: 0.3262982666492462
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.9917355371900827
F_score: 0.995850622406639
validating...
acc: 0.8947368421052632
precision: 0.8833333333333333
recall: 0.8548387096774194
F_score: 0.8688524590163934
******fold 4******
[240, 368]
training...
setp: 0, Loss: 0.7033156156539917
setp: 100, Loss: 0.6864937543869019
setp: 200, Loss: 0.5848707556724548
setp: 300, Loss: 0.5035327672958374
setp: 400, Loss: 0.46243056654930115
setp: 500, Loss: 0.43269023299217224
setp: 600, Loss: 0.4104249179363251
setp: 700, Loss: 0.41928645968437195
setp: 800, Loss: 0.4167911112308502
setp: 900, Loss: 0.38964980840682983
setp: 1000, Loss: 0.33502838015556335
setp: 1100, Loss: 0.3566684424877167
setp: 1200, Loss: 0.34855517745018005
setp: 1300, Loss: 0.41294628381729126
setp: 1400, Loss: 0.388043075799942
setp: 1500, Loss: 0.38390079140663147
setp: 1600, Loss: 0.35528621077537537
setp: 1700, Loss: 0.3863530457019806
setp: 1800, Loss: 0.4098150134086609
setp: 1900, Loss: 0.3501564860343933
setp: 2000, Loss: 0.31895941495895386
setp: 2100, Loss: 0.318639874458313
setp: 2200, Loss: 0.3186655640602112
setp: 2300, Loss: 0.3496028780937195
setp: 2400, Loss: 0.3190293610095978
setp: 2500, Loss: 0.31891345977783203
setp: 2600, Loss: 0.3232842981815338
setp: 2700, Loss: 0.3239232301712036
setp: 2800, Loss: 0.3200024962425232
setp: 2900, Loss: 0.3224005699157715
setp: 3000, Loss: 0.3243873715400696
setp: 3100, Loss: 0.3206433355808258
setp: 3200, Loss: 0.3200764060020447
setp: 3300, Loss: 0.3254414200782776
setp: 3400, Loss: 0.3363390862941742
setp: 3500, Loss: 0.3194621801376343
setp: 3600, Loss: 0.3236633837223053
setp: 3700, Loss: 0.32043561339378357
setp: 3800, Loss: 0.31857064366340637
setp: 3900, Loss: 0.31870409846305847
setp: 4000, Loss: 0.31902649998664856
setp: 4100, Loss: 0.31894350051879883
setp: 4200, Loss: 0.34991034865379333
setp: 4300, Loss: 0.31878089904785156
setp: 4400, Loss: 0.3187902271747589
setp: 4500, Loss: 0.31986790895462036
setp: 4600, Loss: 0.31944790482521057
setp: 4700, Loss: 0.31912919878959656
setp: 4800, Loss: 0.3198705315589905
setp: 4900, Loss: 0.3887788653373718
training successfully ended.
validating...
acc: 0.9555921052631579
precision: 0.9907834101382489
recall: 0.8958333333333334
F_score: 0.9409190371991248
validating...
acc: 0.7631578947368421
precision: 0.8181818181818182
recall: 0.5625
F_score: 0.6666666666666666
model saved.
avg_acc: 0.8776315789473685, avg_f_score: 0.8367976578817471
-------------subject: 3-------------
==========valence==========
******fold 0******
[257, 351]
training...
setp: 0, Loss: 0.6907240152359009
setp: 100, Loss: 0.6619420051574707
setp: 200, Loss: 0.5974577069282532
setp: 300, Loss: 0.5453067421913147
setp: 400, Loss: 0.4647456705570221
setp: 500, Loss: 0.44421035051345825
setp: 600, Loss: 0.38607439398765564
setp: 700, Loss: 0.3579140305519104
setp: 800, Loss: 0.366015762090683
setp: 900, Loss: 0.32822319865226746
setp: 1000, Loss: 0.32224422693252563
setp: 1100, Loss: 0.3577403724193573
setp: 1200, Loss: 0.3197645843029022
setp: 1300, Loss: 0.35066115856170654
setp: 1400, Loss: 0.38365477323532104
setp: 1500, Loss: 0.35983508825302124
setp: 1600, Loss: 0.3173525035381317
setp: 1700, Loss: 0.31867557764053345
setp: 1800, Loss: 0.34802308678627014
setp: 1900, Loss: 0.34685567021369934
setp: 2000, Loss: 0.37982192635536194
setp: 2100, Loss: 0.36575847864151
setp: 2200, Loss: 0.3201187551021576
setp: 2300, Loss: 0.3477201461791992
setp: 2400, Loss: 0.31760382652282715
setp: 2500, Loss: 0.31974291801452637
setp: 2600, Loss: 0.3226478695869446
setp: 2700, Loss: 0.31948864459991455
setp: 2800, Loss: 0.31622815132141113
setp: 2900, Loss: 0.31802061200141907
setp: 3000, Loss: 0.3166813552379608
setp: 3100, Loss: 0.31725651025772095
setp: 3200, Loss: 0.3201196491718292
setp: 3300, Loss: 0.3462940752506256
setp: 3400, Loss: 0.32094714045524597
setp: 3500, Loss: 0.3163006603717804
setp: 3600, Loss: 0.31684479117393494
setp: 3700, Loss: 0.318508505821228
setp: 3800, Loss: 0.3186553120613098
setp: 3900, Loss: 0.35097286105155945
setp: 4000, Loss: 0.3235722482204437
setp: 4100, Loss: 0.32431191205978394
setp: 4200, Loss: 0.347159206867218
setp: 4300, Loss: 0.31750696897506714
setp: 4400, Loss: 0.3179127275943756
setp: 4500, Loss: 0.31746241450309753
setp: 4600, Loss: 0.31699883937835693
setp: 4700, Loss: 0.35691627860069275
setp: 4800, Loss: 0.46059924364089966
setp: 4900, Loss: 0.3166554272174835
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9961089494163424
F_score: 0.9980506822612086
validating...
acc: 0.9473684210526315
precision: 0.963855421686747
recall: 0.9411764705882353
F_score: 0.9523809523809523
******fold 1******
[275, 333]
training...
setp: 0, Loss: 0.6821387410163879
setp: 100, Loss: 0.6918216943740845
setp: 200, Loss: 0.6162838339805603
setp: 300, Loss: 0.3996512293815613
setp: 400, Loss: 0.4436182379722595
setp: 500, Loss: 0.41613930463790894
setp: 600, Loss: 0.3988412320613861
setp: 700, Loss: 0.3570328652858734
setp: 800, Loss: 0.328688383102417
setp: 900, Loss: 0.35477596521377563
setp: 1000, Loss: 0.35155922174453735
setp: 1100, Loss: 0.36410120129585266
setp: 1200, Loss: 0.3474116027355194
setp: 1300, Loss: 0.35466301441192627
setp: 1400, Loss: 0.33824437856674194
setp: 1500, Loss: 0.3572103679180145
setp: 1600, Loss: 0.3501650393009186
setp: 1700, Loss: 0.35193800926208496
setp: 1800, Loss: 0.3497927486896515
setp: 1900, Loss: 0.38158029317855835
setp: 2000, Loss: 0.34845805168151855
setp: 2100, Loss: 0.34769701957702637
setp: 2200, Loss: 0.3241700232028961
setp: 2300, Loss: 0.349185973405838
setp: 2400, Loss: 0.3172425627708435
setp: 2500, Loss: 0.3529338240623474
setp: 2600, Loss: 0.3467491567134857
setp: 2700, Loss: 0.3282604217529297
setp: 2800, Loss: 0.3219463527202606
setp: 2900, Loss: 0.3488350510597229
setp: 3000, Loss: 0.3484064042568207
setp: 3100, Loss: 0.3510969281196594
setp: 3200, Loss: 0.3513531982898712
setp: 3300, Loss: 0.332938015460968
setp: 3400, Loss: 0.3607136309146881
setp: 3500, Loss: 0.3487941324710846
setp: 3600, Loss: 0.3484276235103607
setp: 3700, Loss: 0.36262547969818115
setp: 3800, Loss: 0.35220715403556824
setp: 3900, Loss: 0.34727075695991516
setp: 4000, Loss: 0.31823524832725525
setp: 4100, Loss: 0.3182668387889862
setp: 4200, Loss: 0.3500036597251892
setp: 4300, Loss: 0.3397608995437622
setp: 4400, Loss: 0.32122620940208435
setp: 4500, Loss: 0.33162182569503784
setp: 4600, Loss: 0.3185187578201294
setp: 4700, Loss: 0.34307458996772766
setp: 4800, Loss: 0.31750309467315674
setp: 4900, Loss: 0.3602030277252197
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 1.0
recall: 0.9890909090909091
F_score: 0.9945155393053016
validating...
acc: 0.9605263157894737
precision: 0.9420289855072463
recall: 0.9701492537313433
F_score: 0.9558823529411764
******fold 2******
[276, 332]
training...
setp: 0, Loss: 0.7533554434776306
setp: 100, Loss: 0.6265705823898315
setp: 200, Loss: 0.671629786491394
setp: 300, Loss: 0.6092762351036072
setp: 400, Loss: 0.42347338795661926
setp: 500, Loss: 0.33889931440353394
setp: 600, Loss: 0.42286229133605957
setp: 700, Loss: 0.36579591035842896
setp: 800, Loss: 0.36847493052482605
setp: 900, Loss: 0.3730863630771637
setp: 1000, Loss: 0.3547061085700989
setp: 1100, Loss: 0.3802976906299591
setp: 1200, Loss: 0.3511175215244293
setp: 1300, Loss: 0.35099300742149353
setp: 1400, Loss: 0.32324081659317017
setp: 1500, Loss: 0.3511849343776703
setp: 1600, Loss: 0.35352081060409546
setp: 1700, Loss: 0.3641281723976135
setp: 1800, Loss: 0.36984941363334656
setp: 1900, Loss: 0.35124367475509644
setp: 2000, Loss: 0.34916648268699646
setp: 2100, Loss: 0.34700509905815125
setp: 2200, Loss: 0.3185678720474243
setp: 2300, Loss: 0.35158082842826843
setp: 2400, Loss: 0.3166245222091675
setp: 2500, Loss: 0.35235148668289185
setp: 2600, Loss: 0.346953809261322
setp: 2700, Loss: 0.38091331720352173
setp: 2800, Loss: 0.31997668743133545
setp: 2900, Loss: 0.317425936460495
setp: 3000, Loss: 0.3182971775531769
setp: 3100, Loss: 0.3172317147254944
setp: 3200, Loss: 0.31856289505958557
setp: 3300, Loss: 0.3236525356769562
setp: 3400, Loss: 0.3273850977420807
setp: 3500, Loss: 0.3180685043334961
setp: 3600, Loss: 0.3178011178970337
setp: 3700, Loss: 0.3167647123336792
setp: 3800, Loss: 0.3183439075946808
setp: 3900, Loss: 0.3168922960758209
setp: 4000, Loss: 0.3172157406806946
setp: 4100, Loss: 0.3188423216342926
setp: 4200, Loss: 0.3186834454536438
setp: 4300, Loss: 0.31947416067123413
setp: 4400, Loss: 0.3465053141117096
setp: 4500, Loss: 0.3644118905067444
setp: 4600, Loss: 0.32242274284362793
setp: 4700, Loss: 0.3202797770500183
setp: 4800, Loss: 0.31705760955810547
setp: 4900, Loss: 0.3173012435436249
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.9666666666666667
recall: 0.8787878787878788
F_score: 0.9206349206349207
******fold 3******
[273, 335]
training...
setp: 0, Loss: 0.6872841715812683
setp: 100, Loss: 0.6861922144889832
setp: 200, Loss: 0.6490778923034668
setp: 300, Loss: 0.5967354774475098
setp: 400, Loss: 0.39641672372817993
setp: 500, Loss: 0.3752346932888031
setp: 600, Loss: 0.4616988003253937
setp: 700, Loss: 0.3497540056705475
setp: 800, Loss: 0.3239037096500397
setp: 900, Loss: 0.3220300078392029
setp: 1000, Loss: 0.3209916651248932
setp: 1100, Loss: 0.3183453679084778
setp: 1200, Loss: 0.32186612486839294
setp: 1300, Loss: 0.31704089045524597
setp: 1400, Loss: 0.3175348937511444
setp: 1500, Loss: 0.3513513207435608
setp: 1600, Loss: 0.39886146783828735
setp: 1700, Loss: 0.3836750090122223
setp: 1800, Loss: 0.31861966848373413
setp: 1900, Loss: 0.3201330006122589
setp: 2000, Loss: 0.31826087832450867
setp: 2100, Loss: 0.3181239664554596
setp: 2200, Loss: 0.319862961769104
setp: 2300, Loss: 0.3222668468952179
setp: 2400, Loss: 0.3165394067764282
setp: 2500, Loss: 0.40342146158218384
setp: 2600, Loss: 0.3169648349285126
setp: 2700, Loss: 0.31878727674484253
setp: 2800, Loss: 0.31816792488098145
setp: 2900, Loss: 0.3175826072692871
setp: 3000, Loss: 0.31700754165649414
setp: 3100, Loss: 0.31700459122657776
setp: 3200, Loss: 0.3174590766429901
setp: 3300, Loss: 0.3250623345375061
setp: 3400, Loss: 0.34650683403015137
setp: 3500, Loss: 0.31595170497894287
setp: 3600, Loss: 0.3173872232437134
setp: 3700, Loss: 0.3188989460468292
setp: 3800, Loss: 0.31781867146492004
setp: 3900, Loss: 0.3175232410430908
setp: 4000, Loss: 0.38269057869911194
setp: 4100, Loss: 0.3291023075580597
setp: 4200, Loss: 0.33716073632240295
setp: 4300, Loss: 0.3173741102218628
setp: 4400, Loss: 0.31786835193634033
setp: 4500, Loss: 0.3171297311782837
setp: 4600, Loss: 0.31802240014076233
setp: 4700, Loss: 0.3180527687072754
setp: 4800, Loss: 0.31773340702056885
setp: 4900, Loss: 0.3161594271659851
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9736842105263158
precision: 0.9577464788732394
recall: 0.9855072463768116
F_score: 0.9714285714285714
******fold 4******
[287, 321]
training...
setp: 0, Loss: 0.6943536996841431
setp: 100, Loss: 0.667097806930542
setp: 200, Loss: 0.5950374007225037
setp: 300, Loss: 0.45262473821640015
setp: 400, Loss: 0.5772573351860046
setp: 500, Loss: 0.40780192613601685
setp: 600, Loss: 0.4376954734325409
setp: 700, Loss: 0.33569759130477905
setp: 800, Loss: 0.354973703622818
setp: 900, Loss: 0.33544662594795227
setp: 1000, Loss: 0.47420749068260193
setp: 1100, Loss: 0.367009699344635
setp: 1200, Loss: 0.350064754486084
setp: 1300, Loss: 0.35065382719039917
setp: 1400, Loss: 0.345576673746109
setp: 1500, Loss: 0.4087332785129547
setp: 1600, Loss: 0.3915458917617798
setp: 1700, Loss: 0.34882643818855286
setp: 1800, Loss: 0.3746359944343567
setp: 1900, Loss: 0.3275831937789917
setp: 2000, Loss: 0.34730657935142517
setp: 2100, Loss: 0.31755349040031433
setp: 2200, Loss: 0.32040297985076904
setp: 2300, Loss: 0.3181796669960022
setp: 2400, Loss: 0.31940633058547974
setp: 2500, Loss: 0.31928130984306335
setp: 2600, Loss: 0.31712907552719116
setp: 2700, Loss: 0.3181210160255432
setp: 2800, Loss: 0.3300289809703827
setp: 2900, Loss: 0.3262394070625305
setp: 3000, Loss: 0.33203253149986267
setp: 3100, Loss: 0.3176005780696869
setp: 3200, Loss: 0.31784921884536743
setp: 3300, Loss: 0.3169633746147156
setp: 3400, Loss: 0.3191608190536499
setp: 3500, Loss: 0.31794649362564087
setp: 3600, Loss: 0.31684306263923645
setp: 3700, Loss: 0.3221912682056427
setp: 3800, Loss: 0.318851113319397
setp: 3900, Loss: 0.3480166792869568
setp: 4000, Loss: 0.3177846074104309
setp: 4100, Loss: 0.32015088200569153
setp: 4200, Loss: 0.31738200783729553
setp: 4300, Loss: 0.318789005279541
setp: 4400, Loss: 0.3196341097354889
setp: 4500, Loss: 0.3171948492527008
setp: 4600, Loss: 0.3170127272605896
setp: 4700, Loss: 0.3227699100971222
setp: 4800, Loss: 0.32064276933670044
setp: 4900, Loss: 0.32539257407188416
training successfully ended.
validating...
acc: 0.9325657894736842
precision: 1.0
recall: 0.8571428571428571
F_score: 0.923076923076923
validating...
acc: 0.9210526315789473
precision: 1.0
recall: 0.7818181818181819
F_score: 0.8775510204081634
model saved.
avg_acc: 0.9473684210526315, avg_f_score: 0.9355755635587568
==========arousal==========
******fold 0******
[492, 116]
training...
setp: 0, Loss: 0.6935546398162842
setp: 100, Loss: 0.5370871424674988
setp: 200, Loss: 0.49480780959129333
setp: 300, Loss: 0.4113224446773529
setp: 400, Loss: 0.39288467168807983
setp: 500, Loss: 0.35394883155822754
setp: 600, Loss: 0.38143375515937805
setp: 700, Loss: 0.3240091800689697
setp: 800, Loss: 0.3289080262184143
setp: 900, Loss: 0.3192145824432373
setp: 1000, Loss: 0.3205152451992035
setp: 1100, Loss: 0.34660741686820984
setp: 1200, Loss: 0.316290944814682
setp: 1300, Loss: 0.31759050488471985
setp: 1400, Loss: 0.34843194484710693
setp: 1500, Loss: 0.3474310338497162
setp: 1600, Loss: 0.34813663363456726
setp: 1700, Loss: 0.34859323501586914
setp: 1800, Loss: 0.3176784813404083
setp: 1900, Loss: 0.31622034311294556
setp: 2000, Loss: 0.31730008125305176
setp: 2100, Loss: 0.3166428804397583
setp: 2200, Loss: 0.3172566294670105
setp: 2300, Loss: 0.34785500168800354
setp: 2400, Loss: 0.31677648425102234
setp: 2500, Loss: 0.3165959417819977
setp: 2600, Loss: 0.3170693516731262
setp: 2700, Loss: 0.5948218107223511
setp: 2800, Loss: 0.3566558361053467
setp: 2900, Loss: 0.33266523480415344
setp: 3000, Loss: 0.3405894339084625
setp: 3100, Loss: 0.32743409276008606
setp: 3200, Loss: 0.3257465362548828
setp: 3300, Loss: 0.3594962954521179
setp: 3400, Loss: 0.35513705015182495
setp: 3500, Loss: 0.3614148497581482
setp: 3600, Loss: 0.3531198501586914
setp: 3700, Loss: 0.3247169852256775
setp: 3800, Loss: 0.3239615559577942
setp: 3900, Loss: 0.32769691944122314
setp: 4000, Loss: 0.3222704231739044
setp: 4100, Loss: 0.32191306352615356
setp: 4200, Loss: 0.3221943974494934
setp: 4300, Loss: 0.3210521340370178
setp: 4400, Loss: 0.32192617654800415
setp: 4500, Loss: 0.3573351204395294
setp: 4600, Loss: 0.3523331880569458
setp: 4700, Loss: 0.3519831895828247
setp: 4800, Loss: 0.3529769480228424
setp: 4900, Loss: 0.31950172781944275
training successfully ended.
validating...
acc: 0.9908536585365854
precision: 0.9839679358717435
recall: 0.9979674796747967
F_score: 0.9909182643794147
validating...
acc: 0.9539473684210527
precision: 0.9504132231404959
recall: 0.9913793103448276
F_score: 0.9704641350210972
******fold 1******
[486, 122]
training...
setp: 0, Loss: 0.6934127807617188
setp: 100, Loss: 0.5367752313613892
setp: 200, Loss: 0.4787566065788269
setp: 300, Loss: 0.34664759039878845
setp: 400, Loss: 0.3850246071815491
setp: 500, Loss: 0.3542962372303009
setp: 600, Loss: 0.35186293721199036
setp: 700, Loss: 0.3705712556838989
setp: 800, Loss: 0.31912142038345337
setp: 900, Loss: 0.31849390268325806
setp: 1000, Loss: 0.31760746240615845
setp: 1100, Loss: 0.3173166811466217
setp: 1200, Loss: 0.3319709300994873
setp: 1300, Loss: 0.3755982518196106
setp: 1400, Loss: 0.3169209361076355
setp: 1500, Loss: 0.3173394203186035
setp: 1600, Loss: 0.3162234127521515
setp: 1700, Loss: 0.31860533356666565
setp: 1800, Loss: 0.31635603308677673
setp: 1900, Loss: 0.31810975074768066
setp: 2000, Loss: 0.35964614152908325
setp: 2100, Loss: 0.380423903465271
setp: 2200, Loss: 0.31710055470466614
setp: 2300, Loss: 0.31653091311454773
setp: 2400, Loss: 0.316731721162796
setp: 2500, Loss: 0.3167635500431061
setp: 2600, Loss: 0.3169756829738617
setp: 2700, Loss: 0.3160926103591919
setp: 2800, Loss: 0.35280436277389526
setp: 2900, Loss: 0.3197937309741974
setp: 3000, Loss: 0.3165871202945709
setp: 3100, Loss: 0.31649306416511536
setp: 3200, Loss: 0.31669166684150696
setp: 3300, Loss: 0.3162187337875366
setp: 3400, Loss: 0.3165672719478607
setp: 3500, Loss: 0.3174860179424286
setp: 3600, Loss: 0.4205019474029541
setp: 3700, Loss: 0.3190414011478424
setp: 3800, Loss: 0.3167487680912018
setp: 3900, Loss: 0.31618380546569824
setp: 4000, Loss: 0.3159840703010559
setp: 4100, Loss: 0.3164461851119995
setp: 4200, Loss: 0.31620413064956665
setp: 4300, Loss: 0.3166528046131134
setp: 4400, Loss: 0.3156435191631317
setp: 4500, Loss: 0.32143959403038025
setp: 4600, Loss: 0.3161836564540863
setp: 4700, Loss: 0.3159167468547821
setp: 4800, Loss: 0.31675270199775696
setp: 4900, Loss: 0.31609588861465454
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9736842105263158
precision: 0.9758064516129032
recall: 0.9918032786885246
F_score: 0.983739837398374
******fold 2******
[486, 122]
training...
setp: 0, Loss: 0.7130078077316284
setp: 100, Loss: 0.6186158061027527
setp: 200, Loss: 0.44597071409225464
setp: 300, Loss: 0.3912830054759979
setp: 400, Loss: 0.44696468114852905
setp: 500, Loss: 0.41562485694885254
setp: 600, Loss: 0.3841259181499481
setp: 700, Loss: 0.33525845408439636
setp: 800, Loss: 0.36143016815185547
setp: 900, Loss: 0.34906116127967834
setp: 1000, Loss: 0.3505811095237732
setp: 1100, Loss: 0.3205109238624573
setp: 1200, Loss: 0.3474911153316498
setp: 1300, Loss: 0.31815463304519653
setp: 1400, Loss: 0.31908559799194336
setp: 1500, Loss: 0.3217395842075348
setp: 1600, Loss: 0.31666603684425354
setp: 1700, Loss: 0.31679606437683105
setp: 1800, Loss: 0.32192009687423706
setp: 1900, Loss: 0.3179381489753723
setp: 2000, Loss: 0.31732377409935
setp: 2100, Loss: 0.32435256242752075
setp: 2200, Loss: 0.3184508979320526
setp: 2300, Loss: 0.3217226564884186
setp: 2400, Loss: 0.31627604365348816
setp: 2500, Loss: 0.31706029176712036
setp: 2600, Loss: 0.3174513876438141
setp: 2700, Loss: 0.316637247800827
setp: 2800, Loss: 0.3179861605167389
setp: 2900, Loss: 0.316615492105484
setp: 3000, Loss: 0.31638556718826294
setp: 3100, Loss: 0.3176085948944092
setp: 3200, Loss: 0.33275309205055237
setp: 3300, Loss: 0.31636324524879456
setp: 3400, Loss: 0.3156905770301819
setp: 3500, Loss: 0.31733399629592896
setp: 3600, Loss: 0.3218749463558197
setp: 3700, Loss: 0.31638410687446594
setp: 3800, Loss: 0.3167964816093445
setp: 3900, Loss: 0.6688215136528015
setp: 4000, Loss: 0.38182616233825684
setp: 4100, Loss: 0.34247857332229614
setp: 4200, Loss: 0.3341107666492462
setp: 4300, Loss: 0.3270871639251709
setp: 4400, Loss: 0.32267338037490845
setp: 4500, Loss: 0.32153967022895813
setp: 4600, Loss: 0.3217427134513855
setp: 4700, Loss: 0.32035937905311584
setp: 4800, Loss: 0.3203638792037964
setp: 4900, Loss: 0.32053402066230774
training successfully ended.
validating...
acc: 0.5
precision: 0.5
recall: 1.0
F_score: 0.6666666666666666
validating...
acc: 0.8026315789473685
precision: 0.8026315789473685
recall: 1.0
F_score: 0.8905109489051095
******fold 3******
[493, 115]
training...
setp: 0, Loss: 0.6951586008071899
setp: 100, Loss: 0.5955212712287903
setp: 200, Loss: 0.429533451795578
setp: 300, Loss: 0.4144882261753082
setp: 400, Loss: 0.37289905548095703
setp: 500, Loss: 0.42509734630584717
setp: 600, Loss: 0.3715614676475525
setp: 700, Loss: 0.31901994347572327
setp: 800, Loss: 0.3388228714466095
setp: 900, Loss: 0.42690202593803406
setp: 1000, Loss: 0.3176032602787018
setp: 1100, Loss: 0.3181057572364807
setp: 1200, Loss: 0.31713637709617615
setp: 1300, Loss: 0.3323117792606354
setp: 1400, Loss: 0.33153167366981506
setp: 1500, Loss: 0.31651195883750916
setp: 1600, Loss: 0.34723424911499023
setp: 1700, Loss: 0.35038211941719055
setp: 1800, Loss: 0.31826066970825195
setp: 1900, Loss: 0.31677135825157166
setp: 2000, Loss: 0.32130956649780273
setp: 2100, Loss: 0.3160172700881958
setp: 2200, Loss: 0.3174137473106384
setp: 2300, Loss: 0.3161280155181885
setp: 2400, Loss: 0.3163674473762512
setp: 2500, Loss: 0.31762444972991943
setp: 2600, Loss: 0.3163304328918457
setp: 2700, Loss: 0.31618615984916687
setp: 2800, Loss: 0.38731980323791504
setp: 2900, Loss: 0.3251473307609558
setp: 3000, Loss: 0.31593459844589233
setp: 3100, Loss: 0.3171845078468323
setp: 3200, Loss: 0.3163464665412903
setp: 3300, Loss: 0.31659451127052307
setp: 3400, Loss: 0.31753575801849365
setp: 3500, Loss: 0.3176555931568146
setp: 3600, Loss: 0.3587456941604614
setp: 3700, Loss: 0.34997040033340454
setp: 3800, Loss: 0.31624382734298706
setp: 3900, Loss: 0.31622588634490967
setp: 4000, Loss: 0.31727170944213867
setp: 4100, Loss: 0.31578528881073
setp: 4200, Loss: 0.3167109191417694
setp: 4300, Loss: 0.31991127133369446
setp: 4400, Loss: 0.34581395983695984
setp: 4500, Loss: 0.31763285398483276
setp: 4600, Loss: 0.31591302156448364
setp: 4700, Loss: 0.3470206558704376
setp: 4800, Loss: 0.34844309091567993
setp: 4900, Loss: 0.31707996129989624
training successfully ended.
validating...
acc: 0.8853955375253549
precision: 0.9896907216494846
recall: 0.7789046653144016
F_score: 0.8717366628830874
validating...
acc: 0.8289473684210527
precision: 1.0
recall: 0.7739130434782608
F_score: 0.8725490196078431
******fold 4******
[475, 133]
training...
setp: 0, Loss: 0.7061895132064819
setp: 100, Loss: 0.6147706508636475
setp: 200, Loss: 0.43902766704559326
setp: 300, Loss: 0.37133705615997314
setp: 400, Loss: 0.32691535353660583
setp: 500, Loss: 0.3262661397457123
setp: 600, Loss: 0.3673260807991028
setp: 700, Loss: 0.3245386481285095
setp: 800, Loss: 0.3236328363418579
setp: 900, Loss: 0.32214444875717163
setp: 1000, Loss: 0.31801095604896545
setp: 1100, Loss: 0.31799963116645813
setp: 1200, Loss: 0.31661275029182434
setp: 1300, Loss: 0.31787505745887756
setp: 1400, Loss: 0.31812378764152527
setp: 1500, Loss: 0.32820749282836914
setp: 1600, Loss: 0.3188142776489258
setp: 1700, Loss: 0.3173176348209381
setp: 1800, Loss: 0.3163553476333618
setp: 1900, Loss: 0.31670063734054565
setp: 2000, Loss: 0.317416250705719
setp: 2100, Loss: 0.316594660282135
setp: 2200, Loss: 0.317356139421463
setp: 2300, Loss: 0.322293758392334
setp: 2400, Loss: 0.3173683285713196
setp: 2500, Loss: 0.316279798746109
setp: 2600, Loss: 0.3171427547931671
setp: 2700, Loss: 0.3160606026649475
setp: 2800, Loss: 0.31630462408065796
setp: 2900, Loss: 0.31676506996154785
setp: 3000, Loss: 0.32336974143981934
setp: 3100, Loss: 0.316093385219574
setp: 3200, Loss: 0.31682711839675903
setp: 3300, Loss: 0.31652137637138367
setp: 3400, Loss: 0.3170625865459442
setp: 3500, Loss: 0.31744521856307983
setp: 3600, Loss: 0.3159104883670807
setp: 3700, Loss: 0.316408634185791
setp: 3800, Loss: 0.31697601079940796
setp: 3900, Loss: 0.3326176404953003
setp: 4000, Loss: 0.3157966434955597
setp: 4100, Loss: 0.3161288797855377
setp: 4200, Loss: 0.3161793053150177
setp: 4300, Loss: 0.3160455524921417
setp: 4400, Loss: 0.35696396231651306
setp: 4500, Loss: 0.3157692551612854
setp: 4600, Loss: 0.3155439496040344
setp: 4700, Loss: 0.3163931369781494
setp: 4800, Loss: 0.31659120321273804
setp: 4900, Loss: 0.3215286135673523
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9868421052631579
precision: 0.9924812030075187
recall: 0.9924812030075187
F_score: 0.9924812030075187
model saved.
avg_acc: 0.9092105263157896, avg_f_score: 0.9419490287879885
-------------subject: 4-------------
==========valence==========
******fold 0******
[356, 252]
training...
setp: 0, Loss: 0.6973088383674622
setp: 100, Loss: 0.6639710068702698
setp: 200, Loss: 0.6014420390129089
setp: 300, Loss: 0.5717282891273499
setp: 400, Loss: 0.5574013590812683
setp: 500, Loss: 0.41719070076942444
setp: 600, Loss: 0.3567473292350769
setp: 700, Loss: 0.35764163732528687
setp: 800, Loss: 0.3857501447200775
setp: 900, Loss: 0.3558069169521332
setp: 1000, Loss: 0.36330369114875793
setp: 1100, Loss: 0.35478684306144714
setp: 1200, Loss: 0.33084627985954285
setp: 1300, Loss: 0.3523728847503662
setp: 1400, Loss: 0.3402288556098938
setp: 1500, Loss: 0.3680509924888611
setp: 1600, Loss: 0.3274610936641693
setp: 1700, Loss: 0.34895405173301697
setp: 1800, Loss: 0.3251216411590576
setp: 1900, Loss: 0.32169488072395325
setp: 2000, Loss: 0.351178914308548
setp: 2100, Loss: 0.3356683552265167
setp: 2200, Loss: 0.3256824314594269
setp: 2300, Loss: 0.3286023736000061
setp: 2400, Loss: 0.3331039547920227
setp: 2500, Loss: 0.33365246653556824
setp: 2600, Loss: 0.35107383131980896
setp: 2700, Loss: 0.32644733786582947
setp: 2800, Loss: 0.33087679743766785
setp: 2900, Loss: 0.321084588766098
setp: 3000, Loss: 0.3182763457298279
setp: 3100, Loss: 0.31878262758255005
setp: 3200, Loss: 0.32238438725471497
setp: 3300, Loss: 0.3191942274570465
setp: 3400, Loss: 0.3220565617084503
setp: 3500, Loss: 0.31974154710769653
setp: 3600, Loss: 0.32109561562538147
setp: 3700, Loss: 0.3187086880207062
setp: 3800, Loss: 0.32094255089759827
setp: 3900, Loss: 0.320974737405777
setp: 4000, Loss: 0.3197726607322693
setp: 4100, Loss: 0.38252153992652893
setp: 4200, Loss: 0.3360527753829956
setp: 4300, Loss: 0.33203333616256714
setp: 4400, Loss: 0.31965869665145874
setp: 4500, Loss: 0.31935739517211914
setp: 4600, Loss: 0.3265002369880676
setp: 4700, Loss: 0.3458612561225891
setp: 4800, Loss: 0.31958484649658203
setp: 4900, Loss: 0.32010501623153687
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9971988795518207
recall: 1.0
F_score: 0.9985974754558204
validating...
acc: 0.8289473684210527
precision: 0.9204545454545454
recall: 0.81
F_score: 0.8617021276595745
******fold 1******
[367, 241]
training...
setp: 0, Loss: 0.67509526014328
setp: 100, Loss: 0.6632646322250366
setp: 200, Loss: 0.5538192391395569
setp: 300, Loss: 0.5203875303268433
setp: 400, Loss: 0.5187287330627441
setp: 500, Loss: 0.4476061165332794
setp: 600, Loss: 0.4266236424446106
setp: 700, Loss: 0.4309265613555908
setp: 800, Loss: 0.3742121458053589
setp: 900, Loss: 0.39295169711112976
setp: 1000, Loss: 0.3676486313343048
setp: 1100, Loss: 0.39528554677963257
setp: 1200, Loss: 0.3257385194301605
setp: 1300, Loss: 0.3235563039779663
setp: 1400, Loss: 0.3271859884262085
setp: 1500, Loss: 0.34026265144348145
setp: 1600, Loss: 0.3205118477344513
setp: 1700, Loss: 0.326604962348938
setp: 1800, Loss: 0.3211926221847534
setp: 1900, Loss: 0.32227060198783875
setp: 2000, Loss: 0.3199644982814789
setp: 2100, Loss: 0.32424208521842957
setp: 2200, Loss: 0.3254436254501343
setp: 2300, Loss: 0.3301341235637665
setp: 2400, Loss: 0.32320648431777954
setp: 2500, Loss: 0.3319207429885864
setp: 2600, Loss: 0.319588303565979
setp: 2700, Loss: 0.3184659481048584
setp: 2800, Loss: 0.320739209651947
setp: 2900, Loss: 0.32472339272499084
setp: 3000, Loss: 0.31890034675598145
setp: 3100, Loss: 0.32112202048301697
setp: 3200, Loss: 0.3189297616481781
setp: 3300, Loss: 0.3220813572406769
setp: 3400, Loss: 0.3260619044303894
setp: 3500, Loss: 0.33921822905540466
setp: 3600, Loss: 0.327432245016098
setp: 3700, Loss: 0.3209573030471802
setp: 3800, Loss: 0.31894731521606445
setp: 3900, Loss: 0.3190002143383026
setp: 4000, Loss: 0.3195819854736328
setp: 4100, Loss: 0.3190905451774597
setp: 4200, Loss: 0.31986668705940247
setp: 4300, Loss: 0.37723565101623535
setp: 4400, Loss: 0.3541984260082245
setp: 4500, Loss: 0.31869977712631226
setp: 4600, Loss: 0.3174535632133484
setp: 4700, Loss: 0.31961601972579956
setp: 4800, Loss: 0.32086309790611267
setp: 4900, Loss: 0.3179713785648346
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9972677595628415
recall: 0.9945504087193461
F_score: 0.9959072305593452
validating...
acc: 0.8618421052631579
precision: 0.8469387755102041
recall: 0.9325842696629213
F_score: 0.8877005347593584
******fold 2******
[365, 243]
training...
setp: 0, Loss: 0.7233651280403137
setp: 100, Loss: 0.6802886128425598
setp: 200, Loss: 0.585763156414032
setp: 300, Loss: 0.5371113419532776
setp: 400, Loss: 0.5656735897064209
setp: 500, Loss: 0.46135473251342773
setp: 600, Loss: 0.4563526511192322
setp: 700, Loss: 0.37331876158714294
setp: 800, Loss: 0.3599867820739746
setp: 900, Loss: 0.3915550708770752
setp: 1000, Loss: 0.3690640926361084
setp: 1100, Loss: 0.3379073739051819
setp: 1200, Loss: 0.35788294672966003
setp: 1300, Loss: 0.3255448341369629
setp: 1400, Loss: 0.3491971790790558
setp: 1500, Loss: 0.3573315143585205
setp: 1600, Loss: 0.32456061244010925
setp: 1700, Loss: 0.3228009343147278
setp: 1800, Loss: 0.38254058361053467
setp: 1900, Loss: 0.39738285541534424
setp: 2000, Loss: 0.33246636390686035
setp: 2100, Loss: 0.3235052227973938
setp: 2200, Loss: 0.3308618664741516
setp: 2300, Loss: 0.3234422504901886
setp: 2400, Loss: 0.327659547328949
setp: 2500, Loss: 0.3267900049686432
setp: 2600, Loss: 0.3202892243862152
setp: 2700, Loss: 0.32095158100128174
setp: 2800, Loss: 0.32080888748168945
setp: 2900, Loss: 0.32151317596435547
setp: 3000, Loss: 0.3199321925640106
setp: 3100, Loss: 0.32041049003601074
setp: 3200, Loss: 0.3195154070854187
setp: 3300, Loss: 0.32294562458992004
setp: 3400, Loss: 0.32025402784347534
setp: 3500, Loss: 0.3205624520778656
setp: 3600, Loss: 0.3207448720932007
setp: 3700, Loss: 0.35594239830970764
setp: 3800, Loss: 0.4350851774215698
setp: 3900, Loss: 0.3268986642360687
setp: 4000, Loss: 0.32213664054870605
setp: 4100, Loss: 0.3192366063594818
setp: 4200, Loss: 0.32146042585372925
setp: 4300, Loss: 0.3198026418685913
setp: 4400, Loss: 0.32035595178604126
setp: 4500, Loss: 0.31985998153686523
setp: 4600, Loss: 0.31996089220046997
setp: 4700, Loss: 0.32152828574180603
setp: 4800, Loss: 0.3924996554851532
setp: 4900, Loss: 0.32720431685447693
training successfully ended.
validating...
acc: 0.993421052631579
precision: 0.9945205479452055
recall: 0.9945205479452055
F_score: 0.9945205479452055
validating...
acc: 0.9407894736842105
precision: 0.9456521739130435
recall: 0.9560439560439561
F_score: 0.9508196721311475
******fold 3******
[363, 245]
training...
setp: 0, Loss: 0.7009907364845276
setp: 100, Loss: 0.6730044484138489
setp: 200, Loss: 0.6097936034202576
setp: 300, Loss: 0.6002013087272644
setp: 400, Loss: 0.6331298351287842
setp: 500, Loss: 0.5920079350471497
setp: 600, Loss: 0.4675081670284271
setp: 700, Loss: 0.3754569888114929
setp: 800, Loss: 0.4079497456550598
setp: 900, Loss: 0.3925522267818451
setp: 1000, Loss: 0.3816872239112854
setp: 1100, Loss: 0.3456546366214752
setp: 1200, Loss: 0.3380182981491089
setp: 1300, Loss: 0.38129308819770813
setp: 1400, Loss: 0.3608861565589905
setp: 1500, Loss: 0.3644189238548279
setp: 1600, Loss: 0.32537803053855896
setp: 1700, Loss: 0.3259846270084381
setp: 1800, Loss: 0.32947874069213867
setp: 1900, Loss: 0.372069388628006
setp: 2000, Loss: 0.3221036195755005
setp: 2100, Loss: 0.3214915990829468
setp: 2200, Loss: 0.3235827088356018
setp: 2300, Loss: 0.322914183139801
setp: 2400, Loss: 0.3194817900657654
setp: 2500, Loss: 0.31994685530662537
setp: 2600, Loss: 0.31895723938941956
setp: 2700, Loss: 0.33323216438293457
setp: 2800, Loss: 0.32782188057899475
setp: 2900, Loss: 0.3216524124145508
setp: 3000, Loss: 0.31976720690727234
setp: 3100, Loss: 0.3199288845062256
setp: 3200, Loss: 0.3205273747444153
setp: 3300, Loss: 0.32383421063423157
setp: 3400, Loss: 0.32158130407333374
setp: 3500, Loss: 0.320710688829422
setp: 3600, Loss: 0.3225446939468384
setp: 3700, Loss: 0.33159351348876953
setp: 3800, Loss: 0.3690697252750397
setp: 3900, Loss: 0.3206172287464142
setp: 4000, Loss: 0.31907275319099426
setp: 4100, Loss: 0.32074910402297974
setp: 4200, Loss: 0.32108694314956665
setp: 4300, Loss: 0.3182350695133209
setp: 4400, Loss: 0.3194749653339386
setp: 4500, Loss: 0.318367063999176
setp: 4600, Loss: 0.32002753019332886
setp: 4700, Loss: 0.3206363022327423
setp: 4800, Loss: 0.3215186893939972
setp: 4900, Loss: 0.320115864276886
training successfully ended.
validating...
acc: 0.9111842105263158
precision: 0.9936102236421726
recall: 0.8567493112947658
F_score: 0.9201183431952663
validating...
acc: 0.8618421052631579
precision: 0.95
recall: 0.8172043010752689
F_score: 0.8786127167630059
******fold 4******
[373, 235]
training...
setp: 0, Loss: 0.7231868505477905
setp: 100, Loss: 0.6149251461029053
setp: 200, Loss: 0.6167229413986206
setp: 300, Loss: 0.5328572392463684
setp: 400, Loss: 0.522253692150116
setp: 500, Loss: 0.4647377133369446
setp: 600, Loss: 0.453793466091156
setp: 700, Loss: 0.40682679414749146
setp: 800, Loss: 0.34003084897994995
setp: 900, Loss: 0.35239121317863464
setp: 1000, Loss: 0.34391355514526367
setp: 1100, Loss: 0.3259652554988861
setp: 1200, Loss: 0.33607664704322815
setp: 1300, Loss: 0.3561413884162903
setp: 1400, Loss: 0.3233603835105896
setp: 1500, Loss: 0.32441246509552
setp: 1600, Loss: 0.3239663541316986
setp: 1700, Loss: 0.32486164569854736
setp: 1800, Loss: 0.3205554485321045
setp: 1900, Loss: 0.3790968954563141
setp: 2000, Loss: 0.32298582792282104
setp: 2100, Loss: 0.4192768633365631
setp: 2200, Loss: 0.34150880575180054
setp: 2300, Loss: 0.32890403270721436
setp: 2400, Loss: 0.32110244035720825
setp: 2500, Loss: 0.323747843503952
setp: 2600, Loss: 0.32075443863868713
setp: 2700, Loss: 0.3207482397556305
setp: 2800, Loss: 0.33071988821029663
setp: 2900, Loss: 0.32320675253868103
setp: 3000, Loss: 0.32371270656585693
setp: 3100, Loss: 0.3201361894607544
setp: 3200, Loss: 0.3524992763996124
setp: 3300, Loss: 0.3332146406173706
setp: 3400, Loss: 0.3308164179325104
setp: 3500, Loss: 0.31947189569473267
setp: 3600, Loss: 0.3492295444011688
setp: 3700, Loss: 0.3283112347126007
setp: 3800, Loss: 0.3564908504486084
setp: 3900, Loss: 0.3208799958229065
setp: 4000, Loss: 0.32438555359840393
setp: 4100, Loss: 0.32086819410324097
setp: 4200, Loss: 0.3447088599205017
setp: 4300, Loss: 0.32245534658432007
setp: 4400, Loss: 0.32030627131462097
setp: 4500, Loss: 0.3238489329814911
setp: 4600, Loss: 0.31905272603034973
setp: 4700, Loss: 0.3202076256275177
setp: 4800, Loss: 0.3202860355377197
setp: 4900, Loss: 0.31909292936325073
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8486842105263158
precision: 0.8333333333333334
recall: 0.9036144578313253
F_score: 0.8670520231213873
model saved.
avg_acc: 0.868421052631579, avg_f_score: 0.8891774148868947
==========arousal==========
******fold 0******
[359, 249]
training...
setp: 0, Loss: 0.7057851552963257
setp: 100, Loss: 0.6336994767189026
setp: 200, Loss: 0.6442699432373047
setp: 300, Loss: 0.5999563932418823
setp: 400, Loss: 0.6254699230194092
setp: 500, Loss: 0.5754160284996033
setp: 600, Loss: 0.4947991371154785
setp: 700, Loss: 0.4846281409263611
setp: 800, Loss: 0.473205029964447
setp: 900, Loss: 0.4213218092918396
setp: 1000, Loss: 0.3635343015193939
setp: 1100, Loss: 0.4021757245063782
setp: 1200, Loss: 0.3550299108028412
setp: 1300, Loss: 0.3618132472038269
setp: 1400, Loss: 0.3338163495063782
setp: 1500, Loss: 0.3333253562450409
setp: 1600, Loss: 0.3257638216018677
setp: 1700, Loss: 0.3659716248512268
setp: 1800, Loss: 0.3550274968147278
setp: 1900, Loss: 0.3654271066188812
setp: 2000, Loss: 0.3278370797634125
setp: 2100, Loss: 0.3523213863372803
setp: 2200, Loss: 0.32640722393989563
setp: 2300, Loss: 0.3304751515388489
setp: 2400, Loss: 0.32398727536201477
setp: 2500, Loss: 0.3199314773082733
setp: 2600, Loss: 0.32084476947784424
setp: 2700, Loss: 0.3230712413787842
setp: 2800, Loss: 0.3227299153804779
setp: 2900, Loss: 0.3196142315864563
setp: 3000, Loss: 0.351644366979599
setp: 3100, Loss: 0.3225313425064087
setp: 3200, Loss: 0.32252201437950134
setp: 3300, Loss: 0.3221924602985382
setp: 3400, Loss: 0.32857048511505127
setp: 3500, Loss: 0.3209632337093353
setp: 3600, Loss: 0.32094839215278625
setp: 3700, Loss: 0.32904210686683655
setp: 3800, Loss: 0.3512657880783081
setp: 3900, Loss: 0.32076698541641235
setp: 4000, Loss: 0.32069677114486694
setp: 4100, Loss: 0.32112476229667664
setp: 4200, Loss: 0.32436394691467285
setp: 4300, Loss: 0.321498841047287
setp: 4400, Loss: 0.31909993290901184
setp: 4500, Loss: 0.3196210563182831
setp: 4600, Loss: 0.3213574290275574
setp: 4700, Loss: 0.3210044801235199
setp: 4800, Loss: 0.3190356194972992
setp: 4900, Loss: 0.35040512681007385
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9972222222222222
recall: 1.0
F_score: 0.9986091794158554
validating...
acc: 0.8881578947368421
precision: 0.9545454545454546
recall: 0.865979381443299
F_score: 0.9081081081081082
******fold 1******
[363, 245]
training...
setp: 0, Loss: 0.6876058578491211
setp: 100, Loss: 0.6204838156700134
setp: 200, Loss: 0.5552311539649963
setp: 300, Loss: 0.5533565878868103
setp: 400, Loss: 0.5161610245704651
setp: 500, Loss: 0.4384523630142212
setp: 600, Loss: 0.4451625347137451
setp: 700, Loss: 0.38463252782821655
setp: 800, Loss: 0.39712679386138916
setp: 900, Loss: 0.3675719201564789
setp: 1000, Loss: 0.36583539843559265
setp: 1100, Loss: 0.3863452672958374
setp: 1200, Loss: 0.3280467092990875
setp: 1300, Loss: 0.32614341378211975
setp: 1400, Loss: 0.3549472689628601
setp: 1500, Loss: 0.40215450525283813
setp: 1600, Loss: 0.3563941717147827
setp: 1700, Loss: 0.3841301202774048
setp: 1800, Loss: 0.35343503952026367
setp: 1900, Loss: 0.32366541028022766
setp: 2000, Loss: 0.32407262921333313
setp: 2100, Loss: 0.3534081280231476
setp: 2200, Loss: 0.3486708998680115
setp: 2300, Loss: 0.32207560539245605
setp: 2400, Loss: 0.3233115077018738
setp: 2500, Loss: 0.3642708659172058
setp: 2600, Loss: 0.3505173921585083
setp: 2700, Loss: 0.32099831104278564
setp: 2800, Loss: 0.34983211755752563
setp: 2900, Loss: 0.3502803146839142
setp: 3000, Loss: 0.34962987899780273
setp: 3100, Loss: 0.32045045495033264
setp: 3200, Loss: 0.3244239389896393
setp: 3300, Loss: 0.3524966537952423
setp: 3400, Loss: 0.4086802899837494
setp: 3500, Loss: 0.3598536252975464
setp: 3600, Loss: 0.35067445039749146
setp: 3700, Loss: 0.3504102826118469
setp: 3800, Loss: 0.3212081789970398
setp: 3900, Loss: 0.31927788257598877
setp: 4000, Loss: 0.35159116983413696
setp: 4100, Loss: 0.3637435734272003
setp: 4200, Loss: 0.3283105492591858
setp: 4300, Loss: 0.3202859163284302
setp: 4400, Loss: 0.353425532579422
setp: 4500, Loss: 0.3521101772785187
setp: 4600, Loss: 0.3190295696258545
setp: 4700, Loss: 0.35040029883384705
setp: 4800, Loss: 0.34935861825942993
setp: 4900, Loss: 0.34899649024009705
training successfully ended.
validating...
acc: 0.9539473684210527
precision: 1.0
recall: 0.9228650137741047
F_score: 0.9598853868194842
validating...
acc: 0.868421052631579
precision: 0.974025974025974
recall: 0.8064516129032258
F_score: 0.8823529411764705
******fold 2******
[369, 239]
training...
setp: 0, Loss: 0.6664950847625732
setp: 100, Loss: 0.6180611252784729
setp: 200, Loss: 0.5286495685577393
setp: 300, Loss: 0.43009689450263977
setp: 400, Loss: 0.44942471385002136
setp: 500, Loss: 0.43874412775039673
setp: 600, Loss: 0.38695815205574036
setp: 700, Loss: 0.3877750635147095
setp: 800, Loss: 0.3948175609111786
setp: 900, Loss: 0.38551539182662964
setp: 1000, Loss: 0.3586671054363251
setp: 1100, Loss: 0.36029040813446045
setp: 1200, Loss: 0.35344088077545166
setp: 1300, Loss: 0.39499524235725403
setp: 1400, Loss: 0.31933435797691345
setp: 1500, Loss: 0.35298168659210205
setp: 1600, Loss: 0.35051560401916504
setp: 1700, Loss: 0.3516106903553009
setp: 1800, Loss: 0.34954947233200073
setp: 1900, Loss: 0.37184908986091614
setp: 2000, Loss: 0.35142838954925537
setp: 2100, Loss: 0.3530709147453308
setp: 2200, Loss: 0.3236522078514099
setp: 2300, Loss: 0.35325098037719727
setp: 2400, Loss: 0.37448203563690186
setp: 2500, Loss: 0.34893104434013367
setp: 2600, Loss: 0.3487814664840698
setp: 2700, Loss: 0.35057029128074646
setp: 2800, Loss: 0.350554496049881
setp: 2900, Loss: 0.3496575653553009
setp: 3000, Loss: 0.35004347562789917
setp: 3100, Loss: 0.3497520089149475
setp: 3200, Loss: 0.381294846534729
setp: 3300, Loss: 0.32101789116859436
setp: 3400, Loss: 0.35057157278060913
setp: 3500, Loss: 0.3494197428226471
setp: 3600, Loss: 0.35035088658332825
setp: 3700, Loss: 0.3485119342803955
setp: 3800, Loss: 0.35351917147636414
setp: 3900, Loss: 0.3503369092941284
setp: 4000, Loss: 0.3532705008983612
setp: 4100, Loss: 0.3170783221721649
setp: 4200, Loss: 0.36149197816848755
setp: 4300, Loss: 0.35037821531295776
setp: 4400, Loss: 0.34918227791786194
setp: 4500, Loss: 0.3485584259033203
setp: 4600, Loss: 0.35140129923820496
setp: 4700, Loss: 0.3506971299648285
setp: 4800, Loss: 0.40300846099853516
setp: 4900, Loss: 0.3506969213485718
training successfully ended.
validating...
acc: 0.9720394736842105
precision: 1.0
recall: 0.9539295392953929
F_score: 0.9764216366158114
validating...
acc: 0.9144736842105263
precision: 0.9021739130434783
recall: 0.9540229885057471
F_score: 0.9273743016759777
******fold 3******
[362, 246]
training...
setp: 0, Loss: 0.6927251815795898
setp: 100, Loss: 0.6799783706665039
setp: 200, Loss: 0.6311853528022766
setp: 300, Loss: 0.5881679654121399
setp: 400, Loss: 0.618017852306366
setp: 500, Loss: 0.4954856038093567
setp: 600, Loss: 0.4284572899341583
setp: 700, Loss: 0.4456552565097809
setp: 800, Loss: 0.37782707810401917
setp: 900, Loss: 0.3656163513660431
setp: 1000, Loss: 0.35749417543411255
setp: 1100, Loss: 0.32558244466781616
setp: 1200, Loss: 0.34290868043899536
setp: 1300, Loss: 0.33513182401657104
setp: 1400, Loss: 0.3279327154159546
setp: 1500, Loss: 0.3589612543582916
setp: 1600, Loss: 0.32673969864845276
setp: 1700, Loss: 0.3235531747341156
setp: 1800, Loss: 0.31809306144714355
setp: 1900, Loss: 0.342679500579834
setp: 2000, Loss: 0.3262194097042084
setp: 2100, Loss: 0.3581483066082001
setp: 2200, Loss: 0.31747832894325256
setp: 2300, Loss: 0.31906765699386597
setp: 2400, Loss: 0.3172244131565094
setp: 2500, Loss: 0.32464131712913513
setp: 2600, Loss: 0.3190270960330963
setp: 2700, Loss: 0.31772202253341675
setp: 2800, Loss: 0.31672611832618713
setp: 2900, Loss: 0.31633660197257996
setp: 3000, Loss: 0.31692829728126526
setp: 3100, Loss: 0.31707796454429626
setp: 3200, Loss: 0.36292120814323425
setp: 3300, Loss: 0.32506921887397766
setp: 3400, Loss: 0.41487011313438416
setp: 3500, Loss: 0.3200637400150299
setp: 3600, Loss: 0.32142186164855957
setp: 3700, Loss: 0.31758028268814087
setp: 3800, Loss: 0.3197202980518341
setp: 3900, Loss: 0.3185979723930359
setp: 4000, Loss: 0.3504137694835663
setp: 4100, Loss: 0.3186225891113281
setp: 4200, Loss: 0.3196748197078705
setp: 4300, Loss: 0.31849560141563416
setp: 4400, Loss: 0.3164249062538147
setp: 4500, Loss: 0.3165701627731323
setp: 4600, Loss: 0.3180510699748993
setp: 4700, Loss: 0.3163999021053314
setp: 4800, Loss: 0.37402647733688354
setp: 4900, Loss: 0.31733018159866333
training successfully ended.
validating...
acc: 0.9671052631578947
precision: 0.9913793103448276
recall: 0.9530386740331491
F_score: 0.971830985915493
validating...
acc: 0.9210526315789473
precision: 0.9659090909090909
recall: 0.9042553191489362
F_score: 0.9340659340659342
******fold 4******
[371, 237]
training...
setp: 0, Loss: 0.6685217618942261
setp: 100, Loss: 0.6329298615455627
setp: 200, Loss: 0.6144775152206421
setp: 300, Loss: 0.5082882046699524
setp: 400, Loss: 0.45987629890441895
setp: 500, Loss: 0.41697609424591064
setp: 600, Loss: 0.4473944306373596
setp: 700, Loss: 0.413767009973526
setp: 800, Loss: 0.4164568781852722
setp: 900, Loss: 0.3555654287338257
setp: 1000, Loss: 0.3567429184913635
setp: 1100, Loss: 0.4069312512874603
setp: 1200, Loss: 0.4279457628726959
setp: 1300, Loss: 0.38099464774131775
setp: 1400, Loss: 0.3803540766239166
setp: 1500, Loss: 0.32338088750839233
setp: 1600, Loss: 0.4087633490562439
setp: 1700, Loss: 0.35374563932418823
setp: 1800, Loss: 0.3614467978477478
setp: 1900, Loss: 0.3595868647098541
setp: 2000, Loss: 0.3536100387573242
setp: 2100, Loss: 0.3317871689796448
setp: 2200, Loss: 0.32012662291526794
setp: 2300, Loss: 0.35173529386520386
setp: 2400, Loss: 0.3171525001525879
setp: 2500, Loss: 0.35729312896728516
setp: 2600, Loss: 0.35029321908950806
setp: 2700, Loss: 0.35471582412719727
setp: 2800, Loss: 0.34962010383605957
setp: 2900, Loss: 0.3508169651031494
setp: 3000, Loss: 0.3183269500732422
setp: 3100, Loss: 0.35504403710365295
setp: 3200, Loss: 0.3485438823699951
setp: 3300, Loss: 0.34919148683547974
setp: 3400, Loss: 0.3181631565093994
setp: 3500, Loss: 0.3501218259334564
setp: 3600, Loss: 0.3491010069847107
setp: 3700, Loss: 0.3505150377750397
setp: 3800, Loss: 0.35207700729370117
setp: 3900, Loss: 0.34861642122268677
setp: 4000, Loss: 0.37360724806785583
setp: 4100, Loss: 0.32277220487594604
setp: 4200, Loss: 0.3547041118144989
setp: 4300, Loss: 0.31784674525260925
setp: 4400, Loss: 0.3526975214481354
setp: 4500, Loss: 0.34887683391571045
setp: 4600, Loss: 0.34935832023620605
setp: 4700, Loss: 0.35028892755508423
setp: 4800, Loss: 0.348796010017395
setp: 4900, Loss: 0.3184034526348114
training successfully ended.
validating...
acc: 0.9769736842105263
precision: 0.9972144846796658
recall: 0.9649595687331537
F_score: 0.9808219178082191
validating...
acc: 0.868421052631579
precision: 0.8421052631578947
recall: 0.9411764705882353
F_score: 0.8888888888888888
model saved.
avg_acc: 0.8921052631578948, avg_f_score: 0.9081580347830759
-------------subject: 5-------------
==========valence==========
******fold 0******
[231, 377]
training...
setp: 0, Loss: 0.6753411889076233
setp: 100, Loss: 0.659837543964386
setp: 200, Loss: 0.6631254553794861
setp: 300, Loss: 0.5187550783157349
setp: 400, Loss: 0.4939808249473572
setp: 500, Loss: 0.505681574344635
setp: 600, Loss: 0.5340416431427002
setp: 700, Loss: 0.4899864196777344
setp: 800, Loss: 0.3959886133670807
setp: 900, Loss: 0.4734930694103241
setp: 1000, Loss: 0.4203576445579529
setp: 1100, Loss: 0.4302768409252167
setp: 1200, Loss: 0.4289441406726837
setp: 1300, Loss: 0.44085076451301575
setp: 1400, Loss: 0.44799208641052246
setp: 1500, Loss: 0.4593507647514343
setp: 1600, Loss: 0.351729154586792
setp: 1700, Loss: 0.35798895359039307
setp: 1800, Loss: 0.34384724497795105
setp: 1900, Loss: 0.32875022292137146
setp: 2000, Loss: 0.35069596767425537
setp: 2100, Loss: 0.3247220814228058
setp: 2200, Loss: 0.3222138583660126
setp: 2300, Loss: 0.31635433435440063
setp: 2400, Loss: 0.32044458389282227
setp: 2500, Loss: 0.31691357493400574
setp: 2600, Loss: 0.3309563398361206
setp: 2700, Loss: 0.3164204955101013
setp: 2800, Loss: 0.32903236150741577
setp: 2900, Loss: 0.33461424708366394
setp: 3000, Loss: 0.3210146427154541
setp: 3100, Loss: 0.3501805067062378
setp: 3200, Loss: 0.3486778140068054
setp: 3300, Loss: 0.3489028215408325
setp: 3400, Loss: 0.31976330280303955
setp: 3500, Loss: 0.32135099172592163
setp: 3600, Loss: 0.31824564933776855
setp: 3700, Loss: 0.31879922747612
setp: 3800, Loss: 0.31786665320396423
setp: 3900, Loss: 0.3165942132472992
setp: 4000, Loss: 0.31934812664985657
setp: 4100, Loss: 0.3699836730957031
setp: 4200, Loss: 0.3181625306606293
setp: 4300, Loss: 0.31678342819213867
setp: 4400, Loss: 0.32330241799354553
setp: 4500, Loss: 0.31962692737579346
setp: 4600, Loss: 0.31674280762672424
setp: 4700, Loss: 0.31817588210105896
setp: 4800, Loss: 0.3179812431335449
setp: 4900, Loss: 0.31853294372558594
training successfully ended.
validating...
acc: 0.993421052631579
precision: 1.0
recall: 0.9826839826839827
F_score: 0.9912663755458515
validating...
acc: 0.9078947368421053
precision: 1.0
recall: 0.8082191780821918
F_score: 0.8939393939393939
******fold 1******
[249, 359]
training...
setp: 0, Loss: 0.7104619741439819
setp: 100, Loss: 0.6849239468574524
setp: 200, Loss: 0.6347547173500061
setp: 300, Loss: 0.46917426586151123
setp: 400, Loss: 0.4728415608406067
setp: 500, Loss: 0.8301488757133484
setp: 600, Loss: 0.4952818751335144
setp: 700, Loss: 0.5158102512359619
setp: 800, Loss: 0.4307883083820343
setp: 900, Loss: 0.4308622479438782
setp: 1000, Loss: 0.40148815512657166
setp: 1100, Loss: 0.4652712643146515
setp: 1200, Loss: 0.4472227096557617
setp: 1300, Loss: 0.4389611482620239
setp: 1400, Loss: 0.4550693929195404
setp: 1500, Loss: 0.4709106385707855
setp: 1600, Loss: 0.41647210717201233
setp: 1700, Loss: 0.40070149302482605
setp: 1800, Loss: 0.3719237148761749
setp: 1900, Loss: 0.34903737902641296
setp: 2000, Loss: 0.3190988600254059
setp: 2100, Loss: 0.3541581928730011
setp: 2200, Loss: 0.3599250018596649
setp: 2300, Loss: 0.33743131160736084
setp: 2400, Loss: 0.32816535234451294
setp: 2500, Loss: 0.32115501165390015
setp: 2600, Loss: 0.31980860233306885
setp: 2700, Loss: 0.3189270794391632
setp: 2800, Loss: 0.3196275532245636
setp: 2900, Loss: 0.3188301622867584
setp: 3000, Loss: 0.32289958000183105
setp: 3100, Loss: 0.3203125298023224
setp: 3200, Loss: 0.32612019777297974
setp: 3300, Loss: 0.35117751359939575
setp: 3400, Loss: 0.31748074293136597
setp: 3500, Loss: 0.31733939051628113
setp: 3600, Loss: 0.3189699053764343
setp: 3700, Loss: 0.31983813643455505
setp: 3800, Loss: 0.3175472021102905
setp: 3900, Loss: 0.31870999932289124
setp: 4000, Loss: 0.4136892259120941
setp: 4100, Loss: 0.33116018772125244
setp: 4200, Loss: 0.31800898909568787
setp: 4300, Loss: 0.31728553771972656
setp: 4400, Loss: 0.3183070719242096
setp: 4500, Loss: 0.31836628913879395
setp: 4600, Loss: 0.31755152344703674
setp: 4700, Loss: 0.31712543964385986
setp: 4800, Loss: 0.3190702199935913
setp: 4900, Loss: 0.31908857822418213
training successfully ended.
validating...
acc: 0.9851973684210527
precision: 1.0
recall: 0.963855421686747
F_score: 0.9815950920245399
validating...
acc: 0.8552631578947368
precision: 0.9024390243902439
recall: 0.6727272727272727
F_score: 0.7708333333333333
******fold 2******
[242, 366]
training...
setp: 0, Loss: 0.7193734049797058
setp: 100, Loss: 0.6737458109855652
setp: 200, Loss: 0.6562511324882507
setp: 300, Loss: 0.5924850106239319
setp: 400, Loss: 0.45239222049713135
setp: 500, Loss: 0.5633967518806458
setp: 600, Loss: 0.536554217338562
setp: 700, Loss: 0.43610501289367676
setp: 800, Loss: 0.38334035873413086
setp: 900, Loss: 0.469388872385025
setp: 1000, Loss: 0.44535285234451294
setp: 1100, Loss: 0.534424901008606
setp: 1200, Loss: 0.38503557443618774
setp: 1300, Loss: 0.4399358928203583
setp: 1400, Loss: 0.49269726872444153
setp: 1500, Loss: 0.3978448808193207
setp: 1600, Loss: 0.37497615814208984
setp: 1700, Loss: 0.3714655339717865
setp: 1800, Loss: 0.3625948429107666
setp: 1900, Loss: 0.33742785453796387
setp: 2000, Loss: 0.3268113434314728
setp: 2100, Loss: 0.34962597489356995
setp: 2200, Loss: 0.32324856519699097
setp: 2300, Loss: 0.31727513670921326
setp: 2400, Loss: 0.3265301287174225
setp: 2500, Loss: 0.32012447714805603
setp: 2600, Loss: 0.3210124671459198
setp: 2700, Loss: 0.3172091543674469
setp: 2800, Loss: 0.31830069422721863
setp: 2900, Loss: 0.3187180459499359
setp: 3000, Loss: 0.32192784547805786
setp: 3100, Loss: 0.3881991505622864
setp: 3200, Loss: 0.318126380443573
setp: 3300, Loss: 0.35191527009010315
setp: 3400, Loss: 0.3186382055282593
setp: 3500, Loss: 0.3182113468647003
setp: 3600, Loss: 0.31672099232673645
setp: 3700, Loss: 0.341938316822052
setp: 3800, Loss: 0.31978657841682434
setp: 3900, Loss: 0.33187419176101685
setp: 4000, Loss: 0.31901487708091736
setp: 4100, Loss: 0.31678733229637146
setp: 4200, Loss: 0.31664299964904785
setp: 4300, Loss: 0.31949135661125183
setp: 4400, Loss: 0.3167591392993927
setp: 4500, Loss: 0.3178531229496002
setp: 4600, Loss: 0.31608447432518005
setp: 4700, Loss: 0.32103198766708374
setp: 4800, Loss: 0.3202612102031708
setp: 4900, Loss: 0.3220732808113098
training successfully ended.
validating...
acc: 0.9572368421052632
precision: 1.0
recall: 0.8925619834710744
F_score: 0.9432314410480349
validating...
acc: 0.9144736842105263
precision: 1.0
recall: 0.7903225806451613
F_score: 0.8828828828828829
******fold 3******
[241, 367]
training...
setp: 0, Loss: 0.759708821773529
setp: 100, Loss: 0.675261378288269
setp: 200, Loss: 0.6442448496818542
setp: 300, Loss: 0.5424787998199463
setp: 400, Loss: 0.47423601150512695
setp: 500, Loss: 0.6237381100654602
setp: 600, Loss: 0.5207859873771667
setp: 700, Loss: 0.48698192834854126
setp: 800, Loss: 0.4282354414463043
setp: 900, Loss: 0.4609890878200531
setp: 1000, Loss: 0.3512673079967499
setp: 1100, Loss: 0.4353504180908203
setp: 1200, Loss: 0.38518160581588745
setp: 1300, Loss: 0.4014355540275574
setp: 1400, Loss: 0.35545286536216736
setp: 1500, Loss: 0.3898943364620209
setp: 1600, Loss: 0.33871686458587646
setp: 1700, Loss: 0.37972646951675415
setp: 1800, Loss: 0.33387303352355957
setp: 1900, Loss: 0.3346196413040161
setp: 2000, Loss: 0.32080501317977905
setp: 2100, Loss: 0.36026278138160706
setp: 2200, Loss: 0.3177003562450409
setp: 2300, Loss: 0.3163347840309143
setp: 2400, Loss: 0.3171354830265045
setp: 2500, Loss: 0.3238649070262909
setp: 2600, Loss: 0.3959859311580658
setp: 2700, Loss: 0.32451316714286804
setp: 2800, Loss: 0.3199986517429352
setp: 2900, Loss: 0.3209269344806671
setp: 3000, Loss: 0.31865233182907104
setp: 3100, Loss: 0.32017433643341064
setp: 3200, Loss: 0.318316787481308
setp: 3300, Loss: 0.31974494457244873
setp: 3400, Loss: 0.3177982270717621
setp: 3500, Loss: 0.31833285093307495
setp: 3600, Loss: 0.31909990310668945
setp: 3700, Loss: 0.31680867075920105
setp: 3800, Loss: 0.3172052800655365
setp: 3900, Loss: 0.32004502415657043
setp: 4000, Loss: 0.3863052427768707
setp: 4100, Loss: 0.31680744886398315
setp: 4200, Loss: 0.3180755376815796
setp: 4300, Loss: 0.3158475160598755
setp: 4400, Loss: 0.3175777494907379
setp: 4500, Loss: 0.3188115060329437
setp: 4600, Loss: 0.31693604588508606
setp: 4700, Loss: 0.31844648718833923
setp: 4800, Loss: 0.3181251287460327
setp: 4900, Loss: 0.31921491026878357
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 1.0
recall: 0.9875518672199171
F_score: 0.9937369519832986
validating...
acc: 0.9013157894736842
precision: 0.98
recall: 0.7777777777777778
F_score: 0.8672566371681417
******fold 4******
[253, 355]
training...
setp: 0, Loss: 0.6977707147598267
setp: 100, Loss: 0.6807295680046082
setp: 200, Loss: 0.5730412006378174
setp: 300, Loss: 0.6173794865608215
setp: 400, Loss: 0.5051221251487732
setp: 500, Loss: 0.4647829532623291
setp: 600, Loss: 0.47601792216300964
setp: 700, Loss: 0.46731138229370117
setp: 800, Loss: 0.46479353308677673
setp: 900, Loss: 0.44736215472221375
setp: 1000, Loss: 0.3685896396636963
setp: 1100, Loss: 0.41338831186294556
setp: 1200, Loss: 0.37633511424064636
setp: 1300, Loss: 0.3573496341705322
setp: 1400, Loss: 0.3699342906475067
setp: 1500, Loss: 0.35996130108833313
setp: 1600, Loss: 0.3207343518733978
setp: 1700, Loss: 0.3191693425178528
setp: 1800, Loss: 0.3223598897457123
setp: 1900, Loss: 0.3177832365036011
setp: 2000, Loss: 0.3194366693496704
setp: 2100, Loss: 0.32059308886528015
setp: 2200, Loss: 0.31947311758995056
setp: 2300, Loss: 0.31897515058517456
setp: 2400, Loss: 0.38213515281677246
setp: 2500, Loss: 0.3210512697696686
setp: 2600, Loss: 0.3586549162864685
setp: 2700, Loss: 0.31848597526550293
setp: 2800, Loss: 0.32036057114601135
setp: 2900, Loss: 0.31759220361709595
setp: 3000, Loss: 0.31867095828056335
setp: 3100, Loss: 0.31708723306655884
setp: 3200, Loss: 0.3178495764732361
setp: 3300, Loss: 0.3198484778404236
setp: 3400, Loss: 0.3477909564971924
setp: 3500, Loss: 0.38733768463134766
setp: 3600, Loss: 0.334358274936676
setp: 3700, Loss: 0.32974135875701904
setp: 3800, Loss: 0.31646206974983215
setp: 3900, Loss: 0.3182735741138458
setp: 4000, Loss: 0.3200310468673706
setp: 4100, Loss: 0.31798818707466125
setp: 4200, Loss: 0.3186163008213043
setp: 4300, Loss: 0.3166802227497101
setp: 4400, Loss: 0.3177880644798279
setp: 4500, Loss: 0.3930225968360901
setp: 4600, Loss: 0.3256451189517975
setp: 4700, Loss: 0.3193363845348358
setp: 4800, Loss: 0.31732994318008423
setp: 4900, Loss: 0.31792545318603516
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9960474308300395
F_score: 0.998019801980198
validating...
acc: 0.9473684210526315
precision: 0.9574468085106383
recall: 0.8823529411764706
F_score: 0.9183673469387754
model saved.
avg_acc: 0.9052631578947368, avg_f_score: 0.8666559188525053
==========arousal==========
******fold 0******
[311, 297]
training...
setp: 0, Loss: 0.7148658037185669
setp: 100, Loss: 0.6731438040733337
setp: 200, Loss: 0.6131548285484314
setp: 300, Loss: 0.4702175259590149
setp: 400, Loss: 0.4844403564929962
setp: 500, Loss: 0.4636084735393524
setp: 600, Loss: 0.34642720222473145
setp: 700, Loss: 0.44337698817253113
setp: 800, Loss: 0.3525799810886383
setp: 900, Loss: 0.41720688343048096
setp: 1000, Loss: 0.3534802198410034
setp: 1100, Loss: 0.3333534300327301
setp: 1200, Loss: 0.3182830214500427
setp: 1300, Loss: 0.31967398524284363
setp: 1400, Loss: 0.3245263993740082
setp: 1500, Loss: 0.3727315664291382
setp: 1600, Loss: 0.34901997447013855
setp: 1700, Loss: 0.3189699351787567
setp: 1800, Loss: 0.32676786184310913
setp: 1900, Loss: 0.3188454210758209
setp: 2000, Loss: 0.32952946424484253
setp: 2100, Loss: 0.3856630027294159
setp: 2200, Loss: 0.34942498803138733
setp: 2300, Loss: 0.33706656098365784
setp: 2400, Loss: 0.33242931962013245
setp: 2500, Loss: 0.33167436718940735
setp: 2600, Loss: 0.36446377635002136
setp: 2700, Loss: 0.31850987672805786
setp: 2800, Loss: 0.37492045760154724
setp: 2900, Loss: 0.31692612171173096
setp: 3000, Loss: 0.31811127066612244
setp: 3100, Loss: 0.3164424002170563
setp: 3200, Loss: 0.31803542375564575
setp: 3300, Loss: 0.31960612535476685
setp: 3400, Loss: 0.3178582787513733
setp: 3500, Loss: 0.3183726668357849
setp: 3600, Loss: 0.31795480847358704
setp: 3700, Loss: 0.31680309772491455
setp: 3800, Loss: 0.3628450632095337
setp: 3900, Loss: 0.31659841537475586
setp: 4000, Loss: 0.3176836371421814
setp: 4100, Loss: 0.3161029517650604
setp: 4200, Loss: 0.31924688816070557
setp: 4300, Loss: 0.31766441464424133
setp: 4400, Loss: 0.3167766034603119
setp: 4500, Loss: 0.6652621626853943
setp: 4600, Loss: 0.4011528491973877
setp: 4700, Loss: 0.43570801615715027
setp: 4800, Loss: 0.39067134261131287
setp: 4900, Loss: 0.35935088992118835
training successfully ended.
validating...
acc: 0.9851973684210527
precision: 0.9839743589743589
recall: 0.9871382636655949
F_score: 0.985553772070626
validating...
acc: 0.881578947368421
precision: 0.926829268292683
recall: 0.8636363636363636
F_score: 0.8941176470588236
******fold 1******
[317, 291]
training...
setp: 0, Loss: 0.7119923830032349
setp: 100, Loss: 0.6909282803535461
setp: 200, Loss: 0.6976267695426941
setp: 300, Loss: 0.6240390539169312
setp: 400, Loss: 0.5578385591506958
setp: 500, Loss: 0.4918493926525116
setp: 600, Loss: 0.4406777322292328
setp: 700, Loss: 0.438626229763031
setp: 800, Loss: 0.35216933488845825
setp: 900, Loss: 0.41032087802886963
setp: 1000, Loss: 0.3337160646915436
setp: 1100, Loss: 0.32518815994262695
setp: 1200, Loss: 0.32581818103790283
setp: 1300, Loss: 0.32239848375320435
setp: 1400, Loss: 0.3245565891265869
setp: 1500, Loss: 0.3551252484321594
setp: 1600, Loss: 0.32184919714927673
setp: 1700, Loss: 0.34772202372550964
setp: 1800, Loss: 0.3345800042152405
setp: 1900, Loss: 0.3199961483478546
setp: 2000, Loss: 0.3173983693122864
setp: 2100, Loss: 0.3494621515274048
setp: 2200, Loss: 0.3198004364967346
setp: 2300, Loss: 0.3168105483055115
setp: 2400, Loss: 0.31699612736701965
setp: 2500, Loss: 0.31745532155036926
setp: 2600, Loss: 0.3192444443702698
setp: 2700, Loss: 0.31704041361808777
setp: 2800, Loss: 0.31836068630218506
setp: 2900, Loss: 0.31874024868011475
setp: 3000, Loss: 0.3168482184410095
setp: 3100, Loss: 0.3165654242038727
setp: 3200, Loss: 0.3190332055091858
setp: 3300, Loss: 0.31951236724853516
setp: 3400, Loss: 0.319246381521225
setp: 3500, Loss: 0.31896471977233887
setp: 3600, Loss: 0.3176247477531433
setp: 3700, Loss: 0.3762378692626953
setp: 3800, Loss: 0.322380393743515
setp: 3900, Loss: 0.31665295362472534
setp: 4000, Loss: 0.3492887318134308
setp: 4100, Loss: 0.31828808784484863
setp: 4200, Loss: 0.3165052831172943
setp: 4300, Loss: 0.3164025843143463
setp: 4400, Loss: 0.3170659840106964
setp: 4500, Loss: 0.31915175914764404
setp: 4600, Loss: 0.31566187739372253
setp: 4700, Loss: 0.7119204998016357
setp: 4800, Loss: 0.5623541474342346
setp: 4900, Loss: 0.42065125703811646
training successfully ended.
validating...
acc: 0.90625
precision: 0.9850746268656716
recall: 0.832807570977918
F_score: 0.9025641025641027
validating...
acc: 0.8157894736842105
precision: 0.9354838709677419
recall: 0.7073170731707317
F_score: 0.8055555555555556
******fold 2******
[321, 287]
training...
setp: 0, Loss: 0.7275265455245972
setp: 100, Loss: 0.6762940287590027
setp: 200, Loss: 0.6923096179962158
setp: 300, Loss: 0.5923117399215698
setp: 400, Loss: 0.485839307308197
setp: 500, Loss: 0.4608894884586334
setp: 600, Loss: 0.4207710921764374
setp: 700, Loss: 0.40745508670806885
setp: 800, Loss: 0.4825509786605835
setp: 900, Loss: 0.359912633895874
setp: 1000, Loss: 0.3531051278114319
setp: 1100, Loss: 0.3232322633266449
setp: 1200, Loss: 0.34896624088287354
setp: 1300, Loss: 0.3558492660522461
setp: 1400, Loss: 0.3286578059196472
setp: 1500, Loss: 0.3591815233230591
setp: 1600, Loss: 0.3493236303329468
setp: 1700, Loss: 0.3176729083061218
setp: 1800, Loss: 0.3180343508720398
setp: 1900, Loss: 0.3178388774394989
setp: 2000, Loss: 0.35115107893943787
setp: 2100, Loss: 0.3180393576622009
setp: 2200, Loss: 0.36821573972702026
setp: 2300, Loss: 0.34355613589286804
setp: 2400, Loss: 0.3165057599544525
setp: 2500, Loss: 0.31641432642936707
setp: 2600, Loss: 0.3195294141769409
setp: 2700, Loss: 0.31693097949028015
setp: 2800, Loss: 0.3179002106189728
setp: 2900, Loss: 0.34738636016845703
setp: 3000, Loss: 0.31802162528038025
setp: 3100, Loss: 0.3175317049026489
setp: 3200, Loss: 0.31750401854515076
setp: 3300, Loss: 0.6889843344688416
setp: 3400, Loss: 0.5670738816261292
setp: 3500, Loss: 0.5487985610961914
setp: 3600, Loss: 0.3899863660335541
setp: 3700, Loss: 0.36485669016838074
setp: 3800, Loss: 0.41035208106040955
setp: 3900, Loss: 0.3955818712711334
setp: 4000, Loss: 0.42931511998176575
setp: 4100, Loss: 0.33606404066085815
setp: 4200, Loss: 0.3290627598762512
setp: 4300, Loss: 0.33640140295028687
setp: 4400, Loss: 0.3223656713962555
setp: 4500, Loss: 0.32466670870780945
setp: 4600, Loss: 0.31894955039024353
setp: 4700, Loss: 0.3198704719543457
setp: 4800, Loss: 0.3196997344493866
setp: 4900, Loss: 0.34925445914268494
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9938080495356038
recall: 1.0
F_score: 0.9968944099378882
validating...
acc: 0.881578947368421
precision: 0.8333333333333334
recall: 0.9615384615384616
F_score: 0.8928571428571429
******fold 3******
[321, 287]
training...
setp: 0, Loss: 0.6972412467002869
setp: 100, Loss: 0.6400354504585266
setp: 200, Loss: 0.6180906295776367
setp: 300, Loss: 0.564699113368988
setp: 400, Loss: 0.5399201512336731
setp: 500, Loss: 0.43130651116371155
setp: 600, Loss: 0.4114835262298584
setp: 700, Loss: 0.4302610158920288
setp: 800, Loss: 0.40651047229766846
setp: 900, Loss: 0.3675605058670044
setp: 1000, Loss: 0.4219195246696472
setp: 1100, Loss: 0.42894700169563293
setp: 1200, Loss: 0.35774534940719604
setp: 1300, Loss: 0.3833979666233063
setp: 1400, Loss: 0.41275495290756226
setp: 1500, Loss: 0.33577585220336914
setp: 1600, Loss: 0.3253088593482971
setp: 1700, Loss: 0.3509191870689392
setp: 1800, Loss: 0.32001054286956787
setp: 1900, Loss: 0.31741151213645935
setp: 2000, Loss: 0.3480049669742584
setp: 2100, Loss: 0.33161479234695435
setp: 2200, Loss: 0.31784212589263916
setp: 2300, Loss: 0.33761078119277954
setp: 2400, Loss: 0.3183239698410034
setp: 2500, Loss: 0.3176541030406952
setp: 2600, Loss: 0.3177184760570526
setp: 2700, Loss: 0.31902992725372314
setp: 2800, Loss: 0.32083967328071594
setp: 2900, Loss: 0.34981146454811096
setp: 3000, Loss: 0.3422428071498871
setp: 3100, Loss: 0.3184032142162323
setp: 3200, Loss: 0.3816957473754883
setp: 3300, Loss: 0.32012978196144104
setp: 3400, Loss: 0.3242836594581604
setp: 3500, Loss: 0.31836503744125366
setp: 3600, Loss: 0.35502615571022034
setp: 3700, Loss: 0.3182249367237091
setp: 3800, Loss: 0.31894421577453613
setp: 3900, Loss: 0.3188047409057617
setp: 4000, Loss: 0.32807376980781555
setp: 4100, Loss: 0.31870540976524353
setp: 4200, Loss: 0.320185124874115
setp: 4300, Loss: 0.3198661506175995
setp: 4400, Loss: 0.32306143641471863
setp: 4500, Loss: 0.31646066904067993
setp: 4600, Loss: 0.3280704617500305
setp: 4700, Loss: 0.3162699341773987
setp: 4800, Loss: 0.3204115927219391
setp: 4900, Loss: 0.3167913854122162
training successfully ended.
validating...
acc: 0.9605263157894737
precision: 0.9933554817275747
recall: 0.9314641744548287
F_score: 0.9614147909967845
validating...
acc: 0.8881578947368421
precision: 0.9552238805970149
recall: 0.8205128205128205
F_score: 0.8827586206896552
******fold 4******
[326, 282]
training...
setp: 0, Loss: 0.6978198885917664
setp: 100, Loss: 0.6919166445732117
setp: 200, Loss: 0.630843460559845
setp: 300, Loss: 0.6118137240409851
setp: 400, Loss: 0.5557587742805481
setp: 500, Loss: 0.5186067223548889
setp: 600, Loss: 0.4125034809112549
setp: 700, Loss: 0.4427444636821747
setp: 800, Loss: 0.4451882839202881
setp: 900, Loss: 0.45386451482772827
setp: 1000, Loss: 0.41896429657936096
setp: 1100, Loss: 0.3845914900302887
setp: 1200, Loss: 0.41705721616744995
setp: 1300, Loss: 0.4159727096557617
setp: 1400, Loss: 0.3520399332046509
setp: 1500, Loss: 0.32795408368110657
setp: 1600, Loss: 0.39186277985572815
setp: 1700, Loss: 0.375507116317749
setp: 1800, Loss: 0.3650972247123718
setp: 1900, Loss: 0.35056814551353455
setp: 2000, Loss: 0.3493497669696808
setp: 2100, Loss: 0.32105523347854614
setp: 2200, Loss: 0.3590773046016693
setp: 2300, Loss: 0.35021287202835083
setp: 2400, Loss: 0.323424369096756
setp: 2500, Loss: 0.3288840055465698
setp: 2600, Loss: 0.3186488747596741
setp: 2700, Loss: 0.3170223832130432
setp: 2800, Loss: 0.3199862837791443
setp: 2900, Loss: 0.34584927558898926
setp: 3000, Loss: 0.3282890319824219
setp: 3100, Loss: 0.31792834401130676
setp: 3200, Loss: 0.3472735285758972
setp: 3300, Loss: 0.3190139830112457
setp: 3400, Loss: 0.32645711302757263
setp: 3500, Loss: 0.32015129923820496
setp: 3600, Loss: 0.32307538390159607
setp: 3700, Loss: 0.3184516131877899
setp: 3800, Loss: 0.3171185255050659
setp: 3900, Loss: 0.3223426938056946
setp: 4000, Loss: 0.31713151931762695
setp: 4100, Loss: 0.32154521346092224
setp: 4200, Loss: 0.33953791856765747
setp: 4300, Loss: 0.32360199093818665
setp: 4400, Loss: 0.3224092721939087
setp: 4500, Loss: 0.3180389106273651
setp: 4600, Loss: 0.31728434562683105
setp: 4700, Loss: 0.32135921716690063
setp: 4800, Loss: 0.3198964297771454
setp: 4900, Loss: 0.32316339015960693
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9908814589665653
recall: 1.0
F_score: 0.9954198473282443
validating...
acc: 0.8881578947368421
precision: 0.8414634146341463
recall: 0.9452054794520548
F_score: 0.8903225806451612
model saved.
avg_acc: 0.8710526315789473, avg_f_score: 0.8731223093612677
-------------subject: 6-------------
==========valence==========
******fold 0******
[147, 461]
training...
setp: 0, Loss: 0.7163028120994568
setp: 100, Loss: 0.651563286781311
setp: 200, Loss: 0.5023056268692017
setp: 300, Loss: 0.4254591166973114
setp: 400, Loss: 0.3708912134170532
setp: 500, Loss: 0.37382498383522034
setp: 600, Loss: 0.32911890745162964
setp: 700, Loss: 0.3354177176952362
setp: 800, Loss: 0.3400171399116516
setp: 900, Loss: 0.31823793053627014
setp: 1000, Loss: 0.3207663595676422
setp: 1100, Loss: 0.3200092315673828
setp: 1200, Loss: 0.320492684841156
setp: 1300, Loss: 0.32019683718681335
setp: 1400, Loss: 0.31996986269950867
setp: 1500, Loss: 0.31808698177337646
setp: 1600, Loss: 0.31838691234588623
setp: 1700, Loss: 0.3178187310695648
setp: 1800, Loss: 0.3171193301677704
setp: 1900, Loss: 0.317199170589447
setp: 2000, Loss: 0.3169136047363281
setp: 2100, Loss: 0.6435438394546509
setp: 2200, Loss: 0.47644951939582825
setp: 2300, Loss: 0.3659331500530243
setp: 2400, Loss: 0.34936246275901794
setp: 2500, Loss: 0.3467143177986145
setp: 2600, Loss: 0.34817102551460266
setp: 2700, Loss: 0.329181432723999
setp: 2800, Loss: 0.3316568434238434
setp: 2900, Loss: 0.3330390155315399
setp: 3000, Loss: 0.3324333131313324
setp: 3100, Loss: 0.32427698373794556
setp: 3200, Loss: 0.3805381953716278
setp: 3300, Loss: 0.3245559632778168
setp: 3400, Loss: 0.33620771765708923
setp: 3500, Loss: 0.32124412059783936
setp: 3600, Loss: 0.32514461874961853
setp: 3700, Loss: 0.32498109340667725
setp: 3800, Loss: 0.3232852816581726
setp: 3900, Loss: 0.464813768863678
setp: 4000, Loss: 0.3283660113811493
setp: 4100, Loss: 0.3228166997432709
setp: 4200, Loss: 0.3250696361064911
setp: 4300, Loss: 0.32207557559013367
setp: 4400, Loss: 0.32456332445144653
setp: 4500, Loss: 0.32005757093429565
setp: 4600, Loss: 0.32201945781707764
setp: 4700, Loss: 0.3194584250450134
setp: 4800, Loss: 0.32154080271720886
setp: 4900, Loss: 0.3209979832172394
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9671052631578947
precision: 0.975
recall: 0.9069767441860465
F_score: 0.9397590361445783
******fold 1******
[158, 450]
training...
setp: 0, Loss: 0.6940091848373413
setp: 100, Loss: 0.6909973621368408
setp: 200, Loss: 0.6546193361282349
setp: 300, Loss: 0.4879665672779083
setp: 400, Loss: 0.35936054587364197
setp: 500, Loss: 0.3417089283466339
setp: 600, Loss: 0.3270368278026581
setp: 700, Loss: 0.32429981231689453
setp: 800, Loss: 0.32228168845176697
setp: 900, Loss: 0.3204265832901001
setp: 1000, Loss: 0.5478526949882507
setp: 1100, Loss: 0.32471275329589844
setp: 1200, Loss: 0.32266294956207275
setp: 1300, Loss: 0.3206798732280731
setp: 1400, Loss: 0.3203677237033844
setp: 1500, Loss: 0.32055026292800903
setp: 1600, Loss: 0.32076817750930786
setp: 1700, Loss: 0.3184729218482971
setp: 1800, Loss: 0.3191397190093994
setp: 1900, Loss: 0.32351696491241455
setp: 2000, Loss: 0.3167490065097809
setp: 2100, Loss: 0.31852200627326965
setp: 2200, Loss: 0.3175719678401947
setp: 2300, Loss: 0.3190951347351074
setp: 2400, Loss: 0.3185652196407318
setp: 2500, Loss: 0.31962722539901733
setp: 2600, Loss: 0.3192926049232483
setp: 2700, Loss: 0.3197014331817627
setp: 2800, Loss: 0.3186444044113159
setp: 2900, Loss: 0.39688271284103394
setp: 3000, Loss: 0.32021772861480713
setp: 3100, Loss: 0.3172639310359955
setp: 3200, Loss: 0.31842145323753357
setp: 3300, Loss: 0.3189356327056885
setp: 3400, Loss: 0.31819581985473633
setp: 3500, Loss: 0.3187435269355774
setp: 3600, Loss: 0.3187718689441681
setp: 3700, Loss: 0.31772518157958984
setp: 3800, Loss: 0.3186204135417938
setp: 3900, Loss: 0.31848523020744324
setp: 4000, Loss: 0.31864848732948303
setp: 4100, Loss: 0.3176593780517578
setp: 4200, Loss: 0.3182777762413025
setp: 4300, Loss: 0.31850895285606384
setp: 4400, Loss: 0.3182772099971771
setp: 4500, Loss: 0.31946566700935364
setp: 4600, Loss: 0.3178284764289856
setp: 4700, Loss: 0.32351288199424744
setp: 4800, Loss: 0.31866592168807983
setp: 4900, Loss: 0.31711286306381226
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9605263157894737
precision: 0.8823529411764706
recall: 0.9375
F_score: 0.9090909090909091
******fold 2******
[149, 459]
training...
setp: 0, Loss: 0.694271981716156
setp: 100, Loss: 0.6287009119987488
setp: 200, Loss: 0.514496922492981
setp: 300, Loss: 0.4819779396057129
setp: 400, Loss: 0.5317211151123047
setp: 500, Loss: 0.5205625891685486
setp: 600, Loss: 0.3764285743236542
setp: 700, Loss: 0.3478132486343384
setp: 800, Loss: 0.3703022003173828
setp: 900, Loss: 0.33464959263801575
setp: 1000, Loss: 0.34765133261680603
setp: 1100, Loss: 0.3791870176792145
setp: 1200, Loss: 0.3205338716506958
setp: 1300, Loss: 0.3206789791584015
setp: 1400, Loss: 0.3202042877674103
setp: 1500, Loss: 0.32019802927970886
setp: 1600, Loss: 0.31820181012153625
setp: 1700, Loss: 0.31893089413642883
setp: 1800, Loss: 0.3181805908679962
setp: 1900, Loss: 0.31952446699142456
setp: 2000, Loss: 0.3173641860485077
setp: 2100, Loss: 0.32029446959495544
setp: 2200, Loss: 0.3621193766593933
setp: 2300, Loss: 0.31725090742111206
setp: 2400, Loss: 0.3182683289051056
setp: 2500, Loss: 0.3195623755455017
setp: 2600, Loss: 0.3176978826522827
setp: 2700, Loss: 0.31860145926475525
setp: 2800, Loss: 0.3182952404022217
setp: 2900, Loss: 0.34861087799072266
setp: 3000, Loss: 0.31909310817718506
setp: 3100, Loss: 0.31601274013519287
setp: 3200, Loss: 0.339081346988678
setp: 3300, Loss: 0.31849244236946106
setp: 3400, Loss: 0.3183945417404175
setp: 3500, Loss: 0.31744128465652466
setp: 3600, Loss: 0.3178541660308838
setp: 3700, Loss: 0.31651976704597473
setp: 3800, Loss: 0.31740042567253113
setp: 3900, Loss: 0.31703513860702515
setp: 4000, Loss: 0.6986422538757324
setp: 4100, Loss: 0.3692129850387573
setp: 4200, Loss: 0.33164289593696594
setp: 4300, Loss: 0.33176833391189575
setp: 4400, Loss: 0.346208393573761
setp: 4500, Loss: 0.322824090719223
setp: 4600, Loss: 0.3207192122936249
setp: 4700, Loss: 0.3202710449695587
setp: 4800, Loss: 0.32177838683128357
setp: 4900, Loss: 0.31875574588775635
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
******fold 3******
[148, 460]
training...
setp: 0, Loss: 0.6929473280906677
setp: 100, Loss: 0.7126949429512024
setp: 200, Loss: 0.6881323456764221
setp: 300, Loss: 0.6015604138374329
setp: 400, Loss: 0.3906446397304535
setp: 500, Loss: 0.401454359292984
setp: 600, Loss: 0.3410705029964447
setp: 700, Loss: 0.3345346450805664
setp: 800, Loss: 0.3239195942878723
setp: 900, Loss: 0.3191618025302887
setp: 1000, Loss: 0.3199203610420227
setp: 1100, Loss: 0.3176910877227783
setp: 1200, Loss: 0.31841230392456055
setp: 1300, Loss: 0.3400439918041229
setp: 1400, Loss: 0.32117795944213867
setp: 1500, Loss: 0.3170810043811798
setp: 1600, Loss: 0.3176690936088562
setp: 1700, Loss: 0.31600603461265564
setp: 1800, Loss: 0.317117303609848
setp: 1900, Loss: 0.31784385442733765
setp: 2000, Loss: 0.31618204712867737
setp: 2100, Loss: 0.317554771900177
setp: 2200, Loss: 0.6898283958435059
setp: 2300, Loss: 0.5804581046104431
setp: 2400, Loss: 0.4880920648574829
setp: 2500, Loss: 0.353769987821579
setp: 2600, Loss: 0.3298923969268799
setp: 2700, Loss: 0.32928746938705444
setp: 2800, Loss: 0.32899484038352966
setp: 2900, Loss: 0.3257138133049011
setp: 3000, Loss: 0.32247745990753174
setp: 3100, Loss: 0.323019802570343
setp: 3200, Loss: 0.32350677251815796
setp: 3300, Loss: 0.3219742774963379
setp: 3400, Loss: 0.32556790113449097
setp: 3500, Loss: 0.32565420866012573
setp: 3600, Loss: 0.3216518759727478
setp: 3700, Loss: 0.32285115122795105
setp: 3800, Loss: 0.32401254773139954
setp: 3900, Loss: 0.3250837028026581
setp: 4000, Loss: 0.32207566499710083
setp: 4100, Loss: 0.3229430317878723
setp: 4200, Loss: 0.3254835605621338
setp: 4300, Loss: 0.3229309618473053
setp: 4400, Loss: 0.3274751305580139
setp: 4500, Loss: 0.32036668062210083
setp: 4600, Loss: 0.3199647068977356
setp: 4700, Loss: 0.3223959803581238
setp: 4800, Loss: 0.33046233654022217
setp: 4900, Loss: 0.3570547103881836
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.8636363636363636
recall: 0.9047619047619048
F_score: 0.8837209302325582
******fold 4******
[158, 450]
training...
setp: 0, Loss: 0.6994337439537048
setp: 100, Loss: 0.6514633893966675
setp: 200, Loss: 0.5543336868286133
setp: 300, Loss: 0.78016197681427
setp: 400, Loss: 0.39191219210624695
setp: 500, Loss: 0.5080075263977051
setp: 600, Loss: 0.3804425299167633
setp: 700, Loss: 0.4984672963619232
setp: 800, Loss: 0.38432765007019043
setp: 900, Loss: 0.34355559945106506
setp: 1000, Loss: 0.3614131212234497
setp: 1100, Loss: 0.3766769468784332
setp: 1200, Loss: 0.3242899775505066
setp: 1300, Loss: 0.3228442668914795
setp: 1400, Loss: 0.34741470217704773
setp: 1500, Loss: 0.32086825370788574
setp: 1600, Loss: 0.32166171073913574
setp: 1700, Loss: 0.3187086284160614
setp: 1800, Loss: 0.32219669222831726
setp: 1900, Loss: 0.316722571849823
setp: 2000, Loss: 0.3159770369529724
setp: 2100, Loss: 0.3176085650920868
setp: 2200, Loss: 0.31768810749053955
setp: 2300, Loss: 0.3202045261859894
setp: 2400, Loss: 0.31827402114868164
setp: 2500, Loss: 0.3194720149040222
setp: 2600, Loss: 0.3160957098007202
setp: 2700, Loss: 0.3192838728427887
setp: 2800, Loss: 0.317878395318985
setp: 2900, Loss: 0.31870028376579285
setp: 3000, Loss: 0.615252673625946
setp: 3100, Loss: 0.32502493262290955
setp: 3200, Loss: 0.3303155303001404
setp: 3300, Loss: 0.3176811635494232
setp: 3400, Loss: 0.3534221649169922
setp: 3500, Loss: 0.31696218252182007
setp: 3600, Loss: 0.3170582354068756
setp: 3700, Loss: 0.3159157335758209
setp: 3800, Loss: 0.3180293142795563
setp: 3900, Loss: 0.31717434525489807
setp: 4000, Loss: 0.31759193539619446
setp: 4100, Loss: 0.31715649366378784
setp: 4200, Loss: 0.317059725522995
setp: 4300, Loss: 0.31643420457839966
setp: 4400, Loss: 0.31805217266082764
setp: 4500, Loss: 0.31861045956611633
setp: 4600, Loss: 0.31581342220306396
setp: 4700, Loss: 0.3192056119441986
setp: 4800, Loss: 0.31663778424263
setp: 4900, Loss: 0.31552639603614807
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9144736842105263
precision: 0.7567567567567568
recall: 0.875
F_score: 0.8115942028985507
model saved.
avg_acc: 0.9552631578947368, avg_f_score: 0.9088330156733193
==========arousal==========
******fold 0******
[350, 258]
training...
setp: 0, Loss: 0.7310885190963745
setp: 100, Loss: 0.6856033802032471
setp: 200, Loss: 0.6641688346862793
setp: 300, Loss: 0.6572867035865784
setp: 400, Loss: 0.57783442735672
setp: 500, Loss: 0.5449932813644409
setp: 600, Loss: 0.4977020025253296
setp: 700, Loss: 0.6012725234031677
setp: 800, Loss: 0.4197918176651001
setp: 900, Loss: 0.4102539122104645
setp: 1000, Loss: 0.42027512192726135
setp: 1100, Loss: 0.3719020485877991
setp: 1200, Loss: 0.39394909143447876
setp: 1300, Loss: 0.33325427770614624
setp: 1400, Loss: 0.3342941701412201
setp: 1500, Loss: 0.38039910793304443
setp: 1600, Loss: 0.3209328055381775
setp: 1700, Loss: 0.31858205795288086
setp: 1800, Loss: 0.37005138397216797
setp: 1900, Loss: 0.3178398013114929
setp: 2000, Loss: 0.3333786725997925
setp: 2100, Loss: 0.3492267429828644
setp: 2200, Loss: 0.3188258111476898
setp: 2300, Loss: 0.31866568326950073
setp: 2400, Loss: 0.3172738254070282
setp: 2500, Loss: 0.3489780128002167
setp: 2600, Loss: 0.3183313310146332
setp: 2700, Loss: 0.31579864025115967
setp: 2800, Loss: 0.31784453988075256
setp: 2900, Loss: 0.3185635805130005
setp: 3000, Loss: 0.31938788294792175
setp: 3100, Loss: 0.3483753800392151
setp: 3200, Loss: 0.318635493516922
setp: 3300, Loss: 0.31734997034072876
setp: 3400, Loss: 0.3380550444126129
setp: 3500, Loss: 0.37586742639541626
setp: 3600, Loss: 0.33219417929649353
setp: 3700, Loss: 0.32351168990135193
setp: 3800, Loss: 0.3166208267211914
setp: 3900, Loss: 0.3174348771572113
setp: 4000, Loss: 0.3484869599342346
setp: 4100, Loss: 0.318181574344635
setp: 4200, Loss: 0.3187570571899414
setp: 4300, Loss: 0.31813737750053406
setp: 4400, Loss: 0.34907734394073486
setp: 4500, Loss: 0.31763505935668945
setp: 4600, Loss: 0.3160991370677948
setp: 4700, Loss: 0.3172137141227722
setp: 4800, Loss: 0.31900081038475037
setp: 4900, Loss: 0.318678081035614
training successfully ended.
validating...
acc: 0.9358552631578947
precision: 0.9968051118210862
recall: 0.8914285714285715
F_score: 0.9411764705882353
validating...
acc: 0.8552631578947368
precision: 0.9452054794520548
recall: 0.7931034482758621
F_score: 0.8624999999999999
******fold 1******
[339, 269]
training...
setp: 0, Loss: 0.709941029548645
setp: 100, Loss: 0.6846802234649658
setp: 200, Loss: 0.6819800138473511
setp: 300, Loss: 0.5106642842292786
setp: 400, Loss: 0.44227415323257446
setp: 500, Loss: 0.4443433880805969
setp: 600, Loss: 0.33844193816185
setp: 700, Loss: 0.32984229922294617
setp: 800, Loss: 0.3223755955696106
setp: 900, Loss: 0.3208569288253784
setp: 1000, Loss: 0.3226369619369507
setp: 1100, Loss: 0.3217821419239044
setp: 1200, Loss: 0.3286844491958618
setp: 1300, Loss: 0.3667899966239929
setp: 1400, Loss: 0.3430183231830597
setp: 1500, Loss: 0.323696106672287
setp: 1600, Loss: 0.3198951184749603
setp: 1700, Loss: 0.3180180490016937
setp: 1800, Loss: 0.3191045820713043
setp: 1900, Loss: 0.31761011481285095
setp: 2000, Loss: 0.3191753923892975
setp: 2100, Loss: 0.31885549426078796
setp: 2200, Loss: 0.319538414478302
setp: 2300, Loss: 0.31915223598480225
setp: 2400, Loss: 0.3192502558231354
setp: 2500, Loss: 0.3180398643016815
setp: 2600, Loss: 0.3190605342388153
setp: 2700, Loss: 0.31859537959098816
setp: 2800, Loss: 0.3983919620513916
setp: 2900, Loss: 0.33801645040512085
setp: 3000, Loss: 0.325515478849411
setp: 3100, Loss: 0.32074618339538574
setp: 3200, Loss: 0.3227460980415344
setp: 3300, Loss: 0.32093191146850586
setp: 3400, Loss: 0.32016921043395996
setp: 3500, Loss: 0.3197510540485382
setp: 3600, Loss: 0.3197900354862213
setp: 3700, Loss: 0.32037436962127686
setp: 3800, Loss: 0.320371150970459
setp: 3900, Loss: 0.3200531005859375
setp: 4000, Loss: 0.31991177797317505
setp: 4100, Loss: 0.32046234607696533
setp: 4200, Loss: 0.3203897774219513
setp: 4300, Loss: 0.31961357593536377
setp: 4400, Loss: 0.31928351521492004
setp: 4500, Loss: 0.3551461100578308
setp: 4600, Loss: 0.32445117831230164
setp: 4700, Loss: 0.38422828912734985
setp: 4800, Loss: 0.31966400146484375
setp: 4900, Loss: 0.3204297125339508
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.9578947368421052
recall: 0.9285714285714286
F_score: 0.9430051813471502
******fold 2******
[349, 259]
training...
setp: 0, Loss: 0.6923322081565857
setp: 100, Loss: 0.6082385182380676
setp: 200, Loss: 0.5384345650672913
setp: 300, Loss: 0.581301212310791
setp: 400, Loss: 0.4383765757083893
setp: 500, Loss: 0.4709564447402954
setp: 600, Loss: 0.482330322265625
setp: 700, Loss: 0.5599691867828369
setp: 800, Loss: 0.42334386706352234
setp: 900, Loss: 0.4854732155799866
setp: 1000, Loss: 0.39405757188796997
setp: 1100, Loss: 0.35481593012809753
setp: 1200, Loss: 0.35684579610824585
setp: 1300, Loss: 0.319570392370224
setp: 1400, Loss: 0.31840020418167114
setp: 1500, Loss: 0.35006073117256165
setp: 1600, Loss: 0.31774216890335083
setp: 1700, Loss: 0.35155925154685974
setp: 1800, Loss: 0.32025328278541565
setp: 1900, Loss: 0.34208551049232483
setp: 2000, Loss: 0.3166496455669403
setp: 2100, Loss: 0.3483494222164154
setp: 2200, Loss: 0.31924235820770264
setp: 2300, Loss: 0.31802845001220703
setp: 2400, Loss: 0.31813809275627136
setp: 2500, Loss: 0.31838259100914
setp: 2600, Loss: 0.318785697221756
setp: 2700, Loss: 0.31763648986816406
setp: 2800, Loss: 0.31956496834754944
setp: 2900, Loss: 0.3195614218711853
setp: 3000, Loss: 0.31855514645576477
setp: 3100, Loss: 0.3478408753871918
setp: 3200, Loss: 0.31750500202178955
setp: 3300, Loss: 0.31811320781707764
setp: 3400, Loss: 0.37572813034057617
setp: 3500, Loss: 0.3484874367713928
setp: 3600, Loss: 0.3188886344432831
setp: 3700, Loss: 0.3178306818008423
setp: 3800, Loss: 0.3172054588794708
setp: 3900, Loss: 0.31580206751823425
setp: 4000, Loss: 0.34812647104263306
setp: 4100, Loss: 0.31785863637924194
setp: 4200, Loss: 0.3174610733985901
setp: 4300, Loss: 0.31765493750572205
setp: 4400, Loss: 0.3181092143058777
setp: 4500, Loss: 0.3188903331756592
setp: 4600, Loss: 0.3176836669445038
setp: 4700, Loss: 0.318162739276886
setp: 4800, Loss: 0.31882888078689575
setp: 4900, Loss: 0.31865981221199036
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9971264367816092
recall: 0.994269340974212
F_score: 0.9956958393113342
validating...
acc: 0.9210526315789473
precision: 0.9523809523809523
recall: 0.9090909090909091
F_score: 0.9302325581395349
******fold 3******
[350, 258]
training...
setp: 0, Loss: 0.7396146059036255
setp: 100, Loss: 0.6994300484657288
setp: 200, Loss: 0.6579462289810181
setp: 300, Loss: 0.6178838014602661
setp: 400, Loss: 0.5935190916061401
setp: 500, Loss: 0.46784111857414246
setp: 600, Loss: 0.5197106599807739
setp: 700, Loss: 0.509707510471344
setp: 800, Loss: 0.3822785019874573
setp: 900, Loss: 0.3400893211364746
setp: 1000, Loss: 0.41218554973602295
setp: 1100, Loss: 0.35652798414230347
setp: 1200, Loss: 0.3538324534893036
setp: 1300, Loss: 0.3599254786968231
setp: 1400, Loss: 0.3272537589073181
setp: 1500, Loss: 0.4858910143375397
setp: 1600, Loss: 0.32141953706741333
setp: 1700, Loss: 0.3202698528766632
setp: 1800, Loss: 0.3219287395477295
setp: 1900, Loss: 0.31806403398513794
setp: 2000, Loss: 0.3193280100822449
setp: 2100, Loss: 0.3182133436203003
setp: 2200, Loss: 0.31816965341567993
setp: 2300, Loss: 0.3190177083015442
setp: 2400, Loss: 0.3491668105125427
setp: 2500, Loss: 0.3484232723712921
setp: 2600, Loss: 0.3484971523284912
setp: 2700, Loss: 0.31691622734069824
setp: 2800, Loss: 0.3178368806838989
setp: 2900, Loss: 0.4623030722141266
setp: 3000, Loss: 0.3216802775859833
setp: 3100, Loss: 0.3262512683868408
setp: 3200, Loss: 0.3214779496192932
setp: 3300, Loss: 0.316785603761673
setp: 3400, Loss: 0.35022464394569397
setp: 3500, Loss: 0.31721529364585876
setp: 3600, Loss: 0.3180610239505768
setp: 3700, Loss: 0.31993940472602844
setp: 3800, Loss: 0.31721073389053345
setp: 3900, Loss: 0.318488746881485
setp: 4000, Loss: 0.31775373220443726
setp: 4100, Loss: 0.31758439540863037
setp: 4200, Loss: 0.3182969093322754
setp: 4300, Loss: 0.3490579128265381
setp: 4400, Loss: 0.3487794101238251
setp: 4500, Loss: 0.31779658794403076
setp: 4600, Loss: 0.3170373737812042
setp: 4700, Loss: 0.5386909246444702
setp: 4800, Loss: 0.36834385991096497
setp: 4900, Loss: 0.3280072510242462
training successfully ended.
validating...
acc: 0.9786184210526315
precision: 0.9693593314763231
recall: 0.9942857142857143
F_score: 0.9816643159379409
validating...
acc: 0.8552631578947368
precision: 0.8651685393258427
recall: 0.8850574712643678
F_score: 0.875
******fold 4******
[360, 248]
training...
setp: 0, Loss: 0.6987889409065247
setp: 100, Loss: 0.613273024559021
setp: 200, Loss: 0.6718263030052185
setp: 300, Loss: 0.6136123538017273
setp: 400, Loss: 0.47735217213630676
setp: 500, Loss: 0.6107256412506104
setp: 600, Loss: 0.4590582847595215
setp: 700, Loss: 0.3745304346084595
setp: 800, Loss: 0.3546828329563141
setp: 900, Loss: 0.3253133296966553
setp: 1000, Loss: 0.3524424433708191
setp: 1100, Loss: 0.323696106672287
setp: 1200, Loss: 0.3505609631538391
setp: 1300, Loss: 0.34132352471351624
setp: 1400, Loss: 0.31719255447387695
setp: 1500, Loss: 0.3497275710105896
setp: 1600, Loss: 0.3207128643989563
setp: 1700, Loss: 0.3184036910533905
setp: 1800, Loss: 0.3497132956981659
setp: 1900, Loss: 0.31847354769706726
setp: 2000, Loss: 0.3260636627674103
setp: 2100, Loss: 0.3621474802494049
setp: 2200, Loss: 0.3177815079689026
setp: 2300, Loss: 0.3161535859107971
setp: 2400, Loss: 0.3176199197769165
setp: 2500, Loss: 0.3480134904384613
setp: 2600, Loss: 0.31808173656463623
setp: 2700, Loss: 0.3163675367832184
setp: 2800, Loss: 0.31790491938591003
setp: 2900, Loss: 0.31852173805236816
setp: 3000, Loss: 0.3188496530056
setp: 3100, Loss: 0.3480731248855591
setp: 3200, Loss: 0.3181402087211609
setp: 3300, Loss: 0.33315369486808777
setp: 3400, Loss: 0.31740549206733704
setp: 3500, Loss: 0.31623589992523193
setp: 3600, Loss: 0.3173474371433258
setp: 3700, Loss: 0.3190515637397766
setp: 3800, Loss: 0.3173637390136719
setp: 3900, Loss: 0.3168643116950989
setp: 4000, Loss: 0.31741705536842346
setp: 4100, Loss: 0.3179374039173126
setp: 4200, Loss: 0.31739869713783264
setp: 4300, Loss: 0.31948035955429077
setp: 4400, Loss: 0.3486417829990387
setp: 4500, Loss: 0.38016656041145325
setp: 4600, Loss: 0.31629815697669983
setp: 4700, Loss: 0.3162860870361328
setp: 4800, Loss: 0.3170982599258423
setp: 4900, Loss: 0.31803545355796814
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9972222222222222
recall: 0.9972222222222222
F_score: 0.9972222222222222
validating...
acc: 0.875
precision: 0.8625
recall: 0.8961038961038961
F_score: 0.8789808917197451
model saved.
avg_acc: 0.8868421052631579, avg_f_score: 0.897943726241286
-------------subject: 7-------------
==========valence==========
******fold 0******
[172, 436]
training...
setp: 0, Loss: 0.7094466090202332
setp: 100, Loss: 0.6003972291946411
setp: 200, Loss: 0.4959074556827545
setp: 300, Loss: 0.45405706763267517
setp: 400, Loss: 0.3967371881008148
setp: 500, Loss: 0.4528002142906189
setp: 600, Loss: 0.3731832504272461
setp: 700, Loss: 0.3502652049064636
setp: 800, Loss: 0.3874426484107971
setp: 900, Loss: 0.35759207606315613
setp: 1000, Loss: 0.347224622964859
setp: 1100, Loss: 0.41442984342575073
setp: 1200, Loss: 0.35440924763679504
setp: 1300, Loss: 0.3490787148475647
setp: 1400, Loss: 0.3211035430431366
setp: 1500, Loss: 0.3891970217227936
setp: 1600, Loss: 0.35010042786598206
setp: 1700, Loss: 0.35850989818573
setp: 1800, Loss: 0.31776106357574463
setp: 1900, Loss: 0.32116588950157166
setp: 2000, Loss: 0.3475923240184784
setp: 2100, Loss: 0.31680265069007874
setp: 2200, Loss: 0.32737669348716736
setp: 2300, Loss: 0.3418903052806854
setp: 2400, Loss: 0.3466220498085022
setp: 2500, Loss: 0.3173423111438751
setp: 2600, Loss: 0.3173067271709442
setp: 2700, Loss: 0.3477143943309784
setp: 2800, Loss: 0.3173980712890625
setp: 2900, Loss: 0.31918609142303467
setp: 3000, Loss: 0.3167434334754944
setp: 3100, Loss: 0.34678199887275696
setp: 3200, Loss: 0.31680288910865784
setp: 3300, Loss: 0.3171725273132324
setp: 3400, Loss: 0.3486015200614929
setp: 3500, Loss: 0.3169690668582916
setp: 3600, Loss: 0.5991748571395874
setp: 3700, Loss: 0.3307095170021057
setp: 3800, Loss: 0.34651410579681396
setp: 3900, Loss: 0.32011401653289795
setp: 4000, Loss: 0.32642170786857605
setp: 4100, Loss: 0.34659820795059204
setp: 4200, Loss: 0.31634554266929626
setp: 4300, Loss: 0.31741878390312195
setp: 4400, Loss: 0.31615686416625977
setp: 4500, Loss: 0.31756022572517395
setp: 4600, Loss: 0.3169982135295868
setp: 4700, Loss: 0.3166661262512207
setp: 4800, Loss: 0.3472806513309479
setp: 4900, Loss: 0.31658849120140076
training successfully ended.
validating...
acc: 0.9954128440366973
precision: 0.9954128440366973
recall: 0.9954128440366973
F_score: 0.9954128440366973
validating...
acc: 0.881578947368421
precision: 0.9318181818181818
recall: 0.7321428571428571
F_score: 0.82
******fold 1******
[183, 425]
training...
setp: 0, Loss: 0.8868131637573242
setp: 100, Loss: 0.5434998869895935
setp: 200, Loss: 0.5678964257240295
setp: 300, Loss: 0.5948303937911987
setp: 400, Loss: 0.5951970815658569
setp: 500, Loss: 0.5716555118560791
setp: 600, Loss: 0.6034566164016724
setp: 700, Loss: 0.46193912625312805
setp: 800, Loss: 0.48906514048576355
setp: 900, Loss: 0.5008559823036194
setp: 1000, Loss: 0.4460929036140442
setp: 1100, Loss: 0.4136415719985962
setp: 1200, Loss: 0.45490333437919617
setp: 1300, Loss: 0.4242711663246155
setp: 1400, Loss: 0.4081355333328247
setp: 1500, Loss: 0.47453153133392334
setp: 1600, Loss: 0.38143742084503174
setp: 1700, Loss: 0.4413728415966034
setp: 1800, Loss: 0.40557557344436646
setp: 1900, Loss: 0.47691693902015686
setp: 2000, Loss: 0.41063931584358215
setp: 2100, Loss: 0.44100677967071533
setp: 2200, Loss: 0.38087886571884155
setp: 2300, Loss: 0.45901328325271606
setp: 2400, Loss: 0.44055020809173584
setp: 2500, Loss: 0.4438875615596771
setp: 2600, Loss: 0.44114023447036743
setp: 2700, Loss: 0.38235461711883545
setp: 2800, Loss: 0.4429875910282135
setp: 2900, Loss: 0.4407641291618347
setp: 3000, Loss: 0.40989765524864197
setp: 3100, Loss: 0.4416530728340149
setp: 3200, Loss: 0.4095182418823242
setp: 3300, Loss: 0.37983131408691406
setp: 3400, Loss: 0.47326207160949707
setp: 3500, Loss: 0.37833815813064575
setp: 3600, Loss: 0.4405367970466614
setp: 3700, Loss: 0.3840360939502716
setp: 3800, Loss: 0.47279584407806396
setp: 3900, Loss: 0.4090966582298279
setp: 4000, Loss: 0.4409087598323822
setp: 4100, Loss: 0.37869933247566223
setp: 4200, Loss: 0.44101619720458984
setp: 4300, Loss: 0.4413406252861023
setp: 4400, Loss: 0.4432312548160553
setp: 4500, Loss: 0.44028884172439575
setp: 4600, Loss: 0.3818858563899994
setp: 4700, Loss: 0.4417671859264374
setp: 4800, Loss: 0.4996905028820038
setp: 4900, Loss: 0.42799124121665955
training successfully ended.
validating...
acc: 0.8898026315789473
precision: 0.881578947368421
recall: 0.73224043715847
F_score: 0.8
validating...
acc: 0.8486842105263158
precision: 0.8666666666666667
recall: 0.5777777777777777
F_score: 0.6933333333333332
******fold 2******
[187, 421]
training...
setp: 0, Loss: 0.7241466641426086
setp: 100, Loss: 0.6202569007873535
setp: 200, Loss: 0.6258388757705688
setp: 300, Loss: 0.5687904357910156
setp: 400, Loss: 0.5221834182739258
setp: 500, Loss: 0.5079158544540405
setp: 600, Loss: 0.48586657643318176
setp: 700, Loss: 0.5082271099090576
setp: 800, Loss: 0.5263420343399048
setp: 900, Loss: 0.5048962831497192
setp: 1000, Loss: 0.45883500576019287
setp: 1100, Loss: 0.535097062587738
setp: 1200, Loss: 0.41513141989707947
setp: 1300, Loss: 0.44910112023353577
setp: 1400, Loss: 0.4638884663581848
setp: 1500, Loss: 0.4444093406200409
setp: 1600, Loss: 0.44471442699432373
setp: 1700, Loss: 0.4138053059577942
setp: 1800, Loss: 0.4161551892757416
setp: 1900, Loss: 0.410609632730484
setp: 2000, Loss: 0.40910446643829346
setp: 2100, Loss: 0.47237929701805115
setp: 2200, Loss: 0.3799085021018982
setp: 2300, Loss: 0.3785407841205597
setp: 2400, Loss: 0.4719703197479248
setp: 2500, Loss: 0.44292348623275757
setp: 2600, Loss: 0.44152969121932983
setp: 2700, Loss: 0.4137411117553711
setp: 2800, Loss: 0.4717135429382324
setp: 2900, Loss: 0.4107135236263275
setp: 3000, Loss: 0.3794933557510376
setp: 3100, Loss: 0.40992239117622375
setp: 3200, Loss: 0.37864893674850464
setp: 3300, Loss: 0.41009512543678284
setp: 3400, Loss: 0.5277562737464905
setp: 3500, Loss: 0.4801524579524994
setp: 3600, Loss: 0.4112193286418915
setp: 3700, Loss: 0.41050565242767334
setp: 3800, Loss: 0.4126422703266144
setp: 3900, Loss: 0.40979406237602234
setp: 4000, Loss: 0.47441527247428894
setp: 4100, Loss: 0.39956843852996826
setp: 4200, Loss: 0.37960389256477356
setp: 4300, Loss: 0.4718441367149353
setp: 4400, Loss: 0.4411779046058655
setp: 4500, Loss: 0.44036629796028137
setp: 4600, Loss: 0.4123653471469879
setp: 4700, Loss: 0.4712045192718506
setp: 4800, Loss: 0.4103831648826599
setp: 4900, Loss: 0.3790549635887146
training successfully ended.
validating...
acc: 0.8980263157894737
precision: 0.9084967320261438
recall: 0.7433155080213903
F_score: 0.8176470588235294
validating...
acc: 0.8552631578947368
precision: 0.8064516129032258
recall: 0.6097560975609756
F_score: 0.6944444444444445
******fold 3******
[178, 430]
training...
setp: 0, Loss: 0.7042728066444397
setp: 100, Loss: 0.6472297310829163
setp: 200, Loss: 0.4646519124507904
setp: 300, Loss: 0.47418442368507385
setp: 400, Loss: 0.4831453561782837
setp: 500, Loss: 0.4022063910961151
setp: 600, Loss: 0.4147217273712158
setp: 700, Loss: 0.3823263347148895
setp: 800, Loss: 0.3521360754966736
setp: 900, Loss: 0.4748585820198059
setp: 1000, Loss: 0.378842294216156
setp: 1100, Loss: 0.385820597410202
setp: 1200, Loss: 0.36671698093414307
setp: 1300, Loss: 0.32283052802085876
setp: 1400, Loss: 0.37926071882247925
setp: 1500, Loss: 0.3157387375831604
setp: 1600, Loss: 0.3793031573295593
setp: 1700, Loss: 0.39521268010139465
setp: 1800, Loss: 0.3881417512893677
setp: 1900, Loss: 0.3394498825073242
setp: 2000, Loss: 0.37857985496520996
setp: 2100, Loss: 0.3534512221813202
setp: 2200, Loss: 0.3780866265296936
setp: 2300, Loss: 0.37859174609184265
setp: 2400, Loss: 0.37967249751091003
setp: 2500, Loss: 0.380422979593277
setp: 2600, Loss: 0.3791809678077698
setp: 2700, Loss: 0.37880685925483704
setp: 2800, Loss: 0.37978437542915344
setp: 2900, Loss: 0.37815868854522705
setp: 3000, Loss: 0.3476068079471588
setp: 3100, Loss: 0.34860190749168396
setp: 3200, Loss: 0.3470954895019531
setp: 3300, Loss: 0.34934768080711365
setp: 3400, Loss: 0.48048633337020874
setp: 3500, Loss: 0.4081868827342987
setp: 3600, Loss: 0.4208539128303528
setp: 3700, Loss: 0.39152976870536804
setp: 3800, Loss: 0.4078710973262787
setp: 3900, Loss: 0.3240927755832672
setp: 4000, Loss: 0.32907089591026306
setp: 4100, Loss: 0.3222072124481201
setp: 4200, Loss: 0.32463544607162476
setp: 4300, Loss: 0.3295990526676178
setp: 4400, Loss: 0.3209552764892578
setp: 4500, Loss: 0.32217341661453247
setp: 4600, Loss: 0.32006189227104187
setp: 4700, Loss: 0.3190433084964752
setp: 4800, Loss: 0.32070955634117126
setp: 4900, Loss: 0.3193962275981903
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9144736842105263
precision: 0.9512195121951219
recall: 0.78
F_score: 0.8571428571428571
******fold 4******
[192, 416]
training...
setp: 0, Loss: 0.7113408446311951
setp: 100, Loss: 0.6382508873939514
setp: 200, Loss: 0.5711731910705566
setp: 300, Loss: 0.5620680451393127
setp: 400, Loss: 0.5194342136383057
setp: 500, Loss: 0.4450368285179138
setp: 600, Loss: 0.4023391306400299
setp: 700, Loss: 0.4289169907569885
setp: 800, Loss: 0.4526163339614868
setp: 900, Loss: 0.43424564599990845
setp: 1000, Loss: 0.41321861743927
setp: 1100, Loss: 0.479928195476532
setp: 1200, Loss: 0.48455095291137695
setp: 1300, Loss: 0.44340211153030396
setp: 1400, Loss: 0.4275977313518524
setp: 1500, Loss: 0.43322139978408813
setp: 1600, Loss: 0.4687739312648773
setp: 1700, Loss: 0.41016316413879395
setp: 1800, Loss: 0.44215214252471924
setp: 1900, Loss: 0.38142335414886475
setp: 2000, Loss: 0.410433828830719
setp: 2100, Loss: 0.4326043725013733
setp: 2200, Loss: 0.43986764550209045
setp: 2300, Loss: 0.3819321393966675
setp: 2400, Loss: 0.37788739800453186
setp: 2500, Loss: 0.3197018802165985
setp: 2600, Loss: 0.35514652729034424
setp: 2700, Loss: 0.4539757966995239
setp: 2800, Loss: 0.4115617275238037
setp: 2900, Loss: 0.3819178342819214
setp: 3000, Loss: 0.4398038685321808
setp: 3100, Loss: 0.38416406512260437
setp: 3200, Loss: 0.36282050609588623
setp: 3300, Loss: 0.4022032618522644
setp: 3400, Loss: 0.34328052401542664
setp: 3500, Loss: 0.35397452116012573
setp: 3600, Loss: 0.3162834644317627
setp: 3700, Loss: 0.31953558325767517
setp: 3800, Loss: 0.3492316007614136
setp: 3900, Loss: 0.3179025948047638
setp: 4000, Loss: 0.34688183665275574
setp: 4100, Loss: 0.3173866868019104
setp: 4200, Loss: 0.34846970438957214
setp: 4300, Loss: 0.31653258204460144
setp: 4400, Loss: 0.3164234757423401
setp: 4500, Loss: 0.31578391790390015
setp: 4600, Loss: 0.33177727460861206
setp: 4700, Loss: 0.31753820180892944
setp: 4800, Loss: 0.3178611099720001
setp: 4900, Loss: 0.31717926263809204
training successfully ended.
validating...
acc: 0.9736842105263158
precision: 1.0
recall: 0.9166666666666666
F_score: 0.9565217391304348
validating...
acc: 0.9276315789473685
precision: 1.0
recall: 0.6944444444444444
F_score: 0.819672131147541
model saved.
avg_acc: 0.8855263157894736, avg_f_score: 0.7769185532136352
==========arousal==========
******fold 0******
[219, 389]
training...
setp: 0, Loss: 0.697246253490448
setp: 100, Loss: 0.6615852117538452
setp: 200, Loss: 0.6612967252731323
setp: 300, Loss: 0.6014538407325745
setp: 400, Loss: 0.5768970251083374
setp: 500, Loss: 0.4457755982875824
setp: 600, Loss: 0.5233309268951416
setp: 700, Loss: 0.5223715305328369
setp: 800, Loss: 0.5139622688293457
setp: 900, Loss: 0.4395673871040344
setp: 1000, Loss: 0.40867894887924194
setp: 1100, Loss: 0.3937961161136627
setp: 1200, Loss: 0.372153639793396
setp: 1300, Loss: 0.3695429861545563
setp: 1400, Loss: 0.32370734214782715
setp: 1500, Loss: 0.37896329164505005
setp: 1600, Loss: 0.3303045630455017
setp: 1700, Loss: 0.3191007673740387
setp: 1800, Loss: 0.3501991927623749
setp: 1900, Loss: 0.3183683156967163
setp: 2000, Loss: 0.31851881742477417
setp: 2100, Loss: 0.3469483256340027
setp: 2200, Loss: 0.34626060724258423
setp: 2300, Loss: 0.31960055232048035
setp: 2400, Loss: 0.32406798005104065
setp: 2500, Loss: 0.47673386335372925
setp: 2600, Loss: 0.3516596257686615
setp: 2700, Loss: 0.31673210859298706
setp: 2800, Loss: 0.34808892011642456
setp: 2900, Loss: 0.3777163624763489
setp: 3000, Loss: 0.34712326526641846
setp: 3100, Loss: 0.3168317973613739
setp: 3200, Loss: 0.31651851534843445
setp: 3300, Loss: 0.317537397146225
setp: 3400, Loss: 0.37867075204849243
setp: 3500, Loss: 0.31614407896995544
setp: 3600, Loss: 0.3164233863353729
setp: 3700, Loss: 0.3478274941444397
setp: 3800, Loss: 0.31602007150650024
setp: 3900, Loss: 0.3176193833351135
setp: 4000, Loss: 0.3465782403945923
setp: 4100, Loss: 0.34613901376724243
setp: 4200, Loss: 0.36597907543182373
setp: 4300, Loss: 0.3423265218734741
setp: 4400, Loss: 0.38235336542129517
setp: 4500, Loss: 0.3671014904975891
setp: 4600, Loss: 0.3559659421443939
setp: 4700, Loss: 0.34086552262306213
setp: 4800, Loss: 0.43561750650405884
setp: 4900, Loss: 0.3158329725265503
training successfully ended.
validating...
acc: 0.9884868421052632
precision: 1.0
recall: 0.9680365296803652
F_score: 0.9837587006960556
validating...
acc: 0.9276315789473685
precision: 0.9230769230769231
recall: 0.9090909090909091
F_score: 0.9160305343511451
******fold 1******
[232, 376]
training...
setp: 0, Loss: 0.7273406982421875
setp: 100, Loss: 0.6313737630844116
setp: 200, Loss: 0.5570502877235413
setp: 300, Loss: 0.6502623558044434
setp: 400, Loss: 0.4959131181240082
setp: 500, Loss: 0.4361329674720764
setp: 600, Loss: 0.4433954060077667
setp: 700, Loss: 0.558148205280304
setp: 800, Loss: 0.41125765442848206
setp: 900, Loss: 0.479532927274704
setp: 1000, Loss: 0.3678702712059021
setp: 1100, Loss: 0.36882203817367554
setp: 1200, Loss: 0.3501584827899933
setp: 1300, Loss: 0.37867531180381775
setp: 1400, Loss: 0.3541393578052521
setp: 1500, Loss: 0.3637631833553314
setp: 1600, Loss: 0.3273293972015381
setp: 1700, Loss: 0.33167555928230286
setp: 1800, Loss: 0.3197624981403351
setp: 1900, Loss: 0.316536009311676
setp: 2000, Loss: 0.34436461329460144
setp: 2100, Loss: 0.3264666497707367
setp: 2200, Loss: 0.318380743265152
setp: 2300, Loss: 0.3181469738483429
setp: 2400, Loss: 0.3210238218307495
setp: 2500, Loss: 0.31778863072395325
setp: 2600, Loss: 0.35268229246139526
setp: 2700, Loss: 0.3175102174282074
setp: 2800, Loss: 0.3173072040081024
setp: 2900, Loss: 0.31713488698005676
setp: 3000, Loss: 0.3203015923500061
setp: 3100, Loss: 0.36478275060653687
setp: 3200, Loss: 0.43939971923828125
setp: 3300, Loss: 0.33070728182792664
setp: 3400, Loss: 0.3474074602127075
setp: 3500, Loss: 0.3157154619693756
setp: 3600, Loss: 0.3155689537525177
setp: 3700, Loss: 0.3167963922023773
setp: 3800, Loss: 0.3163599967956543
setp: 3900, Loss: 0.31843990087509155
setp: 4000, Loss: 0.31572476029396057
setp: 4100, Loss: 0.3155878782272339
setp: 4200, Loss: 0.31747788190841675
setp: 4300, Loss: 0.31744033098220825
setp: 4400, Loss: 0.3162592053413391
setp: 4500, Loss: 0.34744712710380554
setp: 4600, Loss: 0.31712785363197327
setp: 4700, Loss: 0.3168296813964844
setp: 4800, Loss: 0.3163515031337738
setp: 4900, Loss: 0.31566688418388367
training successfully ended.
validating...
acc: 0.9917763157894737
precision: 0.9871244635193133
recall: 0.9913793103448276
F_score: 0.989247311827957
validating...
acc: 0.868421052631579
precision: 0.7619047619047619
recall: 0.9056603773584906
F_score: 0.8275862068965516
******fold 2******
[223, 385]
training...
setp: 0, Loss: 0.7261697053909302
setp: 100, Loss: 0.6103805303573608
setp: 200, Loss: 0.6756191253662109
setp: 300, Loss: 0.591054379940033
setp: 400, Loss: 0.4146914482116699
setp: 500, Loss: 0.4075912535190582
setp: 600, Loss: 0.32629844546318054
setp: 700, Loss: 0.32364052534103394
setp: 800, Loss: 0.33192887902259827
setp: 900, Loss: 0.319162517786026
setp: 1000, Loss: 0.3758426308631897
setp: 1100, Loss: 0.3196032643318176
setp: 1200, Loss: 0.35151442885398865
setp: 1300, Loss: 0.31625425815582275
setp: 1400, Loss: 0.35895252227783203
setp: 1500, Loss: 0.3487580418586731
setp: 1600, Loss: 0.3169316053390503
setp: 1700, Loss: 0.3513399064540863
setp: 1800, Loss: 0.31750449538230896
setp: 1900, Loss: 0.3199765682220459
setp: 2000, Loss: 0.3275684118270874
setp: 2100, Loss: 0.3318462371826172
setp: 2200, Loss: 0.34948691725730896
setp: 2300, Loss: 0.34514185786247253
setp: 2400, Loss: 0.3198792338371277
setp: 2500, Loss: 0.31957656145095825
setp: 2600, Loss: 0.31602704524993896
setp: 2700, Loss: 0.3517625331878662
setp: 2800, Loss: 0.31676244735717773
setp: 2900, Loss: 0.3583771288394928
setp: 3000, Loss: 0.3159530460834503
setp: 3100, Loss: 0.32884564995765686
setp: 3200, Loss: 0.3184451758861542
setp: 3300, Loss: 0.3209396004676819
setp: 3400, Loss: 0.34983721375465393
setp: 3500, Loss: 0.3295198678970337
setp: 3600, Loss: 0.32119008898735046
setp: 3700, Loss: 0.32003292441368103
setp: 3800, Loss: 0.32294130325317383
setp: 3900, Loss: 0.3184865117073059
setp: 4000, Loss: 0.3198147118091583
setp: 4100, Loss: 0.3160381019115448
setp: 4200, Loss: 0.31689417362213135
setp: 4300, Loss: 0.31763213872909546
setp: 4400, Loss: 0.3161623179912567
setp: 4500, Loss: 0.31627586483955383
setp: 4600, Loss: 0.31636691093444824
setp: 4700, Loss: 0.3177430331707001
setp: 4800, Loss: 0.3164738118648529
setp: 4900, Loss: 0.31625205278396606
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9955156950672646
F_score: 0.9977528089887641
validating...
acc: 0.9276315789473685
precision: 0.8805970149253731
recall: 0.9516129032258065
F_score: 0.9147286821705426
******fold 3******
[236, 372]
training...
setp: 0, Loss: 0.7107862830162048
setp: 100, Loss: 0.6720585227012634
setp: 200, Loss: 0.4679777920246124
setp: 300, Loss: 0.4574064016342163
setp: 400, Loss: 0.3878902494907379
setp: 500, Loss: 0.3556235134601593
setp: 600, Loss: 0.3509528338909149
setp: 700, Loss: 0.3313598036766052
setp: 800, Loss: 0.323016494512558
setp: 900, Loss: 0.3218654990196228
setp: 1000, Loss: 0.3204304873943329
setp: 1100, Loss: 0.32145002484321594
setp: 1200, Loss: 0.321928471326828
setp: 1300, Loss: 0.320652037858963
setp: 1400, Loss: 0.31787940859794617
setp: 1500, Loss: 0.32073673605918884
setp: 1600, Loss: 0.3179110288619995
setp: 1700, Loss: 0.3177250325679779
setp: 1800, Loss: 0.3177662491798401
setp: 1900, Loss: 0.5770543813705444
setp: 2000, Loss: 0.3424673080444336
setp: 2100, Loss: 0.3471970856189728
setp: 2200, Loss: 0.33091434836387634
setp: 2300, Loss: 0.3266924321651459
setp: 2400, Loss: 0.32968947291374207
setp: 2500, Loss: 0.3310895264148712
setp: 2600, Loss: 0.33189019560813904
setp: 2700, Loss: 0.3248392641544342
setp: 2800, Loss: 0.33888185024261475
setp: 2900, Loss: 0.33840444684028625
setp: 3000, Loss: 0.3224250376224518
setp: 3100, Loss: 0.32234612107276917
setp: 3200, Loss: 0.3219897747039795
setp: 3300, Loss: 0.3216298520565033
setp: 3400, Loss: 0.3228549659252167
setp: 3500, Loss: 0.3220004439353943
setp: 3600, Loss: 0.32175180315971375
setp: 3700, Loss: 0.3218252956867218
setp: 3800, Loss: 0.3228127658367157
setp: 3900, Loss: 0.3229559361934662
setp: 4000, Loss: 0.32253941893577576
setp: 4100, Loss: 0.32300665974617004
setp: 4200, Loss: 0.3231240212917328
setp: 4300, Loss: 0.3229156732559204
setp: 4400, Loss: 0.32455119490623474
setp: 4500, Loss: 0.3230118751525879
setp: 4600, Loss: 0.389778196811676
setp: 4700, Loss: 0.3436925709247589
setp: 4800, Loss: 0.3222278952598572
setp: 4900, Loss: 0.32192012667655945
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.881578947368421
precision: 0.746031746031746
recall: 0.9591836734693877
F_score: 0.8392857142857143
******fold 4******
[230, 378]
training...
setp: 0, Loss: 0.6976040601730347
setp: 100, Loss: 0.677075982093811
setp: 200, Loss: 0.561805009841919
setp: 300, Loss: 0.5178181529045105
setp: 400, Loss: 0.3385263681411743
setp: 500, Loss: 0.3603776693344116
setp: 600, Loss: 0.3351168930530548
setp: 700, Loss: 0.33088865876197815
setp: 800, Loss: 0.3217323422431946
setp: 900, Loss: 0.3189433217048645
setp: 1000, Loss: 0.32135769724845886
setp: 1100, Loss: 0.3192860186100006
setp: 1200, Loss: 0.31987088918685913
setp: 1300, Loss: 0.31972020864486694
setp: 1400, Loss: 0.3183146119117737
setp: 1500, Loss: 0.3373568058013916
setp: 1600, Loss: 0.3169625997543335
setp: 1700, Loss: 0.3170139491558075
setp: 1800, Loss: 0.3169730305671692
setp: 1900, Loss: 0.31799036264419556
setp: 2000, Loss: 0.3163885772228241
setp: 2100, Loss: 0.3623667061328888
setp: 2200, Loss: 0.3240129053592682
setp: 2300, Loss: 0.3162064552307129
setp: 2400, Loss: 0.32004982233047485
setp: 2500, Loss: 0.31683793663978577
setp: 2600, Loss: 0.3177490532398224
setp: 2700, Loss: 0.3185056447982788
setp: 2800, Loss: 0.3168749213218689
setp: 2900, Loss: 0.32611921429634094
setp: 3000, Loss: 0.3203147351741791
setp: 3100, Loss: 0.3182157278060913
setp: 3200, Loss: 0.3179610073566437
setp: 3300, Loss: 0.3166866600513458
setp: 3400, Loss: 0.31842368841171265
setp: 3500, Loss: 0.33586615324020386
setp: 3600, Loss: 0.3169032037258148
setp: 3700, Loss: 0.31566154956817627
setp: 3800, Loss: 0.31723248958587646
setp: 3900, Loss: 0.31610268354415894
setp: 4000, Loss: 0.31681060791015625
setp: 4100, Loss: 0.3178408145904541
setp: 4200, Loss: 0.3245844542980194
setp: 4300, Loss: 0.33968231081962585
setp: 4400, Loss: 0.32312411069869995
setp: 4500, Loss: 0.32260292768478394
setp: 4600, Loss: 0.3303011655807495
setp: 4700, Loss: 0.3167051672935486
setp: 4800, Loss: 0.3187541365623474
setp: 4900, Loss: 0.320437490940094
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9871244635193133
recall: 1.0
F_score: 0.9935205183585313
validating...
acc: 0.9144736842105263
precision: 0.8387096774193549
recall: 0.9454545454545454
F_score: 0.8888888888888888
model saved.
avg_acc: 0.9039473684210527, avg_f_score: 0.8773040053185686
-------------subject: 8-------------
==========valence==========
******fold 0******
[263, 345]
training...
setp: 0, Loss: 0.8138033151626587
setp: 100, Loss: 0.6720526218414307
setp: 200, Loss: 0.6357102394104004
setp: 300, Loss: 0.4409923255443573
setp: 400, Loss: 0.38599851727485657
setp: 500, Loss: 0.3607725501060486
setp: 600, Loss: 0.32755255699157715
setp: 700, Loss: 0.32896360754966736
setp: 800, Loss: 0.3594295084476471
setp: 900, Loss: 0.3231316804885864
setp: 1000, Loss: 0.4052051901817322
setp: 1100, Loss: 0.32652023434638977
setp: 1200, Loss: 0.32162654399871826
setp: 1300, Loss: 0.32140058279037476
setp: 1400, Loss: 0.3234800696372986
setp: 1500, Loss: 0.32061049342155457
setp: 1600, Loss: 0.319868803024292
setp: 1700, Loss: 0.31889641284942627
setp: 1800, Loss: 0.32002076506614685
setp: 1900, Loss: 0.32235270738601685
setp: 2000, Loss: 0.31988319754600525
setp: 2100, Loss: 0.3198258876800537
setp: 2200, Loss: 0.31987079977989197
setp: 2300, Loss: 0.3218119144439697
setp: 2400, Loss: 0.3206039369106293
setp: 2500, Loss: 0.3302564322948456
setp: 2600, Loss: 0.3248280882835388
setp: 2700, Loss: 0.3413350284099579
setp: 2800, Loss: 0.3186410367488861
setp: 2900, Loss: 0.34771400690078735
setp: 3000, Loss: 0.32002532482147217
setp: 3100, Loss: 0.31929418444633484
setp: 3200, Loss: 0.3203488886356354
setp: 3300, Loss: 0.31923046708106995
setp: 3400, Loss: 0.3201160430908203
setp: 3500, Loss: 0.3191613554954529
setp: 3600, Loss: 0.318614661693573
setp: 3700, Loss: 0.31963402032852173
setp: 3800, Loss: 0.321785569190979
setp: 3900, Loss: 0.3195812702178955
setp: 4000, Loss: 0.3193603456020355
setp: 4100, Loss: 0.33016684651374817
setp: 4200, Loss: 0.3208579421043396
setp: 4300, Loss: 0.32197219133377075
setp: 4400, Loss: 0.31827735900878906
setp: 4500, Loss: 0.32306310534477234
setp: 4600, Loss: 0.31976187229156494
setp: 4700, Loss: 0.3201955556869507
setp: 4800, Loss: 0.3489185869693756
setp: 4900, Loss: 0.31909987330436707
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8947368421052632
precision: 0.92
recall: 0.8734177215189873
F_score: 0.896103896103896
******fold 1******
[275, 333]
training...
setp: 0, Loss: 0.7037555575370789
setp: 100, Loss: 0.6633279323577881
setp: 200, Loss: 0.525793731212616
setp: 300, Loss: 0.4866425395011902
setp: 400, Loss: 0.5893658399581909
setp: 500, Loss: 0.5090200304985046
setp: 600, Loss: 0.3448774814605713
setp: 700, Loss: 0.3980240225791931
setp: 800, Loss: 0.35516053438186646
setp: 900, Loss: 0.331001877784729
setp: 1000, Loss: 0.36620858311653137
setp: 1100, Loss: 0.3317423164844513
setp: 1200, Loss: 0.3205333352088928
setp: 1300, Loss: 0.32256239652633667
setp: 1400, Loss: 0.3486376702785492
setp: 1500, Loss: 0.3228927254676819
setp: 1600, Loss: 0.393078088760376
setp: 1700, Loss: 0.3221997618675232
setp: 1800, Loss: 0.31789523363113403
setp: 1900, Loss: 0.3189127445220947
setp: 2000, Loss: 0.3184138536453247
setp: 2100, Loss: 0.3171612620353699
setp: 2200, Loss: 0.3171590268611908
setp: 2300, Loss: 0.31939151883125305
setp: 2400, Loss: 0.31747567653656006
setp: 2500, Loss: 0.31557130813598633
setp: 2600, Loss: 0.31810659170150757
setp: 2700, Loss: 0.3768506646156311
setp: 2800, Loss: 0.3260415494441986
setp: 2900, Loss: 0.3172420859336853
setp: 3000, Loss: 0.31798088550567627
setp: 3100, Loss: 0.3183651268482208
setp: 3200, Loss: 0.31835559010505676
setp: 3300, Loss: 0.3179084360599518
setp: 3400, Loss: 0.3196815848350525
setp: 3500, Loss: 0.3174583315849304
setp: 3600, Loss: 0.3185328245162964
setp: 3700, Loss: 0.31704556941986084
setp: 3800, Loss: 0.3184647560119629
setp: 3900, Loss: 0.31830117106437683
setp: 4000, Loss: 0.3184487223625183
setp: 4100, Loss: 0.347800076007843
setp: 4200, Loss: 0.3170662820339203
setp: 4300, Loss: 0.317522794008255
setp: 4400, Loss: 0.3154888451099396
setp: 4500, Loss: 0.31763502955436707
setp: 4600, Loss: 0.3168260157108307
setp: 4700, Loss: 0.3188636898994446
setp: 4800, Loss: 0.3181605339050293
setp: 4900, Loss: 0.3347416818141937
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9963768115942029
recall: 1.0
F_score: 0.9981851179673321
validating...
acc: 0.9473684210526315
precision: 0.927536231884058
recall: 0.9552238805970149
F_score: 0.9411764705882353
******fold 2******
[277, 331]
training...
setp: 0, Loss: 0.7487662434577942
setp: 100, Loss: 0.6702548861503601
setp: 200, Loss: 0.5891530513763428
setp: 300, Loss: 0.5468219518661499
setp: 400, Loss: 0.5364722609519958
setp: 500, Loss: 0.4588579833507538
setp: 600, Loss: 0.36993688344955444
setp: 700, Loss: 0.36209413409233093
setp: 800, Loss: 0.32989516854286194
setp: 900, Loss: 0.3384132981300354
setp: 1000, Loss: 0.31993594765663147
setp: 1100, Loss: 0.3220193386077881
setp: 1200, Loss: 0.3212313950061798
setp: 1300, Loss: 0.33110156655311584
setp: 1400, Loss: 0.328795850276947
setp: 1500, Loss: 0.3212924301624298
setp: 1600, Loss: 0.3481026887893677
setp: 1700, Loss: 0.32217440009117126
setp: 1800, Loss: 0.31775856018066406
setp: 1900, Loss: 0.31836768984794617
setp: 2000, Loss: 0.3181701600551605
setp: 2100, Loss: 0.3184134364128113
setp: 2200, Loss: 0.3176654577255249
setp: 2300, Loss: 0.3192598819732666
setp: 2400, Loss: 0.31867724657058716
setp: 2500, Loss: 0.31760281324386597
setp: 2600, Loss: 0.31832990050315857
setp: 2700, Loss: 0.5574016571044922
setp: 2800, Loss: 0.32059410214424133
setp: 2900, Loss: 0.31825029850006104
setp: 3000, Loss: 0.3170712888240814
setp: 3100, Loss: 0.3178020417690277
setp: 3200, Loss: 0.3200528919696808
setp: 3300, Loss: 0.31774306297302246
setp: 3400, Loss: 0.318548321723938
setp: 3500, Loss: 0.5860884189605713
setp: 3600, Loss: 0.3443574905395508
setp: 3700, Loss: 0.32824137806892395
setp: 3800, Loss: 0.3255496919155121
setp: 3900, Loss: 0.32329264283180237
setp: 4000, Loss: 0.32329604029655457
setp: 4100, Loss: 0.32185134291648865
setp: 4200, Loss: 0.321932315826416
setp: 4300, Loss: 0.322161465883255
setp: 4400, Loss: 0.3188199996948242
setp: 4500, Loss: 0.32079389691352844
setp: 4600, Loss: 0.3203360438346863
setp: 4700, Loss: 0.3197597563266754
setp: 4800, Loss: 0.31808537244796753
setp: 4900, Loss: 0.31934067606925964
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9516129032258065
recall: 0.9076923076923077
F_score: 0.9291338582677167
******fold 3******
[274, 334]
training...
setp: 0, Loss: 0.6909872889518738
setp: 100, Loss: 0.636294960975647
setp: 200, Loss: 0.6027726531028748
setp: 300, Loss: 0.49400174617767334
setp: 400, Loss: 0.5156066417694092
setp: 500, Loss: 0.45987996459007263
setp: 600, Loss: 0.39529579877853394
setp: 700, Loss: 0.4658350348472595
setp: 800, Loss: 0.382661908864975
setp: 900, Loss: 0.40316036343574524
setp: 1000, Loss: 0.32463011145591736
setp: 1100, Loss: 0.3732583224773407
setp: 1200, Loss: 0.35201746225357056
setp: 1300, Loss: 0.3811565637588501
setp: 1400, Loss: 0.3215811550617218
setp: 1500, Loss: 0.32131436467170715
setp: 1600, Loss: 0.317158579826355
setp: 1700, Loss: 0.3181992173194885
setp: 1800, Loss: 0.31642040610313416
setp: 1900, Loss: 0.3190615475177765
setp: 2000, Loss: 0.3190690279006958
setp: 2100, Loss: 0.35285407304763794
setp: 2200, Loss: 0.33693426847457886
setp: 2300, Loss: 0.32028263807296753
setp: 2400, Loss: 0.31598857045173645
setp: 2500, Loss: 0.31673550605773926
setp: 2600, Loss: 0.31851619482040405
setp: 2700, Loss: 0.3188144862651825
setp: 2800, Loss: 0.31886810064315796
setp: 2900, Loss: 0.3176921606063843
setp: 3000, Loss: 0.3182780146598816
setp: 3100, Loss: 0.3515735864639282
setp: 3200, Loss: 0.3792448937892914
setp: 3300, Loss: 0.3165976107120514
setp: 3400, Loss: 0.33835065364837646
setp: 3500, Loss: 0.3207929730415344
setp: 3600, Loss: 0.31651368737220764
setp: 3700, Loss: 0.3162582516670227
setp: 3800, Loss: 0.3173257112503052
setp: 3900, Loss: 0.3168568015098572
setp: 4000, Loss: 0.31741541624069214
setp: 4100, Loss: 0.3177546560764313
setp: 4200, Loss: 0.3185618817806244
setp: 4300, Loss: 0.32708534598350525
setp: 4400, Loss: 0.3157052993774414
setp: 4500, Loss: 0.31842172145843506
setp: 4600, Loss: 0.31744620203971863
setp: 4700, Loss: 0.31724515557289124
setp: 4800, Loss: 0.31793832778930664
setp: 4900, Loss: 0.31777164340019226
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9927272727272727
recall: 0.9963503649635036
F_score: 0.9945355191256832
validating...
acc: 0.875
precision: 0.855072463768116
recall: 0.8676470588235294
F_score: 0.8613138686131386
******fold 4******
[279, 329]
training...
setp: 0, Loss: 0.6887146830558777
setp: 100, Loss: 0.6776527166366577
setp: 200, Loss: 0.47984564304351807
setp: 300, Loss: 0.36427587270736694
setp: 400, Loss: 0.3478577136993408
setp: 500, Loss: 0.336136132478714
setp: 600, Loss: 0.32328346371650696
setp: 700, Loss: 0.3275136351585388
setp: 800, Loss: 0.3238923251628876
setp: 900, Loss: 0.32475546002388
setp: 1000, Loss: 0.31955888867378235
setp: 1100, Loss: 0.31866511702537537
setp: 1200, Loss: 0.31913718581199646
setp: 1300, Loss: 0.3185388743877411
setp: 1400, Loss: 0.31871047616004944
setp: 1500, Loss: 0.32056164741516113
setp: 1600, Loss: 0.31828781962394714
setp: 1700, Loss: 0.3197006583213806
setp: 1800, Loss: 0.3177538216114044
setp: 1900, Loss: 0.321206659078598
setp: 2000, Loss: 0.3185972571372986
setp: 2100, Loss: 0.31853166222572327
setp: 2200, Loss: 0.35111016035079956
setp: 2300, Loss: 0.3259519338607788
setp: 2400, Loss: 0.3852267265319824
setp: 2500, Loss: 0.3200213313102722
setp: 2600, Loss: 0.320872038602829
setp: 2700, Loss: 0.32147708535194397
setp: 2800, Loss: 0.3210808336734772
setp: 2900, Loss: 0.31919676065444946
setp: 3000, Loss: 0.3214033842086792
setp: 3100, Loss: 0.3203848898410797
setp: 3200, Loss: 0.3194671869277954
setp: 3300, Loss: 0.3200504779815674
setp: 3400, Loss: 0.3216777443885803
setp: 3500, Loss: 0.3205190896987915
setp: 3600, Loss: 0.3209623694419861
setp: 3700, Loss: 0.31953009963035583
setp: 3800, Loss: 0.32249942421913147
setp: 3900, Loss: 0.32067424058914185
setp: 4000, Loss: 0.32007890939712524
setp: 4100, Loss: 0.37734857201576233
setp: 4200, Loss: 0.35796377062797546
setp: 4300, Loss: 0.3317860960960388
setp: 4400, Loss: 0.3272969126701355
setp: 4500, Loss: 0.32883143424987793
setp: 4600, Loss: 0.32942938804626465
setp: 4700, Loss: 0.3336482346057892
setp: 4800, Loss: 0.32157641649246216
setp: 4900, Loss: 0.3218070864677429
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9078947368421053
precision: 0.8656716417910447
recall: 0.9206349206349206
F_score: 0.8923076923076922
model saved.
avg_acc: 0.9131578947368422, avg_f_score: 0.9040071571761358
==========arousal==========
******fold 0******
[224, 384]
training...
setp: 0, Loss: 0.7541952729225159
setp: 100, Loss: 0.6280055642127991
setp: 200, Loss: 0.6513746976852417
setp: 300, Loss: 0.5990303754806519
setp: 400, Loss: 0.5103395581245422
setp: 500, Loss: 0.40948644280433655
setp: 600, Loss: 0.3368869721889496
setp: 700, Loss: 0.3504824638366699
setp: 800, Loss: 0.32391446828842163
setp: 900, Loss: 0.35102754831314087
setp: 1000, Loss: 0.3332158327102661
setp: 1100, Loss: 0.31994011998176575
setp: 1200, Loss: 0.3186923861503601
setp: 1300, Loss: 0.3365437686443329
setp: 1400, Loss: 0.3174842596054077
setp: 1500, Loss: 0.31923598051071167
setp: 1600, Loss: 0.31917548179626465
setp: 1700, Loss: 0.3221762478351593
setp: 1800, Loss: 0.33243250846862793
setp: 1900, Loss: 0.3272767961025238
setp: 2000, Loss: 0.3324802815914154
setp: 2100, Loss: 0.3185349404811859
setp: 2200, Loss: 0.3177639842033386
setp: 2300, Loss: 0.31964266300201416
setp: 2400, Loss: 0.32100534439086914
setp: 2500, Loss: 0.31652936339378357
setp: 2600, Loss: 0.3179508149623871
setp: 2700, Loss: 0.31759151816368103
setp: 2800, Loss: 0.31812584400177
setp: 2900, Loss: 0.31755781173706055
setp: 3000, Loss: 0.3190488815307617
setp: 3100, Loss: 0.31700581312179565
setp: 3200, Loss: 0.3194916844367981
setp: 3300, Loss: 0.3176000416278839
setp: 3400, Loss: 0.31847864389419556
setp: 3500, Loss: 0.31702205538749695
setp: 3600, Loss: 0.3186286687850952
setp: 3700, Loss: 0.32177579402923584
setp: 3800, Loss: 0.32045823335647583
setp: 3900, Loss: 0.3167566657066345
setp: 4000, Loss: 0.31807762384414673
setp: 4100, Loss: 0.3179984390735626
setp: 4200, Loss: 0.3176986277103424
setp: 4300, Loss: 0.3169620633125305
setp: 4400, Loss: 0.31627243757247925
setp: 4500, Loss: 0.329470157623291
setp: 4600, Loss: 0.3169409930706024
setp: 4700, Loss: 0.31710588932037354
setp: 4800, Loss: 0.31725069880485535
setp: 4900, Loss: 0.31771913170814514
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9955555555555555
recall: 1.0
F_score: 0.9977728285077951
validating...
acc: 0.9473684210526315
precision: 0.9390243902439024
recall: 0.9625
F_score: 0.9506172839506174
******fold 1******
[250, 358]
training...
setp: 0, Loss: 0.7004857063293457
setp: 100, Loss: 0.6034950613975525
setp: 200, Loss: 0.5473229885101318
setp: 300, Loss: 0.47069692611694336
setp: 400, Loss: 0.3367058038711548
setp: 500, Loss: 0.3737702965736389
setp: 600, Loss: 0.4539855420589447
setp: 700, Loss: 0.3451606035232544
setp: 800, Loss: 0.32249003648757935
setp: 900, Loss: 0.32281845808029175
setp: 1000, Loss: 0.3214544653892517
setp: 1100, Loss: 0.32103991508483887
setp: 1200, Loss: 0.31726157665252686
setp: 1300, Loss: 0.3198361098766327
setp: 1400, Loss: 0.3182487189769745
setp: 1500, Loss: 0.32003217935562134
setp: 1600, Loss: 0.31762972474098206
setp: 1700, Loss: 0.32022979855537415
setp: 1800, Loss: 0.4718674421310425
setp: 1900, Loss: 0.337338924407959
setp: 2000, Loss: 0.31750550866127014
setp: 2100, Loss: 0.31681713461875916
setp: 2200, Loss: 0.3187192380428314
setp: 2300, Loss: 0.3167365789413452
setp: 2400, Loss: 0.3183403015136719
setp: 2500, Loss: 0.3171992599964142
setp: 2600, Loss: 0.3200095295906067
setp: 2700, Loss: 0.31788352131843567
setp: 2800, Loss: 0.31809350848197937
setp: 2900, Loss: 0.7857277989387512
setp: 3000, Loss: 0.5156298279762268
setp: 3100, Loss: 0.37910619378089905
setp: 3200, Loss: 0.3924260437488556
setp: 3300, Loss: 0.37354663014411926
setp: 3400, Loss: 0.34423649311065674
setp: 3500, Loss: 0.3318440318107605
setp: 3600, Loss: 0.32945606112480164
setp: 3700, Loss: 0.3337937891483307
setp: 3800, Loss: 0.34701499342918396
setp: 3900, Loss: 0.3234196901321411
setp: 4000, Loss: 0.32112178206443787
setp: 4100, Loss: 0.32621243596076965
setp: 4200, Loss: 0.32308125495910645
setp: 4300, Loss: 0.3220405876636505
setp: 4400, Loss: 0.33950063586235046
setp: 4500, Loss: 0.32312077283859253
setp: 4600, Loss: 0.32181811332702637
setp: 4700, Loss: 0.32303568720817566
setp: 4800, Loss: 0.3218761384487152
setp: 4900, Loss: 0.32065123319625854
training successfully ended.
validating...
acc: 0.9555921052631579
precision: 0.9025270758122743
recall: 1.0
F_score: 0.9487666034155597
validating...
acc: 0.868421052631579
precision: 0.7297297297297297
recall: 1.0
F_score: 0.8437499999999999
******fold 2******
[237, 371]
training...
setp: 0, Loss: 0.6444946527481079
setp: 100, Loss: 0.6426133513450623
setp: 200, Loss: 0.5413084626197815
setp: 300, Loss: 0.5005043745040894
setp: 400, Loss: 0.4238132834434509
setp: 500, Loss: 0.35550200939178467
setp: 600, Loss: 0.3775777816772461
setp: 700, Loss: 0.41230568289756775
setp: 800, Loss: 0.3279283940792084
setp: 900, Loss: 0.3232310712337494
setp: 1000, Loss: 0.32228606939315796
setp: 1100, Loss: 0.3234253227710724
setp: 1200, Loss: 0.3265722990036011
setp: 1300, Loss: 0.35957515239715576
setp: 1400, Loss: 0.34927648305892944
setp: 1500, Loss: 0.3199124038219452
setp: 1600, Loss: 0.31845176219940186
setp: 1700, Loss: 0.31722190976142883
setp: 1800, Loss: 0.31988900899887085
setp: 1900, Loss: 0.32079893350601196
setp: 2000, Loss: 0.31632664799690247
setp: 2100, Loss: 0.31718605756759644
setp: 2200, Loss: 0.317894846200943
setp: 2300, Loss: 0.31846413016319275
setp: 2400, Loss: 0.42389023303985596
setp: 2500, Loss: 0.3187546730041504
setp: 2600, Loss: 0.31698372960090637
setp: 2700, Loss: 0.3168330490589142
setp: 2800, Loss: 0.31758618354797363
setp: 2900, Loss: 0.31834274530410767
setp: 3000, Loss: 0.32467421889305115
setp: 3100, Loss: 0.3434011936187744
setp: 3200, Loss: 0.3487989902496338
setp: 3300, Loss: 0.31751036643981934
setp: 3400, Loss: 0.3180735111236572
setp: 3500, Loss: 0.316346675157547
setp: 3600, Loss: 0.3190564215183258
setp: 3700, Loss: 0.31862974166870117
setp: 3800, Loss: 0.3227781057357788
setp: 3900, Loss: 0.315900981426239
setp: 4000, Loss: 0.3166474401950836
setp: 4100, Loss: 0.3181784749031067
setp: 4200, Loss: 0.31824666261672974
setp: 4300, Loss: 0.31742459535598755
setp: 4400, Loss: 0.31710752844810486
setp: 4500, Loss: 0.3169919550418854
setp: 4600, Loss: 0.3187570869922638
setp: 4700, Loss: 0.3178926408290863
setp: 4800, Loss: 0.33078545331954956
setp: 4900, Loss: 0.3180312514305115
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9802631578947368
precision: 0.9705882352941176
recall: 0.9850746268656716
F_score: 0.9777777777777777
******fold 3******
[249, 359]
training...
setp: 0, Loss: 0.7128944396972656
setp: 100, Loss: 0.684581458568573
setp: 200, Loss: 0.5525861382484436
setp: 300, Loss: 0.512139618396759
setp: 400, Loss: 0.38424021005630493
setp: 500, Loss: 0.333156943321228
setp: 600, Loss: 0.3568880259990692
setp: 700, Loss: 0.32508745789527893
setp: 800, Loss: 0.3188352882862091
setp: 900, Loss: 0.3207325339317322
setp: 1000, Loss: 0.3203853964805603
setp: 1100, Loss: 0.32180914282798767
setp: 1200, Loss: 0.3797609508037567
setp: 1300, Loss: 0.3508271276950836
setp: 1400, Loss: 0.32028114795684814
setp: 1500, Loss: 0.3213619589805603
setp: 1600, Loss: 0.3187620937824249
setp: 1700, Loss: 0.3188839852809906
setp: 1800, Loss: 0.32021573185920715
setp: 1900, Loss: 0.38392144441604614
setp: 2000, Loss: 0.3239363133907318
setp: 2100, Loss: 0.3251021206378937
setp: 2200, Loss: 0.3194410800933838
setp: 2300, Loss: 0.3204231262207031
setp: 2400, Loss: 0.3174140453338623
setp: 2500, Loss: 0.3220837116241455
setp: 2600, Loss: 0.31908273696899414
setp: 2700, Loss: 0.3391405940055847
setp: 2800, Loss: 0.32619133591651917
setp: 2900, Loss: 0.3182459771633148
setp: 3000, Loss: 0.3188862204551697
setp: 3100, Loss: 0.3190637230873108
setp: 3200, Loss: 0.3199447989463806
setp: 3300, Loss: 0.31977906823158264
setp: 3400, Loss: 0.32091909646987915
setp: 3500, Loss: 0.31866374611854553
setp: 3600, Loss: 0.31890276074409485
setp: 3700, Loss: 0.3190558850765228
setp: 3800, Loss: 0.320646196603775
setp: 3900, Loss: 0.3188653290271759
setp: 4000, Loss: 0.3177092373371124
setp: 4100, Loss: 0.3195951581001282
setp: 4200, Loss: 0.4244031310081482
setp: 4300, Loss: 0.329729825258255
setp: 4400, Loss: 0.33284392952919006
setp: 4500, Loss: 0.3224894106388092
setp: 4600, Loss: 0.3212484121322632
setp: 4700, Loss: 0.3213746249675751
setp: 4800, Loss: 0.3220391571521759
setp: 4900, Loss: 0.3235747516155243
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9078947368421053
precision: 0.9361702127659575
recall: 0.8
F_score: 0.8627450980392157
******fold 4******
[256, 352]
training...
setp: 0, Loss: 0.6859390735626221
setp: 100, Loss: 0.6379472017288208
setp: 200, Loss: 0.5867577791213989
setp: 300, Loss: 0.4943704903125763
setp: 400, Loss: 0.4964248538017273
setp: 500, Loss: 0.44275394082069397
setp: 600, Loss: 0.4555774927139282
setp: 700, Loss: 0.37190568447113037
setp: 800, Loss: 0.33434346318244934
setp: 900, Loss: 0.32804280519485474
setp: 1000, Loss: 0.39448025822639465
setp: 1100, Loss: 0.3435691297054291
setp: 1200, Loss: 0.3284701704978943
setp: 1300, Loss: 0.3272767961025238
setp: 1400, Loss: 0.3182041347026825
setp: 1500, Loss: 0.3243499994277954
setp: 1600, Loss: 0.31650885939598083
setp: 1700, Loss: 0.3178432881832123
setp: 1800, Loss: 0.32770276069641113
setp: 1900, Loss: 0.32218387722969055
setp: 2000, Loss: 0.31866884231567383
setp: 2100, Loss: 0.3177339732646942
setp: 2200, Loss: 0.3208886981010437
setp: 2300, Loss: 0.3181069791316986
setp: 2400, Loss: 0.31981074810028076
setp: 2500, Loss: 0.3171118497848511
setp: 2600, Loss: 0.3186942934989929
setp: 2700, Loss: 0.3182557225227356
setp: 2800, Loss: 0.31799110770225525
setp: 2900, Loss: 0.31785815954208374
setp: 3000, Loss: 0.318526029586792
setp: 3100, Loss: 0.3174561858177185
setp: 3200, Loss: 0.3185531795024872
setp: 3300, Loss: 0.3332136273384094
setp: 3400, Loss: 0.32308828830718994
setp: 3500, Loss: 0.3163129389286041
setp: 3600, Loss: 0.3175268769264221
setp: 3700, Loss: 0.3185010552406311
setp: 3800, Loss: 0.3196897804737091
setp: 3900, Loss: 0.3178401589393616
setp: 4000, Loss: 0.31863951683044434
setp: 4100, Loss: 0.31833943724632263
setp: 4200, Loss: 0.3175823986530304
setp: 4300, Loss: 0.3170213997364044
setp: 4400, Loss: 0.316938579082489
setp: 4500, Loss: 0.31886208057403564
setp: 4600, Loss: 0.31796160340309143
setp: 4700, Loss: 0.3179643154144287
setp: 4800, Loss: 0.31706559658050537
setp: 4900, Loss: 0.31819644570350647
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9473684210526315
precision: 0.8846153846153846
recall: 0.9583333333333334
F_score: 0.9199999999999999
model saved.
avg_acc: 0.9302631578947368, avg_f_score: 0.9109780319535222
-------------subject: 9-------------
==========valence==========
******fold 0******
[277, 331]
training...
setp: 0, Loss: 0.7037089467048645
setp: 100, Loss: 0.6909352540969849
setp: 200, Loss: 0.6748093366622925
setp: 300, Loss: 0.6845586895942688
setp: 400, Loss: 0.5082799196243286
setp: 500, Loss: 0.5321353077888489
setp: 600, Loss: 0.5604436993598938
setp: 700, Loss: 0.522994339466095
setp: 800, Loss: 0.5041142702102661
setp: 900, Loss: 0.5146403908729553
setp: 1000, Loss: 0.4384382367134094
setp: 1100, Loss: 0.3477528691291809
setp: 1200, Loss: 0.38007810711860657
setp: 1300, Loss: 0.4730328917503357
setp: 1400, Loss: 0.36617201566696167
setp: 1500, Loss: 0.3711302876472473
setp: 1600, Loss: 0.42255347967147827
setp: 1700, Loss: 0.3740265667438507
setp: 1800, Loss: 0.36002111434936523
setp: 1900, Loss: 0.383705735206604
setp: 2000, Loss: 0.37966057658195496
setp: 2100, Loss: 0.3902738690376282
setp: 2200, Loss: 0.3771665394306183
setp: 2300, Loss: 0.3655315637588501
setp: 2400, Loss: 0.34826067090034485
setp: 2500, Loss: 0.35091638565063477
setp: 2600, Loss: 0.40697458386421204
setp: 2700, Loss: 0.3493659496307373
setp: 2800, Loss: 0.3491101861000061
setp: 2900, Loss: 0.39284369349479675
setp: 3000, Loss: 0.31659770011901855
setp: 3100, Loss: 0.31917643547058105
setp: 3200, Loss: 0.34754106402397156
setp: 3300, Loss: 0.31701865792274475
setp: 3400, Loss: 0.3494352102279663
setp: 3500, Loss: 0.3799087405204773
setp: 3600, Loss: 0.3487606346607208
setp: 3700, Loss: 0.3492758870124817
setp: 3800, Loss: 0.4446461796760559
setp: 3900, Loss: 0.3673448860645294
setp: 4000, Loss: 0.39303380250930786
setp: 4100, Loss: 0.3475179374217987
setp: 4200, Loss: 0.3168502151966095
setp: 4300, Loss: 0.31643322110176086
setp: 4400, Loss: 0.35009056329727173
setp: 4500, Loss: 0.34852707386016846
setp: 4600, Loss: 0.3479653596878052
setp: 4700, Loss: 0.3486747741699219
setp: 4800, Loss: 0.3501930236816406
setp: 4900, Loss: 0.31664547324180603
training successfully ended.
validating...
acc: 0.9720394736842105
precision: 0.9924242424242424
recall: 0.9458483754512635
F_score: 0.9685767097966729
validating...
acc: 0.8486842105263158
precision: 0.9178082191780822
recall: 0.7976190476190477
F_score: 0.8535031847133758
******fold 1******
[289, 319]
training...
setp: 0, Loss: 0.7141013145446777
setp: 100, Loss: 0.650251030921936
setp: 200, Loss: 0.6649558544158936
setp: 300, Loss: 0.5635999441146851
setp: 400, Loss: 0.5024321675300598
setp: 500, Loss: 0.5084356069564819
setp: 600, Loss: 0.5972499251365662
setp: 700, Loss: 0.41986969113349915
setp: 800, Loss: 0.4680563807487488
setp: 900, Loss: 0.44237008690834045
setp: 1000, Loss: 0.6454031467437744
setp: 1100, Loss: 0.4339255094528198
setp: 1200, Loss: 0.4551888704299927
setp: 1300, Loss: 0.44120392203330994
setp: 1400, Loss: 0.4140219986438751
setp: 1500, Loss: 0.45521119236946106
setp: 1600, Loss: 0.37385350465774536
setp: 1700, Loss: 0.3942568600177765
setp: 1800, Loss: 0.37850281596183777
setp: 1900, Loss: 0.37441128492355347
setp: 2000, Loss: 0.4099026918411255
setp: 2100, Loss: 0.3966711461544037
setp: 2200, Loss: 0.3519691824913025
setp: 2300, Loss: 0.3811846375465393
setp: 2400, Loss: 0.38040539622306824
setp: 2500, Loss: 0.35375067591667175
setp: 2600, Loss: 0.349902868270874
setp: 2700, Loss: 0.3524317145347595
setp: 2800, Loss: 0.38216322660446167
setp: 2900, Loss: 0.40074247121810913
setp: 3000, Loss: 0.34727799892425537
setp: 3100, Loss: 0.35670366883277893
setp: 3200, Loss: 0.39426541328430176
setp: 3300, Loss: 0.38022544980049133
setp: 3400, Loss: 0.35127902030944824
setp: 3500, Loss: 0.31917938590049744
setp: 3600, Loss: 0.3805718421936035
setp: 3700, Loss: 0.3477187752723694
setp: 3800, Loss: 0.3824586570262909
setp: 3900, Loss: 0.3798035681247711
setp: 4000, Loss: 0.39181679487228394
setp: 4100, Loss: 0.3536298871040344
setp: 4200, Loss: 0.3788878917694092
setp: 4300, Loss: 0.35103070735931396
setp: 4400, Loss: 0.3506465554237366
setp: 4500, Loss: 0.3220441937446594
setp: 4600, Loss: 0.3481975197792053
setp: 4700, Loss: 0.36311760544776917
setp: 4800, Loss: 0.3190285563468933
setp: 4900, Loss: 0.3491760790348053
training successfully ended.
validating...
acc: 0.9572368421052632
precision: 1.0
recall: 0.9100346020761245
F_score: 0.9528985507246377
validating...
acc: 0.8618421052631579
precision: 0.9636363636363636
recall: 0.7361111111111112
F_score: 0.8346456692913387
******fold 2******
[290, 318]
training...
setp: 0, Loss: 0.6821062564849854
setp: 100, Loss: 0.5808330774307251
setp: 200, Loss: 0.613574206829071
setp: 300, Loss: 0.5635287165641785
setp: 400, Loss: 0.5038103461265564
setp: 500, Loss: 0.5050672292709351
setp: 600, Loss: 0.616546630859375
setp: 700, Loss: 0.5230803489685059
setp: 800, Loss: 0.46796363592147827
setp: 900, Loss: 0.4729093015193939
setp: 1000, Loss: 0.44545015692710876
setp: 1100, Loss: 0.37594765424728394
setp: 1200, Loss: 0.40725868940353394
setp: 1300, Loss: 0.42244336009025574
setp: 1400, Loss: 0.35518506169319153
setp: 1500, Loss: 0.35967183113098145
setp: 1600, Loss: 0.488107830286026
setp: 1700, Loss: 0.35332542657852173
setp: 1800, Loss: 0.35588380694389343
setp: 1900, Loss: 0.3902142345905304
setp: 2000, Loss: 0.327948659658432
setp: 2100, Loss: 0.3548625409603119
setp: 2200, Loss: 0.3202039897441864
setp: 2300, Loss: 0.36405879259109497
setp: 2400, Loss: 0.3502751886844635
setp: 2500, Loss: 0.36053594946861267
setp: 2600, Loss: 0.36102771759033203
setp: 2700, Loss: 0.3193162679672241
setp: 2800, Loss: 0.35131022334098816
setp: 2900, Loss: 0.3654937148094177
setp: 3000, Loss: 0.3489592373371124
setp: 3100, Loss: 0.31736910343170166
setp: 3200, Loss: 0.32021990418434143
setp: 3300, Loss: 0.35036489367485046
setp: 3400, Loss: 0.37321242690086365
setp: 3500, Loss: 0.3480677902698517
setp: 3600, Loss: 0.35071027278900146
setp: 3700, Loss: 0.3621380031108856
setp: 3800, Loss: 0.39052966237068176
setp: 3900, Loss: 0.3179812729358673
setp: 4000, Loss: 0.35052740573883057
setp: 4100, Loss: 0.3190869688987732
setp: 4200, Loss: 0.35739389061927795
setp: 4300, Loss: 0.34917235374450684
setp: 4400, Loss: 0.3508918881416321
setp: 4500, Loss: 0.3549354672431946
setp: 4600, Loss: 0.3215115964412689
setp: 4700, Loss: 0.31927329301834106
setp: 4800, Loss: 0.3181327283382416
setp: 4900, Loss: 0.3210025727748871
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.993103448275862
F_score: 0.9965397923875432
validating...
acc: 0.868421052631579
precision: 0.8493150684931506
recall: 0.8732394366197183
F_score: 0.861111111111111
******fold 3******
[291, 317]
training...
setp: 0, Loss: 0.7019898891448975
setp: 100, Loss: 0.6876233816146851
setp: 200, Loss: 0.6647412180900574
setp: 300, Loss: 0.5766214728355408
setp: 400, Loss: 0.5062403678894043
setp: 500, Loss: 0.4932277798652649
setp: 600, Loss: 0.5552034378051758
setp: 700, Loss: 0.4950922727584839
setp: 800, Loss: 0.5576260685920715
setp: 900, Loss: 0.43554002046585083
setp: 1000, Loss: 0.504392147064209
setp: 1100, Loss: 0.3866059482097626
setp: 1200, Loss: 0.419840931892395
setp: 1300, Loss: 0.3863387405872345
setp: 1400, Loss: 0.3648224174976349
setp: 1500, Loss: 0.40291300415992737
setp: 1600, Loss: 0.37849295139312744
setp: 1700, Loss: 0.3239341378211975
setp: 1800, Loss: 0.430629163980484
setp: 1900, Loss: 0.39188385009765625
setp: 2000, Loss: 0.3583967685699463
setp: 2100, Loss: 0.34920594096183777
setp: 2200, Loss: 0.4257160723209381
setp: 2300, Loss: 0.356108158826828
setp: 2400, Loss: 0.3234156668186188
setp: 2500, Loss: 0.34956347942352295
setp: 2600, Loss: 0.3513016104698181
setp: 2700, Loss: 0.34871771931648254
setp: 2800, Loss: 0.35025554895401
setp: 2900, Loss: 0.31850624084472656
setp: 3000, Loss: 0.3643057942390442
setp: 3100, Loss: 0.3681113123893738
setp: 3200, Loss: 0.3493669927120209
setp: 3300, Loss: 0.3493768274784088
setp: 3400, Loss: 0.3499743342399597
setp: 3500, Loss: 0.34926745295524597
setp: 3600, Loss: 0.3171303868293762
setp: 3700, Loss: 0.37920308113098145
setp: 3800, Loss: 0.38390326499938965
setp: 3900, Loss: 0.3497515320777893
setp: 4000, Loss: 0.3489878177642822
setp: 4100, Loss: 0.4067663550376892
setp: 4200, Loss: 0.38256925344467163
setp: 4300, Loss: 0.32799622416496277
setp: 4400, Loss: 0.350778728723526
setp: 4500, Loss: 0.35145291686058044
setp: 4600, Loss: 0.3477461040019989
setp: 4700, Loss: 0.34848442673683167
setp: 4800, Loss: 0.31781983375549316
setp: 4900, Loss: 0.3472788333892822
training successfully ended.
validating...
acc: 0.9703947368421053
precision: 0.9927797833935018
recall: 0.9450171821305842
F_score: 0.9683098591549295
validating...
acc: 0.881578947368421
precision: 0.90625
recall: 0.8285714285714286
F_score: 0.8656716417910447
******fold 4******
[297, 311]
training...
setp: 0, Loss: 0.6932947635650635
setp: 100, Loss: 0.5924872756004333
setp: 200, Loss: 0.5833287835121155
setp: 300, Loss: 0.6100732088088989
setp: 400, Loss: 0.5256021618843079
setp: 500, Loss: 0.4952666759490967
setp: 600, Loss: 0.5975797176361084
setp: 700, Loss: 0.5085671544075012
setp: 800, Loss: 0.48445039987564087
setp: 900, Loss: 0.5076438784599304
setp: 1000, Loss: 0.4196193814277649
setp: 1100, Loss: 0.3371976315975189
setp: 1200, Loss: 0.4705606997013092
setp: 1300, Loss: 0.4385438561439514
setp: 1400, Loss: 0.4265373945236206
setp: 1500, Loss: 0.39216992259025574
setp: 1600, Loss: 0.3966768682003021
setp: 1700, Loss: 0.3628481328487396
setp: 1800, Loss: 0.3921579122543335
setp: 1900, Loss: 0.36491596698760986
setp: 2000, Loss: 0.3540741801261902
setp: 2100, Loss: 0.37885570526123047
setp: 2200, Loss: 0.3604045808315277
setp: 2300, Loss: 0.35544049739837646
setp: 2400, Loss: 0.3832973837852478
setp: 2500, Loss: 0.32130151987075806
setp: 2600, Loss: 0.34832942485809326
setp: 2700, Loss: 0.3500405550003052
setp: 2800, Loss: 0.4128004014492035
setp: 2900, Loss: 0.32512280344963074
setp: 3000, Loss: 0.3172556459903717
setp: 3100, Loss: 0.3189265727996826
setp: 3200, Loss: 0.34912097454071045
setp: 3300, Loss: 0.3487432301044464
setp: 3400, Loss: 0.40883928537368774
setp: 3500, Loss: 0.36796391010284424
setp: 3600, Loss: 0.3687933087348938
setp: 3700, Loss: 0.37910935282707214
setp: 3800, Loss: 0.3215825855731964
setp: 3900, Loss: 0.31782710552215576
setp: 4000, Loss: 0.3533197343349457
setp: 4100, Loss: 0.34938645362854004
setp: 4200, Loss: 0.3705695867538452
setp: 4300, Loss: 0.35368290543556213
setp: 4400, Loss: 0.3177245557308197
setp: 4500, Loss: 0.3853541910648346
setp: 4600, Loss: 0.3597393333911896
setp: 4700, Loss: 0.3780052661895752
setp: 4800, Loss: 0.3196899890899658
setp: 4900, Loss: 0.31832149624824524
training successfully ended.
validating...
acc: 0.9539473684210527
precision: 1.0
recall: 0.9057239057239057
F_score: 0.950530035335689
validating...
acc: 0.881578947368421
precision: 0.9791666666666666
recall: 0.734375
F_score: 0.8392857142857143
model saved.
avg_acc: 0.868421052631579, avg_f_score: 0.8508434642385169
==========arousal==========
******fold 0******
[221, 387]
training...
setp: 0, Loss: 0.6799507141113281
setp: 100, Loss: 0.678628146648407
setp: 200, Loss: 0.6616852879524231
setp: 300, Loss: 0.6622886061668396
setp: 400, Loss: 0.648447573184967
setp: 500, Loss: 0.6757155656814575
setp: 600, Loss: 0.5839811563491821
setp: 700, Loss: 0.4982461929321289
setp: 800, Loss: 0.49196237325668335
setp: 900, Loss: 0.459225594997406
setp: 1000, Loss: 0.39270034432411194
setp: 1100, Loss: 0.3408512473106384
setp: 1200, Loss: 0.33107393980026245
setp: 1300, Loss: 0.36251989006996155
setp: 1400, Loss: 0.326224684715271
setp: 1500, Loss: 0.3330233097076416
setp: 1600, Loss: 0.32425010204315186
setp: 1700, Loss: 0.3405833840370178
setp: 1800, Loss: 0.34097516536712646
setp: 1900, Loss: 0.36879634857177734
setp: 2000, Loss: 0.3780337870121002
setp: 2100, Loss: 0.3273407816886902
setp: 2200, Loss: 0.3206753134727478
setp: 2300, Loss: 0.32053324580192566
setp: 2400, Loss: 0.32561400532722473
setp: 2500, Loss: 0.32405808568000793
setp: 2600, Loss: 0.31991517543792725
setp: 2700, Loss: 0.32031235098838806
setp: 2800, Loss: 0.320737361907959
setp: 2900, Loss: 0.3202783167362213
setp: 3000, Loss: 0.32004645466804504
setp: 3100, Loss: 0.3212072551250458
setp: 3200, Loss: 0.3509441912174225
setp: 3300, Loss: 0.32554832100868225
setp: 3400, Loss: 0.32876643538475037
setp: 3500, Loss: 0.3213040232658386
setp: 3600, Loss: 0.3250139653682709
setp: 3700, Loss: 0.3244893252849579
setp: 3800, Loss: 0.3244975209236145
setp: 3900, Loss: 0.3463340401649475
setp: 4000, Loss: 0.33722829818725586
setp: 4100, Loss: 0.3201298117637634
setp: 4200, Loss: 0.32539060711860657
setp: 4300, Loss: 0.347065806388855
setp: 4400, Loss: 0.3207494914531708
setp: 4500, Loss: 0.31930550932884216
setp: 4600, Loss: 0.31980815529823303
setp: 4700, Loss: 0.3204073905944824
setp: 4800, Loss: 0.3192163109779358
setp: 4900, Loss: 0.31948384642601013
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9954954954954955
recall: 1.0
F_score: 0.9977426636568849
validating...
acc: 0.9407894736842105
precision: 0.9230769230769231
recall: 0.9375
F_score: 0.9302325581395349
******fold 1******
[229, 379]
training...
setp: 0, Loss: 0.7172331809997559
setp: 100, Loss: 0.6478431224822998
setp: 200, Loss: 0.6997900009155273
setp: 300, Loss: 0.6164442300796509
setp: 400, Loss: 0.43613201379776
setp: 500, Loss: 0.41010725498199463
setp: 600, Loss: 0.36984387040138245
setp: 700, Loss: 0.35006818175315857
setp: 800, Loss: 0.3265886902809143
setp: 900, Loss: 0.35642990469932556
setp: 1000, Loss: 0.323228657245636
setp: 1100, Loss: 0.3217962980270386
setp: 1200, Loss: 0.3568597733974457
setp: 1300, Loss: 0.3214710056781769
setp: 1400, Loss: 0.3209821879863739
setp: 1500, Loss: 0.3223632872104645
setp: 1600, Loss: 0.32657286524772644
setp: 1700, Loss: 0.32287636399269104
setp: 1800, Loss: 0.32160425186157227
setp: 1900, Loss: 0.38159942626953125
setp: 2000, Loss: 0.39590176939964294
setp: 2100, Loss: 0.3227446675300598
setp: 2200, Loss: 0.3198992908000946
setp: 2300, Loss: 0.35074546933174133
setp: 2400, Loss: 0.3253211975097656
setp: 2500, Loss: 0.3195198178291321
setp: 2600, Loss: 0.3187868595123291
setp: 2700, Loss: 0.322430819272995
setp: 2800, Loss: 0.35332033038139343
setp: 2900, Loss: 0.3186098039150238
setp: 3000, Loss: 0.3191060423851013
setp: 3100, Loss: 0.3199198544025421
setp: 3200, Loss: 0.3191572427749634
setp: 3300, Loss: 0.31917861104011536
setp: 3400, Loss: 0.3191419541835785
setp: 3500, Loss: 0.4117162227630615
setp: 3600, Loss: 0.341754674911499
setp: 3700, Loss: 0.3230384588241577
setp: 3800, Loss: 0.3293781280517578
setp: 3900, Loss: 0.3201906681060791
setp: 4000, Loss: 0.3191949427127838
setp: 4100, Loss: 0.3198191523551941
setp: 4200, Loss: 0.3200596272945404
setp: 4300, Loss: 0.31927037239074707
setp: 4400, Loss: 0.32072827219963074
setp: 4500, Loss: 0.3195745050907135
setp: 4600, Loss: 0.3194740116596222
setp: 4700, Loss: 0.35044655203819275
setp: 4800, Loss: 0.3192075192928314
setp: 4900, Loss: 0.3199600577354431
training successfully ended.
validating...
acc: 0.9078947368421053
precision: 0.9061032863849765
recall: 0.8427947598253275
F_score: 0.8733031674208145
validating...
acc: 0.8421052631578947
precision: 0.7857142857142857
recall: 0.7857142857142857
F_score: 0.7857142857142857
******fold 2******
[232, 376]
training...
setp: 0, Loss: 0.682940661907196
setp: 100, Loss: 0.6616933941841125
setp: 200, Loss: 0.6442656517028809
setp: 300, Loss: 0.5722270011901855
setp: 400, Loss: 0.41421809792518616
setp: 500, Loss: 0.41647517681121826
setp: 600, Loss: 0.33515167236328125
setp: 700, Loss: 0.3543259799480438
setp: 800, Loss: 0.3303554654121399
setp: 900, Loss: 0.3655424416065216
setp: 1000, Loss: 0.3221081793308258
setp: 1100, Loss: 0.3221455514431
setp: 1200, Loss: 0.3592606782913208
setp: 1300, Loss: 0.3241649270057678
setp: 1400, Loss: 0.32568174600601196
setp: 1500, Loss: 0.33692312240600586
setp: 1600, Loss: 0.32135623693466187
setp: 1700, Loss: 0.3198537528514862
setp: 1800, Loss: 0.31900009512901306
setp: 1900, Loss: 0.3497684895992279
setp: 2000, Loss: 0.32090792059898376
setp: 2100, Loss: 0.31893211603164673
setp: 2200, Loss: 0.3195086717605591
setp: 2300, Loss: 0.32017210125923157
setp: 2400, Loss: 0.3194146156311035
setp: 2500, Loss: 0.3187594413757324
setp: 2600, Loss: 0.3197968006134033
setp: 2700, Loss: 0.34439072012901306
setp: 2800, Loss: 0.4454781115055084
setp: 2900, Loss: 0.3185250163078308
setp: 3000, Loss: 0.3183317184448242
setp: 3100, Loss: 0.3188318610191345
setp: 3200, Loss: 0.32002580165863037
setp: 3300, Loss: 0.3198431730270386
setp: 3400, Loss: 0.31876063346862793
setp: 3500, Loss: 0.319815993309021
setp: 3600, Loss: 0.320169061422348
setp: 3700, Loss: 0.31894606351852417
setp: 3800, Loss: 0.34989190101623535
setp: 3900, Loss: 0.31972116231918335
setp: 4000, Loss: 0.3188806176185608
setp: 4100, Loss: 0.38756781816482544
setp: 4200, Loss: 0.34413835406303406
setp: 4300, Loss: 0.37542104721069336
setp: 4400, Loss: 0.3276222050189972
setp: 4500, Loss: 0.31830546259880066
setp: 4600, Loss: 0.31816715002059937
setp: 4700, Loss: 0.31987446546554565
setp: 4800, Loss: 0.3185077905654907
setp: 4900, Loss: 0.3183198571205139
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9956896551724138
F_score: 0.9978401727861772
validating...
acc: 0.8486842105263158
precision: 0.7678571428571429
recall: 0.8113207547169812
F_score: 0.7889908256880735
******fold 3******
[232, 376]
training...
setp: 0, Loss: 0.688011884689331
setp: 100, Loss: 0.6562005281448364
setp: 200, Loss: 0.6334462761878967
setp: 300, Loss: 0.5309150815010071
setp: 400, Loss: 0.43353942036628723
setp: 500, Loss: 0.42848414182662964
setp: 600, Loss: 0.3796784281730652
setp: 700, Loss: 0.36538246273994446
setp: 800, Loss: 0.38253623247146606
setp: 900, Loss: 0.358839750289917
setp: 1000, Loss: 0.350857675075531
setp: 1100, Loss: 0.32427978515625
setp: 1200, Loss: 0.32208749651908875
setp: 1300, Loss: 0.32129034399986267
setp: 1400, Loss: 0.32189226150512695
setp: 1500, Loss: 0.32153621315956116
setp: 1600, Loss: 0.3203214704990387
setp: 1700, Loss: 0.32054653763771057
setp: 1800, Loss: 0.35507732629776
setp: 1900, Loss: 0.3558982312679291
setp: 2000, Loss: 0.33076000213623047
setp: 2100, Loss: 0.32479920983314514
setp: 2200, Loss: 0.32803013920783997
setp: 2300, Loss: 0.36207151412963867
setp: 2400, Loss: 0.35157209634780884
setp: 2500, Loss: 0.34996020793914795
setp: 2600, Loss: 0.31927698850631714
setp: 2700, Loss: 0.31935203075408936
setp: 2800, Loss: 0.3515818417072296
setp: 2900, Loss: 0.3192495107650757
setp: 3000, Loss: 0.3194960355758667
setp: 3100, Loss: 0.32037675380706787
setp: 3200, Loss: 0.3194287419319153
setp: 3300, Loss: 0.3204761743545532
setp: 3400, Loss: 0.3195083737373352
setp: 3500, Loss: 0.3195241689682007
setp: 3600, Loss: 0.3200989067554474
setp: 3700, Loss: 0.6253089904785156
setp: 3800, Loss: 0.3540378510951996
setp: 3900, Loss: 0.35970592498779297
setp: 4000, Loss: 0.3270173966884613
setp: 4100, Loss: 0.3401890695095062
setp: 4200, Loss: 0.3357585370540619
setp: 4300, Loss: 0.3367265462875366
setp: 4400, Loss: 0.3563927412033081
setp: 4500, Loss: 0.3348604142665863
setp: 4600, Loss: 0.33645516633987427
setp: 4700, Loss: 0.3539736568927765
setp: 4800, Loss: 0.3215115964412689
setp: 4900, Loss: 0.3233439326286316
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.9913793103448276
F_score: 0.9956709956709957
validating...
acc: 0.9210526315789473
precision: 0.8867924528301887
recall: 0.8867924528301887
F_score: 0.8867924528301887
******fold 4******
[226, 382]
training...
setp: 0, Loss: 0.8066567182540894
setp: 100, Loss: 0.6620835661888123
setp: 200, Loss: 0.6445347666740417
setp: 300, Loss: 0.6207882761955261
setp: 400, Loss: 0.6312038898468018
setp: 500, Loss: 0.533118486404419
setp: 600, Loss: 0.4795285761356354
setp: 700, Loss: 0.3457530438899994
setp: 800, Loss: 0.4216420650482178
setp: 900, Loss: 0.33880770206451416
setp: 1000, Loss: 0.3434763550758362
setp: 1100, Loss: 0.3438465893268585
setp: 1200, Loss: 0.34430381655693054
setp: 1300, Loss: 0.322425901889801
setp: 1400, Loss: 0.3611055910587311
setp: 1500, Loss: 0.34702131152153015
setp: 1600, Loss: 0.31957101821899414
setp: 1700, Loss: 0.3184935450553894
setp: 1800, Loss: 0.31868278980255127
setp: 1900, Loss: 0.31976547837257385
setp: 2000, Loss: 0.31989166140556335
setp: 2100, Loss: 0.3202614486217499
setp: 2200, Loss: 0.31826913356781006
setp: 2300, Loss: 0.3191472291946411
setp: 2400, Loss: 0.320394903421402
setp: 2500, Loss: 0.32017824053764343
setp: 2600, Loss: 0.31874915957450867
setp: 2700, Loss: 0.3179282248020172
setp: 2800, Loss: 0.40979814529418945
setp: 2900, Loss: 0.38896048069000244
setp: 3000, Loss: 0.3296787440776825
setp: 3100, Loss: 0.3265224099159241
setp: 3200, Loss: 0.3274966776371002
setp: 3300, Loss: 0.32370898127555847
setp: 3400, Loss: 0.3241724669933319
setp: 3500, Loss: 0.3231865167617798
setp: 3600, Loss: 0.3211490213871002
setp: 3700, Loss: 0.36282816529273987
setp: 3800, Loss: 0.3437890112400055
setp: 3900, Loss: 0.3601717948913574
setp: 4000, Loss: 0.3234122395515442
setp: 4100, Loss: 0.32164761424064636
setp: 4200, Loss: 0.3203805088996887
setp: 4300, Loss: 0.3213050067424774
setp: 4400, Loss: 0.32118576765060425
setp: 4500, Loss: 0.3201393485069275
setp: 4600, Loss: 0.31974878907203674
setp: 4700, Loss: 0.321551650762558
setp: 4800, Loss: 0.3208259642124176
setp: 4900, Loss: 0.32012319564819336
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.881578947368421
precision: 0.9361702127659575
recall: 0.7457627118644068
F_score: 0.8301886792452831
model saved.
avg_acc: 0.8868421052631579, avg_f_score: 0.8443837603234732
-------------subject: 10-------------
==========valence==========
******fold 0******
[292, 316]
training...
setp: 0, Loss: 0.7065711617469788
setp: 100, Loss: 0.6559182405471802
setp: 200, Loss: 0.6058627963066101
setp: 300, Loss: 0.46735310554504395
setp: 400, Loss: 0.5279748439788818
setp: 500, Loss: 0.46135276556015015
setp: 600, Loss: 0.38887807726860046
setp: 700, Loss: 0.4055175483226776
setp: 800, Loss: 0.43433740735054016
setp: 900, Loss: 0.4418637156486511
setp: 1000, Loss: 0.41784724593162537
setp: 1100, Loss: 0.3933388888835907
setp: 1200, Loss: 0.4111412465572357
setp: 1300, Loss: 0.35661888122558594
setp: 1400, Loss: 0.35737279057502747
setp: 1500, Loss: 0.3549613058567047
setp: 1600, Loss: 0.3778691291809082
setp: 1700, Loss: 0.3682441711425781
setp: 1800, Loss: 0.351325660943985
setp: 1900, Loss: 0.3850908875465393
setp: 2000, Loss: 0.34802430868148804
setp: 2100, Loss: 0.32491013407707214
setp: 2200, Loss: 0.3174607455730438
setp: 2300, Loss: 0.38034558296203613
setp: 2400, Loss: 0.34842270612716675
setp: 2500, Loss: 0.35177910327911377
setp: 2600, Loss: 0.31785911321640015
setp: 2700, Loss: 0.3479766845703125
setp: 2800, Loss: 0.4685673713684082
setp: 2900, Loss: 0.32704588770866394
setp: 3000, Loss: 0.3229951560497284
setp: 3100, Loss: 0.31806766986846924
setp: 3200, Loss: 0.31629326939582825
setp: 3300, Loss: 0.31873154640197754
setp: 3400, Loss: 0.3202217221260071
setp: 3500, Loss: 0.31743162870407104
setp: 3600, Loss: 0.3158012330532074
setp: 3700, Loss: 0.32626157999038696
setp: 3800, Loss: 0.3201299011707306
setp: 3900, Loss: 0.3164314925670624
setp: 4000, Loss: 0.31898143887519836
setp: 4100, Loss: 0.3200729489326477
setp: 4200, Loss: 0.31598445773124695
setp: 4300, Loss: 0.31670597195625305
setp: 4400, Loss: 0.3156396448612213
setp: 4500, Loss: 0.3163551092147827
setp: 4600, Loss: 0.31624722480773926
setp: 4700, Loss: 0.3175181746482849
setp: 4800, Loss: 0.3156430721282959
setp: 4900, Loss: 0.31585633754730225
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9876543209876543
recall: 0.9090909090909091
F_score: 0.9467455621301774
******fold 1******
[305, 303]
training...
setp: 0, Loss: 0.6918965578079224
setp: 100, Loss: 0.628019392490387
setp: 200, Loss: 0.5509239435195923
setp: 300, Loss: 0.49489036202430725
setp: 400, Loss: 0.4568525552749634
setp: 500, Loss: 0.4245546758174896
setp: 600, Loss: 0.35905545949935913
setp: 700, Loss: 0.3503252863883972
setp: 800, Loss: 0.3273601233959198
setp: 900, Loss: 0.32712802290916443
setp: 1000, Loss: 0.3998103439807892
setp: 1100, Loss: 0.31752464175224304
setp: 1200, Loss: 0.31976231932640076
setp: 1300, Loss: 0.3186698257923126
setp: 1400, Loss: 0.31657183170318604
setp: 1500, Loss: 0.3186008334159851
setp: 1600, Loss: 0.35397869348526
setp: 1700, Loss: 0.3158118426799774
setp: 1800, Loss: 0.3156450390815735
setp: 1900, Loss: 0.31953975558280945
setp: 2000, Loss: 0.3180164694786072
setp: 2100, Loss: 0.3168560862541199
setp: 2200, Loss: 0.31849876046180725
setp: 2300, Loss: 0.3485161364078522
setp: 2400, Loss: 0.3170006573200226
setp: 2500, Loss: 0.31745949387550354
setp: 2600, Loss: 0.3170316219329834
setp: 2700, Loss: 0.31739771366119385
setp: 2800, Loss: 0.3209705948829651
setp: 2900, Loss: 0.3560763895511627
setp: 3000, Loss: 0.3163216710090637
setp: 3100, Loss: 0.317964106798172
setp: 3200, Loss: 0.31615912914276123
setp: 3300, Loss: 0.31609970331192017
setp: 3400, Loss: 0.31786784529685974
setp: 3500, Loss: 0.31670883297920227
setp: 3600, Loss: 0.3163054585456848
setp: 3700, Loss: 0.3171539902687073
setp: 3800, Loss: 0.3177173137664795
setp: 3900, Loss: 0.31765836477279663
setp: 4000, Loss: 0.32334834337234497
setp: 4100, Loss: 0.3359954059123993
setp: 4200, Loss: 0.3452639579772949
setp: 4300, Loss: 0.3158554136753082
setp: 4400, Loss: 0.31981152296066284
setp: 4500, Loss: 0.3162994980812073
setp: 4600, Loss: 0.3171559274196625
setp: 4700, Loss: 0.31612759828567505
setp: 4800, Loss: 0.3163439631462097
setp: 4900, Loss: 0.3162384629249573
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.9210526315789473
recall: 0.9333333333333333
F_score: 0.9271523178807947
******fold 2******
[309, 299]
training...
setp: 0, Loss: 0.7106704711914062
setp: 100, Loss: 0.5619065761566162
setp: 200, Loss: 0.4935781955718994
setp: 300, Loss: 0.5271039605140686
setp: 400, Loss: 0.5182938575744629
setp: 500, Loss: 0.4478147625923157
setp: 600, Loss: 0.3816770017147064
setp: 700, Loss: 0.39789676666259766
setp: 800, Loss: 0.38727840781211853
setp: 900, Loss: 0.3548690676689148
setp: 1000, Loss: 0.3801133930683136
setp: 1100, Loss: 0.38145244121551514
setp: 1200, Loss: 0.3801967203617096
setp: 1300, Loss: 0.38701558113098145
setp: 1400, Loss: 0.37956443428993225
setp: 1500, Loss: 0.3518831431865692
setp: 1600, Loss: 0.34628409147262573
setp: 1700, Loss: 0.38047510385513306
setp: 1800, Loss: 0.35400205850601196
setp: 1900, Loss: 0.35050562024116516
setp: 2000, Loss: 0.34804046154022217
setp: 2100, Loss: 0.3176155090332031
setp: 2200, Loss: 0.37261760234832764
setp: 2300, Loss: 0.3824157416820526
setp: 2400, Loss: 0.3201879560947418
setp: 2500, Loss: 0.32399651408195496
setp: 2600, Loss: 0.333259642124176
setp: 2700, Loss: 0.3464459180831909
setp: 2800, Loss: 0.31563547253608704
setp: 2900, Loss: 0.3167344331741333
setp: 3000, Loss: 0.31602898240089417
setp: 3100, Loss: 0.34695303440093994
setp: 3200, Loss: 0.34769436717033386
setp: 3300, Loss: 0.3175252377986908
setp: 3400, Loss: 0.31720152497291565
setp: 3500, Loss: 0.31621599197387695
setp: 3600, Loss: 0.31845611333847046
setp: 3700, Loss: 0.31841710209846497
setp: 3800, Loss: 0.3169710636138916
setp: 3900, Loss: 0.3159000277519226
setp: 4000, Loss: 0.48724067211151123
setp: 4100, Loss: 0.32468128204345703
setp: 4200, Loss: 0.4302401840686798
setp: 4300, Loss: 0.3157252371311188
setp: 4400, Loss: 0.3162425756454468
setp: 4500, Loss: 0.31599873304367065
setp: 4600, Loss: 0.3467874825000763
setp: 4700, Loss: 0.3158385157585144
setp: 4800, Loss: 0.3163084387779236
setp: 4900, Loss: 0.3160167634487152
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9935691318327974
recall: 1.0
F_score: 0.996774193548387
validating...
acc: 0.9539473684210527
precision: 0.9444444444444444
recall: 0.9577464788732394
F_score: 0.951048951048951
******fold 3******
[300, 308]
training...
setp: 0, Loss: 0.6953087449073792
setp: 100, Loss: 0.5965087413787842
setp: 200, Loss: 0.5311139225959778
setp: 300, Loss: 0.5196287035942078
setp: 400, Loss: 0.4808867871761322
setp: 500, Loss: 0.3924007713794708
setp: 600, Loss: 0.41587740182876587
setp: 700, Loss: 0.44930633902549744
setp: 800, Loss: 0.3963988125324249
setp: 900, Loss: 0.4190700948238373
setp: 1000, Loss: 0.3690660297870636
setp: 1100, Loss: 0.3732556402683258
setp: 1200, Loss: 0.4274190068244934
setp: 1300, Loss: 0.37227392196655273
setp: 1400, Loss: 0.36049655079841614
setp: 1500, Loss: 0.43205633759498596
setp: 1600, Loss: 0.3947802782058716
setp: 1700, Loss: 0.38112980127334595
setp: 1800, Loss: 0.3480236828327179
setp: 1900, Loss: 0.3590438961982727
setp: 2000, Loss: 0.3222777545452118
setp: 2100, Loss: 0.3500669598579407
setp: 2200, Loss: 0.3427833914756775
setp: 2300, Loss: 0.35588547587394714
setp: 2400, Loss: 0.33480721712112427
setp: 2500, Loss: 0.3409535586833954
setp: 2600, Loss: 0.3174186646938324
setp: 2700, Loss: 0.34942910075187683
setp: 2800, Loss: 0.3164125084877014
setp: 2900, Loss: 0.3363803029060364
setp: 3000, Loss: 0.31717902421951294
setp: 3100, Loss: 0.3484697639942169
setp: 3200, Loss: 0.3166552782058716
setp: 3300, Loss: 0.32843267917633057
setp: 3400, Loss: 0.3985816538333893
setp: 3500, Loss: 0.33126944303512573
setp: 3600, Loss: 0.3688381016254425
setp: 3700, Loss: 0.31559401750564575
setp: 3800, Loss: 0.3205462396144867
setp: 3900, Loss: 0.3159947097301483
setp: 4000, Loss: 0.3168380856513977
setp: 4100, Loss: 0.31673651933670044
setp: 4200, Loss: 0.31815212965011597
setp: 4300, Loss: 0.3171446621417999
setp: 4400, Loss: 0.31582701206207275
setp: 4500, Loss: 0.31859898567199707
setp: 4600, Loss: 0.3155514597892761
setp: 4700, Loss: 0.3181525766849518
setp: 4800, Loss: 0.31592273712158203
setp: 4900, Loss: 0.31788960099220276
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9966777408637874
recall: 1.0
F_score: 0.9983361064891847
validating...
acc: 0.9210526315789473
precision: 0.8777777777777778
recall: 0.9875
F_score: 0.9294117647058824
******fold 4******
[314, 294]
training...
setp: 0, Loss: 0.7231900095939636
setp: 100, Loss: 0.6688790917396545
setp: 200, Loss: 0.5473341941833496
setp: 300, Loss: 0.5154392123222351
setp: 400, Loss: 0.5002049803733826
setp: 500, Loss: 0.4982783794403076
setp: 600, Loss: 0.3463127911090851
setp: 700, Loss: 0.372994065284729
setp: 800, Loss: 0.36588624119758606
setp: 900, Loss: 0.3961816132068634
setp: 1000, Loss: 0.3811706006526947
setp: 1100, Loss: 0.3358452320098877
setp: 1200, Loss: 0.35282260179519653
setp: 1300, Loss: 0.3815760910511017
setp: 1400, Loss: 0.34871017932891846
setp: 1500, Loss: 0.41096046566963196
setp: 1600, Loss: 0.4089444577693939
setp: 1700, Loss: 0.3424229621887207
setp: 1800, Loss: 0.32170358300209045
setp: 1900, Loss: 0.44545525312423706
setp: 2000, Loss: 0.34942200779914856
setp: 2100, Loss: 0.35120531916618347
setp: 2200, Loss: 0.34934160113334656
setp: 2300, Loss: 0.3524963855743408
setp: 2400, Loss: 0.35305941104888916
setp: 2500, Loss: 0.31642279028892517
setp: 2600, Loss: 0.3259238302707672
setp: 2700, Loss: 0.31816256046295166
setp: 2800, Loss: 0.31921544671058655
setp: 2900, Loss: 0.32696443796157837
setp: 3000, Loss: 0.32275184988975525
setp: 3100, Loss: 0.34869930148124695
setp: 3200, Loss: 0.3754655122756958
setp: 3300, Loss: 0.3194787800312042
setp: 3400, Loss: 0.34701937437057495
setp: 3500, Loss: 0.378732293844223
setp: 3600, Loss: 0.31759440898895264
setp: 3700, Loss: 0.316293865442276
setp: 3800, Loss: 0.3469855785369873
setp: 3900, Loss: 0.3204546570777893
setp: 4000, Loss: 0.317045122385025
setp: 4100, Loss: 0.3504270017147064
setp: 4200, Loss: 0.31625670194625854
setp: 4300, Loss: 0.31763947010040283
setp: 4400, Loss: 0.31617259979248047
setp: 4500, Loss: 0.3166912794113159
setp: 4600, Loss: 0.3166964054107666
setp: 4700, Loss: 0.3162820637226105
setp: 4800, Loss: 0.3165837526321411
setp: 4900, Loss: 0.31632518768310547
training successfully ended.
validating...
acc: 0.993421052631579
precision: 0.990506329113924
recall: 0.9968152866242038
F_score: 0.9936507936507937
validating...
acc: 0.9144736842105263
precision: 0.8955223880597015
recall: 0.9090909090909091
F_score: 0.9022556390977443
model saved.
avg_acc: 0.9315789473684211, avg_f_score: 0.9313228469727101
==========arousal==========
******fold 0******
[270, 338]
training...
setp: 0, Loss: 0.7305359244346619
setp: 100, Loss: 0.6835424900054932
setp: 200, Loss: 0.6244922876358032
setp: 300, Loss: 0.5906686782836914
setp: 400, Loss: 0.46512728929519653
setp: 500, Loss: 0.38349947333335876
setp: 600, Loss: 0.3282760679721832
setp: 700, Loss: 0.3623826503753662
setp: 800, Loss: 0.3218823969364166
setp: 900, Loss: 0.32145431637763977
setp: 1000, Loss: 0.3217720687389374
setp: 1100, Loss: 0.3210946321487427
setp: 1200, Loss: 0.3189532160758972
setp: 1300, Loss: 0.31750720739364624
setp: 1400, Loss: 0.3172615170478821
setp: 1500, Loss: 0.3183106780052185
setp: 1600, Loss: 0.31793418526649475
setp: 1700, Loss: 0.3173702657222748
setp: 1800, Loss: 0.7097412347793579
setp: 1900, Loss: 0.33758345246315
setp: 2000, Loss: 0.32338449358940125
setp: 2100, Loss: 0.3400665819644928
setp: 2200, Loss: 0.3423348367214203
setp: 2300, Loss: 0.31675639748573303
setp: 2400, Loss: 0.31692370772361755
setp: 2500, Loss: 0.31651806831359863
setp: 2600, Loss: 0.3190273642539978
setp: 2700, Loss: 0.3174870014190674
setp: 2800, Loss: 0.31791579723358154
setp: 2900, Loss: 0.3180902600288391
setp: 3000, Loss: 0.35117775201797485
setp: 3100, Loss: 0.3165559768676758
setp: 3200, Loss: 0.3173925280570984
setp: 3300, Loss: 0.3163595497608185
setp: 3400, Loss: 0.31717556715011597
setp: 3500, Loss: 0.3170834183692932
setp: 3600, Loss: 0.31690576672554016
setp: 3700, Loss: 0.31748145818710327
setp: 3800, Loss: 0.3169776499271393
setp: 3900, Loss: 0.31645554304122925
setp: 4000, Loss: 0.3250527083873749
setp: 4100, Loss: 0.31713202595710754
setp: 4200, Loss: 0.3171967566013336
setp: 4300, Loss: 0.31613460183143616
setp: 4400, Loss: 0.31643974781036377
setp: 4500, Loss: 0.31858181953430176
setp: 4600, Loss: 0.31778693199157715
setp: 4700, Loss: 0.3177231550216675
setp: 4800, Loss: 0.3166539669036865
setp: 4900, Loss: 0.3243405818939209
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9671052631578947
precision: 0.958904109589041
recall: 0.9722222222222222
F_score: 0.9655172413793104
******fold 1******
[277, 331]
training...
setp: 0, Loss: 0.6768227219581604
setp: 100, Loss: 0.7072238922119141
setp: 200, Loss: 0.6421188116073608
setp: 300, Loss: 0.43585875630378723
setp: 400, Loss: 0.37971940636634827
setp: 500, Loss: 0.37689945101737976
setp: 600, Loss: 0.35822948813438416
setp: 700, Loss: 0.33856990933418274
setp: 800, Loss: 0.3243756592273712
setp: 900, Loss: 0.32099229097366333
setp: 1000, Loss: 0.31979691982269287
setp: 1100, Loss: 0.3205895721912384
setp: 1200, Loss: 0.3499755263328552
setp: 1300, Loss: 0.32122164964675903
setp: 1400, Loss: 0.3205561935901642
setp: 1500, Loss: 0.31994590163230896
setp: 1600, Loss: 0.32980775833129883
setp: 1700, Loss: 0.31973281502723694
setp: 1800, Loss: 0.3180370330810547
setp: 1900, Loss: 0.3196399509906769
setp: 2000, Loss: 0.3193274438381195
setp: 2100, Loss: 0.32039687037467957
setp: 2200, Loss: 0.319597452878952
setp: 2300, Loss: 0.3195642828941345
setp: 2400, Loss: 0.31953004002571106
setp: 2500, Loss: 0.31937581300735474
setp: 2600, Loss: 0.3212859332561493
setp: 2700, Loss: 0.31994110345840454
setp: 2800, Loss: 0.31968364119529724
setp: 2900, Loss: 0.3199129104614258
setp: 3000, Loss: 0.3196016550064087
setp: 3100, Loss: 0.3193102180957794
setp: 3200, Loss: 0.3207097053527832
setp: 3300, Loss: 0.3192675709724426
setp: 3400, Loss: 0.4065402150154114
setp: 3500, Loss: 0.32518938183784485
setp: 3600, Loss: 0.32161375880241394
setp: 3700, Loss: 0.32194915413856506
setp: 3800, Loss: 0.3210981786251068
setp: 3900, Loss: 0.32123422622680664
setp: 4000, Loss: 0.3223065137863159
setp: 4100, Loss: 0.32053935527801514
setp: 4200, Loss: 0.32046353816986084
setp: 4300, Loss: 0.3205409348011017
setp: 4400, Loss: 0.32141390442848206
setp: 4500, Loss: 0.3222348093986511
setp: 4600, Loss: 0.3208922743797302
setp: 4700, Loss: 0.3212280571460724
setp: 4800, Loss: 0.3207106292247772
setp: 4900, Loss: 0.3204417824745178
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9964028776978417
recall: 1.0
F_score: 0.9981981981981982
validating...
acc: 0.881578947368421
precision: 0.8507462686567164
recall: 0.8769230769230769
F_score: 0.8636363636363636
******fold 2******
[265, 343]
training...
setp: 0, Loss: 0.7303559184074402
setp: 100, Loss: 0.6685014367103577
setp: 200, Loss: 0.6917250752449036
setp: 300, Loss: 0.5242551565170288
setp: 400, Loss: 0.4283255934715271
setp: 500, Loss: 0.3457815945148468
setp: 600, Loss: 0.3465630114078522
setp: 700, Loss: 0.37460657954216003
setp: 800, Loss: 0.32276007533073425
setp: 900, Loss: 0.3214402496814728
setp: 1000, Loss: 0.32306382060050964
setp: 1100, Loss: 0.32352152466773987
setp: 1200, Loss: 0.32004913687705994
setp: 1300, Loss: 0.32106781005859375
setp: 1400, Loss: 0.3198264539241791
setp: 1500, Loss: 0.319619357585907
setp: 1600, Loss: 0.32018813490867615
setp: 1700, Loss: 0.4095863401889801
setp: 1800, Loss: 0.34416496753692627
setp: 1900, Loss: 0.3596593737602234
setp: 2000, Loss: 0.32135143876075745
setp: 2100, Loss: 0.32331582903862
setp: 2200, Loss: 0.3218279182910919
setp: 2300, Loss: 0.335109144449234
setp: 2400, Loss: 0.31930845975875854
setp: 2500, Loss: 0.32317331433296204
setp: 2600, Loss: 0.3235148787498474
setp: 2700, Loss: 0.319413423538208
setp: 2800, Loss: 0.3191468417644501
setp: 2900, Loss: 0.31948918104171753
setp: 3000, Loss: 0.31959572434425354
setp: 3100, Loss: 0.3192580044269562
setp: 3200, Loss: 0.3208702504634857
setp: 3300, Loss: 0.31949305534362793
setp: 3400, Loss: 0.3196374177932739
setp: 3500, Loss: 0.31994864344596863
setp: 3600, Loss: 0.31863486766815186
setp: 3700, Loss: 0.3195067048072815
setp: 3800, Loss: 0.3199913203716278
setp: 3900, Loss: 0.3194516599178314
setp: 4000, Loss: 0.35711869597435
setp: 4100, Loss: 0.3426423966884613
setp: 4200, Loss: 0.3409618139266968
setp: 4300, Loss: 0.32028138637542725
setp: 4400, Loss: 0.3199543356895447
setp: 4500, Loss: 0.3220026195049286
setp: 4600, Loss: 0.33538156747817993
setp: 4700, Loss: 0.31972789764404297
setp: 4800, Loss: 0.32002416253089905
setp: 4900, Loss: 0.32039621472358704
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9144736842105263
precision: 0.9
recall: 0.935064935064935
F_score: 0.9171974522292993
******fold 3******
[284, 324]
training...
setp: 0, Loss: 0.7448546886444092
setp: 100, Loss: 0.6958436369895935
setp: 200, Loss: 0.6086301803588867
setp: 300, Loss: 0.47945013642311096
setp: 400, Loss: 0.4078815281391144
setp: 500, Loss: 0.3875844478607178
setp: 600, Loss: 0.3452034890651703
setp: 700, Loss: 0.33636045455932617
setp: 800, Loss: 0.36171191930770874
setp: 900, Loss: 0.33157944679260254
setp: 1000, Loss: 0.3334364593029022
setp: 1100, Loss: 0.3235808312892914
setp: 1200, Loss: 0.38069191575050354
setp: 1300, Loss: 0.3331705331802368
setp: 1400, Loss: 0.31823888421058655
setp: 1500, Loss: 0.3195699155330658
setp: 1600, Loss: 0.31916993856430054
setp: 1700, Loss: 0.32034072279930115
setp: 1800, Loss: 0.3207937777042389
setp: 1900, Loss: 0.32062482833862305
setp: 2000, Loss: 0.3191479444503784
setp: 2100, Loss: 0.3195140063762665
setp: 2200, Loss: 0.319749116897583
setp: 2300, Loss: 0.3205339014530182
setp: 2400, Loss: 0.3193817138671875
setp: 2500, Loss: 0.3184124231338501
setp: 2600, Loss: 0.3206545412540436
setp: 2700, Loss: 0.32173269987106323
setp: 2800, Loss: 0.3881966471672058
setp: 2900, Loss: 0.33932870626449585
setp: 3000, Loss: 0.3266667127609253
setp: 3100, Loss: 0.3385918140411377
setp: 3200, Loss: 0.3259924650192261
setp: 3300, Loss: 0.32757246494293213
setp: 3400, Loss: 0.3322909474372864
setp: 3500, Loss: 0.3256637156009674
setp: 3600, Loss: 0.338491290807724
setp: 3700, Loss: 0.3221413791179657
setp: 3800, Loss: 0.32802262902259827
setp: 3900, Loss: 0.3330058455467224
setp: 4000, Loss: 0.32448217272758484
setp: 4100, Loss: 0.32137930393218994
setp: 4200, Loss: 0.3238736093044281
setp: 4300, Loss: 0.32194581627845764
setp: 4400, Loss: 0.32213711738586426
setp: 4500, Loss: 0.32378414273262024
setp: 4600, Loss: 0.3234248757362366
setp: 4700, Loss: 0.3222755193710327
setp: 4800, Loss: 0.3219671845436096
setp: 4900, Loss: 0.3218849003314972
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.8852459016393442
recall: 0.9310344827586207
F_score: 0.9075630252100839
******fold 4******
[272, 336]
training...
setp: 0, Loss: 0.6931597590446472
setp: 100, Loss: 0.6850863695144653
setp: 200, Loss: 0.6920380592346191
setp: 300, Loss: 0.6805295944213867
setp: 400, Loss: 0.6385343074798584
setp: 500, Loss: 0.42577579617500305
setp: 600, Loss: 0.37402796745300293
setp: 700, Loss: 0.3794958293437958
setp: 800, Loss: 0.32886284589767456
setp: 900, Loss: 0.3217971920967102
setp: 1000, Loss: 0.32198232412338257
setp: 1100, Loss: 0.3221920430660248
setp: 1200, Loss: 0.32246750593185425
setp: 1300, Loss: 0.3210941255092621
setp: 1400, Loss: 0.3185800313949585
setp: 1500, Loss: 0.34368079900741577
setp: 1600, Loss: 0.3200209438800812
setp: 1700, Loss: 0.3187525272369385
setp: 1800, Loss: 0.3200680613517761
setp: 1900, Loss: 0.32271572947502136
setp: 2000, Loss: 0.34929534792900085
setp: 2100, Loss: 0.31874701380729675
setp: 2200, Loss: 0.3217240273952484
setp: 2300, Loss: 0.3203498423099518
setp: 2400, Loss: 0.31776976585388184
setp: 2500, Loss: 0.31796008348464966
setp: 2600, Loss: 0.31998464465141296
setp: 2700, Loss: 0.31834742426872253
setp: 2800, Loss: 0.3183264136314392
setp: 2900, Loss: 0.3414846956729889
setp: 3000, Loss: 0.3315722346305847
setp: 3100, Loss: 0.3177426755428314
setp: 3200, Loss: 0.31716257333755493
setp: 3300, Loss: 0.3164726495742798
setp: 3400, Loss: 0.31839680671691895
setp: 3500, Loss: 0.31870874762535095
setp: 3600, Loss: 0.31841591000556946
setp: 3700, Loss: 0.3179338276386261
setp: 3800, Loss: 0.3194199800491333
setp: 3900, Loss: 0.3166392147541046
setp: 4000, Loss: 0.31889432668685913
setp: 4100, Loss: 0.31813088059425354
setp: 4200, Loss: 0.31935977935791016
setp: 4300, Loss: 0.3172828257083893
setp: 4400, Loss: 0.31810009479522705
setp: 4500, Loss: 0.31871578097343445
setp: 4600, Loss: 0.33776089549064636
setp: 4700, Loss: 0.33668768405914307
setp: 4800, Loss: 0.35424262285232544
setp: 4900, Loss: 0.3173215687274933
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9552238805970149
recall: 0.9142857142857143
F_score: 0.9343065693430657
model saved.
avg_acc: 0.9263157894736842, avg_f_score: 0.9176441303596246
-------------subject: 11-------------
==========valence==========
******fold 0******
[233, 375]
training...
setp: 0, Loss: 0.6501058340072632
setp: 100, Loss: 0.6493777632713318
setp: 200, Loss: 0.6628453731536865
setp: 300, Loss: 0.5818507075309753
setp: 400, Loss: 0.5334959626197815
setp: 500, Loss: 0.5344952344894409
setp: 600, Loss: 0.5942022800445557
setp: 700, Loss: 0.5615580081939697
setp: 800, Loss: 0.60493403673172
setp: 900, Loss: 0.5822529196739197
setp: 1000, Loss: 0.5266159176826477
setp: 1100, Loss: 0.5768031477928162
setp: 1200, Loss: 0.5459872484207153
setp: 1300, Loss: 0.5148267149925232
setp: 1400, Loss: 0.48020586371421814
setp: 1500, Loss: 0.4778960347175598
setp: 1600, Loss: 0.44608980417251587
setp: 1700, Loss: 0.43466514348983765
setp: 1800, Loss: 0.4121830463409424
setp: 1900, Loss: 0.39775681495666504
setp: 2000, Loss: 0.36888816952705383
setp: 2100, Loss: 0.41118231415748596
setp: 2200, Loss: 0.3410946726799011
setp: 2300, Loss: 0.3551415801048279
setp: 2400, Loss: 0.3383820354938507
setp: 2500, Loss: 0.34722188115119934
setp: 2600, Loss: 0.36575016379356384
setp: 2700, Loss: 0.44193416833877563
setp: 2800, Loss: 0.36723005771636963
setp: 2900, Loss: 0.3735545575618744
setp: 3000, Loss: 0.3294641077518463
setp: 3100, Loss: 0.3642880916595459
setp: 3200, Loss: 0.3472941219806671
setp: 3300, Loss: 0.32430511713027954
setp: 3400, Loss: 0.3256887197494507
setp: 3500, Loss: 0.3233419954776764
setp: 3600, Loss: 0.32817545533180237
setp: 3700, Loss: 0.32513123750686646
setp: 3800, Loss: 0.3257565498352051
setp: 3900, Loss: 0.3229222595691681
setp: 4000, Loss: 0.3286915719509125
setp: 4100, Loss: 0.322113037109375
setp: 4200, Loss: 0.32533690333366394
setp: 4300, Loss: 0.3232213258743286
setp: 4400, Loss: 0.32345035672187805
setp: 4500, Loss: 0.35159385204315186
setp: 4600, Loss: 0.3225465416908264
setp: 4700, Loss: 0.43488743901252747
setp: 4800, Loss: 0.3594908118247986
setp: 4900, Loss: 0.3297453224658966
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9957081545064378
F_score: 0.9978494623655914
validating...
acc: 0.868421052631579
precision: 0.8923076923076924
recall: 0.8169014084507042
F_score: 0.8529411764705882
******fold 1******
[251, 357]
training...
setp: 0, Loss: 0.6858259439468384
setp: 100, Loss: 0.6654781699180603
setp: 200, Loss: 0.6653189659118652
setp: 300, Loss: 0.6418288946151733
setp: 400, Loss: 0.6381725668907166
setp: 500, Loss: 0.644182562828064
setp: 600, Loss: 0.5485863089561462
setp: 700, Loss: 0.4589156210422516
setp: 800, Loss: 0.5750811696052551
setp: 900, Loss: 0.6286678314208984
setp: 1000, Loss: 0.44222894310951233
setp: 1100, Loss: 0.49801427125930786
setp: 1200, Loss: 0.4328613579273224
setp: 1300, Loss: 0.44300833344459534
setp: 1400, Loss: 0.46324220299720764
setp: 1500, Loss: 0.4240872859954834
setp: 1600, Loss: 0.4374191164970398
setp: 1700, Loss: 0.43299850821495056
setp: 1800, Loss: 0.36411234736442566
setp: 1900, Loss: 0.35807889699935913
setp: 2000, Loss: 0.35759347677230835
setp: 2100, Loss: 0.39340606331825256
setp: 2200, Loss: 0.32773908972740173
setp: 2300, Loss: 0.3238942325115204
setp: 2400, Loss: 0.3202465772628784
setp: 2500, Loss: 0.3493557572364807
setp: 2600, Loss: 0.3264770209789276
setp: 2700, Loss: 0.3214639127254486
setp: 2800, Loss: 0.32328924536705017
setp: 2900, Loss: 0.32155197858810425
setp: 3000, Loss: 0.3320561349391937
setp: 3100, Loss: 0.34967127442359924
setp: 3200, Loss: 0.3215654492378235
setp: 3300, Loss: 0.37936270236968994
setp: 3400, Loss: 0.3312877416610718
setp: 3500, Loss: 0.3501756489276886
setp: 3600, Loss: 0.3245422840118408
setp: 3700, Loss: 0.31946131587028503
setp: 3800, Loss: 0.34876158833503723
setp: 3900, Loss: 0.3195749819278717
setp: 4000, Loss: 0.3506946563720703
setp: 4100, Loss: 0.3196670711040497
setp: 4200, Loss: 0.45519283413887024
setp: 4300, Loss: 0.32313528656959534
setp: 4400, Loss: 0.3184318542480469
setp: 4500, Loss: 0.31719857454299927
setp: 4600, Loss: 0.3219199478626251
setp: 4700, Loss: 0.33016252517700195
setp: 4800, Loss: 0.36485013365745544
setp: 4900, Loss: 0.317308247089386
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.9920318725099602
F_score: 0.996
validating...
acc: 0.9078947368421053
precision: 0.8979591836734694
recall: 0.8301886792452831
F_score: 0.8627450980392157
******fold 2******
[240, 368]
training...
setp: 0, Loss: 0.7619936466217041
setp: 100, Loss: 0.6450681686401367
setp: 200, Loss: 0.6505621075630188
setp: 300, Loss: 0.6019648909568787
setp: 400, Loss: 0.6100583672523499
setp: 500, Loss: 0.6157832741737366
setp: 600, Loss: 0.6110956072807312
setp: 700, Loss: 0.5104779005050659
setp: 800, Loss: 0.5614606738090515
setp: 900, Loss: 0.5563468933105469
setp: 1000, Loss: 0.4512314796447754
setp: 1100, Loss: 0.42026981711387634
setp: 1200, Loss: 0.4206498861312866
setp: 1300, Loss: 0.39476069808006287
setp: 1400, Loss: 0.3823070824146271
setp: 1500, Loss: 0.41747140884399414
setp: 1600, Loss: 0.34583204984664917
setp: 1700, Loss: 0.44514894485473633
setp: 1800, Loss: 0.36039164662361145
setp: 1900, Loss: 0.3443715572357178
setp: 2000, Loss: 0.3571358025074005
setp: 2100, Loss: 0.371020644903183
setp: 2200, Loss: 0.31982123851776123
setp: 2300, Loss: 0.3663925230503082
setp: 2400, Loss: 0.32772910594940186
setp: 2500, Loss: 0.38423699140548706
setp: 2600, Loss: 0.3211213946342468
setp: 2700, Loss: 0.32420051097869873
setp: 2800, Loss: 0.3215688169002533
setp: 2900, Loss: 0.3205565810203552
setp: 3000, Loss: 0.33871328830718994
setp: 3100, Loss: 0.32337871193885803
setp: 3200, Loss: 0.3638666570186615
setp: 3300, Loss: 0.32264310121536255
setp: 3400, Loss: 0.3184148073196411
setp: 3500, Loss: 0.3202134370803833
setp: 3600, Loss: 0.3215482831001282
setp: 3700, Loss: 0.35091957449913025
setp: 3800, Loss: 0.32230567932128906
setp: 3900, Loss: 0.3212839961051941
setp: 4000, Loss: 0.32122623920440674
setp: 4100, Loss: 0.34147170186042786
setp: 4200, Loss: 0.3655504286289215
setp: 4300, Loss: 0.430254727602005
setp: 4400, Loss: 0.3221392035484314
setp: 4500, Loss: 0.31867682933807373
setp: 4600, Loss: 0.3190034031867981
setp: 4700, Loss: 0.3196955621242523
setp: 4800, Loss: 0.31904691457748413
setp: 4900, Loss: 0.3193983733654022
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9958333333333333
F_score: 0.9979123173277662
validating...
acc: 0.8223684210526315
precision: 0.7681159420289855
recall: 0.828125
F_score: 0.7969924812030075
******fold 3******
[234, 374]
training...
setp: 0, Loss: 0.668903112411499
setp: 100, Loss: 0.6329650282859802
setp: 200, Loss: 0.6528401970863342
setp: 300, Loss: 0.5788983106613159
setp: 400, Loss: 0.6521615982055664
setp: 500, Loss: 0.6177327632904053
setp: 600, Loss: 0.6172727346420288
setp: 700, Loss: 0.590085506439209
setp: 800, Loss: 0.6314066052436829
setp: 900, Loss: 0.6518625020980835
setp: 1000, Loss: 0.5154839158058167
setp: 1100, Loss: 0.569625735282898
setp: 1200, Loss: 0.49142172932624817
setp: 1300, Loss: 0.5580502152442932
setp: 1400, Loss: 0.49305644631385803
setp: 1500, Loss: 0.4670206904411316
setp: 1600, Loss: 0.42611730098724365
setp: 1700, Loss: 0.4958016574382782
setp: 1800, Loss: 0.4005606770515442
setp: 1900, Loss: 0.4985324740409851
setp: 2000, Loss: 0.44989609718322754
setp: 2100, Loss: 0.3454594016075134
setp: 2200, Loss: 0.40261927247047424
setp: 2300, Loss: 0.43312978744506836
setp: 2400, Loss: 0.37409287691116333
setp: 2500, Loss: 0.4022962152957916
setp: 2600, Loss: 0.36877644062042236
setp: 2700, Loss: 0.35449016094207764
setp: 2800, Loss: 0.40789616107940674
setp: 2900, Loss: 0.3331325650215149
setp: 3000, Loss: 0.35266226530075073
setp: 3100, Loss: 0.35603997111320496
setp: 3200, Loss: 0.32702523469924927
setp: 3300, Loss: 0.3979030251502991
setp: 3400, Loss: 0.32027190923690796
setp: 3500, Loss: 0.35171258449554443
setp: 3600, Loss: 0.4208601415157318
setp: 3700, Loss: 0.340327650308609
setp: 3800, Loss: 0.3835321068763733
setp: 3900, Loss: 0.4511413872241974
setp: 4000, Loss: 0.35571351647377014
setp: 4100, Loss: 0.3622247576713562
setp: 4200, Loss: 0.39392784237861633
setp: 4300, Loss: 0.31869426369667053
setp: 4400, Loss: 0.3187862038612366
setp: 4500, Loss: 0.3558254539966583
setp: 4600, Loss: 0.3445540964603424
setp: 4700, Loss: 0.3520246744155884
setp: 4800, Loss: 0.3185712695121765
setp: 4900, Loss: 0.31984803080558777
training successfully ended.
validating...
acc: 0.9851973684210527
precision: 0.9787234042553191
recall: 0.9829059829059829
F_score: 0.9808102345415778
validating...
acc: 0.8355263157894737
precision: 0.835820895522388
recall: 0.8
F_score: 0.8175182481751825
******fold 4******
[258, 350]
training...
setp: 0, Loss: 0.6753881573677063
setp: 100, Loss: 0.6716074347496033
setp: 200, Loss: 0.6558045744895935
setp: 300, Loss: 0.5843055844306946
setp: 400, Loss: 0.5861509442329407
setp: 500, Loss: 0.5717911720275879
setp: 600, Loss: 0.5460193157196045
setp: 700, Loss: 0.541050910949707
setp: 800, Loss: 0.5716990828514099
setp: 900, Loss: 0.5145476460456848
setp: 1000, Loss: 0.47622916102409363
setp: 1100, Loss: 0.4118434488773346
setp: 1200, Loss: 0.43469831347465515
setp: 1300, Loss: 0.4281732439994812
setp: 1400, Loss: 0.4489150047302246
setp: 1500, Loss: 0.3973463177680969
setp: 1600, Loss: 0.35358744859695435
setp: 1700, Loss: 0.433513879776001
setp: 1800, Loss: 0.3593226671218872
setp: 1900, Loss: 0.3610236942768097
setp: 2000, Loss: 0.4164883494377136
setp: 2100, Loss: 0.49461829662323
setp: 2200, Loss: 0.35800901055336
setp: 2300, Loss: 0.47532206773757935
setp: 2400, Loss: 0.418998658657074
setp: 2500, Loss: 0.36134031414985657
setp: 2600, Loss: 0.36114928126335144
setp: 2700, Loss: 0.45170730352401733
setp: 2800, Loss: 0.35983186960220337
setp: 2900, Loss: 0.4446444809436798
setp: 3000, Loss: 0.4472578763961792
setp: 3100, Loss: 0.4570293128490448
setp: 3200, Loss: 0.36334753036499023
setp: 3300, Loss: 0.37891054153442383
setp: 3400, Loss: 0.4113418161869049
setp: 3500, Loss: 0.32528069615364075
setp: 3600, Loss: 0.3838118314743042
setp: 3700, Loss: 0.3499205410480499
setp: 3800, Loss: 0.3521597385406494
setp: 3900, Loss: 0.35425907373428345
setp: 4000, Loss: 0.41631630063056946
setp: 4100, Loss: 0.3484998643398285
setp: 4200, Loss: 0.4331503212451935
setp: 4300, Loss: 0.448173463344574
setp: 4400, Loss: 0.3487261235713959
setp: 4500, Loss: 0.3514120578765869
setp: 4600, Loss: 0.39619752764701843
setp: 4700, Loss: 0.31950077414512634
setp: 4800, Loss: 0.4091129004955292
setp: 4900, Loss: 0.3187597095966339
training successfully ended.
validating...
acc: 0.944078947368421
precision: 0.9912280701754386
recall: 0.875968992248062
F_score: 0.9300411522633745
validating...
acc: 0.7368421052631579
precision: 0.5681818181818182
recall: 0.5434782608695652
F_score: 0.5555555555555556
model saved.
avg_acc: 0.8342105263157894, avg_f_score: 0.7771505118887099
==========arousal==========
******fold 0******
[373, 235]
training...
setp: 0, Loss: 0.6892114281654358
setp: 100, Loss: 0.6111189723014832
setp: 200, Loss: 0.6700443029403687
setp: 300, Loss: 0.6326954960823059
setp: 400, Loss: 0.5347000360488892
setp: 500, Loss: 0.5907370448112488
setp: 600, Loss: 0.530558168888092
setp: 700, Loss: 0.5265350341796875
setp: 800, Loss: 0.5046526789665222
setp: 900, Loss: 0.44076159596443176
setp: 1000, Loss: 0.4153553247451782
setp: 1100, Loss: 0.374478816986084
setp: 1200, Loss: 0.4078596830368042
setp: 1300, Loss: 0.4433360695838928
setp: 1400, Loss: 0.4072626531124115
setp: 1500, Loss: 0.33338072896003723
setp: 1600, Loss: 0.4125473201274872
setp: 1700, Loss: 0.3891978859901428
setp: 1800, Loss: 0.4067097306251526
setp: 1900, Loss: 0.35082173347473145
setp: 2000, Loss: 0.3229859173297882
setp: 2100, Loss: 0.3469547927379608
setp: 2200, Loss: 0.3390277624130249
setp: 2300, Loss: 0.327458918094635
setp: 2400, Loss: 0.33787834644317627
setp: 2500, Loss: 0.3198552131652832
setp: 2600, Loss: 0.3250426650047302
setp: 2700, Loss: 0.3228423297405243
setp: 2800, Loss: 0.32192263007164
setp: 2900, Loss: 0.3260183036327362
setp: 3000, Loss: 0.3214929699897766
setp: 3100, Loss: 0.3233691453933716
setp: 3200, Loss: 0.352634459733963
setp: 3300, Loss: 0.35150080919265747
setp: 3400, Loss: 0.3192858397960663
setp: 3500, Loss: 0.3206270635128021
setp: 3600, Loss: 0.3516910970211029
setp: 3700, Loss: 0.4115389287471771
setp: 3800, Loss: 0.3339843451976776
setp: 3900, Loss: 0.3324996829032898
setp: 4000, Loss: 0.32140615582466125
setp: 4100, Loss: 0.34054452180862427
setp: 4200, Loss: 0.3212461471557617
setp: 4300, Loss: 0.3193052411079407
setp: 4400, Loss: 0.3192460536956787
setp: 4500, Loss: 0.3214901387691498
setp: 4600, Loss: 0.31944072246551514
setp: 4700, Loss: 0.3203899562358856
setp: 4800, Loss: 0.31967559456825256
setp: 4900, Loss: 0.3206850290298462
training successfully ended.
validating...
acc: 0.9786184210526315
precision: 0.9891304347826086
recall: 0.9758713136729222
F_score: 0.9824561403508771
validating...
acc: 0.875
precision: 0.9662921348314607
recall: 0.8431372549019608
F_score: 0.900523560209424
******fold 1******
[383, 225]
training...
setp: 0, Loss: 0.8512017130851746
setp: 100, Loss: 0.6595110297203064
setp: 200, Loss: 0.6613101959228516
setp: 300, Loss: 0.5985596776008606
setp: 400, Loss: 0.5289359092712402
setp: 500, Loss: 0.45308753848075867
setp: 600, Loss: 0.3906996548175812
setp: 700, Loss: 0.4856160581111908
setp: 800, Loss: 0.4150048494338989
setp: 900, Loss: 0.3902447819709778
setp: 1000, Loss: 0.3567117750644684
setp: 1100, Loss: 0.3674902021884918
setp: 1200, Loss: 0.36175280809402466
setp: 1300, Loss: 0.35516053438186646
setp: 1400, Loss: 0.37619608640670776
setp: 1500, Loss: 0.3245438039302826
setp: 1600, Loss: 0.3494316041469574
setp: 1700, Loss: 0.33148467540740967
setp: 1800, Loss: 0.32434535026550293
setp: 1900, Loss: 0.3294945955276489
setp: 2000, Loss: 0.34807395935058594
setp: 2100, Loss: 0.32504311203956604
setp: 2200, Loss: 0.32026344537734985
setp: 2300, Loss: 0.3233313262462616
setp: 2400, Loss: 0.32139623165130615
setp: 2500, Loss: 0.3186354339122772
setp: 2600, Loss: 0.32151201367378235
setp: 2700, Loss: 0.322945773601532
setp: 2800, Loss: 0.3228558301925659
setp: 2900, Loss: 0.3199562430381775
setp: 3000, Loss: 0.3208356499671936
setp: 3100, Loss: 0.3209978938102722
setp: 3200, Loss: 0.6038175225257874
setp: 3300, Loss: 0.411263644695282
setp: 3400, Loss: 0.3282947540283203
setp: 3500, Loss: 0.3278002142906189
setp: 3600, Loss: 0.40143436193466187
setp: 3700, Loss: 0.32572218775749207
setp: 3800, Loss: 0.32982000708580017
setp: 3900, Loss: 0.32362574338912964
setp: 4000, Loss: 0.3286837637424469
setp: 4100, Loss: 0.32123562693595886
setp: 4200, Loss: 0.3251214325428009
setp: 4300, Loss: 0.3221539556980133
setp: 4400, Loss: 0.31911054253578186
setp: 4500, Loss: 0.32228925824165344
setp: 4600, Loss: 0.3242312967777252
setp: 4700, Loss: 0.3230283856391907
setp: 4800, Loss: 0.3208047151565552
setp: 4900, Loss: 0.3209845721721649
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9973958333333334
recall: 1.0
F_score: 0.998696219035202
validating...
acc: 0.9013157894736842
precision: 0.9139784946236559
recall: 0.9239130434782609
F_score: 0.918918918918919
******fold 2******
[380, 228]
training...
setp: 0, Loss: 0.7033988237380981
setp: 100, Loss: 0.6282767057418823
setp: 200, Loss: 0.6234500408172607
setp: 300, Loss: 0.620745837688446
setp: 400, Loss: 0.5926311612129211
setp: 500, Loss: 0.5474768877029419
setp: 600, Loss: 0.522124171257019
setp: 700, Loss: 0.49654924869537354
setp: 800, Loss: 0.4372365176677704
setp: 900, Loss: 0.39391523599624634
setp: 1000, Loss: 0.3906008303165436
setp: 1100, Loss: 0.34510573744773865
setp: 1200, Loss: 0.3459339737892151
setp: 1300, Loss: 0.345992773771286
setp: 1400, Loss: 0.3280973732471466
setp: 1500, Loss: 0.32917213439941406
setp: 1600, Loss: 0.3534461557865143
setp: 1700, Loss: 0.3255687952041626
setp: 1800, Loss: 0.32262369990348816
setp: 1900, Loss: 0.32255446910858154
setp: 2000, Loss: 0.3212904930114746
setp: 2100, Loss: 0.3219621479511261
setp: 2200, Loss: 0.32072684168815613
setp: 2300, Loss: 0.3221319317817688
setp: 2400, Loss: 0.32171911001205444
setp: 2500, Loss: 0.3212699294090271
setp: 2600, Loss: 0.37686946988105774
setp: 2700, Loss: 0.33072832226753235
setp: 2800, Loss: 0.32714560627937317
setp: 2900, Loss: 0.32345494627952576
setp: 3000, Loss: 0.3191750943660736
setp: 3100, Loss: 0.3230065405368805
setp: 3200, Loss: 0.32024094462394714
setp: 3300, Loss: 0.3196450471878052
setp: 3400, Loss: 0.31977567076683044
setp: 3500, Loss: 0.320273756980896
setp: 3600, Loss: 0.320953905582428
setp: 3700, Loss: 0.31951865553855896
setp: 3800, Loss: 0.5098910331726074
setp: 3900, Loss: 0.3334270715713501
setp: 4000, Loss: 0.3294910192489624
setp: 4100, Loss: 0.3333986699581146
setp: 4200, Loss: 0.32338178157806396
setp: 4300, Loss: 0.3206554353237152
setp: 4400, Loss: 0.32077938318252563
setp: 4500, Loss: 0.3208874762058258
setp: 4600, Loss: 0.32061558961868286
setp: 4700, Loss: 0.32190051674842834
setp: 4800, Loss: 0.318975031375885
setp: 4900, Loss: 0.3204694092273712
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8947368421052632
precision: 0.9247311827956989
recall: 0.9052631578947369
F_score: 0.9148936170212766
******fold 3******
[379, 229]
training...
setp: 0, Loss: 0.7486789226531982
setp: 100, Loss: 0.6783941984176636
setp: 200, Loss: 0.5812768340110779
setp: 300, Loss: 0.6196462512016296
setp: 400, Loss: 0.5623697638511658
setp: 500, Loss: 0.534496545791626
setp: 600, Loss: 0.41117411851882935
setp: 700, Loss: 0.499415785074234
setp: 800, Loss: 0.4032052457332611
setp: 900, Loss: 0.3864150047302246
setp: 1000, Loss: 0.35478276014328003
setp: 1100, Loss: 0.39977407455444336
setp: 1200, Loss: 0.35964155197143555
setp: 1300, Loss: 0.36083605885505676
setp: 1400, Loss: 0.3922571837902069
setp: 1500, Loss: 0.34017637372016907
setp: 1600, Loss: 0.3293986916542053
setp: 1700, Loss: 0.33709877729415894
setp: 1800, Loss: 0.3294379413127899
setp: 1900, Loss: 0.3232452869415283
setp: 2000, Loss: 0.32154399156570435
setp: 2100, Loss: 0.32281753420829773
setp: 2200, Loss: 0.3285391628742218
setp: 2300, Loss: 0.3550756275653839
setp: 2400, Loss: 0.33921465277671814
setp: 2500, Loss: 0.3223174214363098
setp: 2600, Loss: 0.33577463030815125
setp: 2700, Loss: 0.32255151867866516
setp: 2800, Loss: 0.34303995966911316
setp: 2900, Loss: 0.32567593455314636
setp: 3000, Loss: 0.32273629307746887
setp: 3100, Loss: 0.3279585540294647
setp: 3200, Loss: 0.3768978416919708
setp: 3300, Loss: 0.3878973722457886
setp: 3400, Loss: 0.3209850490093231
setp: 3500, Loss: 0.3319464325904846
setp: 3600, Loss: 0.3411605954170227
setp: 3700, Loss: 0.32899317145347595
setp: 3800, Loss: 0.3295045495033264
setp: 3900, Loss: 0.3767188787460327
setp: 4000, Loss: 0.37597277760505676
setp: 4100, Loss: 0.34134164452552795
setp: 4200, Loss: 0.3352973759174347
setp: 4300, Loss: 0.3345724046230316
setp: 4400, Loss: 0.3201165199279785
setp: 4500, Loss: 0.320422500371933
setp: 4600, Loss: 0.32114115357398987
setp: 4700, Loss: 0.3214645981788635
setp: 4800, Loss: 0.3197455108165741
setp: 4900, Loss: 0.32094040513038635
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8486842105263158
precision: 0.8613861386138614
recall: 0.90625
F_score: 0.883248730964467
******fold 4******
[385, 223]
training...
setp: 0, Loss: 0.7413421869277954
setp: 100, Loss: 0.6575559973716736
setp: 200, Loss: 0.6281529068946838
setp: 300, Loss: 0.5563840270042419
setp: 400, Loss: 0.6188780069351196
setp: 500, Loss: 0.5405370593070984
setp: 600, Loss: 0.5599642395973206
setp: 700, Loss: 0.532471776008606
setp: 800, Loss: 0.4865255653858185
setp: 900, Loss: 0.39731740951538086
setp: 1000, Loss: 0.3712269961833954
setp: 1100, Loss: 0.38692694902420044
setp: 1200, Loss: 0.34497034549713135
setp: 1300, Loss: 0.3509104251861572
setp: 1400, Loss: 0.3694254159927368
setp: 1500, Loss: 0.3430643379688263
setp: 1600, Loss: 0.3250373601913452
setp: 1700, Loss: 0.3672754466533661
setp: 1800, Loss: 0.34594354033470154
setp: 1900, Loss: 0.35337960720062256
setp: 2000, Loss: 0.322262704372406
setp: 2100, Loss: 0.3244791328907013
setp: 2200, Loss: 0.33332768082618713
setp: 2300, Loss: 0.32585665583610535
setp: 2400, Loss: 0.3345411419868469
setp: 2500, Loss: 0.32968875765800476
setp: 2600, Loss: 0.3364142179489136
setp: 2700, Loss: 0.3285856544971466
setp: 2800, Loss: 0.33885157108306885
setp: 2900, Loss: 0.32617753744125366
setp: 3000, Loss: 0.32056185603141785
setp: 3100, Loss: 0.32654011249542236
setp: 3200, Loss: 0.3698616027832031
setp: 3300, Loss: 0.32300519943237305
setp: 3400, Loss: 0.3269789218902588
setp: 3500, Loss: 0.3187604546546936
setp: 3600, Loss: 0.32638809084892273
setp: 3700, Loss: 0.3209387958049774
setp: 3800, Loss: 0.3212811350822449
setp: 3900, Loss: 0.3195839822292328
setp: 4000, Loss: 0.3216942548751831
setp: 4100, Loss: 0.3204116225242615
setp: 4200, Loss: 0.324097603559494
setp: 4300, Loss: 0.3227667808532715
setp: 4400, Loss: 0.3230341076850891
setp: 4500, Loss: 0.3228664696216583
setp: 4600, Loss: 0.32139256596565247
setp: 4700, Loss: 0.38099581003189087
setp: 4800, Loss: 0.3226676881313324
setp: 4900, Loss: 0.3212308883666992
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8947368421052632
precision: 0.8854166666666666
recall: 0.9444444444444444
F_score: 0.9139784946236558
model saved.
avg_acc: 0.8828947368421053, avg_f_score: 0.9063126643475485
-------------subject: 12-------------
==========valence==========
******fold 0******
[280, 328]
training...
setp: 0, Loss: 0.6915009021759033
setp: 100, Loss: 0.6575589776039124
setp: 200, Loss: 0.5081405639648438
setp: 300, Loss: 0.4339350461959839
setp: 400, Loss: 0.6019082069396973
setp: 500, Loss: 0.5605030655860901
setp: 600, Loss: 0.44941556453704834
setp: 700, Loss: 0.3993062376976013
setp: 800, Loss: 0.4449107348918915
setp: 900, Loss: 0.401335209608078
setp: 1000, Loss: 0.4066786468029022
setp: 1100, Loss: 0.45674607157707214
setp: 1200, Loss: 0.3316531777381897
setp: 1300, Loss: 0.3939925730228424
setp: 1400, Loss: 0.3654698431491852
setp: 1500, Loss: 0.3828188180923462
setp: 1600, Loss: 0.32570183277130127
setp: 1700, Loss: 0.31931906938552856
setp: 1800, Loss: 0.31933531165122986
setp: 1900, Loss: 0.34997016191482544
setp: 2000, Loss: 0.32679107785224915
setp: 2100, Loss: 0.33491048216819763
setp: 2200, Loss: 0.3302193284034729
setp: 2300, Loss: 0.348685622215271
setp: 2400, Loss: 0.3249857723712921
setp: 2500, Loss: 0.35084936022758484
setp: 2600, Loss: 0.31761765480041504
setp: 2700, Loss: 0.323281854391098
setp: 2800, Loss: 0.3178079128265381
setp: 2900, Loss: 0.3192029297351837
setp: 3000, Loss: 0.32914426922798157
setp: 3100, Loss: 0.33110731840133667
setp: 3200, Loss: 0.32340937852859497
setp: 3300, Loss: 0.3583490550518036
setp: 3400, Loss: 0.31851285696029663
setp: 3500, Loss: 0.3256821036338806
setp: 3600, Loss: 0.3395511507987976
setp: 3700, Loss: 0.31678780913352966
setp: 3800, Loss: 0.32078641653060913
setp: 3900, Loss: 0.36185312271118164
setp: 4000, Loss: 0.31625616550445557
setp: 4100, Loss: 0.31612762808799744
setp: 4200, Loss: 0.3199060261249542
setp: 4300, Loss: 0.31970518827438354
setp: 4400, Loss: 0.3185535967350006
setp: 4500, Loss: 0.31661081314086914
setp: 4600, Loss: 0.3173333406448364
setp: 4700, Loss: 0.31842562556266785
setp: 4800, Loss: 0.3166801333427429
setp: 4900, Loss: 0.31925061345100403
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.99644128113879
recall: 1.0
F_score: 0.9982174688057042
validating...
acc: 0.9013157894736842
precision: 0.8928571428571429
recall: 0.9259259259259259
F_score: 0.9090909090909091
******fold 1******
[291, 317]
training...
setp: 0, Loss: 0.7185137271881104
setp: 100, Loss: 0.584136426448822
setp: 200, Loss: 0.5558908581733704
setp: 300, Loss: 0.5058674216270447
setp: 400, Loss: 0.4944593608379364
setp: 500, Loss: 0.5312692523002625
setp: 600, Loss: 0.4084533154964447
setp: 700, Loss: 0.43779999017715454
setp: 800, Loss: 0.48727935552597046
setp: 900, Loss: 0.4019094407558441
setp: 1000, Loss: 0.3337104916572571
setp: 1100, Loss: 0.40268340706825256
setp: 1200, Loss: 0.38146910071372986
setp: 1300, Loss: 0.3856225609779358
setp: 1400, Loss: 0.3782380521297455
setp: 1500, Loss: 0.372343510389328
setp: 1600, Loss: 0.3293587267398834
setp: 1700, Loss: 0.38380342721939087
setp: 1800, Loss: 0.32447633147239685
setp: 1900, Loss: 0.32038426399230957
setp: 2000, Loss: 0.3304051458835602
setp: 2100, Loss: 0.3771408796310425
setp: 2200, Loss: 0.32386085391044617
setp: 2300, Loss: 0.3286578357219696
setp: 2400, Loss: 0.33902081847190857
setp: 2500, Loss: 0.32039037346839905
setp: 2600, Loss: 0.3204343318939209
setp: 2700, Loss: 0.35058867931365967
setp: 2800, Loss: 0.31847402453422546
setp: 2900, Loss: 0.3164825141429901
setp: 3000, Loss: 0.3441222906112671
setp: 3100, Loss: 0.3188886344432831
setp: 3200, Loss: 0.35174840688705444
setp: 3300, Loss: 0.3208027482032776
setp: 3400, Loss: 0.3196716904640198
setp: 3500, Loss: 0.3172885775566101
setp: 3600, Loss: 0.3165469169616699
setp: 3700, Loss: 0.3190317451953888
setp: 3800, Loss: 0.31851500272750854
setp: 3900, Loss: 0.3193892240524292
setp: 4000, Loss: 0.3184336721897125
setp: 4100, Loss: 0.3172791600227356
setp: 4200, Loss: 0.31815385818481445
setp: 4300, Loss: 0.3185802400112152
setp: 4400, Loss: 0.31754571199417114
setp: 4500, Loss: 0.3403635025024414
setp: 4600, Loss: 0.45324456691741943
setp: 4700, Loss: 0.3181728720664978
setp: 4800, Loss: 0.3163420557975769
setp: 4900, Loss: 0.3202516436576843
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9965635738831615
F_score: 0.9982788296041308
validating...
acc: 0.9210526315789473
precision: 0.9264705882352942
recall: 0.9
F_score: 0.9130434782608695
******fold 2******
[288, 320]
training...
setp: 0, Loss: 0.6929619908332825
setp: 100, Loss: 0.6281279921531677
setp: 200, Loss: 0.5773020386695862
setp: 300, Loss: 0.5803609490394592
setp: 400, Loss: 0.5322535634040833
setp: 500, Loss: 0.4859912395477295
setp: 600, Loss: 0.44840094447135925
setp: 700, Loss: 0.42815670371055603
setp: 800, Loss: 0.4436149299144745
setp: 900, Loss: 0.3579481542110443
setp: 1000, Loss: 0.37715303897857666
setp: 1100, Loss: 0.43960708379745483
setp: 1200, Loss: 0.32440444827079773
setp: 1300, Loss: 0.32185831665992737
setp: 1400, Loss: 0.33262577652931213
setp: 1500, Loss: 0.3417665362358093
setp: 1600, Loss: 0.3813071548938751
setp: 1700, Loss: 0.32195061445236206
setp: 1800, Loss: 0.3252827525138855
setp: 1900, Loss: 0.3356570601463318
setp: 2000, Loss: 0.32199209928512573
setp: 2100, Loss: 0.3265957832336426
setp: 2200, Loss: 0.3181842267513275
setp: 2300, Loss: 0.3337322473526001
setp: 2400, Loss: 0.3212336599826813
setp: 2500, Loss: 0.3193221092224121
setp: 2600, Loss: 0.3182356059551239
setp: 2700, Loss: 0.3362117111682892
setp: 2800, Loss: 0.3224852383136749
setp: 2900, Loss: 0.31786882877349854
setp: 3000, Loss: 0.33169105648994446
setp: 3100, Loss: 0.318023145198822
setp: 3200, Loss: 0.337532103061676
setp: 3300, Loss: 0.319222092628479
setp: 3400, Loss: 0.32209640741348267
setp: 3500, Loss: 0.3183339238166809
setp: 3600, Loss: 0.3173369765281677
setp: 3700, Loss: 0.32169875502586365
setp: 3800, Loss: 0.31683576107025146
setp: 3900, Loss: 0.3173372149467468
setp: 4000, Loss: 0.323493629693985
setp: 4100, Loss: 0.31793448328971863
setp: 4200, Loss: 0.3193802833557129
setp: 4300, Loss: 0.3170597553253174
setp: 4400, Loss: 0.31624069809913635
setp: 4500, Loss: 0.3190770447254181
setp: 4600, Loss: 0.34791234135627747
setp: 4700, Loss: 0.31660234928131104
setp: 4800, Loss: 0.3177623450756073
setp: 4900, Loss: 0.32007986307144165
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9078947368421053
precision: 0.9154929577464789
recall: 0.8904109589041096
F_score: 0.9027777777777778
******fold 3******
[286, 322]
training...
setp: 0, Loss: 0.8113239407539368
setp: 100, Loss: 0.5956491231918335
setp: 200, Loss: 0.5530851483345032
setp: 300, Loss: 0.5114957094192505
setp: 400, Loss: 0.500429630279541
setp: 500, Loss: 0.5566396117210388
setp: 600, Loss: 0.37780699133872986
setp: 700, Loss: 0.3744148313999176
setp: 800, Loss: 0.4229390025138855
setp: 900, Loss: 0.3383292257785797
setp: 1000, Loss: 0.4034285247325897
setp: 1100, Loss: 0.3389850854873657
setp: 1200, Loss: 0.3428429663181305
setp: 1300, Loss: 0.34656989574432373
setp: 1400, Loss: 0.329499751329422
setp: 1500, Loss: 0.34475061297416687
setp: 1600, Loss: 0.3276665508747101
setp: 1700, Loss: 0.35142987966537476
setp: 1800, Loss: 0.3331756293773651
setp: 1900, Loss: 0.3396455645561218
setp: 2000, Loss: 0.32481640577316284
setp: 2100, Loss: 0.32878848910331726
setp: 2200, Loss: 0.32578516006469727
setp: 2300, Loss: 0.32389795780181885
setp: 2400, Loss: 0.3580248951911926
setp: 2500, Loss: 0.35943102836608887
setp: 2600, Loss: 0.33460214734077454
setp: 2700, Loss: 0.3384005129337311
setp: 2800, Loss: 0.31715720891952515
setp: 2900, Loss: 0.3203182816505432
setp: 3000, Loss: 0.3201122283935547
setp: 3100, Loss: 0.31868869066238403
setp: 3200, Loss: 0.31960126757621765
setp: 3300, Loss: 0.31937068700790405
setp: 3400, Loss: 0.31904879212379456
setp: 3500, Loss: 0.3168765902519226
setp: 3600, Loss: 0.3168124556541443
setp: 3700, Loss: 0.5792735815048218
setp: 3800, Loss: 0.3188021183013916
setp: 3900, Loss: 0.3180384635925293
setp: 4000, Loss: 0.32011887431144714
setp: 4100, Loss: 0.31697186827659607
setp: 4200, Loss: 0.3181096613407135
setp: 4300, Loss: 0.3221151828765869
setp: 4400, Loss: 0.31915783882141113
setp: 4500, Loss: 0.31916847825050354
setp: 4600, Loss: 0.3591144382953644
setp: 4700, Loss: 0.33397376537323
setp: 4800, Loss: 0.4018264710903168
setp: 4900, Loss: 0.31850168108940125
training successfully ended.
validating...
acc: 0.9703947368421053
precision: 1.0
recall: 0.9370629370629371
F_score: 0.9675090252707581
validating...
acc: 0.8947368421052632
precision: 0.9682539682539683
recall: 0.8133333333333334
F_score: 0.8840579710144927
******fold 4******
[299, 309]
training...
setp: 0, Loss: 0.6942800879478455
setp: 100, Loss: 0.6023593544960022
setp: 200, Loss: 0.5247606635093689
setp: 300, Loss: 0.5411072373390198
setp: 400, Loss: 0.4412098228931427
setp: 500, Loss: 0.5509718060493469
setp: 600, Loss: 0.4075365960597992
setp: 700, Loss: 0.3519159257411957
setp: 800, Loss: 0.3860625624656677
setp: 900, Loss: 0.35005712509155273
setp: 1000, Loss: 0.3359318971633911
setp: 1100, Loss: 0.32734349370002747
setp: 1200, Loss: 0.3423314690589905
setp: 1300, Loss: 0.3233952820301056
setp: 1400, Loss: 0.3276239037513733
setp: 1500, Loss: 0.32232317328453064
setp: 1600, Loss: 0.32104143500328064
setp: 1700, Loss: 0.31829437613487244
setp: 1800, Loss: 0.32926037907600403
setp: 1900, Loss: 0.3631269931793213
setp: 2000, Loss: 0.31836986541748047
setp: 2100, Loss: 0.31911182403564453
setp: 2200, Loss: 0.3241816759109497
setp: 2300, Loss: 0.3214260935783386
setp: 2400, Loss: 0.3211004137992859
setp: 2500, Loss: 0.31750401854515076
setp: 2600, Loss: 0.32111048698425293
setp: 2700, Loss: 0.32074710726737976
setp: 2800, Loss: 0.32941266894340515
setp: 2900, Loss: 0.32357457280158997
setp: 3000, Loss: 0.32543718814849854
setp: 3100, Loss: 0.32226744294166565
setp: 3200, Loss: 0.32342037558555603
setp: 3300, Loss: 0.32114511728286743
setp: 3400, Loss: 0.32115650177001953
setp: 3500, Loss: 0.32133904099464417
setp: 3600, Loss: 0.32014238834381104
setp: 3700, Loss: 0.3241453170776367
setp: 3800, Loss: 0.3200274705886841
setp: 3900, Loss: 0.32308250665664673
setp: 4000, Loss: 0.3186417520046234
setp: 4100, Loss: 0.3175840973854065
setp: 4200, Loss: 0.3168676197528839
setp: 4300, Loss: 0.31971368193626404
setp: 4400, Loss: 0.31883805990219116
setp: 4500, Loss: 0.3174907863140106
setp: 4600, Loss: 0.31822335720062256
setp: 4700, Loss: 0.32039812207221985
setp: 4800, Loss: 0.3236847221851349
setp: 4900, Loss: 0.3384319543838501
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9144736842105263
precision: 0.8450704225352113
recall: 0.967741935483871
F_score: 0.9022556390977443
model saved.
avg_acc: 0.9078947368421053, avg_f_score: 0.9022451550483588
==========arousal==========
******fold 0******
[100, 508]
training...
setp: 0, Loss: 0.695023775100708
setp: 100, Loss: 0.6290819048881531
setp: 200, Loss: 0.6051781177520752
setp: 300, Loss: 0.551106870174408
setp: 400, Loss: 0.5261901021003723
setp: 500, Loss: 0.5047429203987122
setp: 600, Loss: 0.6007696986198425
setp: 700, Loss: 0.38871413469314575
setp: 800, Loss: 0.3522716462612152
setp: 900, Loss: 0.4423550069332123
setp: 1000, Loss: 0.4036085605621338
setp: 1100, Loss: 0.4408375322818756
setp: 1200, Loss: 0.3806018531322479
setp: 1300, Loss: 0.37979039549827576
setp: 1400, Loss: 0.414700984954834
setp: 1500, Loss: 0.351107120513916
setp: 1600, Loss: 0.35233449935913086
setp: 1700, Loss: 0.37852028012275696
setp: 1800, Loss: 0.3805849552154541
setp: 1900, Loss: 0.4400828182697296
setp: 2000, Loss: 0.3793165385723114
setp: 2100, Loss: 0.37844595313072205
setp: 2200, Loss: 0.4104645252227783
setp: 2300, Loss: 0.3494592010974884
setp: 2400, Loss: 0.34747612476348877
setp: 2500, Loss: 0.3787325918674469
setp: 2600, Loss: 0.37914836406707764
setp: 2700, Loss: 0.4404427111148834
setp: 2800, Loss: 0.52088862657547
setp: 2900, Loss: 0.604742705821991
setp: 3000, Loss: 0.41272565722465515
setp: 3100, Loss: 0.3524278998374939
setp: 3200, Loss: 0.3465654253959656
setp: 3300, Loss: 0.37862589955329895
setp: 3400, Loss: 0.3784305453300476
setp: 3500, Loss: 0.44009703397750854
setp: 3600, Loss: 0.3787911534309387
setp: 3700, Loss: 0.37915781140327454
setp: 3800, Loss: 0.4150978624820709
setp: 3900, Loss: 0.3541175127029419
setp: 4000, Loss: 0.3708983361721039
setp: 4100, Loss: 0.40522488951683044
setp: 4200, Loss: 0.3785640597343445
setp: 4300, Loss: 0.44000643491744995
setp: 4400, Loss: 0.37880751490592957
setp: 4500, Loss: 0.3782036304473877
setp: 4600, Loss: 0.40989115834236145
setp: 4700, Loss: 0.3482251763343811
setp: 4800, Loss: 0.3471769094467163
setp: 4900, Loss: 0.3780764639377594
training successfully ended.
validating...
acc: 0.9438976377952756
precision: 0.9977924944812362
recall: 0.889763779527559
F_score: 0.9406867845993756
validating...
acc: 0.8881578947368421
precision: 0.8333333333333334
recall: 0.6060606060606061
F_score: 0.7017543859649124
******fold 1******
[104, 504]
training...
setp: 0, Loss: 0.7316206693649292
setp: 100, Loss: 0.6241881251335144
setp: 200, Loss: 0.42907091975212097
setp: 300, Loss: 0.3790172338485718
setp: 400, Loss: 0.4165700674057007
setp: 500, Loss: 0.354827880859375
setp: 600, Loss: 0.3386252224445343
setp: 700, Loss: 0.31988802552223206
setp: 800, Loss: 0.319236159324646
setp: 900, Loss: 0.3205038905143738
setp: 1000, Loss: 0.3220805823802948
setp: 1100, Loss: 0.32528170943260193
setp: 1200, Loss: 0.33996790647506714
setp: 1300, Loss: 0.31728923320770264
setp: 1400, Loss: 0.3185088634490967
setp: 1500, Loss: 0.3185586631298065
setp: 1600, Loss: 0.3164689838886261
setp: 1700, Loss: 0.317455530166626
setp: 1800, Loss: 0.31743139028549194
setp: 1900, Loss: 0.3181303143501282
setp: 2000, Loss: 0.42296814918518066
setp: 2100, Loss: 0.32115793228149414
setp: 2200, Loss: 0.3162403702735901
setp: 2300, Loss: 0.31695082783699036
setp: 2400, Loss: 0.31639185547828674
setp: 2500, Loss: 0.31730368733406067
setp: 2600, Loss: 0.3177349865436554
setp: 2700, Loss: 0.3208681344985962
setp: 2800, Loss: 0.318040668964386
setp: 2900, Loss: 0.3283630609512329
setp: 3000, Loss: 0.3172033429145813
setp: 3100, Loss: 0.31702351570129395
setp: 3200, Loss: 0.3170258104801178
setp: 3300, Loss: 0.31722351908683777
setp: 3400, Loss: 0.317002534866333
setp: 3500, Loss: 0.31842291355133057
setp: 3600, Loss: 0.31780481338500977
setp: 3700, Loss: 0.3208296597003937
setp: 3800, Loss: 0.31587857007980347
setp: 3900, Loss: 0.31735044717788696
setp: 4000, Loss: 0.3162747621536255
setp: 4100, Loss: 0.3162928819656372
setp: 4200, Loss: 0.3166540563106537
setp: 4300, Loss: 0.6878805160522461
setp: 4400, Loss: 0.5284673571586609
setp: 4500, Loss: 0.39315328001976013
setp: 4600, Loss: 0.3381982445716858
setp: 4700, Loss: 0.3572832942008972
setp: 4800, Loss: 0.34013670682907104
setp: 4900, Loss: 0.33175012469291687
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9736842105263158
precision: 1.0
recall: 0.8620689655172413
F_score: 0.9259259259259259
******fold 2******
[109, 499]
training...
setp: 0, Loss: 0.6936768293380737
setp: 100, Loss: 0.59414142370224
setp: 200, Loss: 0.501331090927124
setp: 300, Loss: 0.5028658509254456
setp: 400, Loss: 0.3787640333175659
setp: 500, Loss: 0.36222290992736816
setp: 600, Loss: 0.3398052453994751
setp: 700, Loss: 0.3242846131324768
setp: 800, Loss: 0.3310692310333252
setp: 900, Loss: 0.32067322731018066
setp: 1000, Loss: 0.3180358111858368
setp: 1100, Loss: 0.3195153474807739
setp: 1200, Loss: 0.31992486119270325
setp: 1300, Loss: 0.3182560205459595
setp: 1400, Loss: 0.32043272256851196
setp: 1500, Loss: 0.31804847717285156
setp: 1600, Loss: 0.31787997484207153
setp: 1700, Loss: 0.31828802824020386
setp: 1800, Loss: 0.3198848366737366
setp: 1900, Loss: 0.3173827826976776
setp: 2000, Loss: 0.31815245747566223
setp: 2100, Loss: 0.3172261118888855
setp: 2200, Loss: 0.32047775387763977
setp: 2300, Loss: 0.3171863853931427
setp: 2400, Loss: 0.31713342666625977
setp: 2500, Loss: 0.3177684247493744
setp: 2600, Loss: 0.3163773715496063
setp: 2700, Loss: 0.3989331126213074
setp: 2800, Loss: 0.3382272720336914
setp: 2900, Loss: 0.33066675066947937
setp: 3000, Loss: 0.3269772529602051
setp: 3100, Loss: 0.32705357670783997
setp: 3200, Loss: 0.3255373239517212
setp: 3300, Loss: 0.4258265495300293
setp: 3400, Loss: 0.3295912444591522
setp: 3500, Loss: 0.34466278553009033
setp: 3600, Loss: 0.3260202407836914
setp: 3700, Loss: 0.32535362243652344
setp: 3800, Loss: 0.32466158270835876
setp: 3900, Loss: 0.3252980411052704
setp: 4000, Loss: 0.3248543441295624
setp: 4100, Loss: 0.3258585035800934
setp: 4200, Loss: 0.3247956335544586
setp: 4300, Loss: 0.3241930603981018
setp: 4400, Loss: 0.3249431848526001
setp: 4500, Loss: 0.3245273530483246
setp: 4600, Loss: 0.32329660654067993
setp: 4700, Loss: 0.3857722878456116
setp: 4800, Loss: 0.32527706027030945
setp: 4900, Loss: 0.32499516010284424
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9736842105263158
precision: 1.0
recall: 0.8333333333333334
F_score: 0.9090909090909091
******fold 3******
[109, 499]
training...
setp: 0, Loss: 0.6942494511604309
setp: 100, Loss: 0.6070574522018433
setp: 200, Loss: 0.509624719619751
setp: 300, Loss: 0.48720669746398926
setp: 400, Loss: 0.4875519573688507
setp: 500, Loss: 0.54043048620224
setp: 600, Loss: 0.48570817708969116
setp: 700, Loss: 0.36977750062942505
setp: 800, Loss: 0.34658917784690857
setp: 900, Loss: 0.40169599652290344
setp: 1000, Loss: 0.4099312424659729
setp: 1100, Loss: 0.3864412307739258
setp: 1200, Loss: 0.3361561894416809
setp: 1300, Loss: 0.32510048151016235
setp: 1400, Loss: 0.37443092465400696
setp: 1500, Loss: 0.3226291537284851
setp: 1600, Loss: 0.3165602684020996
setp: 1700, Loss: 0.32472100853919983
setp: 1800, Loss: 0.3180944621562958
setp: 1900, Loss: 0.3480731248855591
setp: 2000, Loss: 0.3176302909851074
setp: 2100, Loss: 0.32072722911834717
setp: 2200, Loss: 0.31828585267066956
setp: 2300, Loss: 0.318130761384964
setp: 2400, Loss: 0.31600260734558105
setp: 2500, Loss: 0.32137584686279297
setp: 2600, Loss: 0.3159119486808777
setp: 2700, Loss: 0.3565009832382202
setp: 2800, Loss: 0.3215305507183075
setp: 2900, Loss: 0.3163674473762512
setp: 3000, Loss: 0.3173784613609314
setp: 3100, Loss: 0.3169560730457306
setp: 3200, Loss: 0.3157023787498474
setp: 3300, Loss: 0.31576430797576904
setp: 3400, Loss: 0.31632015109062195
setp: 3500, Loss: 0.3472757041454315
setp: 3600, Loss: 0.3550685942173004
setp: 3700, Loss: 0.37638023495674133
setp: 3800, Loss: 0.3265266716480255
setp: 3900, Loss: 0.3277533948421478
setp: 4000, Loss: 0.31547534465789795
setp: 4100, Loss: 0.31516265869140625
setp: 4200, Loss: 0.31588631868362427
setp: 4300, Loss: 0.3470507860183716
setp: 4400, Loss: 0.3169362545013428
setp: 4500, Loss: 0.3177090883255005
setp: 4600, Loss: 0.31960171461105347
setp: 4700, Loss: 0.3170633614063263
setp: 4800, Loss: 0.3153665065765381
setp: 4900, Loss: 0.3155801594257355
training successfully ended.
validating...
acc: 0.9979959919839679
precision: 0.9960079840319361
recall: 1.0
F_score: 0.9980000000000001
validating...
acc: 0.8355263157894737
precision: 0.48148148148148145
recall: 0.5416666666666666
F_score: 0.5098039215686274
******fold 4******
[110, 498]
training...
setp: 0, Loss: 0.6932144165039062
setp: 100, Loss: 0.6236010193824768
setp: 200, Loss: 0.5224630832672119
setp: 300, Loss: 0.5072329640388489
setp: 400, Loss: 0.39059942960739136
setp: 500, Loss: 0.3680209815502167
setp: 600, Loss: 0.3830418288707733
setp: 700, Loss: 0.33727994561195374
setp: 800, Loss: 0.3421907126903534
setp: 900, Loss: 0.34591904282569885
setp: 1000, Loss: 0.3198445737361908
setp: 1100, Loss: 0.46453189849853516
setp: 1200, Loss: 0.3243803381919861
setp: 1300, Loss: 0.3168199956417084
setp: 1400, Loss: 0.3199373781681061
setp: 1500, Loss: 0.3162670433521271
setp: 1600, Loss: 0.3171406686306
setp: 1700, Loss: 0.31665074825286865
setp: 1800, Loss: 0.31649136543273926
setp: 1900, Loss: 0.3212323486804962
setp: 2000, Loss: 0.3272937536239624
setp: 2100, Loss: 0.31770220398902893
setp: 2200, Loss: 0.3167247772216797
setp: 2300, Loss: 0.3160436451435089
setp: 2400, Loss: 0.3164362907409668
setp: 2500, Loss: 0.31628313660621643
setp: 2600, Loss: 0.31831836700439453
setp: 2700, Loss: 0.3640762269496918
setp: 2800, Loss: 0.3198164105415344
setp: 2900, Loss: 0.3355697989463806
setp: 3000, Loss: 0.31616589426994324
setp: 3100, Loss: 0.31563159823417664
setp: 3200, Loss: 0.31649988889694214
setp: 3300, Loss: 0.3163885474205017
setp: 3400, Loss: 0.31603574752807617
setp: 3500, Loss: 0.31732210516929626
setp: 3600, Loss: 0.31671902537345886
setp: 3700, Loss: 0.3170333206653595
setp: 3800, Loss: 0.31740447878837585
setp: 3900, Loss: 0.31690713763237
setp: 4000, Loss: 0.31580406427383423
setp: 4100, Loss: 0.31615450978279114
setp: 4200, Loss: 0.6568740010261536
setp: 4300, Loss: 0.4603704512119293
setp: 4400, Loss: 0.36417245864868164
setp: 4500, Loss: 0.32870179414749146
setp: 4600, Loss: 0.3533081114292145
setp: 4700, Loss: 0.33968305587768555
setp: 4800, Loss: 0.3215090334415436
setp: 4900, Loss: 0.3282892405986786
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9605263157894737
precision: 0.9047619047619048
recall: 0.8260869565217391
F_score: 0.8636363636363636
model saved.
avg_acc: 0.9263157894736842, avg_f_score: 0.7820423012373477
-------------subject: 13-------------
==========valence==========
******fold 0******
[331, 277]
training...
setp: 0, Loss: 0.7026922106742859
setp: 100, Loss: 0.5811247229576111
setp: 200, Loss: 0.6281744837760925
setp: 300, Loss: 0.5127309560775757
setp: 400, Loss: 0.5377405881881714
setp: 500, Loss: 0.5155615210533142
setp: 600, Loss: 0.5149052739143372
setp: 700, Loss: 0.37877750396728516
setp: 800, Loss: 0.5009825229644775
setp: 900, Loss: 0.6015860438346863
setp: 1000, Loss: 0.3883488178253174
setp: 1100, Loss: 0.45624634623527527
setp: 1200, Loss: 0.3432849645614624
setp: 1300, Loss: 0.3526844084262848
setp: 1400, Loss: 0.370579332113266
setp: 1500, Loss: 0.5295212268829346
setp: 1600, Loss: 0.3199116587638855
setp: 1700, Loss: 0.37883079051971436
setp: 1800, Loss: 0.3632369339466095
setp: 1900, Loss: 0.38435104489326477
setp: 2000, Loss: 0.3201785981655121
setp: 2100, Loss: 0.34775716066360474
setp: 2200, Loss: 0.3187670111656189
setp: 2300, Loss: 0.3481362760066986
setp: 2400, Loss: 0.34837862849235535
setp: 2500, Loss: 0.3240773677825928
setp: 2600, Loss: 0.31708207726478577
setp: 2700, Loss: 0.31812527775764465
setp: 2800, Loss: 0.3387228846549988
setp: 2900, Loss: 0.31647369265556335
setp: 3000, Loss: 0.31589511036872864
setp: 3100, Loss: 0.3182945251464844
setp: 3200, Loss: 0.31816157698631287
setp: 3300, Loss: 0.3213540315628052
setp: 3400, Loss: 0.35034507513046265
setp: 3500, Loss: 0.31629106402397156
setp: 3600, Loss: 0.3201490640640259
setp: 3700, Loss: 0.31705743074417114
setp: 3800, Loss: 0.4001128077507019
setp: 3900, Loss: 0.345551073551178
setp: 4000, Loss: 0.317091166973114
setp: 4100, Loss: 0.3167654871940613
setp: 4200, Loss: 0.35182416439056396
setp: 4300, Loss: 0.3474113345146179
setp: 4400, Loss: 0.31792643666267395
setp: 4500, Loss: 0.3229295015335083
setp: 4600, Loss: 0.31699249148368835
setp: 4700, Loss: 0.3223244547843933
setp: 4800, Loss: 0.3173260986804962
setp: 4900, Loss: 0.3265886902809143
training successfully ended.
validating...
acc: 0.9835526315789473
precision: 0.9819819819819819
recall: 0.9879154078549849
F_score: 0.9849397590361446
validating...
acc: 0.8881578947368421
precision: 0.9069767441860465
recall: 0.896551724137931
F_score: 0.9017341040462429
******fold 1******
[342, 266]
training...
setp: 0, Loss: 0.676323413848877
setp: 100, Loss: 0.6484339237213135
setp: 200, Loss: 0.48600542545318604
setp: 300, Loss: 0.48765355348587036
setp: 400, Loss: 0.46509605646133423
setp: 500, Loss: 0.49851474165916443
setp: 600, Loss: 0.43945738673210144
setp: 700, Loss: 0.35943761467933655
setp: 800, Loss: 0.4549693167209625
setp: 900, Loss: 0.3463170826435089
setp: 1000, Loss: 0.42315739393234253
setp: 1100, Loss: 0.4258508086204529
setp: 1200, Loss: 0.36809879541397095
setp: 1300, Loss: 0.38266995549201965
setp: 1400, Loss: 0.331373006105423
setp: 1500, Loss: 0.33769458532333374
setp: 1600, Loss: 0.32944798469543457
setp: 1700, Loss: 0.36451515555381775
setp: 1800, Loss: 0.32449662685394287
setp: 1900, Loss: 0.3994188606739044
setp: 2000, Loss: 0.34015393257141113
setp: 2100, Loss: 0.32552316784858704
setp: 2200, Loss: 0.3474746644496918
setp: 2300, Loss: 0.3158872723579407
setp: 2400, Loss: 0.3176612854003906
setp: 2500, Loss: 0.3484644889831543
setp: 2600, Loss: 0.3236716389656067
setp: 2700, Loss: 0.3876688480377197
setp: 2800, Loss: 0.32180774211883545
setp: 2900, Loss: 0.32588210701942444
setp: 3000, Loss: 0.3389160633087158
setp: 3100, Loss: 0.37610888481140137
setp: 3200, Loss: 0.38500672578811646
setp: 3300, Loss: 0.32489654421806335
setp: 3400, Loss: 0.3233221769332886
setp: 3500, Loss: 0.37326571345329285
setp: 3600, Loss: 0.3558134138584137
setp: 3700, Loss: 0.38205963373184204
setp: 3800, Loss: 0.43736812472343445
setp: 3900, Loss: 0.31776076555252075
setp: 4000, Loss: 0.31660693883895874
setp: 4100, Loss: 0.3162660598754883
setp: 4200, Loss: 0.31651124358177185
setp: 4300, Loss: 0.3168398439884186
setp: 4400, Loss: 0.3211473226547241
setp: 4500, Loss: 0.31680014729499817
setp: 4600, Loss: 0.31648150086402893
setp: 4700, Loss: 0.3186728358268738
setp: 4800, Loss: 0.3165853023529053
setp: 4900, Loss: 0.31786343455314636
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.8809523809523809
recall: 0.9736842105263158
F_score: 0.925
******fold 2******
[330, 278]
training...
setp: 0, Loss: 0.6965529918670654
setp: 100, Loss: 0.6009724140167236
setp: 200, Loss: 0.5496818423271179
setp: 300, Loss: 0.5466376543045044
setp: 400, Loss: 0.5528669357299805
setp: 500, Loss: 0.432168573141098
setp: 600, Loss: 0.45656269788742065
setp: 700, Loss: 0.3489535450935364
setp: 800, Loss: 0.40826067328453064
setp: 900, Loss: 0.36657842993736267
setp: 1000, Loss: 0.32679814100265503
setp: 1100, Loss: 0.39085930585861206
setp: 1200, Loss: 0.34981465339660645
setp: 1300, Loss: 0.3567762076854706
setp: 1400, Loss: 0.31835564970970154
setp: 1500, Loss: 0.34447184205055237
setp: 1600, Loss: 0.36864611506462097
setp: 1700, Loss: 0.3488231599330902
setp: 1800, Loss: 0.3202914297580719
setp: 1900, Loss: 0.3199078142642975
setp: 2000, Loss: 0.34890007972717285
setp: 2100, Loss: 0.3480721712112427
setp: 2200, Loss: 0.3209678828716278
setp: 2300, Loss: 0.3285802900791168
setp: 2400, Loss: 0.3246147334575653
setp: 2500, Loss: 0.3375837504863739
setp: 2600, Loss: 0.32438749074935913
setp: 2700, Loss: 0.34731990098953247
setp: 2800, Loss: 0.35216572880744934
setp: 2900, Loss: 0.316354900598526
setp: 3000, Loss: 0.32216140627861023
setp: 3100, Loss: 0.3168419897556305
setp: 3200, Loss: 0.3245651423931122
setp: 3300, Loss: 0.3189094364643097
setp: 3400, Loss: 0.32295727729797363
setp: 3500, Loss: 0.32074567675590515
setp: 3600, Loss: 0.3502763509750366
setp: 3700, Loss: 0.34020379185676575
setp: 3800, Loss: 0.32112959027290344
setp: 3900, Loss: 0.35417380928993225
setp: 4000, Loss: 0.32437652349472046
setp: 4100, Loss: 0.3303787410259247
setp: 4200, Loss: 0.32650887966156006
setp: 4300, Loss: 0.3167894780635834
setp: 4400, Loss: 0.3428831994533539
setp: 4500, Loss: 0.31524690985679626
setp: 4600, Loss: 0.3208940327167511
setp: 4700, Loss: 0.3181796371936798
setp: 4800, Loss: 0.373855859041214
setp: 4900, Loss: 0.3253083825111389
training successfully ended.
validating...
acc: 0.9736842105263158
precision: 0.953757225433526
recall: 1.0
F_score: 0.9763313609467456
validating...
acc: 0.881578947368421
precision: 0.8365384615384616
recall: 0.9886363636363636
F_score: 0.90625
******fold 3******
[330, 278]
training...
setp: 0, Loss: 0.6927754282951355
setp: 100, Loss: 0.6711094975471497
setp: 200, Loss: 0.5673282742500305
setp: 300, Loss: 0.5352686643600464
setp: 400, Loss: 0.5787929892539978
setp: 500, Loss: 0.48786962032318115
setp: 600, Loss: 0.47879257798194885
setp: 700, Loss: 0.40133967995643616
setp: 800, Loss: 0.43290337920188904
setp: 900, Loss: 0.39121755957603455
setp: 1000, Loss: 0.35128939151763916
setp: 1100, Loss: 0.42767098546028137
setp: 1200, Loss: 0.4199635088443756
setp: 1300, Loss: 0.3537467420101166
setp: 1400, Loss: 0.34319913387298584
setp: 1500, Loss: 0.3439290523529053
setp: 1600, Loss: 0.32289761304855347
setp: 1700, Loss: 0.33555424213409424
setp: 1800, Loss: 0.3220978081226349
setp: 1900, Loss: 0.35272088646888733
setp: 2000, Loss: 0.34863564372062683
setp: 2100, Loss: 0.3167576193809509
setp: 2200, Loss: 0.31837034225463867
setp: 2300, Loss: 0.3597138524055481
setp: 2400, Loss: 0.3352976441383362
setp: 2500, Loss: 0.3350770175457001
setp: 2600, Loss: 0.3210620582103729
setp: 2700, Loss: 0.3167925179004669
setp: 2800, Loss: 0.3198314905166626
setp: 2900, Loss: 0.31870678067207336
setp: 3000, Loss: 0.31666648387908936
setp: 3100, Loss: 0.3172062635421753
setp: 3200, Loss: 0.35226210951805115
setp: 3300, Loss: 0.3163447678089142
setp: 3400, Loss: 0.3167407512664795
setp: 3500, Loss: 0.3349969983100891
setp: 3600, Loss: 0.31557339429855347
setp: 3700, Loss: 0.34009382128715515
setp: 3800, Loss: 0.36823412775993347
setp: 3900, Loss: 0.34685027599334717
setp: 4000, Loss: 0.31860047578811646
setp: 4100, Loss: 0.3242965638637543
setp: 4200, Loss: 0.3190060555934906
setp: 4300, Loss: 0.31971749663352966
setp: 4400, Loss: 0.3167533874511719
setp: 4500, Loss: 0.31753864884376526
setp: 4600, Loss: 0.3178599178791046
setp: 4700, Loss: 0.35602009296417236
setp: 4800, Loss: 0.3165074586868286
setp: 4900, Loss: 0.31609734892845154
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9939577039274925
recall: 0.996969696969697
F_score: 0.9954614220877459
validating...
acc: 0.875
precision: 0.9156626506024096
recall: 0.8636363636363636
F_score: 0.8888888888888888
******fold 4******
[339, 269]
training...
setp: 0, Loss: 0.6881033182144165
setp: 100, Loss: 0.6119335889816284
setp: 200, Loss: 0.5655763745307922
setp: 300, Loss: 0.5695151686668396
setp: 400, Loss: 0.5559619665145874
setp: 500, Loss: 0.47127074003219604
setp: 600, Loss: 0.4281923770904541
setp: 700, Loss: 0.41851720213890076
setp: 800, Loss: 0.38429614901542664
setp: 900, Loss: 0.3826749920845032
setp: 1000, Loss: 0.3991463780403137
setp: 1100, Loss: 0.32733747363090515
setp: 1200, Loss: 0.37070736289024353
setp: 1300, Loss: 0.3498113453388214
setp: 1400, Loss: 0.37180548906326294
setp: 1500, Loss: 0.3383822441101074
setp: 1600, Loss: 0.36488115787506104
setp: 1700, Loss: 0.34590083360671997
setp: 1800, Loss: 0.34292978048324585
setp: 1900, Loss: 0.32550230622291565
setp: 2000, Loss: 0.32725486159324646
setp: 2100, Loss: 0.3160174787044525
setp: 2200, Loss: 0.34577393531799316
setp: 2300, Loss: 0.32355549931526184
setp: 2400, Loss: 0.37896570563316345
setp: 2500, Loss: 0.3277395963668823
setp: 2600, Loss: 0.32581084966659546
setp: 2700, Loss: 0.35043418407440186
setp: 2800, Loss: 0.3166079521179199
setp: 2900, Loss: 0.378695547580719
setp: 3000, Loss: 0.31685709953308105
setp: 3100, Loss: 0.31722450256347656
setp: 3200, Loss: 0.31744468212127686
setp: 3300, Loss: 0.3511165976524353
setp: 3400, Loss: 0.31935951113700867
setp: 3500, Loss: 0.3196929395198822
setp: 3600, Loss: 0.32182055711746216
setp: 3700, Loss: 0.327476441860199
setp: 3800, Loss: 0.38606593012809753
setp: 3900, Loss: 0.3503347635269165
setp: 4000, Loss: 0.3155514597892761
setp: 4100, Loss: 0.34508347511291504
setp: 4200, Loss: 0.31745457649230957
setp: 4300, Loss: 0.3247087001800537
setp: 4400, Loss: 0.31729596853256226
setp: 4500, Loss: 0.3296676278114319
setp: 4600, Loss: 0.34890827536582947
setp: 4700, Loss: 0.37010255455970764
setp: 4800, Loss: 0.3483867347240448
setp: 4900, Loss: 0.3467734754085541
training successfully ended.
validating...
acc: 0.9736842105263158
precision: 0.9627507163323782
recall: 0.9911504424778761
F_score: 0.9767441860465116
validating...
acc: 0.875
precision: 0.8488372093023255
recall: 0.9240506329113924
F_score: 0.8848484848484849
model saved.
avg_acc: 0.8881578947368421, avg_f_score: 0.9013442955567234
==========arousal==========
******fold 0******
[93, 515]
training...
setp: 0, Loss: 0.6936866641044617
setp: 100, Loss: 0.669236421585083
setp: 200, Loss: 0.5149489045143127
setp: 300, Loss: 0.3898162841796875
setp: 400, Loss: 0.39751434326171875
setp: 500, Loss: 0.33658310770988464
setp: 600, Loss: 0.3345026969909668
setp: 700, Loss: 0.3227030634880066
setp: 800, Loss: 0.34900179505348206
setp: 900, Loss: 0.32376226782798767
setp: 1000, Loss: 0.3225781321525574
setp: 1100, Loss: 0.3201388120651245
setp: 1200, Loss: 0.3186616599559784
setp: 1300, Loss: 0.31982922554016113
setp: 1400, Loss: 0.31807106733322144
setp: 1500, Loss: 0.40626803040504456
setp: 1600, Loss: 0.3187267482280731
setp: 1700, Loss: 0.317905992269516
setp: 1800, Loss: 0.31898123025894165
setp: 1900, Loss: 0.3180830776691437
setp: 2000, Loss: 0.3185792565345764
setp: 2100, Loss: 0.34889620542526245
setp: 2200, Loss: 0.3211231529712677
setp: 2300, Loss: 0.31900277733802795
setp: 2400, Loss: 0.3189154267311096
setp: 2500, Loss: 0.3192055821418762
setp: 2600, Loss: 0.3200116753578186
setp: 2700, Loss: 0.32002729177474976
setp: 2800, Loss: 0.34250926971435547
setp: 2900, Loss: 0.3234792947769165
setp: 3000, Loss: 0.31839656829833984
setp: 3100, Loss: 0.32015177607536316
setp: 3200, Loss: 0.31781312823295593
setp: 3300, Loss: 0.3183339238166809
setp: 3400, Loss: 0.3200787901878357
setp: 3500, Loss: 0.3205530643463135
setp: 3600, Loss: 0.3202459216117859
setp: 3700, Loss: 0.321829229593277
setp: 3800, Loss: 0.4513319134712219
setp: 3900, Loss: 0.33786940574645996
setp: 4000, Loss: 0.32493898272514343
setp: 4100, Loss: 0.32329022884368896
setp: 4200, Loss: 0.32325392961502075
setp: 4300, Loss: 0.3228154182434082
setp: 4400, Loss: 0.322653204202652
setp: 4500, Loss: 0.3227235674858093
setp: 4600, Loss: 0.3221210837364197
setp: 4700, Loss: 0.3213179409503937
setp: 4800, Loss: 0.3211219310760498
setp: 4900, Loss: 0.3212389647960663
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.6956521739130435
recall: 0.7619047619047619
F_score: 0.7272727272727272
******fold 1******
[87, 521]
training...
setp: 0, Loss: 0.6950579285621643
setp: 100, Loss: 0.6860848069190979
setp: 200, Loss: 0.579512357711792
setp: 300, Loss: 0.535351574420929
setp: 400, Loss: 0.6236991882324219
setp: 500, Loss: 0.3895072042942047
setp: 600, Loss: 0.4033759832382202
setp: 700, Loss: 0.4044254720211029
setp: 800, Loss: 0.44516992568969727
setp: 900, Loss: 0.4097538888454437
setp: 1000, Loss: 0.41822460293769836
setp: 1100, Loss: 0.36763229966163635
setp: 1200, Loss: 0.34628814458847046
setp: 1300, Loss: 0.45879819989204407
setp: 1400, Loss: 0.3797517716884613
setp: 1500, Loss: 0.34298256039619446
setp: 1600, Loss: 0.5575575232505798
setp: 1700, Loss: 0.319346159696579
setp: 1800, Loss: 0.35196974873542786
setp: 1900, Loss: 0.34987959265708923
setp: 2000, Loss: 0.3162212073802948
setp: 2100, Loss: 0.3820413053035736
setp: 2200, Loss: 0.3478364050388336
setp: 2300, Loss: 0.31735989451408386
setp: 2400, Loss: 0.3751835227012634
setp: 2500, Loss: 0.315965861082077
setp: 2600, Loss: 0.32625818252563477
setp: 2700, Loss: 0.34241393208503723
setp: 2800, Loss: 0.34645313024520874
setp: 2900, Loss: 0.31498441100120544
setp: 3000, Loss: 0.3297848403453827
setp: 3100, Loss: 0.3165290951728821
setp: 3200, Loss: 0.3422708511352539
setp: 3300, Loss: 0.31528982520103455
setp: 3400, Loss: 0.3154517412185669
setp: 3500, Loss: 0.34747740626335144
setp: 3600, Loss: 0.3273160457611084
setp: 3700, Loss: 0.3234993815422058
setp: 3800, Loss: 0.31553155183792114
setp: 3900, Loss: 0.3164346218109131
setp: 4000, Loss: 0.3163083493709564
setp: 4100, Loss: 0.31821632385253906
setp: 4200, Loss: 0.3159719705581665
setp: 4300, Loss: 0.3174479305744171
setp: 4400, Loss: 0.31644508242607117
setp: 4500, Loss: 0.31610628962516785
setp: 4600, Loss: 0.3182946443557739
setp: 4700, Loss: 0.3163706958293915
setp: 4800, Loss: 0.32560256123542786
setp: 4900, Loss: 0.3190777003765106
training successfully ended.
validating...
acc: 0.9289827255278311
precision: 0.9933774834437086
recall: 0.8637236084452975
F_score: 0.9240246406570841
validating...
acc: 0.9276315789473685
precision: 0.9
recall: 0.6666666666666666
F_score: 0.7659574468085106
******fold 2******
[87, 521]
training...
setp: 0, Loss: 0.6933527588844299
setp: 100, Loss: 0.7261928915977478
setp: 200, Loss: 0.5830200910568237
setp: 300, Loss: 0.4291246831417084
setp: 400, Loss: 0.41560986638069153
setp: 500, Loss: 0.33695438504219055
setp: 600, Loss: 0.3322884142398834
setp: 700, Loss: 0.3210506737232208
setp: 800, Loss: 0.3224404454231262
setp: 900, Loss: 0.3202650845050812
setp: 1000, Loss: 0.31909334659576416
setp: 1100, Loss: 0.3197039067745209
setp: 1200, Loss: 0.31986352801322937
setp: 1300, Loss: 0.3204362988471985
setp: 1400, Loss: 0.3170751631259918
setp: 1500, Loss: 0.31914278864860535
setp: 1600, Loss: 0.3188458979129791
setp: 1700, Loss: 0.3207722008228302
setp: 1800, Loss: 0.3232592046260834
setp: 1900, Loss: 0.3179587721824646
setp: 2000, Loss: 0.3186023235321045
setp: 2100, Loss: 0.3186963200569153
setp: 2200, Loss: 0.32011908292770386
setp: 2300, Loss: 0.3184759020805359
setp: 2400, Loss: 0.3185900151729584
setp: 2500, Loss: 0.3182964622974396
setp: 2600, Loss: 0.3476428985595703
setp: 2700, Loss: 0.32180020213127136
setp: 2800, Loss: 0.32072558999061584
setp: 2900, Loss: 0.31938791275024414
setp: 3000, Loss: 0.3198229670524597
setp: 3100, Loss: 0.31985020637512207
setp: 3200, Loss: 0.32104259729385376
setp: 3300, Loss: 0.3182699680328369
setp: 3400, Loss: 0.31916019320487976
setp: 3500, Loss: 0.3194798231124878
setp: 3600, Loss: 0.31901979446411133
setp: 3700, Loss: 0.32132646441459656
setp: 3800, Loss: 0.35781580209732056
setp: 3900, Loss: 0.31950610876083374
setp: 4000, Loss: 0.31849801540374756
setp: 4100, Loss: 0.31892043352127075
setp: 4200, Loss: 0.31868815422058105
setp: 4300, Loss: 0.31919705867767334
setp: 4400, Loss: 0.318756639957428
setp: 4500, Loss: 0.3185015320777893
setp: 4600, Loss: 0.3204003572463989
setp: 4700, Loss: 0.32887157797813416
setp: 4800, Loss: 0.3179977238178253
setp: 4900, Loss: 0.31746214628219604
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9539473684210527
precision: 0.9166666666666666
recall: 0.8148148148148148
F_score: 0.8627450980392156
******fold 3******
[97, 511]
training...
setp: 0, Loss: 0.7005361914634705
setp: 100, Loss: 0.6165462732315063
setp: 200, Loss: 0.5758357644081116
setp: 300, Loss: 0.4988842010498047
setp: 400, Loss: 0.4072980284690857
setp: 500, Loss: 0.34718695282936096
setp: 600, Loss: 0.3589988946914673
setp: 700, Loss: 0.3340798020362854
setp: 800, Loss: 0.3202486038208008
setp: 900, Loss: 0.32278957962989807
setp: 1000, Loss: 0.39388808608055115
setp: 1100, Loss: 0.32046568393707275
setp: 1200, Loss: 0.31989309191703796
setp: 1300, Loss: 0.3165419101715088
setp: 1400, Loss: 0.31863775849342346
setp: 1500, Loss: 0.31674569845199585
setp: 1600, Loss: 0.32283443212509155
setp: 1700, Loss: 0.3338046073913574
setp: 1800, Loss: 0.3213237226009369
setp: 1900, Loss: 0.31763821840286255
setp: 2000, Loss: 0.3170146346092224
setp: 2100, Loss: 0.3155967891216278
setp: 2200, Loss: 0.316928505897522
setp: 2300, Loss: 0.40377143025398254
setp: 2400, Loss: 0.3218802511692047
setp: 2500, Loss: 0.316445529460907
setp: 2600, Loss: 0.31867337226867676
setp: 2700, Loss: 0.3165939450263977
setp: 2800, Loss: 0.31598085165023804
setp: 2900, Loss: 0.3332981467247009
setp: 3000, Loss: 0.32172971963882446
setp: 3100, Loss: 0.31619390845298767
setp: 3200, Loss: 0.31568679213523865
setp: 3300, Loss: 0.31631577014923096
setp: 3400, Loss: 0.31782013177871704
setp: 3500, Loss: 0.316228449344635
setp: 3600, Loss: 0.3569536507129669
setp: 3700, Loss: 0.31648126244544983
setp: 3800, Loss: 0.31675636768341064
setp: 3900, Loss: 0.31618550419807434
setp: 4000, Loss: 0.3159754276275635
setp: 4100, Loss: 0.31660255789756775
setp: 4200, Loss: 0.317659854888916
setp: 4300, Loss: 0.3162204623222351
setp: 4400, Loss: 0.45265695452690125
setp: 4500, Loss: 0.39524000883102417
setp: 4600, Loss: 0.32154738903045654
setp: 4700, Loss: 0.31760433316230774
setp: 4800, Loss: 0.31618958711624146
setp: 4900, Loss: 0.3178735673427582
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8947368421052632
precision: 0.52
recall: 0.7647058823529411
F_score: 0.6190476190476191
******fold 4******
[92, 516]
training...
setp: 0, Loss: 0.698674738407135
setp: 100, Loss: 0.6737028956413269
setp: 200, Loss: 0.6236236691474915
setp: 300, Loss: 0.558834969997406
setp: 400, Loss: 0.5805200338363647
setp: 500, Loss: 0.42550548911094666
setp: 600, Loss: 0.38065478205680847
setp: 700, Loss: 0.3954634666442871
setp: 800, Loss: 0.4476684629917145
setp: 900, Loss: 0.421186625957489
setp: 1000, Loss: 0.6901412010192871
setp: 1100, Loss: 0.37148234248161316
setp: 1200, Loss: 0.32001832127571106
setp: 1300, Loss: 0.3684394657611847
setp: 1400, Loss: 0.3763923645019531
setp: 1500, Loss: 0.3170486390590668
setp: 1600, Loss: 0.36779797077178955
setp: 1700, Loss: 0.3167175054550171
setp: 1800, Loss: 0.38695284724235535
setp: 1900, Loss: 0.31640878319740295
setp: 2000, Loss: 0.34685689210891724
setp: 2100, Loss: 0.31592515110969543
setp: 2200, Loss: 0.3174262046813965
setp: 2300, Loss: 0.31574270129203796
setp: 2400, Loss: 0.3164275586605072
setp: 2500, Loss: 0.38146913051605225
setp: 2600, Loss: 0.32201191782951355
setp: 2700, Loss: 0.3634394109249115
setp: 2800, Loss: 0.3468981981277466
setp: 2900, Loss: 0.32099127769470215
setp: 3000, Loss: 0.31916508078575134
setp: 3100, Loss: 0.3477631211280823
setp: 3200, Loss: 0.31624636054039
setp: 3300, Loss: 0.3160540759563446
setp: 3400, Loss: 0.3157750964164734
setp: 3500, Loss: 0.3499564528465271
setp: 3600, Loss: 0.31639721989631653
setp: 3700, Loss: 0.320464551448822
setp: 3800, Loss: 0.31692415475845337
setp: 3900, Loss: 0.3162553310394287
setp: 4000, Loss: 0.3154487907886505
setp: 4100, Loss: 0.3468019664287567
setp: 4200, Loss: 0.3154630959033966
setp: 4300, Loss: 0.667729914188385
setp: 4400, Loss: 0.49097344279289246
setp: 4500, Loss: 0.4487953782081604
setp: 4600, Loss: 0.36216312646865845
setp: 4700, Loss: 0.3713783621788025
setp: 4800, Loss: 0.37981346249580383
setp: 4900, Loss: 0.3814716339111328
training successfully ended.
validating...
acc: 0.9912790697674418
precision: 0.9941520467836257
recall: 0.9883720930232558
F_score: 0.9912536443148688
validating...
acc: 0.9539473684210527
precision: 0.8260869565217391
recall: 0.8636363636363636
F_score: 0.8444444444444444
model saved.
avg_acc: 0.930263157894737, avg_f_score: 0.7638934671225035
-------------subject: 14-------------
==========valence==========
******fold 0******
[292, 316]
training...
setp: 0, Loss: 0.6922407150268555
setp: 100, Loss: 0.6190872192382812
setp: 200, Loss: 0.528622031211853
setp: 300, Loss: 0.47823095321655273
setp: 400, Loss: 0.46684491634368896
setp: 500, Loss: 0.39370936155319214
setp: 600, Loss: 0.3760469853878021
setp: 700, Loss: 0.46300363540649414
setp: 800, Loss: 0.3437667489051819
setp: 900, Loss: 0.3884349763393402
setp: 1000, Loss: 0.31861165165901184
setp: 1100, Loss: 0.3664216101169586
setp: 1200, Loss: 0.3406725227832794
setp: 1300, Loss: 0.32023903727531433
setp: 1400, Loss: 0.32332488894462585
setp: 1500, Loss: 0.31863147020339966
setp: 1600, Loss: 0.3195473849773407
setp: 1700, Loss: 0.31825169920921326
setp: 1800, Loss: 0.3179873526096344
setp: 1900, Loss: 0.35173261165618896
setp: 2000, Loss: 0.32719355821609497
setp: 2100, Loss: 0.35697782039642334
setp: 2200, Loss: 0.35330066084861755
setp: 2300, Loss: 0.31634390354156494
setp: 2400, Loss: 0.34702157974243164
setp: 2500, Loss: 0.3177536129951477
setp: 2600, Loss: 0.31952086091041565
setp: 2700, Loss: 0.3184373676776886
setp: 2800, Loss: 0.3239128887653351
setp: 2900, Loss: 0.3166072368621826
setp: 3000, Loss: 0.317430317401886
setp: 3100, Loss: 0.3179101347923279
setp: 3200, Loss: 0.3167267441749573
setp: 3300, Loss: 0.31616419553756714
setp: 3400, Loss: 0.3176901936531067
setp: 3500, Loss: 0.3202234208583832
setp: 3600, Loss: 0.31859129667282104
setp: 3700, Loss: 0.31718167662620544
setp: 3800, Loss: 0.3187703490257263
setp: 3900, Loss: 0.5206650495529175
setp: 4000, Loss: 0.3642653822898865
setp: 4100, Loss: 0.31828993558883667
setp: 4200, Loss: 0.5418077111244202
setp: 4300, Loss: 0.324242502450943
setp: 4400, Loss: 0.3180197775363922
setp: 4500, Loss: 0.31900444626808167
setp: 4600, Loss: 0.3191154897212982
setp: 4700, Loss: 0.3187839686870575
setp: 4800, Loss: 0.3174419403076172
setp: 4900, Loss: 0.3184712529182434
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9647058823529412
recall: 0.9318181818181818
F_score: 0.9479768786127167
******fold 1******
[307, 301]
training...
setp: 0, Loss: 0.7361951470375061
setp: 100, Loss: 0.66655433177948
setp: 200, Loss: 0.5798152089118958
setp: 300, Loss: 0.5148077607154846
setp: 400, Loss: 0.460631400346756
setp: 500, Loss: 0.41407719254493713
setp: 600, Loss: 0.42678147554397583
setp: 700, Loss: 0.3769548833370209
setp: 800, Loss: 0.4273093044757843
setp: 900, Loss: 0.4628494381904602
setp: 1000, Loss: 0.3675549626350403
setp: 1100, Loss: 0.35770535469055176
setp: 1200, Loss: 0.3498710095882416
setp: 1300, Loss: 0.3772280812263489
setp: 1400, Loss: 0.3389361500740051
setp: 1500, Loss: 0.3173857033252716
setp: 1600, Loss: 0.34867656230926514
setp: 1700, Loss: 0.31818267703056335
setp: 1800, Loss: 0.3347356617450714
setp: 1900, Loss: 0.34734246134757996
setp: 2000, Loss: 0.34861525893211365
setp: 2100, Loss: 0.31932908296585083
setp: 2200, Loss: 0.31945836544036865
setp: 2300, Loss: 0.3187372088432312
setp: 2400, Loss: 0.3170744478702545
setp: 2500, Loss: 0.3492765724658966
setp: 2600, Loss: 0.3481820225715637
setp: 2700, Loss: 0.33832621574401855
setp: 2800, Loss: 0.3463326394557953
setp: 2900, Loss: 0.3170703947544098
setp: 3000, Loss: 0.3268872797489166
setp: 3100, Loss: 0.32532012462615967
setp: 3200, Loss: 0.36498692631721497
setp: 3300, Loss: 0.31650063395500183
setp: 3400, Loss: 0.3330545425415039
setp: 3500, Loss: 0.32359805703163147
setp: 3600, Loss: 0.3181912899017334
setp: 3700, Loss: 0.31607621908187866
setp: 3800, Loss: 0.3232058584690094
setp: 3900, Loss: 0.31786713004112244
setp: 4000, Loss: 0.316760778427124
setp: 4100, Loss: 0.31590211391448975
setp: 4200, Loss: 0.3185409605503082
setp: 4300, Loss: 0.31800174713134766
setp: 4400, Loss: 0.3174813389778137
setp: 4500, Loss: 0.34702998399734497
setp: 4600, Loss: 0.31633004546165466
setp: 4700, Loss: 0.3203940689563751
setp: 4800, Loss: 0.3178401589393616
setp: 4900, Loss: 0.3177381753921509
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9967532467532467
recall: 1.0
F_score: 0.9983739837398374
validating...
acc: 0.9013157894736842
precision: 0.953125
recall: 0.8356164383561644
F_score: 0.8905109489051095
******fold 2******
[304, 304]
training...
setp: 0, Loss: 0.7113859057426453
setp: 100, Loss: 0.6914166808128357
setp: 200, Loss: 0.549805223941803
setp: 300, Loss: 0.46457892656326294
setp: 400, Loss: 0.4303767681121826
setp: 500, Loss: 0.39039546251296997
setp: 600, Loss: 0.3387972414493561
setp: 700, Loss: 0.37680694460868835
setp: 800, Loss: 0.32933539152145386
setp: 900, Loss: 0.35063570737838745
setp: 1000, Loss: 0.32377439737319946
setp: 1100, Loss: 0.3228139877319336
setp: 1200, Loss: 0.3245369493961334
setp: 1300, Loss: 0.3258163332939148
setp: 1400, Loss: 0.32465672492980957
setp: 1500, Loss: 0.31981131434440613
setp: 1600, Loss: 0.32161620259284973
setp: 1700, Loss: 0.3209812045097351
setp: 1800, Loss: 0.32055172324180603
setp: 1900, Loss: 0.3209756910800934
setp: 2000, Loss: 0.3497743308544159
setp: 2100, Loss: 0.32241931557655334
setp: 2200, Loss: 0.34180891513824463
setp: 2300, Loss: 0.31992068886756897
setp: 2400, Loss: 0.3190707564353943
setp: 2500, Loss: 0.3202693462371826
setp: 2600, Loss: 0.3211396336555481
setp: 2700, Loss: 0.31967446208000183
setp: 2800, Loss: 0.32049262523651123
setp: 2900, Loss: 0.31967127323150635
setp: 3000, Loss: 0.3204653263092041
setp: 3100, Loss: 0.32087114453315735
setp: 3200, Loss: 0.3205934464931488
setp: 3300, Loss: 0.31962698698043823
setp: 3400, Loss: 0.3191826045513153
setp: 3500, Loss: 0.5279948115348816
setp: 3600, Loss: 0.3692642152309418
setp: 3700, Loss: 0.35365355014801025
setp: 3800, Loss: 0.33537861704826355
setp: 3900, Loss: 0.3670976758003235
setp: 4000, Loss: 0.3282371461391449
setp: 4100, Loss: 0.3286992311477661
setp: 4200, Loss: 0.3259422779083252
setp: 4300, Loss: 0.32532644271850586
setp: 4400, Loss: 0.3535013794898987
setp: 4500, Loss: 0.3282087743282318
setp: 4600, Loss: 0.33688995242118835
setp: 4700, Loss: 0.3247707188129425
setp: 4800, Loss: 0.32164353132247925
setp: 4900, Loss: 0.3226693272590637
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9078947368421053
precision: 0.9078947368421053
recall: 0.9078947368421053
F_score: 0.9078947368421053
******fold 3******
[304, 304]
training...
setp: 0, Loss: 0.7000181674957275
setp: 100, Loss: 0.6469496488571167
setp: 200, Loss: 0.49268630146980286
setp: 300, Loss: 0.36201992630958557
setp: 400, Loss: 0.3760240375995636
setp: 500, Loss: 0.3945387303829193
setp: 600, Loss: 0.35423633456230164
setp: 700, Loss: 0.381960391998291
setp: 800, Loss: 0.40742406249046326
setp: 900, Loss: 0.35112154483795166
setp: 1000, Loss: 0.3285210132598877
setp: 1100, Loss: 0.3207454979419708
setp: 1200, Loss: 0.3507659435272217
setp: 1300, Loss: 0.3568449914455414
setp: 1400, Loss: 0.3186744153499603
setp: 1500, Loss: 0.33102959394454956
setp: 1600, Loss: 0.3224567174911499
setp: 1700, Loss: 0.32070040702819824
setp: 1800, Loss: 0.3218577802181244
setp: 1900, Loss: 0.32394465804100037
setp: 2000, Loss: 0.31943440437316895
setp: 2100, Loss: 0.329988956451416
setp: 2200, Loss: 0.31752780079841614
setp: 2300, Loss: 0.31943005323410034
setp: 2400, Loss: 0.3200775384902954
setp: 2500, Loss: 0.3191826343536377
setp: 2600, Loss: 0.41405683755874634
setp: 2700, Loss: 0.35134896636009216
setp: 2800, Loss: 0.3185729682445526
setp: 2900, Loss: 0.317818820476532
setp: 3000, Loss: 0.31825363636016846
setp: 3100, Loss: 0.3201291263103485
setp: 3200, Loss: 0.3210032880306244
setp: 3300, Loss: 0.3178715705871582
setp: 3400, Loss: 0.32026755809783936
setp: 3500, Loss: 0.3199664354324341
setp: 3600, Loss: 0.3187437951564789
setp: 3700, Loss: 0.31883662939071655
setp: 3800, Loss: 0.31971800327301025
setp: 3900, Loss: 0.3205276131629944
setp: 4000, Loss: 0.3198946416378021
setp: 4100, Loss: 0.31786423921585083
setp: 4200, Loss: 0.319487601518631
setp: 4300, Loss: 0.3199242055416107
setp: 4400, Loss: 0.6805763840675354
setp: 4500, Loss: 0.4234481453895569
setp: 4600, Loss: 0.36949387192726135
setp: 4700, Loss: 0.3498799502849579
setp: 4800, Loss: 0.3300473690032959
setp: 4900, Loss: 0.3271183967590332
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9934426229508196
recall: 0.9967105263157895
F_score: 0.9950738916256158
validating...
acc: 0.9078947368421053
precision: 0.8875
recall: 0.9342105263157895
F_score: 0.9102564102564101
******fold 4******
[313, 295]
training...
setp: 0, Loss: 0.6958949565887451
setp: 100, Loss: 0.6722373962402344
setp: 200, Loss: 0.5593289136886597
setp: 300, Loss: 0.5718175768852234
setp: 400, Loss: 0.5191267132759094
setp: 500, Loss: 0.4045915901660919
setp: 600, Loss: 0.3561854958534241
setp: 700, Loss: 0.33369216322898865
setp: 800, Loss: 0.37333622574806213
setp: 900, Loss: 0.3611026108264923
setp: 1000, Loss: 0.40761449933052063
setp: 1100, Loss: 0.3229726254940033
setp: 1200, Loss: 0.3223762810230255
setp: 1300, Loss: 0.3511461019515991
setp: 1400, Loss: 0.31959840655326843
setp: 1500, Loss: 0.32187363505363464
setp: 1600, Loss: 0.34458982944488525
setp: 1700, Loss: 0.3671553134918213
setp: 1800, Loss: 0.32428446412086487
setp: 1900, Loss: 0.3197735846042633
setp: 2000, Loss: 0.3220977783203125
setp: 2100, Loss: 0.31918850541114807
setp: 2200, Loss: 0.3180851936340332
setp: 2300, Loss: 0.351192444562912
setp: 2400, Loss: 0.3197248578071594
setp: 2500, Loss: 0.3194023668766022
setp: 2600, Loss: 0.3190423846244812
setp: 2700, Loss: 0.3195306360721588
setp: 2800, Loss: 0.3202263116836548
setp: 2900, Loss: 0.3793450593948364
setp: 3000, Loss: 0.3409508764743805
setp: 3100, Loss: 0.3317854404449463
setp: 3200, Loss: 0.3190823197364807
setp: 3300, Loss: 0.31737107038497925
setp: 3400, Loss: 0.31925472617149353
setp: 3500, Loss: 0.3191491961479187
setp: 3600, Loss: 0.31968051195144653
setp: 3700, Loss: 0.31822168827056885
setp: 3800, Loss: 0.3197173476219177
setp: 3900, Loss: 0.32092389464378357
setp: 4000, Loss: 0.3187762498855591
setp: 4100, Loss: 0.3181764483451843
setp: 4200, Loss: 0.35325193405151367
setp: 4300, Loss: 0.3594242334365845
setp: 4400, Loss: 0.3180314004421234
setp: 4500, Loss: 0.31804358959198
setp: 4600, Loss: 0.3188989758491516
setp: 4700, Loss: 0.31922203302383423
setp: 4800, Loss: 0.31822702288627625
setp: 4900, Loss: 0.3184627294540405
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9968152866242038
recall: 1.0
F_score: 0.9984051036682616
validating...
acc: 0.9210526315789473
precision: 0.8873239436619719
recall: 0.9402985074626866
F_score: 0.9130434782608696
model saved.
avg_acc: 0.9157894736842106, avg_f_score: 0.9139364905754424
==========arousal==========
******fold 0******
[196, 412]
training...
setp: 0, Loss: 0.7098492383956909
setp: 100, Loss: 0.6004230380058289
setp: 200, Loss: 0.6442528367042542
setp: 300, Loss: 0.5959710478782654
setp: 400, Loss: 0.6450110077857971
setp: 500, Loss: 0.6211907267570496
setp: 600, Loss: 0.5980200171470642
setp: 700, Loss: 0.6459758877754211
setp: 800, Loss: 0.6704847812652588
setp: 900, Loss: 0.6832587718963623
setp: 1000, Loss: 0.6181279420852661
setp: 1100, Loss: 0.6691230535507202
setp: 1200, Loss: 0.6297557353973389
setp: 1300, Loss: 0.5466139316558838
setp: 1400, Loss: 0.5048078298568726
setp: 1500, Loss: 0.5781369209289551
setp: 1600, Loss: 0.49513542652130127
setp: 1700, Loss: 0.4387277364730835
setp: 1800, Loss: 0.47264939546585083
setp: 1900, Loss: 0.41026946902275085
setp: 2000, Loss: 0.41538918018341064
setp: 2100, Loss: 0.3950359523296356
setp: 2200, Loss: 0.35696911811828613
setp: 2300, Loss: 0.35709357261657715
setp: 2400, Loss: 0.33184394240379333
setp: 2500, Loss: 0.35309767723083496
setp: 2600, Loss: 0.3868993818759918
setp: 2700, Loss: 0.3317541182041168
setp: 2800, Loss: 0.4732128083705902
setp: 2900, Loss: 0.3809654414653778
setp: 3000, Loss: 0.35297033190727234
setp: 3100, Loss: 0.38743922114372253
setp: 3200, Loss: 0.35152482986450195
setp: 3300, Loss: 0.41461995244026184
setp: 3400, Loss: 0.39244237542152405
setp: 3500, Loss: 0.5148160457611084
setp: 3600, Loss: 0.33801430463790894
setp: 3700, Loss: 0.35815882682800293
setp: 3800, Loss: 0.3805026113986969
setp: 3900, Loss: 0.3506113588809967
setp: 4000, Loss: 0.34960681200027466
setp: 4100, Loss: 0.34849026799201965
setp: 4200, Loss: 0.34995946288108826
setp: 4300, Loss: 0.3174329996109009
setp: 4400, Loss: 0.3491455912590027
setp: 4500, Loss: 0.38087716698646545
setp: 4600, Loss: 0.31917744874954224
setp: 4700, Loss: 0.38469505310058594
setp: 4800, Loss: 0.3495821952819824
setp: 4900, Loss: 0.36444753408432007
training successfully ended.
validating...
acc: 0.9736842105263158
precision: 0.9891304347826086
recall: 0.9285714285714286
F_score: 0.9578947368421052
validating...
acc: 0.8486842105263158
precision: 0.7692307692307693
recall: 0.7843137254901961
F_score: 0.7766990291262137
******fold 1******
[192, 416]
training...
setp: 0, Loss: 0.7656113505363464
setp: 100, Loss: 0.671345055103302
setp: 200, Loss: 0.5708979368209839
setp: 300, Loss: 0.5907989144325256
setp: 400, Loss: 0.5611167550086975
setp: 500, Loss: 0.5356492400169373
setp: 600, Loss: 0.5712863206863403
setp: 700, Loss: 0.5750077366828918
setp: 800, Loss: 0.6010468006134033
setp: 900, Loss: 0.5598079562187195
setp: 1000, Loss: 0.5442959666252136
setp: 1100, Loss: 0.5479095578193665
setp: 1200, Loss: 0.5246168971061707
setp: 1300, Loss: 0.574968695640564
setp: 1400, Loss: 0.47930899262428284
setp: 1500, Loss: 0.6170266270637512
setp: 1600, Loss: 0.5692458152770996
setp: 1700, Loss: 0.5659732222557068
setp: 1800, Loss: 0.44405144453048706
setp: 1900, Loss: 0.5666811466217041
setp: 2000, Loss: 0.6065028309822083
setp: 2100, Loss: 0.4864168167114258
setp: 2200, Loss: 0.4655059278011322
setp: 2300, Loss: 0.5076187252998352
setp: 2400, Loss: 0.4805448651313782
setp: 2500, Loss: 0.5521221160888672
setp: 2600, Loss: 0.5648810863494873
setp: 2700, Loss: 0.5414157509803772
setp: 2800, Loss: 0.5049265623092651
setp: 2900, Loss: 0.5144562721252441
setp: 3000, Loss: 0.5029099583625793
setp: 3100, Loss: 0.5125156044960022
setp: 3200, Loss: 0.4682389497756958
setp: 3300, Loss: 0.4173544645309448
setp: 3400, Loss: 0.441084623336792
setp: 3500, Loss: 0.407540887594223
setp: 3600, Loss: 0.3314196765422821
setp: 3700, Loss: 0.34848394989967346
setp: 3800, Loss: 0.3292023837566376
setp: 3900, Loss: 0.33337512612342834
setp: 4000, Loss: 0.323304682970047
setp: 4100, Loss: 0.31740865111351013
setp: 4200, Loss: 0.31927430629730225
setp: 4300, Loss: 0.333615243434906
setp: 4400, Loss: 0.34622666239738464
setp: 4500, Loss: 0.3180140554904938
setp: 4600, Loss: 0.322388231754303
setp: 4700, Loss: 0.3197135031223297
setp: 4800, Loss: 0.3219178318977356
setp: 4900, Loss: 0.32425037026405334
training successfully ended.
validating...
acc: 0.9884868421052632
precision: 1.0
recall: 0.9635416666666666
F_score: 0.9814323607427056
validating...
acc: 0.8947368421052632
precision: 0.9534883720930233
recall: 0.7454545454545455
F_score: 0.8367346938775511
******fold 2******
[196, 412]
training...
setp: 0, Loss: 0.740390956401825
setp: 100, Loss: 0.5984722375869751
setp: 200, Loss: 0.6583648324012756
setp: 300, Loss: 0.6009320020675659
setp: 400, Loss: 0.5965913534164429
setp: 500, Loss: 0.5431912541389465
setp: 600, Loss: 0.5724416971206665
setp: 700, Loss: 0.5693424344062805
setp: 800, Loss: 0.5805227160453796
setp: 900, Loss: 0.5415430068969727
setp: 1000, Loss: 0.5702418088912964
setp: 1100, Loss: 0.5742239356040955
setp: 1200, Loss: 0.5510702133178711
setp: 1300, Loss: 0.5652973055839539
setp: 1400, Loss: 0.5262243747711182
setp: 1500, Loss: 0.5712434649467468
setp: 1600, Loss: 0.5157511234283447
setp: 1700, Loss: 0.5085844397544861
setp: 1800, Loss: 0.521296501159668
setp: 1900, Loss: 0.4337905943393707
setp: 2000, Loss: 0.4595050513744354
setp: 2100, Loss: 0.6845415830612183
setp: 2200, Loss: 0.4817414879798889
setp: 2300, Loss: 0.4982050657272339
setp: 2400, Loss: 0.48040544986724854
setp: 2500, Loss: 0.463970810174942
setp: 2600, Loss: 0.46649137139320374
setp: 2700, Loss: 0.5073297619819641
setp: 2800, Loss: 0.4119998514652252
setp: 2900, Loss: 0.3556978106498718
setp: 3000, Loss: 0.46530893445014954
setp: 3100, Loss: 0.5017473697662354
setp: 3200, Loss: 0.4155513048171997
setp: 3300, Loss: 0.3610471189022064
setp: 3400, Loss: 0.3854915201663971
setp: 3500, Loss: 0.4337659180164337
setp: 3600, Loss: 0.3611253499984741
setp: 3700, Loss: 0.3249526023864746
setp: 3800, Loss: 0.350801557302475
setp: 3900, Loss: 0.34352701902389526
setp: 4000, Loss: 0.34731754660606384
setp: 4100, Loss: 0.32594454288482666
setp: 4200, Loss: 0.35071802139282227
setp: 4300, Loss: 0.3425659239292145
setp: 4400, Loss: 0.34913796186447144
setp: 4500, Loss: 0.33525335788726807
setp: 4600, Loss: 0.3498080372810364
setp: 4700, Loss: 0.3530295193195343
setp: 4800, Loss: 0.34787869453430176
setp: 4900, Loss: 0.32032525539398193
training successfully ended.
validating...
acc: 0.993421052631579
precision: 0.9948453608247423
recall: 0.9846938775510204
F_score: 0.9897435897435899
validating...
acc: 0.9078947368421053
precision: 0.8363636363636363
recall: 0.9019607843137255
F_score: 0.8679245283018867
******fold 3******
[199, 409]
training...
setp: 0, Loss: 0.7003931999206543
setp: 100, Loss: 0.6657353043556213
setp: 200, Loss: 0.6210787296295166
setp: 300, Loss: 0.6446290612220764
setp: 400, Loss: 0.643711507320404
setp: 500, Loss: 0.5769336819648743
setp: 600, Loss: 0.6945427060127258
setp: 700, Loss: 0.5986441969871521
setp: 800, Loss: 0.5372816324234009
setp: 900, Loss: 0.5758394598960876
setp: 1000, Loss: 0.6455308794975281
setp: 1100, Loss: 0.5992342829704285
setp: 1200, Loss: 0.6436811685562134
setp: 1300, Loss: 0.6212583184242249
setp: 1400, Loss: 0.6938998699188232
setp: 1500, Loss: 0.6439805030822754
setp: 1600, Loss: 0.6002423763275146
setp: 1700, Loss: 0.6930805444717407
setp: 1800, Loss: 0.6688272356987
setp: 1900, Loss: 0.6658040881156921
setp: 2000, Loss: 0.6661139130592346
setp: 2100, Loss: 0.6211100816726685
setp: 2200, Loss: 0.6445630192756653
setp: 2300, Loss: 0.6437198519706726
setp: 2400, Loss: 0.5770015120506287
setp: 2500, Loss: 0.6942088603973389
setp: 2600, Loss: 0.5987702012062073
setp: 2700, Loss: 0.5374957919120789
setp: 2800, Loss: 0.5760195255279541
setp: 2900, Loss: 0.6454113721847534
setp: 3000, Loss: 0.5993838310241699
setp: 3100, Loss: 0.643697202205658
setp: 3200, Loss: 0.6212762594223022
setp: 3300, Loss: 0.6934658885002136
setp: 3400, Loss: 0.6439619064331055
setp: 3500, Loss: 0.6002037525177002
setp: 3600, Loss: 0.6928895115852356
setp: 3700, Loss: 0.6685867309570312
setp: 3800, Loss: 0.6658651828765869
setp: 3900, Loss: 0.6660416722297668
setp: 4000, Loss: 0.6211141347885132
setp: 4100, Loss: 0.6445255875587463
setp: 4200, Loss: 0.6437135934829712
setp: 4300, Loss: 0.5770119428634644
setp: 4400, Loss: 0.6939103603363037
setp: 4500, Loss: 0.5989667773246765
setp: 4600, Loss: 0.5372971892356873
setp: 4700, Loss: 0.5760282874107361
setp: 4800, Loss: 0.6453332901000977
setp: 4900, Loss: 0.5994119048118591
training successfully ended.
validating...
acc: 0.6726973684210527
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.6842105263157895
precision: 0
recall: 0.0
F_score: 0
******fold 4******
[205, 403]
training...
setp: 0, Loss: 0.6561955809593201
setp: 100, Loss: 0.6434776186943054
setp: 200, Loss: 0.6256228685379028
setp: 300, Loss: 0.6185083389282227
setp: 400, Loss: 0.545712411403656
setp: 500, Loss: 0.562268853187561
setp: 600, Loss: 0.5078794956207275
setp: 700, Loss: 0.538701593875885
setp: 800, Loss: 0.5500587821006775
setp: 900, Loss: 0.5385082364082336
setp: 1000, Loss: 0.5895927548408508
setp: 1100, Loss: 0.5367711782455444
setp: 1200, Loss: 0.6150055527687073
setp: 1300, Loss: 0.5248599052429199
setp: 1400, Loss: 0.470181405544281
setp: 1500, Loss: 0.5879110097885132
setp: 1600, Loss: 0.5433511734008789
setp: 1700, Loss: 0.5412747859954834
setp: 1800, Loss: 0.4167623221874237
setp: 1900, Loss: 0.3972865045070648
setp: 2000, Loss: 0.4673956036567688
setp: 2100, Loss: 0.4213321805000305
setp: 2200, Loss: 0.36958545446395874
setp: 2300, Loss: 0.3630088269710541
setp: 2400, Loss: 0.3516881465911865
setp: 2500, Loss: 0.38567042350769043
setp: 2600, Loss: 0.3581952452659607
setp: 2700, Loss: 0.36733725666999817
setp: 2800, Loss: 0.3211379647254944
setp: 2900, Loss: 0.33464890718460083
setp: 3000, Loss: 0.32087844610214233
setp: 3100, Loss: 0.3516547977924347
setp: 3200, Loss: 0.3203313648700714
setp: 3300, Loss: 0.3195136487483978
setp: 3400, Loss: 0.3824024200439453
setp: 3500, Loss: 0.3207230865955353
setp: 3600, Loss: 0.31938472390174866
setp: 3700, Loss: 0.32209089398384094
setp: 3800, Loss: 0.33855241537094116
setp: 3900, Loss: 0.3448328375816345
setp: 4000, Loss: 0.3185423016548157
setp: 4100, Loss: 0.3184812664985657
setp: 4200, Loss: 0.31827297806739807
setp: 4300, Loss: 0.31767016649246216
setp: 4400, Loss: 0.31865394115448
setp: 4500, Loss: 0.31803813576698303
setp: 4600, Loss: 0.31739065051078796
setp: 4700, Loss: 0.3207413852214813
setp: 4800, Loss: 0.3166002035140991
setp: 4900, Loss: 0.31898990273475647
training successfully ended.
validating...
acc: 0.9490131578947368
precision: 0.967741935483871
recall: 0.8780487804878049
F_score: 0.9207161125319694
validating...
acc: 0.8881578947368421
precision: 0.9032258064516129
recall: 0.6666666666666666
F_score: 0.7671232876712328
model saved.
avg_acc: 0.8447368421052632, avg_f_score: 0.6496963077953769
-------------subject: 15-------------
==========valence==========
******fold 0******
[288, 320]
training...
setp: 0, Loss: 0.7522504925727844
setp: 100, Loss: 0.6786952614784241
setp: 200, Loss: 0.49763181805610657
setp: 300, Loss: 0.42250528931617737
setp: 400, Loss: 0.3911084830760956
setp: 500, Loss: 0.34504446387290955
setp: 600, Loss: 0.33391863107681274
setp: 700, Loss: 0.3303271234035492
setp: 800, Loss: 0.361428827047348
setp: 900, Loss: 0.3655131459236145
setp: 1000, Loss: 0.36012110114097595
setp: 1100, Loss: 0.37567248940467834
setp: 1200, Loss: 0.33250463008880615
setp: 1300, Loss: 0.31939658522605896
setp: 1400, Loss: 0.3204263746738434
setp: 1500, Loss: 0.3209778368473053
setp: 1600, Loss: 0.3213336169719696
setp: 1700, Loss: 0.32070639729499817
setp: 1800, Loss: 0.3209573030471802
setp: 1900, Loss: 0.32182615995407104
setp: 2000, Loss: 0.31952965259552
setp: 2100, Loss: 0.31939470767974854
setp: 2200, Loss: 0.31963691115379333
setp: 2300, Loss: 0.3194677531719208
setp: 2400, Loss: 0.31963637471199036
setp: 2500, Loss: 0.3197256922721863
setp: 2600, Loss: 0.32075539231300354
setp: 2700, Loss: 0.3515876233577728
setp: 2800, Loss: 0.34942561388015747
setp: 2900, Loss: 0.3189689517021179
setp: 3000, Loss: 0.43759194016456604
setp: 3100, Loss: 0.3318692445755005
setp: 3200, Loss: 0.33236128091812134
setp: 3300, Loss: 0.3451111912727356
setp: 3400, Loss: 0.3330289423465729
setp: 3500, Loss: 0.32395678758621216
setp: 3600, Loss: 0.31758517026901245
setp: 3700, Loss: 0.34616491198539734
setp: 3800, Loss: 0.3207021951675415
setp: 3900, Loss: 0.3183460235595703
setp: 4000, Loss: 0.31915679574012756
setp: 4100, Loss: 0.3196606934070587
setp: 4200, Loss: 0.31897401809692383
setp: 4300, Loss: 0.3204631805419922
setp: 4400, Loss: 0.3198031783103943
setp: 4500, Loss: 0.32185617089271545
setp: 4600, Loss: 0.3504577577114105
setp: 4700, Loss: 0.3196215033531189
setp: 4800, Loss: 0.31933677196502686
setp: 4900, Loss: 0.3205711841583252
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9965277777777778
F_score: 0.9982608695652174
validating...
acc: 0.9276315789473685
precision: 0.945054945054945
recall: 0.9347826086956522
F_score: 0.9398907103825138
******fold 1******
[308, 300]
training...
setp: 0, Loss: 0.6923088431358337
setp: 100, Loss: 0.6688997149467468
setp: 200, Loss: 0.56770920753479
setp: 300, Loss: 0.48635950684547424
setp: 400, Loss: 0.49365097284317017
setp: 500, Loss: 0.3898322880268097
setp: 600, Loss: 0.35783520340919495
setp: 700, Loss: 0.3398160934448242
setp: 800, Loss: 0.36412283778190613
setp: 900, Loss: 0.3253895938396454
setp: 1000, Loss: 0.34541112184524536
setp: 1100, Loss: 0.318464070558548
setp: 1200, Loss: 0.33969634771347046
setp: 1300, Loss: 0.31858187913894653
setp: 1400, Loss: 0.3187073767185211
setp: 1500, Loss: 0.3326488435268402
setp: 1600, Loss: 0.3170437514781952
setp: 1700, Loss: 0.31838342547416687
setp: 1800, Loss: 0.3189086616039276
setp: 1900, Loss: 0.3198019862174988
setp: 2000, Loss: 0.3181042969226837
setp: 2100, Loss: 0.3451474606990814
setp: 2200, Loss: 0.35686224699020386
setp: 2300, Loss: 0.3184986710548401
setp: 2400, Loss: 0.31952589750289917
setp: 2500, Loss: 0.31727030873298645
setp: 2600, Loss: 0.3156808018684387
setp: 2700, Loss: 0.32066020369529724
setp: 2800, Loss: 0.31721770763397217
setp: 2900, Loss: 0.3162216544151306
setp: 3000, Loss: 0.3180257976055145
setp: 3100, Loss: 0.33220943808555603
setp: 3200, Loss: 0.3711231052875519
setp: 3300, Loss: 0.33077141642570496
setp: 3400, Loss: 0.3178861141204834
setp: 3500, Loss: 0.31679460406303406
setp: 3600, Loss: 0.316886842250824
setp: 3700, Loss: 0.31755802035331726
setp: 3800, Loss: 0.3187813460826874
setp: 3900, Loss: 0.3177376389503479
setp: 4000, Loss: 0.3169569969177246
setp: 4100, Loss: 0.31673723459243774
setp: 4200, Loss: 0.318692684173584
setp: 4300, Loss: 0.3212331235408783
setp: 4400, Loss: 0.3203248083591461
setp: 4500, Loss: 0.31645846366882324
setp: 4600, Loss: 0.318008154630661
setp: 4700, Loss: 0.31692758202552795
setp: 4800, Loss: 0.3159394860267639
setp: 4900, Loss: 0.3172636032104492
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.881578947368421
precision: 0.9354838709677419
recall: 0.8055555555555556
F_score: 0.8656716417910448
******fold 2******
[306, 302]
training...
setp: 0, Loss: 0.6938163042068481
setp: 100, Loss: 0.6924129724502563
setp: 200, Loss: 0.7025511264801025
setp: 300, Loss: 0.4594227969646454
setp: 400, Loss: 0.5045610666275024
setp: 500, Loss: 0.5383737683296204
setp: 600, Loss: 0.46620118618011475
setp: 700, Loss: 0.49085673689842224
setp: 800, Loss: 0.5406399965286255
setp: 900, Loss: 0.37368279695510864
setp: 1000, Loss: 0.37847790122032166
setp: 1100, Loss: 0.37666067481040955
setp: 1200, Loss: 0.43462705612182617
setp: 1300, Loss: 0.34987181425094604
setp: 1400, Loss: 0.3399200737476349
setp: 1500, Loss: 0.3475249111652374
setp: 1600, Loss: 0.31778571009635925
setp: 1700, Loss: 0.3261553943157196
setp: 1800, Loss: 0.3357098400592804
setp: 1900, Loss: 0.3771779239177704
setp: 2000, Loss: 0.3188048005104065
setp: 2100, Loss: 0.31966549158096313
setp: 2200, Loss: 0.31730884313583374
setp: 2300, Loss: 0.35895660519599915
setp: 2400, Loss: 0.32157739996910095
setp: 2500, Loss: 0.31659796833992004
setp: 2600, Loss: 0.3266400694847107
setp: 2700, Loss: 0.35895857214927673
setp: 2800, Loss: 0.3182796239852905
setp: 2900, Loss: 0.31745025515556335
setp: 3000, Loss: 0.3227563500404358
setp: 3100, Loss: 0.33941012620925903
setp: 3200, Loss: 0.3284156024456024
setp: 3300, Loss: 0.3299836814403534
setp: 3400, Loss: 0.3257175087928772
setp: 3500, Loss: 0.3277030885219574
setp: 3600, Loss: 0.3156660199165344
setp: 3700, Loss: 0.3196925222873688
setp: 3800, Loss: 0.3206103444099426
setp: 3900, Loss: 0.31588631868362427
setp: 4000, Loss: 0.3174886107444763
setp: 4100, Loss: 0.315486878156662
setp: 4200, Loss: 0.3493162989616394
setp: 4300, Loss: 0.318386971950531
setp: 4400, Loss: 0.3161275088787079
setp: 4500, Loss: 0.31955236196517944
setp: 4600, Loss: 0.3512195944786072
setp: 4700, Loss: 0.3176494836807251
setp: 4800, Loss: 0.31674104928970337
setp: 4900, Loss: 0.31701987981796265
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.9934640522875817
F_score: 0.9967213114754099
validating...
acc: 0.9013157894736842
precision: 0.9154929577464789
recall: 0.8783783783783784
F_score: 0.896551724137931
******fold 3******
[298, 310]
training...
setp: 0, Loss: 0.7307659387588501
setp: 100, Loss: 0.6929349899291992
setp: 200, Loss: 0.6017351746559143
setp: 300, Loss: 0.5047305822372437
setp: 400, Loss: 0.5218170881271362
setp: 500, Loss: 0.42161843180656433
setp: 600, Loss: 0.3778964579105377
setp: 700, Loss: 0.37901705503463745
setp: 800, Loss: 0.4093710482120514
setp: 900, Loss: 0.3789384067058563
setp: 1000, Loss: 0.3420976400375366
setp: 1100, Loss: 0.3514947295188904
setp: 1200, Loss: 0.37651658058166504
setp: 1300, Loss: 0.33605775237083435
setp: 1400, Loss: 0.35640543699264526
setp: 1500, Loss: 0.3422791361808777
setp: 1600, Loss: 0.3386150300502777
setp: 1700, Loss: 0.3399755656719208
setp: 1800, Loss: 0.31941261887550354
setp: 1900, Loss: 0.32322192192077637
setp: 2000, Loss: 0.32510483264923096
setp: 2100, Loss: 0.3203636705875397
setp: 2200, Loss: 0.32134556770324707
setp: 2300, Loss: 0.3204898238182068
setp: 2400, Loss: 0.32178688049316406
setp: 2500, Loss: 0.3198738098144531
setp: 2600, Loss: 0.32175830006599426
setp: 2700, Loss: 0.3540036082267761
setp: 2800, Loss: 0.32056137919425964
setp: 2900, Loss: 0.32027313113212585
setp: 3000, Loss: 0.32037559151649475
setp: 3100, Loss: 0.3202013373374939
setp: 3200, Loss: 0.33735528588294983
setp: 3300, Loss: 0.3366921842098236
setp: 3400, Loss: 0.321191668510437
setp: 3500, Loss: 0.3284321129322052
setp: 3600, Loss: 0.3177873194217682
setp: 3700, Loss: 0.31858333945274353
setp: 3800, Loss: 0.32049041986465454
setp: 3900, Loss: 0.32119521498680115
setp: 4000, Loss: 0.31933656334877014
setp: 4100, Loss: 0.3192273676395416
setp: 4200, Loss: 0.31939926743507385
setp: 4300, Loss: 0.320438951253891
setp: 4400, Loss: 0.31895479559898376
setp: 4500, Loss: 0.32002121210098267
setp: 4600, Loss: 0.35158488154411316
setp: 4700, Loss: 0.3191699981689453
setp: 4800, Loss: 0.319585919380188
setp: 4900, Loss: 0.4942389726638794
training successfully ended.
validating...
acc: 0.9819078947368421
precision: 1.0
recall: 0.9630872483221476
F_score: 0.9811965811965813
validating...
acc: 0.881578947368421
precision: 1.0
recall: 0.7804878048780488
F_score: 0.8767123287671234
******fold 4******
[320, 288]
training...
setp: 0, Loss: 0.6911829710006714
setp: 100, Loss: 0.6827694773674011
setp: 200, Loss: 0.6081580519676208
setp: 300, Loss: 0.45795243978500366
setp: 400, Loss: 0.42103198170661926
setp: 500, Loss: 0.37051182985305786
setp: 600, Loss: 0.3342024087905884
setp: 700, Loss: 0.3300754129886627
setp: 800, Loss: 0.3666909635066986
setp: 900, Loss: 0.32486850023269653
setp: 1000, Loss: 0.32183367013931274
setp: 1100, Loss: 0.32075709104537964
setp: 1200, Loss: 0.32198643684387207
setp: 1300, Loss: 0.31862327456474304
setp: 1400, Loss: 0.3204907774925232
setp: 1500, Loss: 0.32011353969573975
setp: 1600, Loss: 0.31889408826828003
setp: 1700, Loss: 0.3195309042930603
setp: 1800, Loss: 0.3185006380081177
setp: 1900, Loss: 0.3523194193840027
setp: 2000, Loss: 0.3186894953250885
setp: 2100, Loss: 0.3568861186504364
setp: 2200, Loss: 0.3224169611930847
setp: 2300, Loss: 0.3237747848033905
setp: 2400, Loss: 0.3188420236110687
setp: 2500, Loss: 0.3181130886077881
setp: 2600, Loss: 0.3188879191875458
setp: 2700, Loss: 0.35030579566955566
setp: 2800, Loss: 0.3185361623764038
setp: 2900, Loss: 0.3179146349430084
setp: 3000, Loss: 0.3182019591331482
setp: 3100, Loss: 0.31891366839408875
setp: 3200, Loss: 0.31797873973846436
setp: 3300, Loss: 0.32010820508003235
setp: 3400, Loss: 0.3201499581336975
setp: 3500, Loss: 0.3519543409347534
setp: 3600, Loss: 0.356965035200119
setp: 3700, Loss: 0.3178764581680298
setp: 3800, Loss: 0.3484661877155304
setp: 3900, Loss: 0.3184667229652405
setp: 4000, Loss: 0.3177117109298706
setp: 4100, Loss: 0.3193056881427765
setp: 4200, Loss: 0.31980782747268677
setp: 4300, Loss: 0.31960973143577576
setp: 4400, Loss: 0.3185495436191559
setp: 4500, Loss: 0.31918731331825256
setp: 4600, Loss: 0.35065650939941406
setp: 4700, Loss: 0.3182852864265442
setp: 4800, Loss: 0.3176504075527191
setp: 4900, Loss: 0.3204699158668518
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.99375
F_score: 0.9968652037617556
validating...
acc: 0.9736842105263158
precision: 0.9516129032258065
recall: 0.9833333333333333
F_score: 0.9672131147540983
model saved.
avg_acc: 0.9131578947368422, avg_f_score: 0.9092079039665422
==========arousal==========
******fold 0******
[288, 320]
training...
setp: 0, Loss: 0.677751362323761
setp: 100, Loss: 0.6911717057228088
setp: 200, Loss: 0.5926201939582825
setp: 300, Loss: 0.6186516880989075
setp: 400, Loss: 0.5417908430099487
setp: 500, Loss: 0.5552852153778076
setp: 600, Loss: 0.43715277314186096
setp: 700, Loss: 0.4089333713054657
setp: 800, Loss: 0.4347730278968811
setp: 900, Loss: 0.34304115176200867
setp: 1000, Loss: 0.3961838185787201
setp: 1100, Loss: 0.4249473214149475
setp: 1200, Loss: 0.38141050934791565
setp: 1300, Loss: 0.32206112146377563
setp: 1400, Loss: 0.32297539710998535
setp: 1500, Loss: 0.3240904211997986
setp: 1600, Loss: 0.3173455595970154
setp: 1700, Loss: 0.3336126208305359
setp: 1800, Loss: 0.31833404302597046
setp: 1900, Loss: 0.32009121775627136
setp: 2000, Loss: 0.3472815155982971
setp: 2100, Loss: 0.32265374064445496
setp: 2200, Loss: 0.363297700881958
setp: 2300, Loss: 0.31850993633270264
setp: 2400, Loss: 0.39025890827178955
setp: 2500, Loss: 0.3259345591068268
setp: 2600, Loss: 0.3469354212284088
setp: 2700, Loss: 0.316889226436615
setp: 2800, Loss: 0.31793928146362305
setp: 2900, Loss: 0.3164100646972656
setp: 3000, Loss: 0.3489714562892914
setp: 3100, Loss: 0.31889021396636963
setp: 3200, Loss: 0.317714661359787
setp: 3300, Loss: 0.31742802262306213
setp: 3400, Loss: 0.3174586892127991
setp: 3500, Loss: 0.31663626432418823
setp: 3600, Loss: 0.3157360553741455
setp: 3700, Loss: 0.36026206612586975
setp: 3800, Loss: 0.38522812724113464
setp: 3900, Loss: 0.4030363857746124
setp: 4000, Loss: 0.5109845995903015
setp: 4100, Loss: 0.3513813018798828
setp: 4200, Loss: 0.33460733294487
setp: 4300, Loss: 0.33830368518829346
setp: 4400, Loss: 0.31640326976776123
setp: 4500, Loss: 0.31910499930381775
setp: 4600, Loss: 0.3337323069572449
setp: 4700, Loss: 0.31631582975387573
setp: 4800, Loss: 0.3164229989051819
setp: 4900, Loss: 0.32394593954086304
training successfully ended.
validating...
acc: 0.993421052631579
precision: 0.9965034965034965
recall: 0.9895833333333334
F_score: 0.9930313588850175
validating...
acc: 0.9144736842105263
precision: 0.9054054054054054
recall: 0.9178082191780822
F_score: 0.9115646258503401
******fold 1******
[277, 331]
training...
setp: 0, Loss: 0.6937709450721741
setp: 100, Loss: 0.6993642449378967
setp: 200, Loss: 0.6799913048744202
setp: 300, Loss: 0.6336658000946045
setp: 400, Loss: 0.6202902793884277
setp: 500, Loss: 0.7594757676124573
setp: 600, Loss: 0.536342203617096
setp: 700, Loss: 0.4823315143585205
setp: 800, Loss: 0.5202198028564453
setp: 900, Loss: 0.4453313648700714
setp: 1000, Loss: 0.36475637555122375
setp: 1100, Loss: 0.3979065418243408
setp: 1200, Loss: 0.33596789836883545
setp: 1300, Loss: 0.38735803961753845
setp: 1400, Loss: 0.36424124240875244
setp: 1500, Loss: 0.38597849011421204
setp: 1600, Loss: 0.3239147663116455
setp: 1700, Loss: 0.3198769688606262
setp: 1800, Loss: 0.3282766342163086
setp: 1900, Loss: 0.3498355746269226
setp: 2000, Loss: 0.33472365140914917
setp: 2100, Loss: 0.3837571144104004
setp: 2200, Loss: 0.32812294363975525
setp: 2300, Loss: 0.4448031485080719
setp: 2400, Loss: 0.320720911026001
setp: 2500, Loss: 0.3278605043888092
setp: 2600, Loss: 0.34090307354927063
setp: 2700, Loss: 0.35225921869277954
setp: 2800, Loss: 0.339756041765213
setp: 2900, Loss: 0.3317562937736511
setp: 3000, Loss: 0.31617310643196106
setp: 3100, Loss: 0.315978080034256
setp: 3200, Loss: 0.35150864720344543
setp: 3300, Loss: 0.31716352701187134
setp: 3400, Loss: 0.32400059700012207
setp: 3500, Loss: 0.32019972801208496
setp: 3600, Loss: 0.31808558106422424
setp: 3700, Loss: 0.3262676000595093
setp: 3800, Loss: 0.32107093930244446
setp: 3900, Loss: 0.32069525122642517
setp: 4000, Loss: 0.3524125814437866
setp: 4100, Loss: 0.31699058413505554
setp: 4200, Loss: 0.3294699788093567
setp: 4300, Loss: 0.3163171708583832
setp: 4400, Loss: 0.32264891266822815
setp: 4500, Loss: 0.31784069538116455
setp: 4600, Loss: 0.3177860677242279
setp: 4700, Loss: 0.31881624460220337
setp: 4800, Loss: 0.316962867975235
setp: 4900, Loss: 0.3162832260131836
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9964028776978417
recall: 1.0
F_score: 0.9981981981981982
validating...
acc: 0.9013157894736842
precision: 0.96
recall: 0.8571428571428571
F_score: 0.9056603773584904
******fold 2******
[291, 317]
training...
setp: 0, Loss: 0.7020204663276672
setp: 100, Loss: 0.6666640043258667
setp: 200, Loss: 0.5856409072875977
setp: 300, Loss: 0.6634836792945862
setp: 400, Loss: 0.6329046487808228
setp: 500, Loss: 0.5826935172080994
setp: 600, Loss: 0.5978079438209534
setp: 700, Loss: 0.4783022701740265
setp: 800, Loss: 0.5614745616912842
setp: 900, Loss: 0.5001314878463745
setp: 1000, Loss: 0.48048144578933716
setp: 1100, Loss: 0.4190841615200043
setp: 1200, Loss: 0.4981091022491455
setp: 1300, Loss: 0.44136109948158264
setp: 1400, Loss: 0.3999510705471039
setp: 1500, Loss: 0.42389747500419617
setp: 1600, Loss: 0.34074392914772034
setp: 1700, Loss: 0.3564094603061676
setp: 1800, Loss: 0.3747013211250305
setp: 1900, Loss: 0.35874050855636597
setp: 2000, Loss: 0.32087022066116333
setp: 2100, Loss: 0.321108877658844
setp: 2200, Loss: 0.31808924674987793
setp: 2300, Loss: 0.31845617294311523
setp: 2400, Loss: 0.31887704133987427
setp: 2500, Loss: 0.31685131788253784
setp: 2600, Loss: 0.38002610206604004
setp: 2700, Loss: 0.3173144459724426
setp: 2800, Loss: 0.3174479305744171
setp: 2900, Loss: 0.31735336780548096
setp: 3000, Loss: 0.31777137517929077
setp: 3100, Loss: 0.31711345911026
setp: 3200, Loss: 0.3183407783508301
setp: 3300, Loss: 0.31654465198516846
setp: 3400, Loss: 0.5025245547294617
setp: 3500, Loss: 0.3190869092941284
setp: 3600, Loss: 0.3165662884712219
setp: 3700, Loss: 0.31640103459358215
setp: 3800, Loss: 0.31884485483169556
setp: 3900, Loss: 0.31711676716804504
setp: 4000, Loss: 0.3184104859828949
setp: 4100, Loss: 0.31627538800239563
setp: 4200, Loss: 0.31566137075424194
setp: 4300, Loss: 0.31715264916419983
setp: 4400, Loss: 0.3176783323287964
setp: 4500, Loss: 0.37926551699638367
setp: 4600, Loss: 0.31694701313972473
setp: 4700, Loss: 0.3172249495983124
setp: 4800, Loss: 0.3171665668487549
setp: 4900, Loss: 0.31708672642707825
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9965635738831615
recall: 0.9965635738831615
F_score: 0.9965635738831615
validating...
acc: 0.9144736842105263
precision: 0.9253731343283582
recall: 0.8857142857142857
F_score: 0.9051094890510949
******fold 3******
[295, 313]
training...
setp: 0, Loss: 0.7189881801605225
setp: 100, Loss: 0.6959254145622253
setp: 200, Loss: 0.691949725151062
setp: 300, Loss: 0.6936588883399963
setp: 400, Loss: 0.6977419257164001
setp: 500, Loss: 0.6896430253982544
setp: 600, Loss: 0.6934812664985657
setp: 700, Loss: 0.6916718482971191
setp: 800, Loss: 0.6900147795677185
setp: 900, Loss: 0.6897501945495605
setp: 1000, Loss: 0.691810131072998
setp: 1100, Loss: 0.6916502714157104
setp: 1200, Loss: 0.6934895515441895
setp: 1300, Loss: 0.6933559775352478
setp: 1400, Loss: 0.6935787200927734
setp: 1500, Loss: 0.6937090754508972
setp: 1600, Loss: 0.6919446587562561
setp: 1700, Loss: 0.6920379996299744
setp: 1800, Loss: 0.6936203837394714
setp: 1900, Loss: 0.6958505511283875
setp: 2000, Loss: 0.6961588859558105
setp: 2100, Loss: 0.6919747591018677
setp: 2200, Loss: 0.6936443448066711
setp: 2300, Loss: 0.6977205872535706
setp: 2400, Loss: 0.6898391246795654
setp: 2500, Loss: 0.6934629082679749
setp: 2600, Loss: 0.6916707158088684
setp: 2700, Loss: 0.6900630593299866
setp: 2800, Loss: 0.6899774074554443
setp: 2900, Loss: 0.6918314099311829
setp: 3000, Loss: 0.6916484832763672
setp: 3100, Loss: 0.6934704780578613
setp: 3200, Loss: 0.6933345794677734
setp: 3300, Loss: 0.6935675144195557
setp: 3400, Loss: 0.6937146782875061
setp: 3500, Loss: 0.6919748783111572
setp: 3600, Loss: 0.6920706033706665
setp: 3700, Loss: 0.6936147212982178
setp: 3800, Loss: 0.69586580991745
setp: 3900, Loss: 0.6960635781288147
setp: 4000, Loss: 0.6919959187507629
setp: 4100, Loss: 0.6936410665512085
setp: 4200, Loss: 0.6977169513702393
setp: 4300, Loss: 0.6899349689483643
setp: 4400, Loss: 0.6934541463851929
setp: 4500, Loss: 0.6916725039482117
setp: 4600, Loss: 0.6900839805603027
setp: 4700, Loss: 0.6900527477264404
setp: 4800, Loss: 0.6918400526046753
setp: 4900, Loss: 0.6916484236717224
training successfully ended.
validating...
acc: 0.5148026315789473
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.5657894736842105
precision: 0
recall: 0.0
F_score: 0
******fold 4******
[293, 315]
training...
setp: 0, Loss: 0.6915079355239868
setp: 100, Loss: 0.6834057569503784
setp: 200, Loss: 0.6621560454368591
setp: 300, Loss: 0.46884685754776
setp: 400, Loss: 0.5937997698783875
setp: 500, Loss: 0.49475863575935364
setp: 600, Loss: 0.45417091250419617
setp: 700, Loss: 0.4664396643638611
setp: 800, Loss: 0.4726175367832184
setp: 900, Loss: 0.3543410003185272
setp: 1000, Loss: 0.40463384985923767
setp: 1100, Loss: 0.3779245615005493
setp: 1200, Loss: 0.4114278256893158
setp: 1300, Loss: 0.3299436867237091
setp: 1400, Loss: 0.34882602095603943
setp: 1500, Loss: 0.33053871989250183
setp: 1600, Loss: 0.340730756521225
setp: 1700, Loss: 0.3331746757030487
setp: 1800, Loss: 0.3194643259048462
setp: 1900, Loss: 0.32083243131637573
setp: 2000, Loss: 0.31781265139579773
setp: 2100, Loss: 0.3200405240058899
setp: 2200, Loss: 0.3160756826400757
setp: 2300, Loss: 0.3183174431324005
setp: 2400, Loss: 0.5912454724311829
setp: 2500, Loss: 0.3444207012653351
setp: 2600, Loss: 0.3548886179924011
setp: 2700, Loss: 0.3967103660106659
setp: 2800, Loss: 0.3223986029624939
setp: 2900, Loss: 0.3169005811214447
setp: 3000, Loss: 0.3402237296104431
setp: 3100, Loss: 0.332133412361145
setp: 3200, Loss: 0.32378777861595154
setp: 3300, Loss: 0.3183392286300659
setp: 3400, Loss: 0.33114802837371826
setp: 3500, Loss: 0.3180687129497528
setp: 3600, Loss: 0.3197645843029022
setp: 3700, Loss: 0.31589752435684204
setp: 3800, Loss: 0.3424382507801056
setp: 3900, Loss: 0.31626737117767334
setp: 4000, Loss: 0.31849032640457153
setp: 4100, Loss: 0.3166261315345764
setp: 4200, Loss: 0.31697362661361694
setp: 4300, Loss: 0.3169812858104706
setp: 4400, Loss: 0.31618937849998474
setp: 4500, Loss: 0.349531888961792
setp: 4600, Loss: 0.31659960746765137
setp: 4700, Loss: 0.3167760968208313
setp: 4800, Loss: 0.31610625982284546
setp: 4900, Loss: 0.31869032979011536
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9965986394557823
recall: 1.0
F_score: 0.9982964224872232
validating...
acc: 0.9671052631578947
precision: 0.9846153846153847
recall: 0.9411764705882353
F_score: 0.962406015037594
model saved.
avg_acc: 0.8526315789473685, avg_f_score: 0.7369481014595038
-------------subject: 16-------------
==========valence==========
******fold 0******
[375, 233]
training...
setp: 0, Loss: 0.7015461921691895
setp: 100, Loss: 0.6730692386627197
setp: 200, Loss: 0.4735606610774994
setp: 300, Loss: 0.5428605675697327
setp: 400, Loss: 0.5645508766174316
setp: 500, Loss: 0.4613688588142395
setp: 600, Loss: 0.5762317776679993
setp: 700, Loss: 0.47331464290618896
setp: 800, Loss: 0.42904484272003174
setp: 900, Loss: 0.6058576703071594
setp: 1000, Loss: 0.4461866021156311
setp: 1100, Loss: 0.440886527299881
setp: 1200, Loss: 0.4667963683605194
setp: 1300, Loss: 0.44563549757003784
setp: 1400, Loss: 0.3792353570461273
setp: 1500, Loss: 0.3627757132053375
setp: 1600, Loss: 0.4521467387676239
setp: 1700, Loss: 0.3996138870716095
setp: 1800, Loss: 0.35714685916900635
setp: 1900, Loss: 0.38965484499931335
setp: 2000, Loss: 0.3484537601470947
setp: 2100, Loss: 0.314983993768692
setp: 2200, Loss: 0.3484683930873871
setp: 2300, Loss: 0.3801077902317047
setp: 2400, Loss: 0.32023537158966064
setp: 2500, Loss: 0.3809727430343628
setp: 2600, Loss: 0.3480232357978821
setp: 2700, Loss: 0.31625813245773315
setp: 2800, Loss: 0.31710007786750793
setp: 2900, Loss: 0.34874793887138367
setp: 3000, Loss: 0.3476306200027466
setp: 3100, Loss: 0.34915366768836975
setp: 3200, Loss: 0.3808108866214752
setp: 3300, Loss: 0.3473759889602661
setp: 3400, Loss: 0.34801414608955383
setp: 3500, Loss: 0.4169735610485077
setp: 3600, Loss: 0.3519532382488251
setp: 3700, Loss: 0.36415940523147583
setp: 3800, Loss: 0.3785143792629242
setp: 3900, Loss: 0.36045098304748535
setp: 4000, Loss: 0.3149818181991577
setp: 4100, Loss: 0.34799206256866455
setp: 4200, Loss: 0.3817288279533386
setp: 4300, Loss: 0.31582728028297424
setp: 4400, Loss: 0.3798963725566864
setp: 4500, Loss: 0.3476538360118866
setp: 4600, Loss: 0.3161739110946655
setp: 4700, Loss: 0.3166276514530182
setp: 4800, Loss: 0.34712228178977966
setp: 4900, Loss: 0.3467259407043457
training successfully ended.
validating...
acc: 0.96875
precision: 0.9517766497461929
recall: 1.0
F_score: 0.9752925877763329
validating...
acc: 0.9736842105263158
precision: 0.9705882352941176
recall: 0.99
F_score: 0.9801980198019802
******fold 1******
[375, 233]
training...
setp: 0, Loss: 0.7306428551673889
setp: 100, Loss: 0.6167663335800171
setp: 200, Loss: 0.5659451484680176
setp: 300, Loss: 0.42024141550064087
setp: 400, Loss: 0.615424394607544
setp: 500, Loss: 0.5773147940635681
setp: 600, Loss: 0.4149068295955658
setp: 700, Loss: 0.3814716339111328
setp: 800, Loss: 0.4155627191066742
setp: 900, Loss: 0.41844791173934937
setp: 1000, Loss: 0.41942262649536133
setp: 1100, Loss: 0.40951207280158997
setp: 1200, Loss: 0.44444552063941956
setp: 1300, Loss: 0.4721527397632599
setp: 1400, Loss: 0.44042590260505676
setp: 1500, Loss: 0.44064030051231384
setp: 1600, Loss: 0.5169922709465027
setp: 1700, Loss: 0.4543948769569397
setp: 1800, Loss: 0.4088892936706543
setp: 1900, Loss: 0.44910669326782227
setp: 2000, Loss: 0.40494489669799805
setp: 2100, Loss: 0.3540264666080475
setp: 2200, Loss: 0.3477782905101776
setp: 2300, Loss: 0.4209131896495819
setp: 2400, Loss: 0.34777331352233887
setp: 2500, Loss: 0.3482430577278137
setp: 2600, Loss: 0.3159594237804413
setp: 2700, Loss: 0.3477834463119507
setp: 2800, Loss: 0.31707563996315
setp: 2900, Loss: 0.34820204973220825
setp: 3000, Loss: 0.3510535657405853
setp: 3100, Loss: 0.3382558226585388
setp: 3200, Loss: 0.34855446219444275
setp: 3300, Loss: 0.3475477993488312
setp: 3400, Loss: 0.34897154569625854
setp: 3500, Loss: 0.3485617935657501
setp: 3600, Loss: 0.3475237488746643
setp: 3700, Loss: 0.3167811334133148
setp: 3800, Loss: 0.34743401408195496
setp: 3900, Loss: 0.3178000748157501
setp: 4000, Loss: 0.3467385768890381
setp: 4100, Loss: 0.34804773330688477
setp: 4200, Loss: 0.40029677748680115
setp: 4300, Loss: 0.34961584210395813
setp: 4400, Loss: 0.3158857524394989
setp: 4500, Loss: 0.3153921067714691
setp: 4600, Loss: 0.34633365273475647
setp: 4700, Loss: 0.3157847821712494
setp: 4800, Loss: 0.34892722964286804
setp: 4900, Loss: 0.3466244339942932
training successfully ended.
validating...
acc: 0.975328947368421
precision: 0.966321243523316
recall: 0.9946666666666667
F_score: 0.9802890932982917
validating...
acc: 0.8618421052631579
precision: 0.9157894736842105
recall: 0.87
F_score: 0.8923076923076922
******fold 2******
[377, 231]
training...
setp: 0, Loss: 0.8013192415237427
setp: 100, Loss: 0.6450437903404236
setp: 200, Loss: 0.4962523281574249
setp: 300, Loss: 0.482755571603775
setp: 400, Loss: 0.42817574739456177
setp: 500, Loss: 0.4421912729740143
setp: 600, Loss: 0.46322935819625854
setp: 700, Loss: 0.4313352108001709
setp: 800, Loss: 0.4013160169124603
setp: 900, Loss: 0.38836508989334106
setp: 1000, Loss: 0.32373562455177307
setp: 1100, Loss: 0.31723541021347046
setp: 1200, Loss: 0.41213899850845337
setp: 1300, Loss: 0.40661942958831787
setp: 1400, Loss: 0.35114404559135437
setp: 1500, Loss: 0.3562633991241455
setp: 1600, Loss: 0.3508422076702118
setp: 1700, Loss: 0.3203166723251343
setp: 1800, Loss: 0.3642277121543884
setp: 1900, Loss: 0.37788125872612
setp: 2000, Loss: 0.3484613299369812
setp: 2100, Loss: 0.31686636805534363
setp: 2200, Loss: 0.34752005338668823
setp: 2300, Loss: 0.3481596112251282
setp: 2400, Loss: 0.3488490581512451
setp: 2500, Loss: 0.34791484475135803
setp: 2600, Loss: 0.34781864285469055
setp: 2700, Loss: 0.3481554388999939
setp: 2800, Loss: 0.3489927351474762
setp: 2900, Loss: 0.3249528706073761
setp: 3000, Loss: 0.3186686933040619
setp: 3100, Loss: 0.36777985095977783
setp: 3200, Loss: 0.4295504689216614
setp: 3300, Loss: 0.34641534090042114
setp: 3400, Loss: 0.34838807582855225
setp: 3500, Loss: 0.3538021147251129
setp: 3600, Loss: 0.3167371153831482
setp: 3700, Loss: 0.3469546437263489
setp: 3800, Loss: 0.35037732124328613
setp: 3900, Loss: 0.3498346507549286
setp: 4000, Loss: 0.3155204951763153
setp: 4100, Loss: 0.35043832659721375
setp: 4200, Loss: 0.3472888469696045
setp: 4300, Loss: 0.34739434719085693
setp: 4400, Loss: 0.34715354442596436
setp: 4500, Loss: 0.3469042479991913
setp: 4600, Loss: 0.34802278876304626
setp: 4700, Loss: 0.34743526577949524
setp: 4800, Loss: 0.31614622473716736
setp: 4900, Loss: 0.3158636689186096
training successfully ended.
validating...
acc: 0.975328947368421
precision: 0.9617346938775511
recall: 1.0
F_score: 0.9804941482444733
validating...
acc: 0.8881578947368421
precision: 0.8932038834951457
recall: 0.9387755102040817
F_score: 0.9154228855721394
******fold 3******
[389, 219]
training...
setp: 0, Loss: 0.7855618596076965
setp: 100, Loss: 0.648396372795105
setp: 200, Loss: 0.5351472496986389
setp: 300, Loss: 0.5603512525558472
setp: 400, Loss: 0.5230002403259277
setp: 500, Loss: 0.4979940950870514
setp: 600, Loss: 0.49346619844436646
setp: 700, Loss: 0.39057764410972595
setp: 800, Loss: 0.4934191107749939
setp: 900, Loss: 0.43827322125434875
setp: 1000, Loss: 0.4599824547767639
setp: 1100, Loss: 0.44869035482406616
setp: 1200, Loss: 0.39089077711105347
setp: 1300, Loss: 0.32567211985588074
setp: 1400, Loss: 0.41068604588508606
setp: 1500, Loss: 0.40428489446640015
setp: 1600, Loss: 0.3537849485874176
setp: 1700, Loss: 0.3784940838813782
setp: 1800, Loss: 0.37947261333465576
setp: 1900, Loss: 0.38133659958839417
setp: 2000, Loss: 0.37956443428993225
setp: 2100, Loss: 0.380970299243927
setp: 2200, Loss: 0.407124787569046
setp: 2300, Loss: 0.47035565972328186
setp: 2400, Loss: 0.3713422417640686
setp: 2500, Loss: 0.3482155501842499
setp: 2600, Loss: 0.348497599363327
setp: 2700, Loss: 0.426926851272583
setp: 2800, Loss: 0.3785546123981476
setp: 2900, Loss: 0.34791693091392517
setp: 3000, Loss: 0.3476455807685852
setp: 3100, Loss: 0.3469987213611603
setp: 3200, Loss: 0.32796624302864075
setp: 3300, Loss: 0.34760913252830505
setp: 3400, Loss: 0.3195245862007141
setp: 3500, Loss: 0.3492967188358307
setp: 3600, Loss: 0.35045507550239563
setp: 3700, Loss: 0.34721386432647705
setp: 3800, Loss: 0.3204326033592224
setp: 3900, Loss: 0.3475935459136963
setp: 4000, Loss: 0.34720277786254883
setp: 4100, Loss: 0.3471840023994446
setp: 4200, Loss: 0.3494950234889984
setp: 4300, Loss: 0.3482065200805664
setp: 4400, Loss: 0.3473522663116455
setp: 4500, Loss: 0.6235474348068237
setp: 4600, Loss: 0.4112813472747803
setp: 4700, Loss: 0.3896028995513916
setp: 4800, Loss: 0.35009676218032837
setp: 4900, Loss: 0.4342898726463318
training successfully ended.
validating...
acc: 0.975328947368421
precision: 0.9628712871287128
recall: 1.0
F_score: 0.9810844892812106
validating...
acc: 0.9605263157894737
precision: 0.9444444444444444
recall: 0.9883720930232558
F_score: 0.9659090909090908
******fold 4******
[384, 224]
training...
setp: 0, Loss: 0.8028025031089783
setp: 100, Loss: 0.6436222791671753
setp: 200, Loss: 0.5664528012275696
setp: 300, Loss: 0.5481685996055603
setp: 400, Loss: 0.4672574996948242
setp: 500, Loss: 0.5053575038909912
setp: 600, Loss: 0.4678887724876404
setp: 700, Loss: 0.38974252343177795
setp: 800, Loss: 0.47019582986831665
setp: 900, Loss: 0.43994003534317017
setp: 1000, Loss: 0.3870157301425934
setp: 1100, Loss: 0.4780378043651581
setp: 1200, Loss: 0.44293588399887085
setp: 1300, Loss: 0.4625450372695923
setp: 1400, Loss: 0.41226163506507874
setp: 1500, Loss: 0.4109300971031189
setp: 1600, Loss: 0.4271441698074341
setp: 1700, Loss: 0.45456862449645996
setp: 1800, Loss: 0.4184613525867462
setp: 1900, Loss: 0.4376382529735565
setp: 2000, Loss: 0.37971994280815125
setp: 2100, Loss: 0.41118913888931274
setp: 2200, Loss: 0.4418110251426697
setp: 2300, Loss: 0.3844798803329468
setp: 2400, Loss: 0.4412451982498169
setp: 2500, Loss: 0.41424670815467834
setp: 2600, Loss: 0.36946871876716614
setp: 2700, Loss: 0.3857458233833313
setp: 2800, Loss: 0.39405930042266846
setp: 2900, Loss: 0.3781915307044983
setp: 3000, Loss: 0.40776872634887695
setp: 3100, Loss: 0.385549396276474
setp: 3200, Loss: 0.4113083481788635
setp: 3300, Loss: 0.39338842034339905
setp: 3400, Loss: 0.41164901852607727
setp: 3500, Loss: 0.41573643684387207
setp: 3600, Loss: 0.36125749349594116
setp: 3700, Loss: 0.31764423847198486
setp: 3800, Loss: 0.3487018346786499
setp: 3900, Loss: 0.31652942299842834
setp: 4000, Loss: 0.34730881452560425
setp: 4100, Loss: 0.35079753398895264
setp: 4200, Loss: 0.3173926770687103
setp: 4300, Loss: 0.3472177982330322
setp: 4400, Loss: 0.3473597466945648
setp: 4500, Loss: 0.39143964648246765
setp: 4600, Loss: 0.34897327423095703
setp: 4700, Loss: 0.34631744027137756
setp: 4800, Loss: 0.3468712568283081
setp: 4900, Loss: 0.34648510813713074
training successfully ended.
validating...
acc: 0.975328947368421
precision: 0.9624060150375939
recall: 1.0
F_score: 0.9808429118773946
validating...
acc: 0.9276315789473685
precision: 0.9
recall: 0.989010989010989
F_score: 0.9424083769633509
model saved.
avg_acc: 0.9223684210526315, avg_f_score: 0.9392492131108506
==========arousal==========
******fold 0******
[305, 303]
training...
setp: 0, Loss: 0.728329062461853
setp: 100, Loss: 0.5933452248573303
setp: 200, Loss: 0.5857921838760376
setp: 300, Loss: 0.4636627435684204
setp: 400, Loss: 0.6331086754798889
setp: 500, Loss: 0.4819248914718628
setp: 600, Loss: 0.5373876690864563
setp: 700, Loss: 0.4190448224544525
setp: 800, Loss: 0.4615766704082489
setp: 900, Loss: 0.44327566027641296
setp: 1000, Loss: 0.44286787509918213
setp: 1100, Loss: 0.4295833110809326
setp: 1200, Loss: 0.41161781549453735
setp: 1300, Loss: 0.4071090519428253
setp: 1400, Loss: 0.47143951058387756
setp: 1500, Loss: 0.3519408106803894
setp: 1600, Loss: 0.4423578679561615
setp: 1700, Loss: 0.40638744831085205
setp: 1800, Loss: 0.5610362887382507
setp: 1900, Loss: 0.41017085313796997
setp: 2000, Loss: 0.46951529383659363
setp: 2100, Loss: 0.4941948652267456
setp: 2200, Loss: 0.41014620661735535
setp: 2300, Loss: 0.4115867018699646
setp: 2400, Loss: 0.4084222614765167
setp: 2500, Loss: 0.4858912527561188
setp: 2600, Loss: 0.3768687844276428
setp: 2700, Loss: 0.3788699805736542
setp: 2800, Loss: 0.43433377146720886
setp: 2900, Loss: 0.38028958439826965
setp: 3000, Loss: 0.3999437987804413
setp: 3100, Loss: 0.4083234965801239
setp: 3200, Loss: 0.3777841031551361
setp: 3300, Loss: 0.4712320566177368
setp: 3400, Loss: 0.35619306564331055
setp: 3500, Loss: 0.34734493494033813
setp: 3600, Loss: 0.37871572375297546
setp: 3700, Loss: 0.36661770939826965
setp: 3800, Loss: 0.3641797602176666
setp: 3900, Loss: 0.40552449226379395
setp: 4000, Loss: 0.3794563412666321
setp: 4100, Loss: 0.4114832580089569
setp: 4200, Loss: 0.3821084797382355
setp: 4300, Loss: 0.34794721007347107
setp: 4400, Loss: 0.35154005885124207
setp: 4500, Loss: 0.3167680501937866
setp: 4600, Loss: 0.3473968207836151
setp: 4700, Loss: 0.347846657037735
setp: 4800, Loss: 0.3466934859752655
setp: 4900, Loss: 0.3492352068424225
training successfully ended.
validating...
acc: 0.96875
precision: 0.941358024691358
recall: 1.0
F_score: 0.9697933227344991
validating...
acc: 0.9078947368421053
precision: 0.8961038961038961
recall: 0.92
F_score: 0.9078947368421053
******fold 1******
[310, 298]
training...
setp: 0, Loss: 0.6915868520736694
setp: 100, Loss: 0.5497995018959045
setp: 200, Loss: 0.6019138097763062
setp: 300, Loss: 0.48347732424736023
setp: 400, Loss: 0.5771298408508301
setp: 500, Loss: 0.48013144731521606
setp: 600, Loss: 0.705310583114624
setp: 700, Loss: 0.5577768683433533
setp: 800, Loss: 0.4423581063747406
setp: 900, Loss: 0.5004197955131531
setp: 1000, Loss: 0.5085158348083496
setp: 1100, Loss: 0.5107489228248596
setp: 1200, Loss: 0.4144759476184845
setp: 1300, Loss: 0.450951486825943
setp: 1400, Loss: 0.4718896150588989
setp: 1500, Loss: 0.36875951290130615
setp: 1600, Loss: 0.4438443183898926
setp: 1700, Loss: 0.3810727894306183
setp: 1800, Loss: 0.4344683587551117
setp: 1900, Loss: 0.4500814378261566
setp: 2000, Loss: 0.4233527183532715
setp: 2100, Loss: 0.4689525067806244
setp: 2200, Loss: 0.4369385540485382
setp: 2300, Loss: 0.5291517972946167
setp: 2400, Loss: 0.4090822637081146
setp: 2500, Loss: 0.4119757413864136
setp: 2600, Loss: 0.4087400734424591
setp: 2700, Loss: 0.38253986835479736
setp: 2800, Loss: 0.4132493734359741
setp: 2900, Loss: 0.3788888454437256
setp: 3000, Loss: 0.3574947416782379
setp: 3100, Loss: 0.4056193232536316
setp: 3200, Loss: 0.41112586855888367
setp: 3300, Loss: 0.4684011936187744
setp: 3400, Loss: 0.34571605920791626
setp: 3500, Loss: 0.34854716062545776
setp: 3600, Loss: 0.34809350967407227
setp: 3700, Loss: 0.34828653931617737
setp: 3800, Loss: 0.3912578523159027
setp: 3900, Loss: 0.36064237356185913
setp: 4000, Loss: 0.3278718590736389
setp: 4100, Loss: 0.3797937333583832
setp: 4200, Loss: 0.40815219283103943
setp: 4300, Loss: 0.34741997718811035
setp: 4400, Loss: 0.37652459740638733
setp: 4500, Loss: 0.3835318088531494
setp: 4600, Loss: 0.3166886270046234
setp: 4700, Loss: 0.3785077631473541
setp: 4800, Loss: 0.3162292242050171
setp: 4900, Loss: 0.3474031388759613
training successfully ended.
validating...
acc: 0.9539473684210527
precision: 0.9196428571428571
recall: 0.9967741935483871
F_score: 0.9566563467492261
validating...
acc: 0.875
precision: 0.7865168539325843
recall: 1.0
F_score: 0.880503144654088
******fold 2******
[311, 297]
training...
setp: 0, Loss: 0.6996876001358032
setp: 100, Loss: 0.5642454624176025
setp: 200, Loss: 0.5571094751358032
setp: 300, Loss: 0.5827040672302246
setp: 400, Loss: 0.5831979513168335
setp: 500, Loss: 0.49505776166915894
setp: 600, Loss: 0.48347723484039307
setp: 700, Loss: 0.5401069521903992
setp: 800, Loss: 0.590287983417511
setp: 900, Loss: 0.40282419323921204
setp: 1000, Loss: 0.4918694496154785
setp: 1100, Loss: 0.5242394804954529
setp: 1200, Loss: 0.4477751851081848
setp: 1300, Loss: 0.43658775091171265
setp: 1400, Loss: 0.46160468459129333
setp: 1500, Loss: 0.4591904282569885
setp: 1600, Loss: 0.4441555440425873
setp: 1700, Loss: 0.4002193808555603
setp: 1800, Loss: 0.3709612190723419
setp: 1900, Loss: 0.4236205518245697
setp: 2000, Loss: 0.5015523433685303
setp: 2100, Loss: 0.49164634943008423
setp: 2200, Loss: 0.42302414774894714
setp: 2300, Loss: 0.48031604290008545
setp: 2400, Loss: 0.3892098069190979
setp: 2500, Loss: 0.43710923194885254
setp: 2600, Loss: 0.4516957104206085
setp: 2700, Loss: 0.4115467667579651
setp: 2800, Loss: 0.3802816569805145
setp: 2900, Loss: 0.4683532118797302
setp: 3000, Loss: 0.32840481400489807
setp: 3100, Loss: 0.4214786887168884
setp: 3200, Loss: 0.3847091495990753
setp: 3300, Loss: 0.46625417470932007
setp: 3400, Loss: 0.3843677043914795
setp: 3500, Loss: 0.3741893470287323
setp: 3600, Loss: 0.3765050172805786
setp: 3700, Loss: 0.35505443811416626
setp: 3800, Loss: 0.34919512271881104
setp: 3900, Loss: 0.4095309376716614
setp: 4000, Loss: 0.37717607617378235
setp: 4100, Loss: 0.3879213333129883
setp: 4200, Loss: 0.4267326891422272
setp: 4300, Loss: 0.37288352847099304
setp: 4400, Loss: 0.35934677720069885
setp: 4500, Loss: 0.4203135371208191
setp: 4600, Loss: 0.3851615786552429
setp: 4700, Loss: 0.3478318750858307
setp: 4800, Loss: 0.36288145184516907
setp: 4900, Loss: 0.3161804974079132
training successfully ended.
validating...
acc: 0.96875
precision: 0.9506172839506173
recall: 0.9903536977491961
F_score: 0.9700787401574803
validating...
acc: 0.9210526315789473
precision: 0.88
recall: 0.9565217391304348
F_score: 0.9166666666666666
******fold 3******
[300, 308]
training...
setp: 0, Loss: 0.699289083480835
setp: 100, Loss: 0.5340279340744019
setp: 200, Loss: 0.5809189081192017
setp: 300, Loss: 0.5430785417556763
setp: 400, Loss: 0.5592594146728516
setp: 500, Loss: 0.46194225549697876
setp: 600, Loss: 0.5718233585357666
setp: 700, Loss: 0.5517212748527527
setp: 800, Loss: 0.5862606763839722
setp: 900, Loss: 0.4334510266780853
setp: 1000, Loss: 0.656385600566864
setp: 1100, Loss: 0.5053161978721619
setp: 1200, Loss: 0.4866391122341156
setp: 1300, Loss: 0.40994346141815186
setp: 1400, Loss: 0.3999428451061249
setp: 1500, Loss: 0.3492184281349182
setp: 1600, Loss: 0.42400017380714417
setp: 1700, Loss: 0.38706937432289124
setp: 1800, Loss: 0.38004642724990845
setp: 1900, Loss: 0.3846028447151184
setp: 2000, Loss: 0.38684335350990295
setp: 2100, Loss: 0.38140004873275757
setp: 2200, Loss: 0.3542177379131317
setp: 2300, Loss: 0.39107781648635864
setp: 2400, Loss: 0.31862401962280273
setp: 2500, Loss: 0.3184646964073181
setp: 2600, Loss: 0.31618231534957886
setp: 2700, Loss: 0.31765908002853394
setp: 2800, Loss: 0.31748485565185547
setp: 2900, Loss: 0.31654539704322815
setp: 3000, Loss: 0.3168741464614868
setp: 3100, Loss: 0.3159927725791931
setp: 3200, Loss: 0.3180575966835022
setp: 3300, Loss: 0.3506298065185547
setp: 3400, Loss: 0.31974008679389954
setp: 3500, Loss: 0.3162693679332733
setp: 3600, Loss: 0.3179672062397003
setp: 3700, Loss: 0.3167876899242401
setp: 3800, Loss: 0.31913328170776367
setp: 3900, Loss: 0.31737959384918213
setp: 4000, Loss: 0.3178389072418213
setp: 4100, Loss: 0.3181960880756378
setp: 4200, Loss: 0.3171404302120209
setp: 4300, Loss: 0.31572195887565613
setp: 4400, Loss: 0.31692013144493103
setp: 4500, Loss: 0.3173777163028717
setp: 4600, Loss: 0.3199158310890198
setp: 4700, Loss: 0.3177693784236908
setp: 4800, Loss: 0.35877904295921326
setp: 4900, Loss: 0.3155800998210907
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9736842105263158
precision: 0.9871794871794872
recall: 0.9625
F_score: 0.9746835443037976
******fold 4******
[294, 314]
training...
setp: 0, Loss: 0.719010055065155
setp: 100, Loss: 0.5627865791320801
setp: 200, Loss: 0.5784807205200195
setp: 300, Loss: 0.4919475018978119
setp: 400, Loss: 0.472709596157074
setp: 500, Loss: 0.5114526748657227
setp: 600, Loss: 0.5084521770477295
setp: 700, Loss: 0.5180413722991943
setp: 800, Loss: 0.4246484339237213
setp: 900, Loss: 0.48787832260131836
setp: 1000, Loss: 0.4448273777961731
setp: 1100, Loss: 0.3591770827770233
setp: 1200, Loss: 0.38578706979751587
setp: 1300, Loss: 0.41050130128860474
setp: 1400, Loss: 0.3803846836090088
setp: 1500, Loss: 0.3796353042125702
setp: 1600, Loss: 0.44294822216033936
setp: 1700, Loss: 0.37955623865127563
setp: 1800, Loss: 0.378401517868042
setp: 1900, Loss: 0.3478020131587982
setp: 2000, Loss: 0.37912219762802124
setp: 2100, Loss: 0.37997594475746155
setp: 2200, Loss: 0.31893426179885864
setp: 2300, Loss: 0.43071919679641724
setp: 2400, Loss: 0.4005039632320404
setp: 2500, Loss: 0.37827345728874207
setp: 2600, Loss: 0.41000011563301086
setp: 2700, Loss: 0.37872228026390076
setp: 2800, Loss: 0.37022197246551514
setp: 2900, Loss: 0.3861354887485504
setp: 3000, Loss: 0.34751006960868835
setp: 3100, Loss: 0.34700170159339905
setp: 3200, Loss: 0.3779061436653137
setp: 3300, Loss: 0.356719434261322
setp: 3400, Loss: 0.34859582781791687
setp: 3500, Loss: 0.3857216238975525
setp: 3600, Loss: 0.34961381554603577
setp: 3700, Loss: 0.34814393520355225
setp: 3800, Loss: 0.39532166719436646
setp: 3900, Loss: 0.34707707166671753
setp: 4000, Loss: 0.3466161787509918
setp: 4100, Loss: 0.3197208344936371
setp: 4200, Loss: 0.37826719880104065
setp: 4300, Loss: 0.31644150614738464
setp: 4400, Loss: 0.34712061285972595
setp: 4500, Loss: 0.3479686975479126
setp: 4600, Loss: 0.34762975573539734
setp: 4700, Loss: 0.37035199999809265
setp: 4800, Loss: 0.3468701243400574
setp: 4900, Loss: 0.3467986583709717
training successfully ended.
validating...
acc: 0.9720394736842105
precision: 0.948220064724919
recall: 0.9965986394557823
F_score: 0.9718076285240463
validating...
acc: 0.9276315789473685
precision: 0.9032258064516129
recall: 0.9767441860465116
F_score: 0.9385474860335195
model saved.
avg_acc: 0.9210526315789475, avg_f_score: 0.9236591157000354
-------------subject: 17-------------
==========valence==========
******fold 0******
[267, 341]
training...
setp: 0, Loss: 0.7281898856163025
setp: 100, Loss: 0.6848774552345276
setp: 200, Loss: 0.6851038336753845
setp: 300, Loss: 0.6633762717247009
setp: 400, Loss: 0.5877552628517151
setp: 500, Loss: 0.4080783426761627
setp: 600, Loss: 0.354154109954834
setp: 700, Loss: 0.417019784450531
setp: 800, Loss: 0.36455854773521423
setp: 900, Loss: 0.35533520579338074
setp: 1000, Loss: 0.3227214515209198
setp: 1100, Loss: 0.3204690217971802
setp: 1200, Loss: 0.32464638352394104
setp: 1300, Loss: 0.32342249155044556
setp: 1400, Loss: 0.3331286907196045
setp: 1500, Loss: 0.32794514298439026
setp: 1600, Loss: 0.32038503885269165
setp: 1700, Loss: 0.3419387936592102
setp: 1800, Loss: 0.31969454884529114
setp: 1900, Loss: 0.35344362258911133
setp: 2000, Loss: 0.3365151286125183
setp: 2100, Loss: 0.32108351588249207
setp: 2200, Loss: 0.31794700026512146
setp: 2300, Loss: 0.3251737952232361
setp: 2400, Loss: 0.3215997815132141
setp: 2500, Loss: 0.31778767704963684
setp: 2600, Loss: 0.3745492994785309
setp: 2700, Loss: 0.37190985679626465
setp: 2800, Loss: 0.32912248373031616
setp: 2900, Loss: 0.34501323103904724
setp: 3000, Loss: 0.31830862164497375
setp: 3100, Loss: 0.3168540596961975
setp: 3200, Loss: 0.3184274137020111
setp: 3300, Loss: 0.3169868588447571
setp: 3400, Loss: 0.31878429651260376
setp: 3500, Loss: 0.3172294497489929
setp: 3600, Loss: 0.3187512755393982
setp: 3700, Loss: 0.3188890814781189
setp: 3800, Loss: 0.32030799984931946
setp: 3900, Loss: 0.31932568550109863
setp: 4000, Loss: 0.31933990120887756
setp: 4100, Loss: 0.31933778524398804
setp: 4200, Loss: 0.3401462733745575
setp: 4300, Loss: 0.31852954626083374
setp: 4400, Loss: 0.3165963888168335
setp: 4500, Loss: 0.3478150963783264
setp: 4600, Loss: 0.31821492314338684
setp: 4700, Loss: 0.3192882239818573
setp: 4800, Loss: 0.31824082136154175
setp: 4900, Loss: 0.3181609511375427
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9962546816479401
F_score: 0.99812382739212
validating...
acc: 0.8881578947368421
precision: 0.9142857142857143
recall: 0.8533333333333334
F_score: 0.8827586206896552
******fold 1******
[261, 347]
training...
setp: 0, Loss: 0.7499206066131592
setp: 100, Loss: 0.6670848727226257
setp: 200, Loss: 0.6638871431350708
setp: 300, Loss: 0.6644600629806519
setp: 400, Loss: 0.6013669371604919
setp: 500, Loss: 0.4445229470729828
setp: 600, Loss: 0.4049687385559082
setp: 700, Loss: 0.4167698919773102
setp: 800, Loss: 0.33649247884750366
setp: 900, Loss: 0.32269856333732605
setp: 1000, Loss: 0.3225209414958954
setp: 1100, Loss: 0.3324817419052124
setp: 1200, Loss: 0.33653685450553894
setp: 1300, Loss: 0.32229048013687134
setp: 1400, Loss: 0.3215737044811249
setp: 1500, Loss: 0.32180050015449524
setp: 1600, Loss: 0.3209657371044159
setp: 1700, Loss: 0.3221632242202759
setp: 1800, Loss: 0.32170048356056213
setp: 1900, Loss: 0.31935766339302063
setp: 2000, Loss: 0.32125329971313477
setp: 2100, Loss: 0.32223576307296753
setp: 2200, Loss: 0.482455849647522
setp: 2300, Loss: 0.32442885637283325
setp: 2400, Loss: 0.31880924105644226
setp: 2500, Loss: 0.3176678717136383
setp: 2600, Loss: 0.3195885121822357
setp: 2700, Loss: 0.3189181089401245
setp: 2800, Loss: 0.31860461831092834
setp: 2900, Loss: 0.32036077976226807
setp: 3000, Loss: 0.32080942392349243
setp: 3100, Loss: 0.31873181462287903
setp: 3200, Loss: 0.4049910008907318
setp: 3300, Loss: 0.33096206188201904
setp: 3400, Loss: 0.3203471899032593
setp: 3500, Loss: 0.3190208375453949
setp: 3600, Loss: 0.32395392656326294
setp: 3700, Loss: 0.31805136799812317
setp: 3800, Loss: 0.3194275498390198
setp: 3900, Loss: 0.3192998468875885
setp: 4000, Loss: 0.319745808839798
setp: 4100, Loss: 0.32071059942245483
setp: 4200, Loss: 0.318727970123291
setp: 4300, Loss: 0.3197784721851349
setp: 4400, Loss: 0.3194214403629303
setp: 4500, Loss: 0.4100281298160553
setp: 4600, Loss: 0.3309321999549866
setp: 4700, Loss: 0.32194599509239197
setp: 4800, Loss: 0.3184863030910492
setp: 4900, Loss: 0.3185518980026245
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.95
recall: 0.9382716049382716
F_score: 0.9440993788819876
******fold 2******
[273, 335]
training...
setp: 0, Loss: 0.7105897068977356
setp: 100, Loss: 0.6855292916297913
setp: 200, Loss: 0.6852397322654724
setp: 300, Loss: 0.6865582466125488
setp: 400, Loss: 0.680267333984375
setp: 500, Loss: 0.6927217245101929
setp: 600, Loss: 0.6982386112213135
setp: 700, Loss: 0.6771319508552551
setp: 800, Loss: 0.6918175220489502
setp: 900, Loss: 0.6999456286430359
setp: 1000, Loss: 0.6917207837104797
setp: 1100, Loss: 0.6767663955688477
setp: 1200, Loss: 0.6791326403617859
setp: 1300, Loss: 0.6925089359283447
setp: 1400, Loss: 0.6915478706359863
setp: 1500, Loss: 0.6811447143554688
setp: 1600, Loss: 0.6992045044898987
setp: 1700, Loss: 0.6923336982727051
setp: 1800, Loss: 0.6862180233001709
setp: 1900, Loss: 0.6970312595367432
setp: 2000, Loss: 0.6853834986686707
setp: 2100, Loss: 0.6855103969573975
setp: 2200, Loss: 0.6864510774612427
setp: 2300, Loss: 0.6800926327705383
setp: 2400, Loss: 0.6926913857460022
setp: 2500, Loss: 0.6983072757720947
setp: 2600, Loss: 0.6773760318756104
setp: 2700, Loss: 0.6918538808822632
setp: 2800, Loss: 0.7002128958702087
setp: 2900, Loss: 0.6917126774787903
setp: 3000, Loss: 0.6772421002388
setp: 3100, Loss: 0.679007887840271
setp: 3200, Loss: 0.6926226615905762
setp: 3300, Loss: 0.691511332988739
setp: 3400, Loss: 0.6813626885414124
setp: 3500, Loss: 0.6994181871414185
setp: 3600, Loss: 0.6923968195915222
setp: 3700, Loss: 0.6863004565238953
setp: 3800, Loss: 0.6969668865203857
setp: 3900, Loss: 0.6853647232055664
setp: 4000, Loss: 0.6855024695396423
setp: 4100, Loss: 0.6865373849868774
setp: 4200, Loss: 0.6800439357757568
setp: 4300, Loss: 0.6927630305290222
setp: 4400, Loss: 0.6982961297035217
setp: 4500, Loss: 0.6776012778282166
setp: 4600, Loss: 0.6918716430664062
setp: 4700, Loss: 0.7003194093704224
setp: 4800, Loss: 0.6916953325271606
setp: 4900, Loss: 0.6773989200592041
training successfully ended.
validating...
acc: 0.5509868421052632
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.5460526315789473
precision: 0
recall: 0.0
F_score: 0
******fold 3******
[274, 334]
training...
setp: 0, Loss: 0.7334105372428894
setp: 100, Loss: 0.6820823550224304
setp: 200, Loss: 0.6965509057044983
setp: 300, Loss: 0.6919440031051636
setp: 400, Loss: 0.6985557675361633
setp: 500, Loss: 0.6823058128356934
setp: 600, Loss: 0.6970599889755249
setp: 700, Loss: 0.6922336220741272
setp: 800, Loss: 0.7041388750076294
setp: 900, Loss: 0.6778857111930847
setp: 1000, Loss: 0.6800923347473145
setp: 1100, Loss: 0.7061774134635925
setp: 1200, Loss: 0.6915947794914246
setp: 1300, Loss: 0.6729919910430908
setp: 1400, Loss: 0.6917833685874939
setp: 1500, Loss: 0.6854604482650757
setp: 1600, Loss: 0.6914163827896118
setp: 1700, Loss: 0.6815261840820312
setp: 1800, Loss: 0.6793332695960999
setp: 1900, Loss: 0.6921675801277161
setp: 2000, Loss: 0.6817049384117126
setp: 2100, Loss: 0.6967865228652954
setp: 2200, Loss: 0.6921282410621643
setp: 2300, Loss: 0.6985757946968079
setp: 2400, Loss: 0.6822054386138916
setp: 2500, Loss: 0.6972312331199646
setp: 2600, Loss: 0.6923019289970398
setp: 2700, Loss: 0.7042796015739441
setp: 2800, Loss: 0.6780357360839844
setp: 2900, Loss: 0.6800011992454529
setp: 3000, Loss: 0.7064274549484253
setp: 3100, Loss: 0.691596269607544
setp: 3200, Loss: 0.6732879877090454
setp: 3300, Loss: 0.6918169260025024
setp: 3400, Loss: 0.6854411363601685
setp: 3500, Loss: 0.691403865814209
setp: 3600, Loss: 0.681601881980896
setp: 3700, Loss: 0.6792535185813904
setp: 3800, Loss: 0.6922014355659485
setp: 3900, Loss: 0.6817984580993652
setp: 4000, Loss: 0.6967849731445312
setp: 4100, Loss: 0.6921656727790833
setp: 4200, Loss: 0.6986144185066223
setp: 4300, Loss: 0.6822870969772339
setp: 4400, Loss: 0.6972612142562866
setp: 4500, Loss: 0.6923329830169678
setp: 4600, Loss: 0.7042920589447021
setp: 4700, Loss: 0.6781150698661804
setp: 4800, Loss: 0.6799593567848206
setp: 4900, Loss: 0.706496000289917
training successfully ended.
validating...
acc: 0.5493421052631579
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.5526315789473685
precision: 0
recall: 0.0
F_score: 0
******fold 4******
[293, 315]
training...
setp: 0, Loss: 0.7016318440437317
setp: 100, Loss: 0.6933888792991638
setp: 200, Loss: 0.6916468143463135
setp: 300, Loss: 0.6914107203483582
setp: 400, Loss: 0.691679060459137
setp: 500, Loss: 0.6887739896774292
setp: 600, Loss: 0.6895397305488586
setp: 700, Loss: 0.6939443945884705
setp: 800, Loss: 0.693601131439209
setp: 900, Loss: 0.6902902722358704
setp: 1000, Loss: 0.6938119530677795
setp: 1100, Loss: 0.6964035034179688
setp: 1200, Loss: 0.695319414138794
setp: 1300, Loss: 0.6918309926986694
setp: 1400, Loss: 0.6939114332199097
setp: 1500, Loss: 0.698496401309967
setp: 1600, Loss: 0.6903051137924194
setp: 1700, Loss: 0.693560004234314
setp: 1800, Loss: 0.6965522170066833
setp: 1900, Loss: 0.6876435875892639
setp: 2000, Loss: 0.6934304237365723
setp: 2100, Loss: 0.6917097568511963
setp: 2200, Loss: 0.6914305686950684
setp: 2300, Loss: 0.6917280554771423
setp: 2400, Loss: 0.6892274022102356
setp: 2500, Loss: 0.6896045207977295
setp: 2600, Loss: 0.693980872631073
setp: 2700, Loss: 0.6935460567474365
setp: 2800, Loss: 0.6905407309532166
setp: 2900, Loss: 0.6938093304634094
setp: 3000, Loss: 0.6964462995529175
setp: 3100, Loss: 0.6951503157615662
setp: 3200, Loss: 0.6918932199478149
setp: 3300, Loss: 0.693922758102417
setp: 3400, Loss: 0.6984618306159973
setp: 3500, Loss: 0.6904703378677368
setp: 3600, Loss: 0.6935312747955322
setp: 3700, Loss: 0.6965909004211426
setp: 3800, Loss: 0.687762439250946
setp: 3900, Loss: 0.6934021711349487
setp: 4000, Loss: 0.6917281150817871
setp: 4100, Loss: 0.6914271116256714
setp: 4200, Loss: 0.6917527318000793
setp: 4300, Loss: 0.6893628239631653
setp: 4400, Loss: 0.68963223695755
setp: 4500, Loss: 0.6939838528633118
setp: 4600, Loss: 0.6935299038887024
setp: 4700, Loss: 0.690586507320404
setp: 4800, Loss: 0.6938042640686035
setp: 4900, Loss: 0.6964419484138489
training successfully ended.
validating...
acc: 0.5180921052631579
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.6776315789473685
precision: 0
recall: 0.0
F_score: 0
model saved.
avg_acc: 0.7210526315789474, avg_f_score: 0.36537159991432855
==========arousal==========
******fold 0******
[224, 384]
training...
setp: 0, Loss: 0.6964103579521179
setp: 100, Loss: 0.6763082146644592
setp: 200, Loss: 0.6122070550918579
setp: 300, Loss: 0.5708568692207336
setp: 400, Loss: 0.4879577159881592
setp: 500, Loss: 0.38360315561294556
setp: 600, Loss: 0.3764318823814392
setp: 700, Loss: 0.3280620872974396
setp: 800, Loss: 0.3264405429363251
setp: 900, Loss: 0.3885544240474701
setp: 1000, Loss: 0.32981085777282715
setp: 1100, Loss: 0.32254189252853394
setp: 1200, Loss: 0.32269755005836487
setp: 1300, Loss: 0.32551196217536926
setp: 1400, Loss: 0.32814157009124756
setp: 1500, Loss: 0.3210945129394531
setp: 1600, Loss: 0.32423633337020874
setp: 1700, Loss: 0.3202497661113739
setp: 1800, Loss: 0.35181310772895813
setp: 1900, Loss: 0.3191656172275543
setp: 2000, Loss: 0.32075363397598267
setp: 2100, Loss: 0.3211575746536255
setp: 2200, Loss: 0.31952354311943054
setp: 2300, Loss: 0.32039904594421387
setp: 2400, Loss: 0.3191457986831665
setp: 2500, Loss: 0.31983208656311035
setp: 2600, Loss: 0.31932586431503296
setp: 2700, Loss: 0.3202594518661499
setp: 2800, Loss: 0.3205902576446533
setp: 2900, Loss: 0.624581515789032
setp: 3000, Loss: 0.5023707151412964
setp: 3100, Loss: 0.38284239172935486
setp: 3200, Loss: 0.3458684980869293
setp: 3300, Loss: 0.3311220407485962
setp: 3400, Loss: 0.33249804377555847
setp: 3500, Loss: 0.3308931887149811
setp: 3600, Loss: 0.3365629315376282
setp: 3700, Loss: 0.37705734372138977
setp: 3800, Loss: 0.3257407248020172
setp: 3900, Loss: 0.32531818747520447
setp: 4000, Loss: 0.3243599534034729
setp: 4100, Loss: 0.3227008283138275
setp: 4200, Loss: 0.3232567012310028
setp: 4300, Loss: 0.32234275341033936
setp: 4400, Loss: 0.3225352168083191
setp: 4500, Loss: 0.32223114371299744
setp: 4600, Loss: 0.32394200563430786
setp: 4700, Loss: 0.3240157961845398
setp: 4800, Loss: 0.5830239653587341
setp: 4900, Loss: 0.35209178924560547
training successfully ended.
validating...
acc: 0.993421052631579
precision: 0.9910714285714286
recall: 0.9910714285714286
F_score: 0.9910714285714286
validating...
acc: 0.8881578947368421
precision: 0.8548387096774194
recall: 0.8688524590163934
F_score: 0.8617886178861789
******fold 1******
[232, 376]
training...
setp: 0, Loss: 0.7210360169410706
setp: 100, Loss: 0.6900120973587036
setp: 200, Loss: 0.6349701285362244
setp: 300, Loss: 0.6039004921913147
setp: 400, Loss: 0.599606454372406
setp: 500, Loss: 0.5152997970581055
setp: 600, Loss: 0.5104055404663086
setp: 700, Loss: 0.4113915264606476
setp: 800, Loss: 0.3525778651237488
setp: 900, Loss: 0.38406264781951904
setp: 1000, Loss: 0.33602359890937805
setp: 1100, Loss: 0.3229396641254425
setp: 1200, Loss: 0.3436419665813446
setp: 1300, Loss: 0.3851388394832611
setp: 1400, Loss: 0.3229353725910187
setp: 1500, Loss: 0.32022804021835327
setp: 1600, Loss: 0.32067811489105225
setp: 1700, Loss: 0.3208794593811035
setp: 1800, Loss: 0.35007214546203613
setp: 1900, Loss: 0.31936711072921753
setp: 2000, Loss: 0.31969812512397766
setp: 2100, Loss: 0.3180724084377289
setp: 2200, Loss: 0.3978527784347534
setp: 2300, Loss: 0.3379625082015991
setp: 2400, Loss: 0.37144234776496887
setp: 2500, Loss: 0.32202693819999695
setp: 2600, Loss: 0.3169647753238678
setp: 2700, Loss: 0.31698793172836304
setp: 2800, Loss: 0.31848153471946716
setp: 2900, Loss: 0.3198310136795044
setp: 3000, Loss: 0.3198893964290619
setp: 3100, Loss: 0.3453972339630127
setp: 3200, Loss: 0.3195633292198181
setp: 3300, Loss: 0.3178827166557312
setp: 3400, Loss: 0.31731241941452026
setp: 3500, Loss: 0.32077887654304504
setp: 3600, Loss: 0.36038267612457275
setp: 3700, Loss: 0.3507816195487976
setp: 3800, Loss: 0.3188774883747101
setp: 3900, Loss: 0.3189050555229187
setp: 4000, Loss: 0.31824609637260437
setp: 4100, Loss: 0.31866973638534546
setp: 4200, Loss: 0.3296588659286499
setp: 4300, Loss: 0.3507637083530426
setp: 4400, Loss: 0.3264635503292084
setp: 4500, Loss: 0.3177051246166229
setp: 4600, Loss: 0.33215197920799255
setp: 4700, Loss: 0.3234567940235138
setp: 4800, Loss: 0.32171592116355896
setp: 4900, Loss: 0.31923654675483704
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9956896551724138
recall: 0.9956896551724138
F_score: 0.9956896551724138
validating...
acc: 0.9144736842105263
precision: 0.8703703703703703
recall: 0.8867924528301887
F_score: 0.8785046728971964
******fold 2******
[222, 386]
training...
setp: 0, Loss: 0.6536511778831482
setp: 100, Loss: 0.6258749961853027
setp: 200, Loss: 0.6174877285957336
setp: 300, Loss: 0.6209701299667358
setp: 400, Loss: 0.5781678557395935
setp: 500, Loss: 0.5581222772598267
setp: 600, Loss: 0.4899284243583679
setp: 700, Loss: 0.47118598222732544
setp: 800, Loss: 0.4891248941421509
setp: 900, Loss: 0.47348710894584656
setp: 1000, Loss: 0.40439096093177795
setp: 1100, Loss: 0.33646515011787415
setp: 1200, Loss: 0.4639699161052704
setp: 1300, Loss: 0.41564875841140747
setp: 1400, Loss: 0.3245352804660797
setp: 1500, Loss: 0.3266047239303589
setp: 1600, Loss: 0.45132362842559814
setp: 1700, Loss: 0.37708207964897156
setp: 1800, Loss: 0.3335284888744354
setp: 1900, Loss: 0.4578065574169159
setp: 2000, Loss: 0.3509540259838104
setp: 2100, Loss: 0.38454365730285645
setp: 2200, Loss: 0.32613101601600647
setp: 2300, Loss: 0.4193192422389984
setp: 2400, Loss: 0.4864865243434906
setp: 2500, Loss: 0.3481088876724243
setp: 2600, Loss: 0.38619452714920044
setp: 2700, Loss: 0.37181171774864197
setp: 2800, Loss: 0.36059868335723877
setp: 2900, Loss: 0.399553120136261
setp: 3000, Loss: 0.32262495160102844
setp: 3100, Loss: 0.3221290409564972
setp: 3200, Loss: 0.32164961099624634
setp: 3300, Loss: 0.3161824941635132
setp: 3400, Loss: 0.3176535367965698
setp: 3500, Loss: 0.3224763870239258
setp: 3600, Loss: 0.3227023780345917
setp: 3700, Loss: 0.3327197730541229
setp: 3800, Loss: 0.3299804925918579
setp: 3900, Loss: 0.3157518208026886
setp: 4000, Loss: 0.31845661997795105
setp: 4100, Loss: 0.31757476925849915
setp: 4200, Loss: 0.31782713532447815
setp: 4300, Loss: 0.34970805048942566
setp: 4400, Loss: 0.31871700286865234
setp: 4500, Loss: 0.3194265365600586
setp: 4600, Loss: 0.3184271454811096
setp: 4700, Loss: 0.3249432444572449
setp: 4800, Loss: 0.35876116156578064
setp: 4900, Loss: 0.3224125802516937
training successfully ended.
validating...
acc: 0.9243421052631579
precision: 1.0
recall: 0.7927927927927928
F_score: 0.8844221105527639
validating...
acc: 0.756578947368421
precision: 0.9642857142857143
recall: 0.42857142857142855
F_score: 0.5934065934065934
******fold 3******
[233, 375]
training...
setp: 0, Loss: 0.6942709684371948
setp: 100, Loss: 0.6897150278091431
setp: 200, Loss: 0.6386139392852783
setp: 300, Loss: 0.6183157563209534
setp: 400, Loss: 0.5843867063522339
setp: 500, Loss: 0.4913790225982666
setp: 600, Loss: 0.44841963052749634
setp: 700, Loss: 0.39137256145477295
setp: 800, Loss: 0.3431471288204193
setp: 900, Loss: 0.34732499718666077
setp: 1000, Loss: 0.35749495029449463
setp: 1100, Loss: 0.3274378478527069
setp: 1200, Loss: 0.3230186104774475
setp: 1300, Loss: 0.3243854343891144
setp: 1400, Loss: 0.34769415855407715
setp: 1500, Loss: 0.3322301208972931
setp: 1600, Loss: 0.31942927837371826
setp: 1700, Loss: 0.32034406065940857
setp: 1800, Loss: 0.31909629702568054
setp: 1900, Loss: 0.31943178176879883
setp: 2000, Loss: 0.31823399662971497
setp: 2100, Loss: 0.32085180282592773
setp: 2200, Loss: 0.34844887256622314
setp: 2300, Loss: 0.31932467222213745
setp: 2400, Loss: 0.3200114369392395
setp: 2500, Loss: 0.31764519214630127
setp: 2600, Loss: 0.31894513964653015
setp: 2700, Loss: 0.31730732321739197
setp: 2800, Loss: 0.31807616353034973
setp: 2900, Loss: 0.31984269618988037
setp: 3000, Loss: 0.3195280432701111
setp: 3100, Loss: 0.3827234208583832
setp: 3200, Loss: 0.3392440974712372
setp: 3300, Loss: 0.3175654113292694
setp: 3400, Loss: 0.31720268726348877
setp: 3500, Loss: 0.31850042939186096
setp: 3600, Loss: 0.31853100657463074
setp: 3700, Loss: 0.31814253330230713
setp: 3800, Loss: 0.3203395903110504
setp: 3900, Loss: 0.3642084002494812
setp: 4000, Loss: 0.33120307326316833
setp: 4100, Loss: 0.3188609778881073
setp: 4200, Loss: 0.31878989934921265
setp: 4300, Loss: 0.31688398122787476
setp: 4400, Loss: 0.317797988653183
setp: 4500, Loss: 0.3187841773033142
setp: 4600, Loss: 0.31709563732147217
setp: 4700, Loss: 0.3176119029521942
setp: 4800, Loss: 0.31872010231018066
setp: 4900, Loss: 0.32100731134414673
training successfully ended.
validating...
acc: 0.774671052631579
precision: 1.0
recall: 0.41201716738197425
F_score: 0.5835866261398177
validating...
acc: 0.7302631578947368
precision: 1.0
recall: 0.21153846153846154
F_score: 0.34920634920634924
******fold 4******
[229, 379]
training...
setp: 0, Loss: 0.6709047555923462
setp: 100, Loss: 0.6426124572753906
setp: 200, Loss: 0.662030816078186
setp: 300, Loss: 0.6146753430366516
setp: 400, Loss: 0.6280627846717834
setp: 500, Loss: 0.6198906898498535
setp: 600, Loss: 0.5763533711433411
setp: 700, Loss: 0.5552563667297363
setp: 800, Loss: 0.3891536295413971
setp: 900, Loss: 0.40644484758377075
setp: 1000, Loss: 0.5614821314811707
setp: 1100, Loss: 0.43111640214920044
setp: 1200, Loss: 0.41640183329582214
setp: 1300, Loss: 0.35123229026794434
setp: 1400, Loss: 0.3282029926776886
setp: 1500, Loss: 0.32554295659065247
setp: 1600, Loss: 0.3256183862686157
setp: 1700, Loss: 0.31989917159080505
setp: 1800, Loss: 0.31999459862709045
setp: 1900, Loss: 0.3180152475833893
setp: 2000, Loss: 0.3162113130092621
setp: 2100, Loss: 0.3603734076023102
setp: 2200, Loss: 0.33677777647972107
setp: 2300, Loss: 0.34728801250457764
setp: 2400, Loss: 0.3198525905609131
setp: 2500, Loss: 0.34746748208999634
setp: 2600, Loss: 0.3180522620677948
setp: 2700, Loss: 0.3161357045173645
setp: 2800, Loss: 0.33886635303497314
setp: 2900, Loss: 0.33894041180610657
setp: 3000, Loss: 0.3190627694129944
setp: 3100, Loss: 0.38215938210487366
setp: 3200, Loss: 0.31787291169166565
setp: 3300, Loss: 0.31770604848861694
setp: 3400, Loss: 0.3169945180416107
setp: 3500, Loss: 0.3203378915786743
setp: 3600, Loss: 0.31770220398902893
setp: 3700, Loss: 0.4421030879020691
setp: 3800, Loss: 0.4016658663749695
setp: 3900, Loss: 0.31684499979019165
setp: 4000, Loss: 0.31787869334220886
setp: 4100, Loss: 0.3157535493373871
setp: 4200, Loss: 0.318615198135376
setp: 4300, Loss: 0.31837913393974304
setp: 4400, Loss: 0.31708282232284546
setp: 4500, Loss: 0.31799277663230896
setp: 4600, Loss: 0.31665971875190735
setp: 4700, Loss: 0.3197847902774811
setp: 4800, Loss: 0.31862306594848633
setp: 4900, Loss: 0.6735132336616516
training successfully ended.
validating...
acc: 0.6726973684210527
precision: 0.734375
recall: 0.2052401746724891
F_score: 0.32081911262798635
validating...
acc: 0.6776315789473685
precision: 0.7692307692307693
recall: 0.17857142857142858
F_score: 0.2898550724637681
model saved.
avg_acc: 0.7934210526315789, avg_f_score: 0.5945522611720172
-------------subject: 18-------------
==========valence==========
******fold 0******
[205, 403]
training...
setp: 0, Loss: 0.6891973614692688
setp: 100, Loss: 0.6646139025688171
setp: 200, Loss: 0.6379971504211426
setp: 300, Loss: 0.5179039239883423
setp: 400, Loss: 0.38415974378585815
setp: 500, Loss: 0.46479523181915283
setp: 600, Loss: 0.4223891794681549
setp: 700, Loss: 0.40673068165779114
setp: 800, Loss: 0.3901740610599518
setp: 900, Loss: 0.40343889594078064
setp: 1000, Loss: 0.3510407507419586
setp: 1100, Loss: 0.35404080152511597
setp: 1200, Loss: 0.35464608669281006
setp: 1300, Loss: 0.38371849060058594
setp: 1400, Loss: 0.34948331117630005
setp: 1500, Loss: 0.3794024586677551
setp: 1600, Loss: 0.3196622431278229
setp: 1700, Loss: 0.3799571692943573
setp: 1800, Loss: 0.3195870816707611
setp: 1900, Loss: 0.4171120524406433
setp: 2000, Loss: 0.3551521599292755
setp: 2100, Loss: 0.36675503849983215
setp: 2200, Loss: 0.3714350759983063
setp: 2300, Loss: 0.3254803717136383
setp: 2400, Loss: 0.3520098924636841
setp: 2500, Loss: 0.37896114587783813
setp: 2600, Loss: 0.32816416025161743
setp: 2700, Loss: 0.37283390760421753
setp: 2800, Loss: 0.35290780663490295
setp: 2900, Loss: 0.34673961997032166
setp: 3000, Loss: 0.3166896402835846
setp: 3100, Loss: 0.3488721251487732
setp: 3200, Loss: 0.3487106263637543
setp: 3300, Loss: 0.34782084822654724
setp: 3400, Loss: 0.34788399934768677
setp: 3500, Loss: 0.3163335919380188
setp: 3600, Loss: 0.3610783815383911
setp: 3700, Loss: 0.32252800464630127
setp: 3800, Loss: 0.37430649995803833
setp: 3900, Loss: 0.31685519218444824
setp: 4000, Loss: 0.3496043384075165
setp: 4100, Loss: 0.3487010598182678
setp: 4200, Loss: 0.31672903895378113
setp: 4300, Loss: 0.3492523729801178
setp: 4400, Loss: 0.38634657859802246
setp: 4500, Loss: 0.31687062978744507
setp: 4600, Loss: 0.3487159013748169
setp: 4700, Loss: 0.34773439168930054
setp: 4800, Loss: 0.34758177399635315
setp: 4900, Loss: 0.3175851106643677
training successfully ended.
validating...
acc: 0.9769736842105263
precision: 1.0
recall: 0.9317073170731708
F_score: 0.9646464646464648
validating...
acc: 0.9473684210526315
precision: 0.9818181818181818
recall: 0.8852459016393442
F_score: 0.9310344827586207
******fold 1******
[219, 389]
training...
setp: 0, Loss: 0.6571958065032959
setp: 100, Loss: 0.6422199010848999
setp: 200, Loss: 0.5535279512405396
setp: 300, Loss: 0.4728172719478607
setp: 400, Loss: 0.40950846672058105
setp: 500, Loss: 0.38673102855682373
setp: 600, Loss: 0.43504735827445984
setp: 700, Loss: 0.466473788022995
setp: 800, Loss: 0.3958896994590759
setp: 900, Loss: 0.3589905798435211
setp: 1000, Loss: 0.32394543290138245
setp: 1100, Loss: 0.3215932548046112
setp: 1200, Loss: 0.3224676549434662
setp: 1300, Loss: 0.3245052099227905
setp: 1400, Loss: 0.3184751272201538
setp: 1500, Loss: 0.31640464067459106
setp: 1600, Loss: 0.31656745076179504
setp: 1700, Loss: 0.3487260639667511
setp: 1800, Loss: 0.31828635931015015
setp: 1900, Loss: 0.3194165825843811
setp: 2000, Loss: 0.32173025608062744
setp: 2100, Loss: 0.33233603835105896
setp: 2200, Loss: 0.31901445984840393
setp: 2300, Loss: 0.34752359986305237
setp: 2400, Loss: 0.3178773820400238
setp: 2500, Loss: 0.3182307779788971
setp: 2600, Loss: 0.3168906569480896
setp: 2700, Loss: 0.3742222785949707
setp: 2800, Loss: 0.3195243775844574
setp: 2900, Loss: 0.3168928325176239
setp: 3000, Loss: 0.31646472215652466
setp: 3100, Loss: 0.3183557391166687
setp: 3200, Loss: 0.31697362661361694
setp: 3300, Loss: 0.31736353039741516
setp: 3400, Loss: 0.31622931361198425
setp: 3500, Loss: 0.316468745470047
setp: 3600, Loss: 0.6978845000267029
setp: 3700, Loss: 0.47005370259284973
setp: 3800, Loss: 0.3933705985546112
setp: 3900, Loss: 0.33225589990615845
setp: 4000, Loss: 0.32902565598487854
setp: 4100, Loss: 0.32144829630851746
setp: 4200, Loss: 0.33997219800949097
setp: 4300, Loss: 0.3221871256828308
setp: 4400, Loss: 0.32086846232414246
setp: 4500, Loss: 0.32032325863838196
setp: 4600, Loss: 0.32096704840660095
setp: 4700, Loss: 0.32026442885398865
setp: 4800, Loss: 0.3195532262325287
setp: 4900, Loss: 0.3197587728500366
training successfully ended.
validating...
acc: 0.6398026315789473
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.6907894736842105
precision: 0
recall: 0.0
F_score: 0
******fold 2******
[209, 399]
training...
setp: 0, Loss: 0.6520476937294006
setp: 100, Loss: 0.6187816262245178
setp: 200, Loss: 0.6714767217636108
setp: 300, Loss: 0.5452340245246887
setp: 400, Loss: 0.45512452721595764
setp: 500, Loss: 0.5928900241851807
setp: 600, Loss: 0.418088436126709
setp: 700, Loss: 0.4730766713619232
setp: 800, Loss: 0.4733176827430725
setp: 900, Loss: 0.4428325891494751
setp: 1000, Loss: 0.4416208565235138
setp: 1100, Loss: 0.4792279005050659
setp: 1200, Loss: 0.45211291313171387
setp: 1300, Loss: 0.47460782527923584
setp: 1400, Loss: 0.37865859270095825
setp: 1500, Loss: 0.4708287715911865
setp: 1600, Loss: 0.3853794038295746
setp: 1700, Loss: 0.4288172125816345
setp: 1800, Loss: 0.4718947112560272
setp: 1900, Loss: 0.4730644226074219
setp: 2000, Loss: 0.43178239464759827
setp: 2100, Loss: 0.47247862815856934
setp: 2200, Loss: 0.4622819125652313
setp: 2300, Loss: 0.39788731932640076
setp: 2400, Loss: 0.5008774399757385
setp: 2500, Loss: 0.41056177020072937
setp: 2600, Loss: 0.41218817234039307
setp: 2700, Loss: 0.455016165971756
setp: 2800, Loss: 0.45716363191604614
setp: 2900, Loss: 0.44629383087158203
setp: 3000, Loss: 0.4724977910518646
setp: 3100, Loss: 0.4474373161792755
setp: 3200, Loss: 0.4727811813354492
setp: 3300, Loss: 0.4060472249984741
setp: 3400, Loss: 0.47384384274482727
setp: 3500, Loss: 0.41399383544921875
setp: 3600, Loss: 0.4098375737667084
setp: 3700, Loss: 0.44115015864372253
setp: 3800, Loss: 0.4712550640106201
setp: 3900, Loss: 0.4176742434501648
setp: 4000, Loss: 0.4412098228931427
setp: 4100, Loss: 0.40964382886886597
setp: 4200, Loss: 0.3785351812839508
setp: 4300, Loss: 0.4128818213939667
setp: 4400, Loss: 0.4107010066509247
setp: 4500, Loss: 0.41146034002304077
setp: 4600, Loss: 0.41304558515548706
setp: 4700, Loss: 0.4117276966571808
setp: 4800, Loss: 0.4404458999633789
setp: 4900, Loss: 0.4931776821613312
training successfully ended.
validating...
acc: 0.8898026315789473
precision: 0.9863013698630136
recall: 0.6889952153110048
F_score: 0.8112676056338027
validating...
acc: 0.7894736842105263
precision: 0.9310344827586207
recall: 0.47368421052631576
F_score: 0.627906976744186
******fold 3******
[203, 405]
training...
setp: 0, Loss: 0.6465029716491699
setp: 100, Loss: 0.6245813965797424
setp: 200, Loss: 0.5941745638847351
setp: 300, Loss: 0.5408035516738892
setp: 400, Loss: 0.5023167729377747
setp: 500, Loss: 0.48653724789619446
setp: 600, Loss: 0.3642352819442749
setp: 700, Loss: 0.3613690137863159
setp: 800, Loss: 0.3575056493282318
setp: 900, Loss: 0.3532027006149292
setp: 1000, Loss: 0.37592893838882446
setp: 1100, Loss: 0.31982412934303284
setp: 1200, Loss: 0.37717512249946594
setp: 1300, Loss: 0.36381492018699646
setp: 1400, Loss: 0.3473277986049652
setp: 1500, Loss: 0.3336816728115082
setp: 1600, Loss: 0.32008445262908936
setp: 1700, Loss: 0.3481297492980957
setp: 1800, Loss: 0.31687238812446594
setp: 1900, Loss: 0.3322571814060211
setp: 2000, Loss: 0.323442280292511
setp: 2100, Loss: 0.3201712667942047
setp: 2200, Loss: 0.3163238763809204
setp: 2300, Loss: 0.36500534415245056
setp: 2400, Loss: 0.3173844516277313
setp: 2500, Loss: 0.34912121295928955
setp: 2600, Loss: 0.3284428119659424
setp: 2700, Loss: 0.3483928143978119
setp: 2800, Loss: 0.31740903854370117
setp: 2900, Loss: 0.3166484236717224
setp: 3000, Loss: 0.31690025329589844
setp: 3100, Loss: 0.3186599910259247
setp: 3200, Loss: 0.31734171509742737
setp: 3300, Loss: 0.3168330192565918
setp: 3400, Loss: 0.316580593585968
setp: 3500, Loss: 0.31702086329460144
setp: 3600, Loss: 0.3643728494644165
setp: 3700, Loss: 0.3191663324832916
setp: 3800, Loss: 0.3188830614089966
setp: 3900, Loss: 0.3173563480377197
setp: 4000, Loss: 0.3163694441318512
setp: 4100, Loss: 0.3169380724430084
setp: 4200, Loss: 0.3488095700740814
setp: 4300, Loss: 0.3179227411746979
setp: 4400, Loss: 0.31825917959213257
setp: 4500, Loss: 0.3171742558479309
setp: 4600, Loss: 0.38580501079559326
setp: 4700, Loss: 0.31782710552215576
setp: 4800, Loss: 0.31665101647377014
setp: 4900, Loss: 0.3176034390926361
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 1.0
recall: 0.9852216748768473
F_score: 0.9925558312655086
validating...
acc: 0.9342105263157895
precision: 0.9649122807017544
recall: 0.873015873015873
F_score: 0.9166666666666667
******fold 4******
[228, 380]
training...
setp: 0, Loss: 0.6777500510215759
setp: 100, Loss: 0.6682381629943848
setp: 200, Loss: 0.5950524210929871
setp: 300, Loss: 0.45168423652648926
setp: 400, Loss: 0.5127463340759277
setp: 500, Loss: 0.46765995025634766
setp: 600, Loss: 0.49163618683815
setp: 700, Loss: 0.514374315738678
setp: 800, Loss: 0.5684913992881775
setp: 900, Loss: 0.46929019689559937
setp: 1000, Loss: 0.5008026361465454
setp: 1100, Loss: 0.4598161280155182
setp: 1200, Loss: 0.4685453474521637
setp: 1300, Loss: 0.4875655472278595
setp: 1400, Loss: 0.412148654460907
setp: 1500, Loss: 0.4872985780239105
setp: 1600, Loss: 0.45757487416267395
setp: 1700, Loss: 0.39661285281181335
setp: 1800, Loss: 0.48001551628112793
setp: 1900, Loss: 0.42454415559768677
setp: 2000, Loss: 0.5080139636993408
setp: 2100, Loss: 0.41579845547676086
setp: 2200, Loss: 0.3774370551109314
setp: 2300, Loss: 0.4422094523906708
setp: 2400, Loss: 0.35803207755088806
setp: 2500, Loss: 0.450176864862442
setp: 2600, Loss: 0.4786135256290436
setp: 2700, Loss: 0.47231921553611755
setp: 2800, Loss: 0.3767763376235962
setp: 2900, Loss: 0.4501199722290039
setp: 3000, Loss: 0.4513165354728699
setp: 3100, Loss: 0.4299832284450531
setp: 3200, Loss: 0.47874903678894043
setp: 3300, Loss: 0.40852439403533936
setp: 3400, Loss: 0.3792949318885803
setp: 3500, Loss: 0.4108189046382904
setp: 3600, Loss: 0.38056695461273193
setp: 3700, Loss: 0.47122642397880554
setp: 3800, Loss: 0.4115748405456543
setp: 3900, Loss: 0.4333963096141815
setp: 4000, Loss: 0.41378888487815857
setp: 4100, Loss: 0.3602485656738281
setp: 4200, Loss: 0.4401842951774597
setp: 4300, Loss: 0.35331952571868896
setp: 4400, Loss: 0.4255853593349457
setp: 4500, Loss: 0.4110284745693207
setp: 4600, Loss: 0.4396462142467499
setp: 4700, Loss: 0.37828364968299866
setp: 4800, Loss: 0.3174852430820465
setp: 4900, Loss: 0.31932979822158813
training successfully ended.
validating...
acc: 0.9884868421052632
precision: 0.9955156950672646
recall: 0.9736842105263158
F_score: 0.9844789356984479
validating...
acc: 0.9144736842105263
precision: 0.8205128205128205
recall: 0.8421052631578947
F_score: 0.8311688311688312
model saved.
avg_acc: 0.8552631578947368, avg_f_score: 0.661355391467661
==========arousal==========
******fold 0******
[217, 391]
training...
setp: 0, Loss: 0.7023698091506958
setp: 100, Loss: 0.6184390187263489
setp: 200, Loss: 0.6001326441764832
setp: 300, Loss: 0.5679845213890076
setp: 400, Loss: 0.5377477407455444
setp: 500, Loss: 0.5936844348907471
setp: 600, Loss: 0.52043616771698
setp: 700, Loss: 0.41301587224006653
setp: 800, Loss: 0.3701987564563751
setp: 900, Loss: 0.4308198392391205
setp: 1000, Loss: 0.40982598066329956
setp: 1100, Loss: 0.3548758625984192
setp: 1200, Loss: 0.32606151700019836
setp: 1300, Loss: 0.32501956820487976
setp: 1400, Loss: 0.35215190052986145
setp: 1500, Loss: 0.32162296772003174
setp: 1600, Loss: 0.3193986415863037
setp: 1700, Loss: 0.3246444761753082
setp: 1800, Loss: 0.35103145241737366
setp: 1900, Loss: 0.32944151759147644
setp: 2000, Loss: 0.3205338418483734
setp: 2100, Loss: 0.32031580805778503
setp: 2200, Loss: 0.3183733820915222
setp: 2300, Loss: 0.3178592324256897
setp: 2400, Loss: 0.319586843252182
setp: 2500, Loss: 0.3476239740848541
setp: 2600, Loss: 0.33384671807289124
setp: 2700, Loss: 0.31972581148147583
setp: 2800, Loss: 0.31757858395576477
setp: 2900, Loss: 0.31799376010894775
setp: 3000, Loss: 0.34636417031288147
setp: 3100, Loss: 0.3174585998058319
setp: 3200, Loss: 0.3193434476852417
setp: 3300, Loss: 0.34767526388168335
setp: 3400, Loss: 0.3170689046382904
setp: 3500, Loss: 0.3165716230869293
setp: 3600, Loss: 0.3190804123878479
setp: 3700, Loss: 0.31909993290901184
setp: 3800, Loss: 0.3269285261631012
setp: 3900, Loss: 0.3284367322921753
setp: 4000, Loss: 0.3781258761882782
setp: 4100, Loss: 0.3168155252933502
setp: 4200, Loss: 0.3167128562927246
setp: 4300, Loss: 0.3180551528930664
setp: 4400, Loss: 0.31631600856781006
setp: 4500, Loss: 0.3180326223373413
setp: 4600, Loss: 0.31740257143974304
setp: 4700, Loss: 0.3173278272151947
setp: 4800, Loss: 0.32854998111724854
setp: 4900, Loss: 0.3481822907924652
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9953917050691244
F_score: 0.997690531177829
validating...
acc: 0.9539473684210527
precision: 0.9841269841269841
recall: 0.9117647058823529
F_score: 0.9465648854961831
******fold 1******
[220, 388]
training...
setp: 0, Loss: 0.6689569354057312
setp: 100, Loss: 0.6826013922691345
setp: 200, Loss: 0.6259599924087524
setp: 300, Loss: 0.5974322557449341
setp: 400, Loss: 0.5510237216949463
setp: 500, Loss: 0.5176924467086792
setp: 600, Loss: 0.45842835307121277
setp: 700, Loss: 0.5340436697006226
setp: 800, Loss: 0.36468929052352905
setp: 900, Loss: 0.32516300678253174
setp: 1000, Loss: 0.34095320105552673
setp: 1100, Loss: 0.3480475842952728
setp: 1200, Loss: 0.32053375244140625
setp: 1300, Loss: 0.3226766586303711
setp: 1400, Loss: 0.3793337941169739
setp: 1500, Loss: 0.3178633749485016
setp: 1600, Loss: 0.34778714179992676
setp: 1700, Loss: 0.3187389373779297
setp: 1800, Loss: 0.32092902064323425
setp: 1900, Loss: 0.3184645473957062
setp: 2000, Loss: 0.3489197790622711
setp: 2100, Loss: 0.33715522289276123
setp: 2200, Loss: 0.3352586627006531
setp: 2300, Loss: 0.31675437092781067
setp: 2400, Loss: 0.34924665093421936
setp: 2500, Loss: 0.3170071542263031
setp: 2600, Loss: 0.3183431923389435
setp: 2700, Loss: 0.4350089430809021
setp: 2800, Loss: 0.3400156497955322
setp: 2900, Loss: 0.32389479875564575
setp: 3000, Loss: 0.3175903856754303
setp: 3100, Loss: 0.31692376732826233
setp: 3200, Loss: 0.31653454899787903
setp: 3300, Loss: 0.3182034492492676
setp: 3400, Loss: 0.3292725384235382
setp: 3500, Loss: 0.34177201986312866
setp: 3600, Loss: 0.3172241449356079
setp: 3700, Loss: 0.31843388080596924
setp: 3800, Loss: 0.3179752826690674
setp: 3900, Loss: 0.35162514448165894
setp: 4000, Loss: 0.315947949886322
setp: 4100, Loss: 0.3174433410167694
setp: 4200, Loss: 0.31664973497390747
setp: 4300, Loss: 0.36139369010925293
setp: 4400, Loss: 0.31910964846611023
setp: 4500, Loss: 0.3187270164489746
setp: 4600, Loss: 0.31619083881378174
setp: 4700, Loss: 0.3167921304702759
setp: 4800, Loss: 0.3186371624469757
setp: 4900, Loss: 0.31824666261672974
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.990909090909091
F_score: 0.995433789954338
validating...
acc: 0.9342105263157895
precision: 0.9508196721311475
recall: 0.8923076923076924
F_score: 0.9206349206349206
******fold 2******
[232, 376]
training...
setp: 0, Loss: 0.6614491939544678
setp: 100, Loss: 0.6470888257026672
setp: 200, Loss: 0.6899283528327942
setp: 300, Loss: 0.6549433469772339
setp: 400, Loss: 0.5409955978393555
setp: 500, Loss: 0.6247442364692688
setp: 600, Loss: 0.5019751787185669
setp: 700, Loss: 0.4494311511516571
setp: 800, Loss: 0.3459080755710602
setp: 900, Loss: 0.35770857334136963
setp: 1000, Loss: 0.3341328203678131
setp: 1100, Loss: 0.3284423053264618
setp: 1200, Loss: 0.32233843207359314
setp: 1300, Loss: 0.3217950165271759
setp: 1400, Loss: 0.3226272463798523
setp: 1500, Loss: 0.32069554924964905
setp: 1600, Loss: 0.33759960532188416
setp: 1700, Loss: 0.32692572474479675
setp: 1800, Loss: 0.3191646933555603
setp: 1900, Loss: 0.31925496459007263
setp: 2000, Loss: 0.3209007680416107
setp: 2100, Loss: 0.3200223743915558
setp: 2200, Loss: 0.3184661865234375
setp: 2300, Loss: 0.31858986616134644
setp: 2400, Loss: 0.3190975785255432
setp: 2500, Loss: 0.3215424418449402
setp: 2600, Loss: 0.3191438913345337
setp: 2700, Loss: 0.3181418180465698
setp: 2800, Loss: 0.3652566075325012
setp: 2900, Loss: 0.33364102244377136
setp: 3000, Loss: 0.32130736112594604
setp: 3100, Loss: 0.3202177882194519
setp: 3200, Loss: 0.31965240836143494
setp: 3300, Loss: 0.32002654671669006
setp: 3400, Loss: 0.3281223475933075
setp: 3500, Loss: 0.31792810559272766
setp: 3600, Loss: 0.31962814927101135
setp: 3700, Loss: 0.31902122497558594
setp: 3800, Loss: 0.3188604414463043
setp: 3900, Loss: 0.32145801186561584
setp: 4000, Loss: 0.31819042563438416
setp: 4100, Loss: 0.3186705708503723
setp: 4200, Loss: 0.31828930974006653
setp: 4300, Loss: 0.3177570104598999
setp: 4400, Loss: 0.4412551522254944
setp: 4500, Loss: 0.390586256980896
setp: 4600, Loss: 0.357829749584198
setp: 4700, Loss: 0.3299182951450348
setp: 4800, Loss: 0.3312634825706482
setp: 4900, Loss: 0.33150070905685425
training successfully ended.
validating...
acc: 0.9884868421052632
precision: 0.9707112970711297
recall: 1.0
F_score: 0.9851380042462845
validating...
acc: 0.9078947368421053
precision: 0.8
recall: 0.9811320754716981
F_score: 0.8813559322033898
******fold 3******
[229, 379]
training...
setp: 0, Loss: 0.7678776383399963
setp: 100, Loss: 0.6617476940155029
setp: 200, Loss: 0.5988875031471252
setp: 300, Loss: 0.614086389541626
setp: 400, Loss: 0.5882257223129272
setp: 500, Loss: 0.6032682061195374
setp: 600, Loss: 0.5365378856658936
setp: 700, Loss: 0.5280047059059143
setp: 800, Loss: 0.3605235815048218
setp: 900, Loss: 0.36895719170570374
setp: 1000, Loss: 0.34457528591156006
setp: 1100, Loss: 0.34160134196281433
setp: 1200, Loss: 0.36844056844711304
setp: 1300, Loss: 0.32203754782676697
setp: 1400, Loss: 0.32233279943466187
setp: 1500, Loss: 0.32087865471839905
setp: 1600, Loss: 0.37727582454681396
setp: 1700, Loss: 0.32463520765304565
setp: 1800, Loss: 0.3496650457382202
setp: 1900, Loss: 0.3216423988342285
setp: 2000, Loss: 0.3182602822780609
setp: 2100, Loss: 0.3174797594547272
setp: 2200, Loss: 0.3193591237068176
setp: 2300, Loss: 0.32116377353668213
setp: 2400, Loss: 0.3189847469329834
setp: 2500, Loss: 0.31770849227905273
setp: 2600, Loss: 0.31927162408828735
setp: 2700, Loss: 0.31681275367736816
setp: 2800, Loss: 0.5501936674118042
setp: 2900, Loss: 0.32024046778678894
setp: 3000, Loss: 0.33184924721717834
setp: 3100, Loss: 0.3172050416469574
setp: 3200, Loss: 0.3160583972930908
setp: 3300, Loss: 0.31881818175315857
setp: 3400, Loss: 0.31773561239242554
setp: 3500, Loss: 0.31814277172088623
setp: 3600, Loss: 0.32009759545326233
setp: 3700, Loss: 0.31836289167404175
setp: 3800, Loss: 0.33799779415130615
setp: 3900, Loss: 0.31709492206573486
setp: 4000, Loss: 0.3194073736667633
setp: 4100, Loss: 0.3174470365047455
setp: 4200, Loss: 0.3170279860496521
setp: 4300, Loss: 0.31781715154647827
setp: 4400, Loss: 0.32405152916908264
setp: 4500, Loss: 0.31944090127944946
setp: 4600, Loss: 0.3191657066345215
setp: 4700, Loss: 0.31832003593444824
setp: 4800, Loss: 0.31636282801628113
setp: 4900, Loss: 0.31766054034233093
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9276315789473685
precision: 0.8688524590163934
recall: 0.9464285714285714
F_score: 0.9059829059829059
******fold 4******
[242, 366]
training...
setp: 0, Loss: 0.6869149208068848
setp: 100, Loss: 0.6333279013633728
setp: 200, Loss: 0.5846835970878601
setp: 300, Loss: 0.45668524503707886
setp: 400, Loss: 0.3993014097213745
setp: 500, Loss: 0.4319498538970947
setp: 600, Loss: 0.32621461153030396
setp: 700, Loss: 0.32403695583343506
setp: 800, Loss: 0.33119675517082214
setp: 900, Loss: 0.3241063356399536
setp: 1000, Loss: 0.321154922246933
setp: 1100, Loss: 0.34440934658050537
setp: 1200, Loss: 0.318789541721344
setp: 1300, Loss: 0.3232754170894623
setp: 1400, Loss: 0.32026389241218567
setp: 1500, Loss: 0.34959712624549866
setp: 1600, Loss: 0.3196910321712494
setp: 1700, Loss: 0.319770485162735
setp: 1800, Loss: 0.3500577509403229
setp: 1900, Loss: 0.3192680776119232
setp: 2000, Loss: 0.3194968104362488
setp: 2100, Loss: 0.3197750151157379
setp: 2200, Loss: 0.3187960684299469
setp: 2300, Loss: 0.3199908137321472
setp: 2400, Loss: 0.32033178210258484
setp: 2500, Loss: 0.31745538115501404
setp: 2600, Loss: 0.31811216473579407
setp: 2700, Loss: 0.3189356029033661
setp: 2800, Loss: 0.318727970123291
setp: 2900, Loss: 0.3210378587245941
setp: 3000, Loss: 0.5458822250366211
setp: 3100, Loss: 0.4038260281085968
setp: 3200, Loss: 0.3397066593170166
setp: 3300, Loss: 0.38554975390434265
setp: 3400, Loss: 0.327070027589798
setp: 3500, Loss: 0.32674187421798706
setp: 3600, Loss: 0.3264983296394348
setp: 3700, Loss: 0.3308584988117218
setp: 3800, Loss: 0.32559216022491455
setp: 3900, Loss: 0.32331570982933044
setp: 4000, Loss: 0.3249650001525879
setp: 4100, Loss: 0.324711412191391
setp: 4200, Loss: 0.3248938322067261
setp: 4300, Loss: 0.32885870337486267
setp: 4400, Loss: 0.33084723353385925
setp: 4500, Loss: 0.3221765458583832
setp: 4600, Loss: 0.3225770592689514
setp: 4700, Loss: 0.3223063051700592
setp: 4800, Loss: 0.3229430019855499
setp: 4900, Loss: 0.323321133852005
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9078947368421053
precision: 0.8717948717948718
recall: 0.7906976744186046
F_score: 0.8292682926829267
model saved.
avg_acc: 0.9263157894736842, avg_f_score: 0.8967613874000652
-------------subject: 19-------------
==========valence==========
******fold 0******
[250, 358]
training...
setp: 0, Loss: 0.6638081669807434
setp: 100, Loss: 0.6111333966255188
setp: 200, Loss: 0.6060441732406616
setp: 300, Loss: 0.5419297218322754
setp: 400, Loss: 0.6315214037895203
setp: 500, Loss: 0.5147935748100281
setp: 600, Loss: 0.4404172897338867
setp: 700, Loss: 0.3301304876804352
setp: 800, Loss: 0.4211373031139374
setp: 900, Loss: 0.43439117074012756
setp: 1000, Loss: 0.3205193877220154
setp: 1100, Loss: 0.32498300075531006
setp: 1200, Loss: 0.33081403374671936
setp: 1300, Loss: 0.337909460067749
setp: 1400, Loss: 0.3964209258556366
setp: 1500, Loss: 0.3771401047706604
setp: 1600, Loss: 0.31896498799324036
setp: 1700, Loss: 0.317557156085968
setp: 1800, Loss: 0.3164283335208893
setp: 1900, Loss: 0.3182631731033325
setp: 2000, Loss: 0.32097387313842773
setp: 2100, Loss: 0.373830109834671
setp: 2200, Loss: 0.3184303343296051
setp: 2300, Loss: 0.3236946761608124
setp: 2400, Loss: 0.3165460526943207
setp: 2500, Loss: 0.31654664874076843
setp: 2600, Loss: 0.3153265118598938
setp: 2700, Loss: 0.3189247250556946
setp: 2800, Loss: 0.31715816259384155
setp: 2900, Loss: 0.3155704140663147
setp: 3000, Loss: 0.31722861528396606
setp: 3100, Loss: 0.31633880734443665
setp: 3200, Loss: 0.318364679813385
setp: 3300, Loss: 0.3177932798862457
setp: 3400, Loss: 0.3204289674758911
setp: 3500, Loss: 0.3170045018196106
setp: 3600, Loss: 0.31596606969833374
setp: 3700, Loss: 0.31711530685424805
setp: 3800, Loss: 0.3666735887527466
setp: 3900, Loss: 0.3225874900817871
setp: 4000, Loss: 0.3703799247741699
setp: 4100, Loss: 0.31874558329582214
setp: 4200, Loss: 0.31606701016426086
setp: 4300, Loss: 0.31588131189346313
setp: 4400, Loss: 0.31636157631874084
setp: 4500, Loss: 0.31554192304611206
setp: 4600, Loss: 0.3174031376838684
setp: 4700, Loss: 0.3166908025741577
setp: 4800, Loss: 0.31569674611091614
setp: 4900, Loss: 0.3170183300971985
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8947368421052632
precision: 0.9253731343283582
recall: 0.8493150684931506
F_score: 0.8857142857142857
******fold 1******
[260, 348]
training...
setp: 0, Loss: 0.6954142451286316
setp: 100, Loss: 0.6713972687721252
setp: 200, Loss: 0.6336961388587952
setp: 300, Loss: 0.6079310178756714
setp: 400, Loss: 0.5005231499671936
setp: 500, Loss: 0.6402440667152405
setp: 600, Loss: 0.5666112899780273
setp: 700, Loss: 0.4162909686565399
setp: 800, Loss: 0.4796024262905121
setp: 900, Loss: 0.4423891603946686
setp: 1000, Loss: 0.3823949098587036
setp: 1100, Loss: 0.3414844572544098
setp: 1200, Loss: 0.33295053243637085
setp: 1300, Loss: 0.34636691212654114
setp: 1400, Loss: 0.35238075256347656
setp: 1500, Loss: 0.3174978196620941
setp: 1600, Loss: 0.32315847277641296
setp: 1700, Loss: 0.3179519474506378
setp: 1800, Loss: 0.3168141841888428
setp: 1900, Loss: 0.319934606552124
setp: 2000, Loss: 0.3260011374950409
setp: 2100, Loss: 0.4033713936805725
setp: 2200, Loss: 0.3158588707447052
setp: 2300, Loss: 0.3245188891887665
setp: 2400, Loss: 0.3173442482948303
setp: 2500, Loss: 0.3169762194156647
setp: 2600, Loss: 0.31614288687705994
setp: 2700, Loss: 0.3159818947315216
setp: 2800, Loss: 0.3165855407714844
setp: 2900, Loss: 0.31718382239341736
setp: 3000, Loss: 0.31731441617012024
setp: 3100, Loss: 0.31841349601745605
setp: 3200, Loss: 0.3172524571418762
setp: 3300, Loss: 0.34870246052742004
setp: 3400, Loss: 0.31709131598472595
setp: 3500, Loss: 0.5580688714981079
setp: 3600, Loss: 0.4229283630847931
setp: 3700, Loss: 0.3347538709640503
setp: 3800, Loss: 0.3629038631916046
setp: 3900, Loss: 0.31894156336784363
setp: 4000, Loss: 0.3176768124103546
setp: 4100, Loss: 0.31531399488449097
setp: 4200, Loss: 0.3179240822792053
setp: 4300, Loss: 0.3160873353481293
setp: 4400, Loss: 0.31631454825401306
setp: 4500, Loss: 0.31616637110710144
setp: 4600, Loss: 0.31591951847076416
setp: 4700, Loss: 0.3168575167655945
setp: 4800, Loss: 0.317091166973114
setp: 4900, Loss: 0.31643903255462646
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9961538461538462
F_score: 0.9980732177263969
validating...
acc: 0.9078947368421053
precision: 0.8769230769230769
recall: 0.9047619047619048
F_score: 0.890625
******fold 2******
[258, 350]
training...
setp: 0, Loss: 0.6766603589057922
setp: 100, Loss: 0.6751123070716858
setp: 200, Loss: 0.6582465171813965
setp: 300, Loss: 0.5334857702255249
setp: 400, Loss: 0.4594382345676422
setp: 500, Loss: 0.4878142476081848
setp: 600, Loss: 0.4305016100406647
setp: 700, Loss: 0.5017210245132446
setp: 800, Loss: 0.3655816614627838
setp: 900, Loss: 0.32416436076164246
setp: 1000, Loss: 0.37919408082962036
setp: 1100, Loss: 0.3213708698749542
setp: 1200, Loss: 0.3208005428314209
setp: 1300, Loss: 0.3183748424053192
setp: 1400, Loss: 0.3208453357219696
setp: 1500, Loss: 0.3487803339958191
setp: 1600, Loss: 0.3175223469734192
setp: 1700, Loss: 0.3618892729282379
setp: 1800, Loss: 0.31662994623184204
setp: 1900, Loss: 0.3319297134876251
setp: 2000, Loss: 0.3187583386898041
setp: 2100, Loss: 0.31735894083976746
setp: 2200, Loss: 0.3167852461338043
setp: 2300, Loss: 0.3177569508552551
setp: 2400, Loss: 0.3502892851829529
setp: 2500, Loss: 0.31762173771858215
setp: 2600, Loss: 0.31847280263900757
setp: 2700, Loss: 0.47154149413108826
setp: 2800, Loss: 0.326663613319397
setp: 2900, Loss: 0.3341671824455261
setp: 3000, Loss: 0.38253286480903625
setp: 3100, Loss: 0.31627270579338074
setp: 3200, Loss: 0.3166234493255615
setp: 3300, Loss: 0.31853169202804565
setp: 3400, Loss: 0.35169583559036255
setp: 3500, Loss: 0.3160383999347687
setp: 3600, Loss: 0.31585532426834106
setp: 3700, Loss: 0.31594476103782654
setp: 3800, Loss: 0.31691181659698486
setp: 3900, Loss: 0.318655788898468
setp: 4000, Loss: 0.3176785707473755
setp: 4100, Loss: 0.31579694151878357
setp: 4200, Loss: 0.3175702691078186
setp: 4300, Loss: 0.3500635325908661
setp: 4400, Loss: 0.3167645335197449
setp: 4500, Loss: 0.3173266649246216
setp: 4600, Loss: 0.3169087767601013
setp: 4700, Loss: 0.31765708327293396
setp: 4800, Loss: 0.41912955045700073
setp: 4900, Loss: 0.40068748593330383
training successfully ended.
validating...
acc: 0.9654605263157895
precision: 0.9372693726937269
recall: 0.9844961240310077
F_score: 0.9603024574669187
validating...
acc: 0.9013157894736842
precision: 0.8378378378378378
recall: 0.9538461538461539
F_score: 0.8920863309352518
******fold 3******
[252, 356]
training...
setp: 0, Loss: 0.7933684587478638
setp: 100, Loss: 0.6131824851036072
setp: 200, Loss: 0.6118571758270264
setp: 300, Loss: 0.4838959276676178
setp: 400, Loss: 0.4386127293109894
setp: 500, Loss: 0.4985983967781067
setp: 600, Loss: 0.5174946784973145
setp: 700, Loss: 0.4664374887943268
setp: 800, Loss: 0.3761204183101654
setp: 900, Loss: 0.4918050765991211
setp: 1000, Loss: 0.347503125667572
setp: 1100, Loss: 0.34281298518180847
setp: 1200, Loss: 0.32168814539909363
setp: 1300, Loss: 0.34930333495140076
setp: 1400, Loss: 0.3188551068305969
setp: 1500, Loss: 0.3498269021511078
setp: 1600, Loss: 0.3586055040359497
setp: 1700, Loss: 0.32063594460487366
setp: 1800, Loss: 0.35010507702827454
setp: 1900, Loss: 0.35149049758911133
setp: 2000, Loss: 0.37004590034484863
setp: 2100, Loss: 0.3701339364051819
setp: 2200, Loss: 0.33825916051864624
setp: 2300, Loss: 0.3494112193584442
setp: 2400, Loss: 0.3636610209941864
setp: 2500, Loss: 0.317381888628006
setp: 2600, Loss: 0.3184916079044342
setp: 2700, Loss: 0.3240857422351837
setp: 2800, Loss: 0.33586233854293823
setp: 2900, Loss: 0.32204172015190125
setp: 3000, Loss: 0.31643402576446533
setp: 3100, Loss: 0.32801488041877747
setp: 3200, Loss: 0.31999868154525757
setp: 3300, Loss: 0.3164766728878021
setp: 3400, Loss: 0.31740131974220276
setp: 3500, Loss: 0.3675767183303833
setp: 3600, Loss: 0.31580013036727905
setp: 3700, Loss: 0.3159826695919037
setp: 3800, Loss: 0.3171488344669342
setp: 3900, Loss: 0.3178628385066986
setp: 4000, Loss: 0.316587895154953
setp: 4100, Loss: 0.31702786684036255
setp: 4200, Loss: 0.318227618932724
setp: 4300, Loss: 0.3188207447528839
setp: 4400, Loss: 0.31641075015068054
setp: 4500, Loss: 0.31657519936561584
setp: 4600, Loss: 0.31714627146720886
setp: 4700, Loss: 0.6139939427375793
setp: 4800, Loss: 0.4939015209674835
setp: 4900, Loss: 0.35344743728637695
training successfully ended.
validating...
acc: 0.993421052631579
precision: 0.9920634920634921
recall: 0.9920634920634921
F_score: 0.9920634920634921
validating...
acc: 0.9473684210526315
precision: 0.9436619718309859
recall: 0.9436619718309859
F_score: 0.9436619718309859
******fold 4******
[272, 336]
training...
setp: 0, Loss: 0.677613377571106
setp: 100, Loss: 0.6854825615882874
setp: 200, Loss: 0.6327887773513794
setp: 300, Loss: 0.5426605939865112
setp: 400, Loss: 0.5444864630699158
setp: 500, Loss: 0.49245351552963257
setp: 600, Loss: 0.5065204501152039
setp: 700, Loss: 0.34875884652137756
setp: 800, Loss: 0.36975833773612976
setp: 900, Loss: 0.38753148913383484
setp: 1000, Loss: 0.35996466875076294
setp: 1100, Loss: 0.3285893201828003
setp: 1200, Loss: 0.32038259506225586
setp: 1300, Loss: 0.3215888440608978
setp: 1400, Loss: 0.35071563720703125
setp: 1500, Loss: 0.3550558090209961
setp: 1600, Loss: 0.31688979268074036
setp: 1700, Loss: 0.31944647431373596
setp: 1800, Loss: 0.3175908625125885
setp: 1900, Loss: 0.3185955882072449
setp: 2000, Loss: 0.3208373486995697
setp: 2100, Loss: 0.3164391815662384
setp: 2200, Loss: 0.3162367045879364
setp: 2300, Loss: 0.31811952590942383
setp: 2400, Loss: 0.348562628030777
setp: 2500, Loss: 0.3172851502895355
setp: 2600, Loss: 0.3174609839916229
setp: 2700, Loss: 0.32433491945266724
setp: 2800, Loss: 0.3677010238170624
setp: 2900, Loss: 0.3154275119304657
setp: 3000, Loss: 0.31645652651786804
setp: 3100, Loss: 0.31678351759910583
setp: 3200, Loss: 0.3164224326610565
setp: 3300, Loss: 0.34701448678970337
setp: 3400, Loss: 0.3190161883831024
setp: 3500, Loss: 0.34823277592658997
setp: 3600, Loss: 0.3277340233325958
setp: 3700, Loss: 0.31724992394447327
setp: 3800, Loss: 0.3262442648410797
setp: 3900, Loss: 0.32432931661605835
setp: 4000, Loss: 0.31575807929039
setp: 4100, Loss: 0.3154504597187042
setp: 4200, Loss: 0.3163455128669739
setp: 4300, Loss: 0.351899653673172
setp: 4400, Loss: 0.31573277711868286
setp: 4500, Loss: 0.31531473994255066
setp: 4600, Loss: 0.31615597009658813
setp: 4700, Loss: 0.317544549703598
setp: 4800, Loss: 0.31664589047431946
setp: 4900, Loss: 0.31703364849090576
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.9926470588235294
F_score: 0.996309963099631
validating...
acc: 0.9078947368421053
precision: 0.8490566037735849
recall: 0.8823529411764706
F_score: 0.8653846153846154
model saved.
avg_acc: 0.9118421052631579, avg_f_score: 0.8954944407730278
==========arousal==========
******fold 0******
[189, 419]
training...
setp: 0, Loss: 0.8009974956512451
setp: 100, Loss: 0.6208910346031189
setp: 200, Loss: 0.6677052974700928
setp: 300, Loss: 0.5682752728462219
setp: 400, Loss: 0.5121265649795532
setp: 500, Loss: 0.43876877427101135
setp: 600, Loss: 0.4240867793560028
setp: 700, Loss: 0.376577764749527
setp: 800, Loss: 0.36630159616470337
setp: 900, Loss: 0.38046371936798096
setp: 1000, Loss: 0.4048166573047638
setp: 1100, Loss: 0.3482334017753601
setp: 1200, Loss: 0.32617199420928955
setp: 1300, Loss: 0.3928985297679901
setp: 1400, Loss: 0.38077160716056824
setp: 1500, Loss: 0.35567575693130493
setp: 1600, Loss: 0.38954809308052063
setp: 1700, Loss: 0.3623702824115753
setp: 1800, Loss: 0.3260590732097626
setp: 1900, Loss: 0.35375717282295227
setp: 2000, Loss: 0.3467223644256592
setp: 2100, Loss: 0.34702515602111816
setp: 2200, Loss: 0.3469479978084564
setp: 2300, Loss: 0.34816181659698486
setp: 2400, Loss: 0.3472766876220703
setp: 2500, Loss: 0.34808066487312317
setp: 2600, Loss: 0.31566980481147766
setp: 2700, Loss: 0.34910300374031067
setp: 2800, Loss: 0.34929245710372925
setp: 2900, Loss: 0.3610116243362427
setp: 3000, Loss: 0.34643465280532837
setp: 3100, Loss: 0.3167027533054352
setp: 3200, Loss: 0.34983474016189575
setp: 3300, Loss: 0.34591278433799744
setp: 3400, Loss: 0.3516715168952942
setp: 3500, Loss: 0.3510112166404724
setp: 3600, Loss: 0.3492765426635742
setp: 3700, Loss: 0.3181096017360687
setp: 3800, Loss: 0.35090234875679016
setp: 3900, Loss: 0.3485119640827179
setp: 4000, Loss: 0.3467784523963928
setp: 4100, Loss: 0.34692612290382385
setp: 4200, Loss: 0.34737998247146606
setp: 4300, Loss: 0.3473494350910187
setp: 4400, Loss: 0.3481186628341675
setp: 4500, Loss: 0.31507641077041626
setp: 4600, Loss: 0.3496829569339752
setp: 4700, Loss: 0.347499281167984
setp: 4800, Loss: 0.3515167832374573
setp: 4900, Loss: 0.3471761643886566
training successfully ended.
validating...
acc: 0.975328947368421
precision: 1.0
recall: 0.9206349206349206
F_score: 0.9586776859504132
validating...
acc: 0.9210526315789473
precision: 0.9791666666666666
recall: 0.8103448275862069
F_score: 0.8867924528301887
******fold 1******
[197, 411]
training...
setp: 0, Loss: 0.6488627791404724
setp: 100, Loss: 0.6209847927093506
setp: 200, Loss: 0.581591010093689
setp: 300, Loss: 0.5302785634994507
setp: 400, Loss: 0.40489551424980164
setp: 500, Loss: 0.43743717670440674
setp: 600, Loss: 0.3842350244522095
setp: 700, Loss: 0.4061054289340973
setp: 800, Loss: 0.33468887209892273
setp: 900, Loss: 0.36757272481918335
setp: 1000, Loss: 0.35697141289711
setp: 1100, Loss: 0.3497881293296814
setp: 1200, Loss: 0.3555253744125366
setp: 1300, Loss: 0.3477923572063446
setp: 1400, Loss: 0.32641613483428955
setp: 1500, Loss: 0.3226366937160492
setp: 1600, Loss: 0.32091793417930603
setp: 1700, Loss: 0.31938913464546204
setp: 1800, Loss: 0.3181106448173523
setp: 1900, Loss: 0.32570701837539673
setp: 2000, Loss: 0.31688085198402405
setp: 2100, Loss: 0.31538355350494385
setp: 2200, Loss: 0.3215801417827606
setp: 2300, Loss: 0.31928619742393494
setp: 2400, Loss: 0.3246048390865326
setp: 2500, Loss: 0.3171318471431732
setp: 2600, Loss: 0.31619125604629517
setp: 2700, Loss: 0.3165387809276581
setp: 2800, Loss: 0.32063934206962585
setp: 2900, Loss: 0.31896910071372986
setp: 3000, Loss: 0.3196091055870056
setp: 3100, Loss: 0.38342419266700745
setp: 3200, Loss: 0.3167993724346161
setp: 3300, Loss: 0.31615617871284485
setp: 3400, Loss: 0.3195471465587616
setp: 3500, Loss: 0.3175205588340759
setp: 3600, Loss: 0.31980690360069275
setp: 3700, Loss: 0.3163178861141205
setp: 3800, Loss: 0.3204992115497589
setp: 3900, Loss: 0.36665940284729004
setp: 4000, Loss: 0.31506645679473877
setp: 4100, Loss: 0.3154582381248474
setp: 4200, Loss: 0.3165145218372345
setp: 4300, Loss: 0.32772716879844666
setp: 4400, Loss: 0.34763485193252563
setp: 4500, Loss: 0.3152008354663849
setp: 4600, Loss: 0.31582096219062805
setp: 4700, Loss: 0.3170092701911926
setp: 4800, Loss: 0.3167111873626709
setp: 4900, Loss: 0.31735238432884216
training successfully ended.
validating...
acc: 0.993421052631579
precision: 0.9800995024875622
recall: 1.0
F_score: 0.9899497487437185
validating...
acc: 0.9473684210526315
precision: 0.8620689655172413
recall: 1.0
F_score: 0.9259259259259259
******fold 2******
[196, 412]
training...
setp: 0, Loss: 0.5828157067298889
setp: 100, Loss: 0.5969724059104919
setp: 200, Loss: 0.6662220358848572
setp: 300, Loss: 0.5283371806144714
setp: 400, Loss: 0.5443286895751953
setp: 500, Loss: 0.3970591425895691
setp: 600, Loss: 0.32863762974739075
setp: 700, Loss: 0.3251105546951294
setp: 800, Loss: 0.325684517621994
setp: 900, Loss: 0.3252817690372467
setp: 1000, Loss: 0.32671448588371277
setp: 1100, Loss: 0.3222283720970154
setp: 1200, Loss: 0.3227687478065491
setp: 1300, Loss: 0.34101274609565735
setp: 1400, Loss: 0.31896743178367615
setp: 1500, Loss: 0.3188639283180237
setp: 1600, Loss: 0.3502061665058136
setp: 1700, Loss: 0.3177183270454407
setp: 1800, Loss: 0.3185517489910126
setp: 1900, Loss: 0.31927916407585144
setp: 2000, Loss: 0.3186339735984802
setp: 2100, Loss: 0.4688379764556885
setp: 2200, Loss: 0.3217701315879822
setp: 2300, Loss: 0.3240395784378052
setp: 2400, Loss: 0.3916124105453491
setp: 2500, Loss: 0.32007449865341187
setp: 2600, Loss: 0.3183802366256714
setp: 2700, Loss: 0.3203345835208893
setp: 2800, Loss: 0.32016685605049133
setp: 2900, Loss: 0.3208092153072357
setp: 3000, Loss: 0.31985124945640564
setp: 3100, Loss: 0.3201369643211365
setp: 3200, Loss: 0.3206256031990051
setp: 3300, Loss: 0.3189038932323456
setp: 3400, Loss: 0.31909388303756714
setp: 3500, Loss: 0.35022494196891785
setp: 3600, Loss: 0.31907257437705994
setp: 3700, Loss: 0.31907618045806885
setp: 3800, Loss: 0.4739031195640564
setp: 3900, Loss: 0.3553355634212494
setp: 4000, Loss: 0.32780399918556213
setp: 4100, Loss: 0.32131174206733704
setp: 4200, Loss: 0.3222483992576599
setp: 4300, Loss: 0.32331639528274536
setp: 4400, Loss: 0.31948748230934143
setp: 4500, Loss: 0.32117119431495667
setp: 4600, Loss: 0.3362308442592621
setp: 4700, Loss: 0.3373875021934509
setp: 4800, Loss: 0.32445028424263
setp: 4900, Loss: 0.31976720690727234
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9605263157894737
precision: 0.9591836734693877
recall: 0.9215686274509803
F_score: 0.9400000000000001
******fold 3******
[197, 411]
training...
setp: 0, Loss: 0.6773999929428101
setp: 100, Loss: 0.6434587240219116
setp: 200, Loss: 0.5957339406013489
setp: 300, Loss: 0.5867201089859009
setp: 400, Loss: 0.45746657252311707
setp: 500, Loss: 0.3997611701488495
setp: 600, Loss: 0.3358813524246216
setp: 700, Loss: 0.32906851172447205
setp: 800, Loss: 0.34249258041381836
setp: 900, Loss: 0.3242323100566864
setp: 1000, Loss: 0.3248985707759857
setp: 1100, Loss: 0.32403743267059326
setp: 1200, Loss: 0.351727157831192
setp: 1300, Loss: 0.3503417670726776
setp: 1400, Loss: 0.320141464471817
setp: 1500, Loss: 0.3241918385028839
setp: 1600, Loss: 0.34926360845565796
setp: 1700, Loss: 0.3200189769268036
setp: 1800, Loss: 0.31735149025917053
setp: 1900, Loss: 0.3203396797180176
setp: 2000, Loss: 0.31835249066352844
setp: 2100, Loss: 0.3174149692058563
setp: 2200, Loss: 0.3177719712257385
setp: 2300, Loss: 0.31956228613853455
setp: 2400, Loss: 0.3192298412322998
setp: 2500, Loss: 0.32387658953666687
setp: 2600, Loss: 0.3283224403858185
setp: 2700, Loss: 0.32274121046066284
setp: 2800, Loss: 0.3185387849807739
setp: 2900, Loss: 0.3189244866371155
setp: 3000, Loss: 0.3180661201477051
setp: 3100, Loss: 0.319988489151001
setp: 3200, Loss: 0.31935209035873413
setp: 3300, Loss: 0.317952424287796
setp: 3400, Loss: 0.3192722499370575
setp: 3500, Loss: 0.31919416785240173
setp: 3600, Loss: 0.31861376762390137
setp: 3700, Loss: 0.36411410570144653
setp: 3800, Loss: 0.34319496154785156
setp: 3900, Loss: 0.31836435198783875
setp: 4000, Loss: 0.3171066641807556
setp: 4100, Loss: 0.31761467456817627
setp: 4200, Loss: 0.319728285074234
setp: 4300, Loss: 0.3193873465061188
setp: 4400, Loss: 0.31862887740135193
setp: 4500, Loss: 0.3178795576095581
setp: 4600, Loss: 0.32095950841903687
setp: 4700, Loss: 0.32077452540397644
setp: 4800, Loss: 0.3193746507167816
setp: 4900, Loss: 0.3182387948036194
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9949494949494949
recall: 1.0
F_score: 0.9974683544303797
validating...
acc: 0.9671052631578947
precision: 0.9591836734693877
recall: 0.94
F_score: 0.9494949494949495
******fold 4******
[209, 399]
training...
setp: 0, Loss: 0.7365537285804749
setp: 100, Loss: 0.6644666790962219
setp: 200, Loss: 0.6240954399108887
setp: 300, Loss: 0.6025571823120117
setp: 400, Loss: 0.6175754070281982
setp: 500, Loss: 0.5880255103111267
setp: 600, Loss: 0.5024539232254028
setp: 700, Loss: 0.47840583324432373
setp: 800, Loss: 0.48341724276542664
setp: 900, Loss: 0.45143255591392517
setp: 1000, Loss: 0.39620986580848694
setp: 1100, Loss: 0.3572002053260803
setp: 1200, Loss: 0.3560976982116699
setp: 1300, Loss: 0.40860363841056824
setp: 1400, Loss: 0.3566812574863434
setp: 1500, Loss: 0.3546016812324524
setp: 1600, Loss: 0.40480223298072815
setp: 1700, Loss: 0.35101690888404846
setp: 1800, Loss: 0.35007229447364807
setp: 1900, Loss: 0.3672257661819458
setp: 2000, Loss: 0.32506248354911804
setp: 2100, Loss: 0.32025444507598877
setp: 2200, Loss: 0.33285966515541077
setp: 2300, Loss: 0.33869102597236633
setp: 2400, Loss: 0.316509872674942
setp: 2500, Loss: 0.32381191849708557
setp: 2600, Loss: 0.31516796350479126
setp: 2700, Loss: 0.3158720135688782
setp: 2800, Loss: 0.3293742537498474
setp: 2900, Loss: 0.3155205547809601
setp: 3000, Loss: 0.31560835242271423
setp: 3100, Loss: 0.32338476181030273
setp: 3200, Loss: 0.31703364849090576
setp: 3300, Loss: 0.31485694646835327
setp: 3400, Loss: 0.31684795022010803
setp: 3500, Loss: 0.3209819197654724
setp: 3600, Loss: 0.32948529720306396
setp: 3700, Loss: 0.31557542085647583
setp: 3800, Loss: 0.32273155450820923
setp: 3900, Loss: 0.3179929554462433
setp: 4000, Loss: 0.31699126958847046
setp: 4100, Loss: 0.31578654050827026
setp: 4200, Loss: 0.3435192108154297
setp: 4300, Loss: 0.3167802393436432
setp: 4400, Loss: 0.32391902804374695
setp: 4500, Loss: 0.31571313738822937
setp: 4600, Loss: 0.32166555523872375
setp: 4700, Loss: 0.31855830550193787
setp: 4800, Loss: 0.3149808943271637
setp: 4900, Loss: 0.3158775269985199
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9952153110047847
F_score: 0.9976019184652278
validating...
acc: 0.9342105263157895
precision: 0.9666666666666667
recall: 0.7631578947368421
F_score: 0.8529411764705883
model saved.
avg_acc: 0.9460526315789473, avg_f_score: 0.9110309009443306
-------------subject: 20-------------
==========valence==========
******fold 0******
[250, 358]
training...
setp: 0, Loss: 0.6909476518630981
setp: 100, Loss: 0.6663811802864075
setp: 200, Loss: 0.6309927105903625
setp: 300, Loss: 0.5513941049575806
setp: 400, Loss: 0.5064767003059387
setp: 500, Loss: 0.4696619212627411
setp: 600, Loss: 0.4278119206428528
setp: 700, Loss: 0.36103713512420654
setp: 800, Loss: 0.3517315983772278
setp: 900, Loss: 0.3999817967414856
setp: 1000, Loss: 0.4077402949333191
setp: 1100, Loss: 0.35078006982803345
setp: 1200, Loss: 0.34169673919677734
setp: 1300, Loss: 0.32221075892448425
setp: 1400, Loss: 0.3295592665672302
setp: 1500, Loss: 0.3698435425758362
setp: 1600, Loss: 0.32005980610847473
setp: 1700, Loss: 0.3236936926841736
setp: 1800, Loss: 0.31836843490600586
setp: 1900, Loss: 0.3189948797225952
setp: 2000, Loss: 0.34131842851638794
setp: 2100, Loss: 0.33470097184181213
setp: 2200, Loss: 0.31806567311286926
setp: 2300, Loss: 0.32580137252807617
setp: 2400, Loss: 0.3177682161331177
setp: 2500, Loss: 0.3536025881767273
setp: 2600, Loss: 0.3168781101703644
setp: 2700, Loss: 0.3167530298233032
setp: 2800, Loss: 0.3256237506866455
setp: 2900, Loss: 0.3171434998512268
setp: 3000, Loss: 0.3175448477268219
setp: 3100, Loss: 0.31895333528518677
setp: 3200, Loss: 0.31972724199295044
setp: 3300, Loss: 0.3173961937427521
setp: 3400, Loss: 0.3784044682979584
setp: 3500, Loss: 0.3175092935562134
setp: 3600, Loss: 0.3175122141838074
setp: 3700, Loss: 0.3170171082019806
setp: 3800, Loss: 0.31811755895614624
setp: 3900, Loss: 0.3176952302455902
setp: 4000, Loss: 0.3170907497406006
setp: 4100, Loss: 0.3187364935874939
setp: 4200, Loss: 0.3275546729564667
setp: 4300, Loss: 0.3376046419143677
setp: 4400, Loss: 0.35348057746887207
setp: 4500, Loss: 0.3167458474636078
setp: 4600, Loss: 0.3162332773208618
setp: 4700, Loss: 0.31816110014915466
setp: 4800, Loss: 0.31671011447906494
setp: 4900, Loss: 0.3175528943538666
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.992
F_score: 0.9959839357429718
validating...
acc: 0.8881578947368421
precision: 0.8783783783783784
recall: 0.8904109589041096
F_score: 0.8843537414965986
******fold 1******
[260, 348]
training...
setp: 0, Loss: 0.7142800688743591
setp: 100, Loss: 0.6659754514694214
setp: 200, Loss: 0.6386575698852539
setp: 300, Loss: 0.5499544143676758
setp: 400, Loss: 0.612022340297699
setp: 500, Loss: 0.47779449820518494
setp: 600, Loss: 0.3858225345611572
setp: 700, Loss: 0.35059189796447754
setp: 800, Loss: 0.3300541937351227
setp: 900, Loss: 0.3620446026325226
setp: 1000, Loss: 0.35594749450683594
setp: 1100, Loss: 0.3211441934108734
setp: 1200, Loss: 0.32083478569984436
setp: 1300, Loss: 0.350276380777359
setp: 1400, Loss: 0.3188475966453552
setp: 1500, Loss: 0.3510688841342926
setp: 1600, Loss: 0.3184632360935211
setp: 1700, Loss: 0.31698644161224365
setp: 1800, Loss: 0.32601940631866455
setp: 1900, Loss: 0.3364681899547577
setp: 2000, Loss: 0.3156449794769287
setp: 2100, Loss: 0.31752219796180725
setp: 2200, Loss: 0.3177860677242279
setp: 2300, Loss: 0.31657254695892334
setp: 2400, Loss: 0.3215618431568146
setp: 2500, Loss: 0.3187294900417328
setp: 2600, Loss: 0.31773841381073
setp: 2700, Loss: 0.3185177147388458
setp: 2800, Loss: 0.3204731047153473
setp: 2900, Loss: 0.31675517559051514
setp: 3000, Loss: 0.3785080015659332
setp: 3100, Loss: 0.31639984250068665
setp: 3200, Loss: 0.3481825292110443
setp: 3300, Loss: 0.31714728474617004
setp: 3400, Loss: 0.3185722827911377
setp: 3500, Loss: 0.3174653649330139
setp: 3600, Loss: 0.3169088661670685
setp: 3700, Loss: 0.31851935386657715
setp: 3800, Loss: 0.3184022307395935
setp: 3900, Loss: 0.31605425477027893
setp: 4000, Loss: 0.31773892045021057
setp: 4100, Loss: 0.31712105870246887
setp: 4200, Loss: 0.3245466947555542
setp: 4300, Loss: 0.31730014085769653
setp: 4400, Loss: 0.3169548213481903
setp: 4500, Loss: 0.3177289366722107
setp: 4600, Loss: 0.3181379437446594
setp: 4700, Loss: 0.31722232699394226
setp: 4800, Loss: 0.3207375407218933
setp: 4900, Loss: 0.31692788004875183
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9090909090909091
recall: 0.9523809523809523
F_score: 0.9302325581395349
******fold 2******
[260, 348]
training...
setp: 0, Loss: 0.6918548941612244
setp: 100, Loss: 0.6739960312843323
setp: 200, Loss: 0.6080276966094971
setp: 300, Loss: 0.4909651279449463
setp: 400, Loss: 0.39286673069000244
setp: 500, Loss: 0.37715810537338257
setp: 600, Loss: 0.4118221700191498
setp: 700, Loss: 0.3343169391155243
setp: 800, Loss: 0.4471476674079895
setp: 900, Loss: 0.5179833769798279
setp: 1000, Loss: 0.4288104772567749
setp: 1100, Loss: 0.33141475915908813
setp: 1200, Loss: 0.3370479643344879
setp: 1300, Loss: 0.3207855820655823
setp: 1400, Loss: 0.31857985258102417
setp: 1500, Loss: 0.38045480847358704
setp: 1600, Loss: 0.31748104095458984
setp: 1700, Loss: 0.3191779553890228
setp: 1800, Loss: 0.37873706221580505
setp: 1900, Loss: 0.349786639213562
setp: 2000, Loss: 0.32325494289398193
setp: 2100, Loss: 0.33078038692474365
setp: 2200, Loss: 0.3194507956504822
setp: 2300, Loss: 0.3169763684272766
setp: 2400, Loss: 0.316830575466156
setp: 2500, Loss: 0.3193111717700958
setp: 2600, Loss: 0.31667137145996094
setp: 2700, Loss: 0.3174588978290558
setp: 2800, Loss: 0.3199080526828766
setp: 2900, Loss: 0.32189372181892395
setp: 3000, Loss: 0.3170601725578308
setp: 3100, Loss: 0.3183533251285553
setp: 3200, Loss: 0.32066768407821655
setp: 3300, Loss: 0.31650015711784363
setp: 3400, Loss: 0.34938353300094604
setp: 3500, Loss: 0.31742730736732483
setp: 3600, Loss: 0.3193630278110504
setp: 3700, Loss: 0.34583407640457153
setp: 3800, Loss: 0.3476961851119995
setp: 3900, Loss: 0.3499884605407715
setp: 4000, Loss: 0.3183385133743286
setp: 4100, Loss: 0.34529340267181396
setp: 4200, Loss: 0.34913113713264465
setp: 4300, Loss: 0.3156414330005646
setp: 4400, Loss: 0.32474470138549805
setp: 4500, Loss: 0.3168996572494507
setp: 4600, Loss: 0.3181808292865753
setp: 4700, Loss: 0.3258756697177887
setp: 4800, Loss: 0.3411578834056854
setp: 4900, Loss: 0.3207375109195709
training successfully ended.
validating...
acc: 0.9851973684210527
precision: 0.9845559845559846
recall: 0.9807692307692307
F_score: 0.9826589595375722
validating...
acc: 0.9407894736842105
precision: 0.9090909090909091
recall: 0.9523809523809523
F_score: 0.9302325581395349
******fold 3******
[250, 358]
training...
setp: 0, Loss: 0.7415692806243896
setp: 100, Loss: 0.6677758097648621
setp: 200, Loss: 0.669307291507721
setp: 300, Loss: 0.5673511028289795
setp: 400, Loss: 0.5365976095199585
setp: 500, Loss: 0.42565709352493286
setp: 600, Loss: 0.4784618318080902
setp: 700, Loss: 0.3807399272918701
setp: 800, Loss: 0.3911794424057007
setp: 900, Loss: 0.3619022071361542
setp: 1000, Loss: 0.3238559067249298
setp: 1100, Loss: 0.3935273289680481
setp: 1200, Loss: 0.36850813031196594
setp: 1300, Loss: 0.32933366298675537
setp: 1400, Loss: 0.32039570808410645
setp: 1500, Loss: 0.3974243998527527
setp: 1600, Loss: 0.3839854896068573
setp: 1700, Loss: 0.33024853467941284
setp: 1800, Loss: 0.3490833044052124
setp: 1900, Loss: 0.34742239117622375
setp: 2000, Loss: 0.3178274631500244
setp: 2100, Loss: 0.31705230474472046
setp: 2200, Loss: 0.3945136070251465
setp: 2300, Loss: 0.3180856704711914
setp: 2400, Loss: 0.330130010843277
setp: 2500, Loss: 0.4140474796295166
setp: 2600, Loss: 0.3246776759624481
setp: 2700, Loss: 0.378803014755249
setp: 2800, Loss: 0.31888076663017273
setp: 2900, Loss: 0.3155583143234253
setp: 3000, Loss: 0.38667869567871094
setp: 3100, Loss: 0.3380027711391449
setp: 3200, Loss: 0.4133983254432678
setp: 3300, Loss: 0.363353967666626
setp: 3400, Loss: 0.342988520860672
setp: 3500, Loss: 0.3187277317047119
setp: 3600, Loss: 0.3164829909801483
setp: 3700, Loss: 0.3206489086151123
setp: 3800, Loss: 0.3481910824775696
setp: 3900, Loss: 0.3163742423057556
setp: 4000, Loss: 0.31638965010643005
setp: 4100, Loss: 0.3165692389011383
setp: 4200, Loss: 0.3176776170730591
setp: 4300, Loss: 0.3165712058544159
setp: 4400, Loss: 0.3505115211009979
setp: 4500, Loss: 0.31736576557159424
setp: 4600, Loss: 0.31995245814323425
setp: 4700, Loss: 0.3174821138381958
setp: 4800, Loss: 0.3174173831939697
setp: 4900, Loss: 0.3176642656326294
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.992
F_score: 0.9959839357429718
validating...
acc: 0.9407894736842105
precision: 0.9571428571428572
recall: 0.9178082191780822
F_score: 0.9370629370629371
******fold 4******
[272, 336]
training...
setp: 0, Loss: 0.7707012891769409
setp: 100, Loss: 0.6860039830207825
setp: 200, Loss: 0.6802860498428345
setp: 300, Loss: 0.6096442937850952
setp: 400, Loss: 0.5750175714492798
setp: 500, Loss: 0.4862700402736664
setp: 600, Loss: 0.42627039551734924
setp: 700, Loss: 0.36697420477867126
setp: 800, Loss: 0.42355111241340637
setp: 900, Loss: 0.4030066430568695
setp: 1000, Loss: 0.37748974561691284
setp: 1100, Loss: 0.34092146158218384
setp: 1200, Loss: 0.32688191533088684
setp: 1300, Loss: 0.34103450179100037
setp: 1400, Loss: 0.32724884152412415
setp: 1500, Loss: 0.4289313852787018
setp: 1600, Loss: 0.33551478385925293
setp: 1700, Loss: 0.3174390494823456
setp: 1800, Loss: 0.3171970844268799
setp: 1900, Loss: 0.32370832562446594
setp: 2000, Loss: 0.31803128123283386
setp: 2100, Loss: 0.31971776485443115
setp: 2200, Loss: 0.37923356890678406
setp: 2300, Loss: 0.31645891070365906
setp: 2400, Loss: 0.32259929180145264
setp: 2500, Loss: 0.35676881670951843
setp: 2600, Loss: 0.32336413860321045
setp: 2700, Loss: 0.3208945691585541
setp: 2800, Loss: 0.35590746998786926
setp: 2900, Loss: 0.35011833906173706
setp: 3000, Loss: 0.31657660007476807
setp: 3100, Loss: 0.3190184235572815
setp: 3200, Loss: 0.3178577423095703
setp: 3300, Loss: 0.3205850422382355
setp: 3400, Loss: 0.41269296407699585
setp: 3500, Loss: 0.31687164306640625
setp: 3600, Loss: 0.33871594071388245
setp: 3700, Loss: 0.33989113569259644
setp: 3800, Loss: 0.32792529463768005
setp: 3900, Loss: 0.3256898522377014
setp: 4000, Loss: 0.3164157569408417
setp: 4100, Loss: 0.3860742449760437
setp: 4200, Loss: 0.3182838261127472
setp: 4300, Loss: 0.317108154296875
setp: 4400, Loss: 0.32307350635528564
setp: 4500, Loss: 0.318099707365036
setp: 4600, Loss: 0.32385122776031494
setp: 4700, Loss: 0.4238375723361969
setp: 4800, Loss: 0.3519915044307709
setp: 4900, Loss: 0.3202532231807709
training successfully ended.
validating...
acc: 0.9901315789473685
precision: 0.9925925925925926
recall: 0.9852941176470589
F_score: 0.988929889298893
validating...
acc: 0.8618421052631579
precision: 0.7678571428571429
recall: 0.8431372549019608
F_score: 0.8037383177570093
model saved.
avg_acc: 0.9144736842105263, avg_f_score: 0.8971240225191229
==========arousal==========
******fold 0******
[132, 476]
training...
setp: 0, Loss: 0.7167943716049194
setp: 100, Loss: 0.6813678741455078
setp: 200, Loss: 0.4943818151950836
setp: 300, Loss: 0.47166338562965393
setp: 400, Loss: 0.3611259460449219
setp: 500, Loss: 0.3307708501815796
setp: 600, Loss: 0.3279113471508026
setp: 700, Loss: 0.3225507140159607
setp: 800, Loss: 0.31976503133773804
setp: 900, Loss: 0.32146257162094116
setp: 1000, Loss: 0.31953927874565125
setp: 1100, Loss: 0.31796422600746155
setp: 1200, Loss: 0.31935417652130127
setp: 1300, Loss: 0.317899227142334
setp: 1400, Loss: 0.31701910495758057
setp: 1500, Loss: 0.31862929463386536
setp: 1600, Loss: 0.3177010715007782
setp: 1700, Loss: 0.3168260455131531
setp: 1800, Loss: 0.3204056918621063
setp: 1900, Loss: 0.322943776845932
setp: 2000, Loss: 0.31646832823753357
setp: 2100, Loss: 0.31825777888298035
setp: 2200, Loss: 0.31763896346092224
setp: 2300, Loss: 0.3169375956058502
setp: 2400, Loss: 0.31925684213638306
setp: 2500, Loss: 0.36012500524520874
setp: 2600, Loss: 0.3167881369590759
setp: 2700, Loss: 0.3176793158054352
setp: 2800, Loss: 0.31797733902931213
setp: 2900, Loss: 0.31663596630096436
setp: 3000, Loss: 0.3178307116031647
setp: 3100, Loss: 0.31754356622695923
setp: 3200, Loss: 0.31963321566581726
setp: 3300, Loss: 0.3360412120819092
setp: 3400, Loss: 0.3173449635505676
setp: 3500, Loss: 0.31691086292266846
setp: 3600, Loss: 0.3181522488594055
setp: 3700, Loss: 0.31739017367362976
setp: 3800, Loss: 0.3166636824607849
setp: 3900, Loss: 0.49460577964782715
setp: 4000, Loss: 0.33093270659446716
setp: 4100, Loss: 0.3470926582813263
setp: 4200, Loss: 0.31955623626708984
setp: 4300, Loss: 0.3190974295139313
setp: 4400, Loss: 0.3178883194923401
setp: 4500, Loss: 0.3190690875053406
setp: 4600, Loss: 0.3187533915042877
setp: 4700, Loss: 0.3178957402706146
setp: 4800, Loss: 0.31914186477661133
setp: 4900, Loss: 0.3192671239376068
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9605263157894737
precision: 0.9459459459459459
recall: 0.8974358974358975
F_score: 0.9210526315789475
******fold 1******
[132, 476]
training...
setp: 0, Loss: 0.7270101308822632
setp: 100, Loss: 0.6825322508811951
setp: 200, Loss: 0.52635657787323
setp: 300, Loss: 0.3952665627002716
setp: 400, Loss: 0.3666517436504364
setp: 500, Loss: 0.3501531183719635
setp: 600, Loss: 0.3254832625389099
setp: 700, Loss: 0.32574743032455444
setp: 800, Loss: 0.3213200867176056
setp: 900, Loss: 0.3203204870223999
setp: 1000, Loss: 0.3187122642993927
setp: 1100, Loss: 0.31960171461105347
setp: 1200, Loss: 0.320157915353775
setp: 1300, Loss: 0.31915903091430664
setp: 1400, Loss: 0.3181438446044922
setp: 1500, Loss: 0.3184715509414673
setp: 1600, Loss: 0.32309406995773315
setp: 1700, Loss: 0.3179457187652588
setp: 1800, Loss: 0.31762900948524475
setp: 1900, Loss: 0.3185894787311554
setp: 2000, Loss: 0.31777289509773254
setp: 2100, Loss: 0.3172203302383423
setp: 2200, Loss: 0.31779903173446655
setp: 2300, Loss: 0.31764933466911316
setp: 2400, Loss: 0.4971116781234741
setp: 2500, Loss: 0.34341612458229065
setp: 2600, Loss: 0.3320058584213257
setp: 2700, Loss: 0.34758758544921875
setp: 2800, Loss: 0.32530251145362854
setp: 2900, Loss: 0.324221134185791
setp: 3000, Loss: 0.3240548074245453
setp: 3100, Loss: 0.32326287031173706
setp: 3200, Loss: 0.3234637975692749
setp: 3300, Loss: 0.32276666164398193
setp: 3400, Loss: 0.32271018624305725
setp: 3500, Loss: 0.3230600357055664
setp: 3600, Loss: 0.3224060535430908
setp: 3700, Loss: 0.3226361870765686
setp: 3800, Loss: 0.32297825813293457
setp: 3900, Loss: 0.322223961353302
setp: 4000, Loss: 0.32261428236961365
setp: 4100, Loss: 0.3227674663066864
setp: 4200, Loss: 0.698969841003418
setp: 4300, Loss: 0.3932466208934784
setp: 4400, Loss: 0.35118725895881653
setp: 4500, Loss: 0.33073437213897705
setp: 4600, Loss: 0.32664069533348083
setp: 4700, Loss: 0.32604724168777466
setp: 4800, Loss: 0.3256325423717499
setp: 4900, Loss: 0.3251177668571472
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9605263157894737
precision: 0.9230769230769231
recall: 0.9230769230769231
F_score: 0.9230769230769231
******fold 2******
[143, 465]
training...
setp: 0, Loss: 0.693183958530426
setp: 100, Loss: 0.6378318071365356
setp: 200, Loss: 0.47243091464042664
setp: 300, Loss: 0.3719748556613922
setp: 400, Loss: 0.3494596779346466
setp: 500, Loss: 0.3371162712574005
setp: 600, Loss: 0.3333303928375244
setp: 700, Loss: 0.32355406880378723
setp: 800, Loss: 0.32425832748413086
setp: 900, Loss: 0.3246638774871826
setp: 1000, Loss: 0.3217293620109558
setp: 1100, Loss: 0.32657650113105774
setp: 1200, Loss: 0.3280467391014099
setp: 1300, Loss: 0.32694295048713684
setp: 1400, Loss: 0.3440451920032501
setp: 1500, Loss: 0.3196367025375366
setp: 1600, Loss: 0.3195481598377228
setp: 1700, Loss: 0.32000458240509033
setp: 1800, Loss: 0.3196291923522949
setp: 1900, Loss: 0.3191303610801697
setp: 2000, Loss: 0.31939029693603516
setp: 2100, Loss: 0.3194907605648041
setp: 2200, Loss: 0.35101574659347534
setp: 2300, Loss: 0.34180793166160583
setp: 2400, Loss: 0.3185374140739441
setp: 2500, Loss: 0.32020464539527893
setp: 2600, Loss: 0.3203057646751404
setp: 2700, Loss: 0.31830164790153503
setp: 2800, Loss: 0.3186471164226532
setp: 2900, Loss: 0.3187362849712372
setp: 3000, Loss: 0.31921517848968506
setp: 3100, Loss: 0.31835082173347473
setp: 3200, Loss: 0.3490990698337555
setp: 3300, Loss: 0.3382532596588135
setp: 3400, Loss: 0.32273244857788086
setp: 3500, Loss: 0.3234955668449402
setp: 3600, Loss: 0.32297641038894653
setp: 3700, Loss: 0.3210256099700928
setp: 3800, Loss: 0.3216349482536316
setp: 3900, Loss: 0.3220226466655731
setp: 4000, Loss: 0.3212810754776001
setp: 4100, Loss: 0.32180145382881165
setp: 4200, Loss: 0.3217782974243164
setp: 4300, Loss: 0.3210894763469696
setp: 4400, Loss: 0.3212073743343353
setp: 4500, Loss: 0.32128310203552246
setp: 4600, Loss: 0.3209761381149292
setp: 4700, Loss: 0.32063257694244385
setp: 4800, Loss: 0.3574366569519043
setp: 4900, Loss: 0.32349276542663574
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.7222222222222222
recall: 0.9285714285714286
F_score: 0.8125000000000001
******fold 3******
[135, 473]
training...
setp: 0, Loss: 0.6929974555969238
setp: 100, Loss: 0.6692761778831482
setp: 200, Loss: 0.5361493825912476
setp: 300, Loss: 0.4894748330116272
setp: 400, Loss: 0.38866865634918213
setp: 500, Loss: 0.3290994465351105
setp: 600, Loss: 0.32622238993644714
setp: 700, Loss: 0.3317519426345825
setp: 800, Loss: 0.3251899778842926
setp: 900, Loss: 0.31954246759414673
setp: 1000, Loss: 0.31863832473754883
setp: 1100, Loss: 0.3175202012062073
setp: 1200, Loss: 0.3178284466266632
setp: 1300, Loss: 0.3290632367134094
setp: 1400, Loss: 0.3175963759422302
setp: 1500, Loss: 0.31776824593544006
setp: 1600, Loss: 0.3174270689487457
setp: 1700, Loss: 0.3166128396987915
setp: 1800, Loss: 0.3475644588470459
setp: 1900, Loss: 0.3254581391811371
setp: 2000, Loss: 0.3161160945892334
setp: 2100, Loss: 0.3172244727611542
setp: 2200, Loss: 0.316903293132782
setp: 2300, Loss: 0.3168063163757324
setp: 2400, Loss: 0.316505491733551
setp: 2500, Loss: 0.3172833025455475
setp: 2600, Loss: 0.3169418275356293
setp: 2700, Loss: 0.3166203796863556
setp: 2800, Loss: 0.31718385219573975
setp: 2900, Loss: 0.3220573961734772
setp: 3000, Loss: 0.31795167922973633
setp: 3100, Loss: 0.31605610251426697
setp: 3200, Loss: 0.31586018204689026
setp: 3300, Loss: 0.31648921966552734
setp: 3400, Loss: 0.31654611229896545
setp: 3500, Loss: 0.31640520691871643
setp: 3600, Loss: 0.3165222108364105
setp: 3700, Loss: 0.3179168403148651
setp: 3800, Loss: 0.3160460293292999
setp: 3900, Loss: 0.3165176212787628
setp: 4000, Loss: 0.31645774841308594
setp: 4100, Loss: 0.31630319356918335
setp: 4200, Loss: 0.3169776201248169
setp: 4300, Loss: 0.31698769330978394
setp: 4400, Loss: 0.3157098889350891
setp: 4500, Loss: 0.3161373436450958
setp: 4600, Loss: 0.3163425624370575
setp: 4700, Loss: 0.316207617521286
setp: 4800, Loss: 0.524627149105072
setp: 4900, Loss: 0.33947813510894775
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9473684210526315
precision: 0.868421052631579
recall: 0.9166666666666666
F_score: 0.8918918918918918
******fold 4******
[142, 466]
training...
setp: 0, Loss: 0.6956267952919006
setp: 100, Loss: 0.6137998700141907
setp: 200, Loss: 0.506342351436615
setp: 300, Loss: 0.3933086097240448
setp: 400, Loss: 0.3274180293083191
setp: 500, Loss: 0.32361674308776855
setp: 600, Loss: 0.331794798374176
setp: 700, Loss: 0.31994178891181946
setp: 800, Loss: 0.31946367025375366
setp: 900, Loss: 0.3211107552051544
setp: 1000, Loss: 0.3190951943397522
setp: 1100, Loss: 0.3188460171222687
setp: 1200, Loss: 0.3200063705444336
setp: 1300, Loss: 0.31865525245666504
setp: 1400, Loss: 0.3235178589820862
setp: 1500, Loss: 0.3641558587551117
setp: 1600, Loss: 0.3280600607395172
setp: 1700, Loss: 0.3191109895706177
setp: 1800, Loss: 0.3192358613014221
setp: 1900, Loss: 0.3186742067337036
setp: 2000, Loss: 0.3191339671611786
setp: 2100, Loss: 0.3198408782482147
setp: 2200, Loss: 0.31883662939071655
setp: 2300, Loss: 0.3191553056240082
setp: 2400, Loss: 0.3203710615634918
setp: 2500, Loss: 0.318922221660614
setp: 2600, Loss: 0.3287516236305237
setp: 2700, Loss: 0.3187246024608612
setp: 2800, Loss: 0.31859439611434937
setp: 2900, Loss: 0.31847503781318665
setp: 3000, Loss: 0.31939223408699036
setp: 3100, Loss: 0.3185189366340637
setp: 3200, Loss: 0.318482369184494
setp: 3300, Loss: 0.3214772641658783
setp: 3400, Loss: 0.3283163011074066
setp: 3500, Loss: 0.31794825196266174
setp: 3600, Loss: 0.3181999623775482
setp: 3700, Loss: 0.31817084550857544
setp: 3800, Loss: 0.3181467354297638
setp: 3900, Loss: 0.3190543055534363
setp: 4000, Loss: 0.3184505105018616
setp: 4100, Loss: 0.31828558444976807
setp: 4200, Loss: 0.3191256523132324
setp: 4300, Loss: 0.32002463936805725
setp: 4400, Loss: 0.3174343407154083
setp: 4500, Loss: 0.3180164098739624
setp: 4600, Loss: 0.31820979714393616
setp: 4700, Loss: 0.31817567348480225
setp: 4800, Loss: 0.31881484389305115
setp: 4900, Loss: 0.3185514807701111
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.84
recall: 0.7241379310344828
F_score: 0.7777777777777777
model saved.
avg_acc: 0.9421052631578947, avg_f_score: 0.865259844865108
-------------subject: 21-------------
==========valence==========
******fold 0******
[277, 331]
training...
setp: 0, Loss: 0.7041218876838684
setp: 100, Loss: 0.6760923862457275
setp: 200, Loss: 0.5897343158721924
setp: 300, Loss: 0.4966107904911041
setp: 400, Loss: 0.41977983713150024
setp: 500, Loss: 0.38064950704574585
setp: 600, Loss: 0.32806476950645447
setp: 700, Loss: 0.3791173994541168
setp: 800, Loss: 0.32509323954582214
setp: 900, Loss: 0.3197883665561676
setp: 1000, Loss: 0.320994108915329
setp: 1100, Loss: 0.31949907541275024
setp: 1200, Loss: 0.31938299536705017
setp: 1300, Loss: 0.3184666633605957
setp: 1400, Loss: 0.3201805055141449
setp: 1500, Loss: 0.3213030695915222
setp: 1600, Loss: 0.3193104565143585
setp: 1700, Loss: 0.32230421900749207
setp: 1800, Loss: 0.3203108012676239
setp: 1900, Loss: 0.6962689161300659
setp: 2000, Loss: 0.3704308271408081
setp: 2100, Loss: 0.4103677272796631
setp: 2200, Loss: 0.3279311954975128
setp: 2300, Loss: 0.32769137620925903
setp: 2400, Loss: 0.3305526077747345
setp: 2500, Loss: 0.32743728160858154
setp: 2600, Loss: 0.3222689628601074
setp: 2700, Loss: 0.3223752975463867
setp: 2800, Loss: 0.32038259506225586
setp: 2900, Loss: 0.32323646545410156
setp: 3000, Loss: 0.3202742040157318
setp: 3100, Loss: 0.3202386200428009
setp: 3200, Loss: 0.3212043344974518
setp: 3300, Loss: 0.3207285404205322
setp: 3400, Loss: 0.3210955560207367
setp: 3500, Loss: 0.32136988639831543
setp: 3600, Loss: 0.32143786549568176
setp: 3700, Loss: 0.3587287664413452
setp: 3800, Loss: 0.3322237730026245
setp: 3900, Loss: 0.3193346858024597
setp: 4000, Loss: 0.3192685544490814
setp: 4100, Loss: 0.3189758360385895
setp: 4200, Loss: 0.3206670880317688
setp: 4300, Loss: 0.32027190923690796
setp: 4400, Loss: 0.31901565194129944
setp: 4500, Loss: 0.3188202381134033
setp: 4600, Loss: 0.3192737400531769
setp: 4700, Loss: 0.4056086540222168
setp: 4800, Loss: 0.3259739577770233
setp: 4900, Loss: 0.3181523382663727
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.9512195121951219
recall: 0.9285714285714286
F_score: 0.9397590361445782
******fold 1******
[295, 313]
training...
setp: 0, Loss: 0.7223184108734131
setp: 100, Loss: 0.6955063939094543
setp: 200, Loss: 0.6477600932121277
setp: 300, Loss: 0.5188912749290466
setp: 400, Loss: 0.4703626334667206
setp: 500, Loss: 0.4386736750602722
setp: 600, Loss: 0.3525497317314148
setp: 700, Loss: 0.33334919810295105
setp: 800, Loss: 0.332170695066452
setp: 900, Loss: 0.3596661686897278
setp: 1000, Loss: 0.32312244176864624
setp: 1100, Loss: 0.3211866617202759
setp: 1200, Loss: 0.3214813768863678
setp: 1300, Loss: 0.3238039016723633
setp: 1400, Loss: 0.32171666622161865
setp: 1500, Loss: 0.3222000002861023
setp: 1600, Loss: 0.32015562057495117
setp: 1700, Loss: 0.32013508677482605
setp: 1800, Loss: 0.3199510872364044
setp: 1900, Loss: 0.3536367118358612
setp: 2000, Loss: 0.32029277086257935
setp: 2100, Loss: 0.32356441020965576
setp: 2200, Loss: 0.34200337529182434
setp: 2300, Loss: 0.3513873815536499
setp: 2400, Loss: 0.31931135058403015
setp: 2500, Loss: 0.3204042911529541
setp: 2600, Loss: 0.31976258754730225
setp: 2700, Loss: 0.3200533390045166
setp: 2800, Loss: 0.3202988803386688
setp: 2900, Loss: 0.3204561471939087
setp: 3000, Loss: 0.31987202167510986
setp: 3100, Loss: 0.31936976313591003
setp: 3200, Loss: 0.32067108154296875
setp: 3300, Loss: 0.3209861218929291
setp: 3400, Loss: 0.3197472095489502
setp: 3500, Loss: 0.31945961713790894
setp: 3600, Loss: 0.31965237855911255
setp: 3700, Loss: 0.4828544557094574
setp: 3800, Loss: 0.38565605878829956
setp: 3900, Loss: 0.32417571544647217
setp: 4000, Loss: 0.327711820602417
setp: 4100, Loss: 0.3209229111671448
setp: 4200, Loss: 0.3224812150001526
setp: 4300, Loss: 0.31962329149246216
setp: 4400, Loss: 0.32084402441978455
setp: 4500, Loss: 0.3196521997451782
setp: 4600, Loss: 0.32002076506614685
setp: 4700, Loss: 0.320709764957428
setp: 4800, Loss: 0.3204975128173828
setp: 4900, Loss: 0.32012757658958435
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8421052631578947
precision: 0.8181818181818182
recall: 0.8181818181818182
F_score: 0.8181818181818182
******fold 2******
[291, 317]
training...
setp: 0, Loss: 0.6860842704772949
setp: 100, Loss: 0.6913273334503174
setp: 200, Loss: 0.6972622871398926
setp: 300, Loss: 0.6960779428482056
setp: 400, Loss: 0.6936789751052856
setp: 500, Loss: 0.6913509368896484
setp: 600, Loss: 0.688572347164154
setp: 700, Loss: 0.6937092542648315
setp: 800, Loss: 0.689397931098938
setp: 900, Loss: 0.694248378276825
setp: 1000, Loss: 0.6968466639518738
setp: 1100, Loss: 0.6898756623268127
setp: 1200, Loss: 0.6914851665496826
setp: 1300, Loss: 0.6913020014762878
setp: 1400, Loss: 0.6914359331130981
setp: 1500, Loss: 0.68833988904953
setp: 1600, Loss: 0.6966930031776428
setp: 1700, Loss: 0.6943420171737671
setp: 1800, Loss: 0.6914998888969421
setp: 1900, Loss: 0.6899688839912415
setp: 2000, Loss: 0.6913686394691467
setp: 2100, Loss: 0.6973540186882019
setp: 2200, Loss: 0.6960241794586182
setp: 2300, Loss: 0.6936475038528442
setp: 2400, Loss: 0.691336452960968
setp: 2500, Loss: 0.6885375380516052
setp: 2600, Loss: 0.6936590671539307
setp: 2700, Loss: 0.6894885897636414
setp: 2800, Loss: 0.6942876577377319
setp: 2900, Loss: 0.6968420147895813
setp: 3000, Loss: 0.6900293827056885
setp: 3100, Loss: 0.6914970278739929
setp: 3200, Loss: 0.6912927627563477
setp: 3300, Loss: 0.6914473176002502
setp: 3400, Loss: 0.6885216236114502
setp: 3500, Loss: 0.6966795921325684
setp: 3600, Loss: 0.6943674683570862
setp: 3700, Loss: 0.6915138959884644
setp: 3800, Loss: 0.6900261044502258
setp: 3900, Loss: 0.6913697123527527
setp: 4000, Loss: 0.697385847568512
setp: 4100, Loss: 0.695982813835144
setp: 4200, Loss: 0.6936404705047607
setp: 4300, Loss: 0.691336452960968
setp: 4400, Loss: 0.6885433793067932
setp: 4500, Loss: 0.6936506032943726
setp: 4600, Loss: 0.6894918084144592
setp: 4700, Loss: 0.6942893862724304
setp: 4800, Loss: 0.6968289017677307
setp: 4900, Loss: 0.6900398135185242
training successfully ended.
validating...
acc: 0.5213815789473685
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.5394736842105263
precision: 0
recall: 0.0
F_score: 0
******fold 3******
[282, 326]
training...
setp: 0, Loss: 0.6933841705322266
setp: 100, Loss: 0.6885893940925598
setp: 200, Loss: 0.6961132287979126
setp: 300, Loss: 0.6149052977561951
setp: 400, Loss: 0.5407852530479431
setp: 500, Loss: 0.4249732494354248
setp: 600, Loss: 0.4244135618209839
setp: 700, Loss: 0.33772802352905273
setp: 800, Loss: 0.3588830530643463
setp: 900, Loss: 0.3263315260410309
setp: 1000, Loss: 0.3384184241294861
setp: 1100, Loss: 0.32529884576797485
setp: 1200, Loss: 0.32246652245521545
setp: 1300, Loss: 0.32154157757759094
setp: 1400, Loss: 0.32909533381462097
setp: 1500, Loss: 0.32692235708236694
setp: 1600, Loss: 0.3324521481990814
setp: 1700, Loss: 0.32470524311065674
setp: 1800, Loss: 0.3292316496372223
setp: 1900, Loss: 0.3227796256542206
setp: 2000, Loss: 0.32129907608032227
setp: 2100, Loss: 0.3196834921836853
setp: 2200, Loss: 0.31958457827568054
setp: 2300, Loss: 0.32168012857437134
setp: 2400, Loss: 0.31930363178253174
setp: 2500, Loss: 0.32011717557907104
setp: 2600, Loss: 0.31780511140823364
setp: 2700, Loss: 0.31980639696121216
setp: 2800, Loss: 0.31969553232192993
setp: 2900, Loss: 0.3209935426712036
setp: 3000, Loss: 0.3189220726490021
setp: 3100, Loss: 0.4037632644176483
setp: 3200, Loss: 0.3761318624019623
setp: 3300, Loss: 0.3426803648471832
setp: 3400, Loss: 0.3301578164100647
setp: 3500, Loss: 0.3254818618297577
setp: 3600, Loss: 0.3353461027145386
setp: 3700, Loss: 0.3456079959869385
setp: 3800, Loss: 0.32475465536117554
setp: 3900, Loss: 0.3238268792629242
setp: 4000, Loss: 0.3295195400714874
setp: 4100, Loss: 0.3204710781574249
setp: 4200, Loss: 0.33245179057121277
setp: 4300, Loss: 0.32058578729629517
setp: 4400, Loss: 0.3223614990711212
setp: 4500, Loss: 0.3204648792743683
setp: 4600, Loss: 0.32237765192985535
setp: 4700, Loss: 0.3222082555294037
setp: 4800, Loss: 0.3230612576007843
setp: 4900, Loss: 0.32183679938316345
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8552631578947368
precision: 0.88
recall: 0.8354430379746836
F_score: 0.8571428571428572
******fold 4******
[299, 309]
training...
setp: 0, Loss: 0.711007297039032
setp: 100, Loss: 0.6431820392608643
setp: 200, Loss: 0.5241144895553589
setp: 300, Loss: 0.4872973561286926
setp: 400, Loss: 0.5282949805259705
setp: 500, Loss: 0.4339917004108429
setp: 600, Loss: 0.5588133335113525
setp: 700, Loss: 0.39648935198783875
setp: 800, Loss: 0.3319619596004486
setp: 900, Loss: 0.3239661157131195
setp: 1000, Loss: 0.3312895894050598
setp: 1100, Loss: 0.3493140637874603
setp: 1200, Loss: 0.3184452950954437
setp: 1300, Loss: 0.3268030285835266
setp: 1400, Loss: 0.32099664211273193
setp: 1500, Loss: 0.32146450877189636
setp: 1600, Loss: 0.317657470703125
setp: 1700, Loss: 0.31760174036026
setp: 1800, Loss: 0.3223625719547272
setp: 1900, Loss: 0.390427827835083
setp: 2000, Loss: 0.31685489416122437
setp: 2100, Loss: 0.31783244013786316
setp: 2200, Loss: 0.3166721761226654
setp: 2300, Loss: 0.3391720950603485
setp: 2400, Loss: 0.32391855120658875
setp: 2500, Loss: 0.3236616551876068
setp: 2600, Loss: 0.31873536109924316
setp: 2700, Loss: 0.31687813997268677
setp: 2800, Loss: 0.31879621744155884
setp: 2900, Loss: 0.31978774070739746
setp: 3000, Loss: 0.32247424125671387
setp: 3100, Loss: 0.3205885589122772
setp: 3200, Loss: 0.31613823771476746
setp: 3300, Loss: 0.3231091797351837
setp: 3400, Loss: 0.32296425104141235
setp: 3500, Loss: 0.32314157485961914
setp: 3600, Loss: 0.31981199979782104
setp: 3700, Loss: 0.3174803853034973
setp: 3800, Loss: 0.33017438650131226
setp: 3900, Loss: 0.359892338514328
setp: 4000, Loss: 0.3157929480075836
setp: 4100, Loss: 0.3165249824523926
setp: 4200, Loss: 0.31820401549339294
setp: 4300, Loss: 0.3173362910747528
setp: 4400, Loss: 0.31837210059165955
setp: 4500, Loss: 0.317452609539032
setp: 4600, Loss: 0.3183128833770752
setp: 4700, Loss: 0.3169950544834137
setp: 4800, Loss: 0.3166356682777405
setp: 4900, Loss: 0.31779131293296814
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8947368421052632
precision: 0.8382352941176471
recall: 0.9193548387096774
F_score: 0.8769230769230769
model saved.
avg_acc: 0.8131578947368421, avg_f_score: 0.6984013576784661
==========arousal==========
******fold 0******
[123, 485]
training...
setp: 0, Loss: 0.724242091178894
setp: 100, Loss: 0.6176446676254272
setp: 200, Loss: 0.5038887858390808
setp: 300, Loss: 0.4490030109882355
setp: 400, Loss: 0.4452959895133972
setp: 500, Loss: 0.3563419580459595
setp: 600, Loss: 0.33852696418762207
setp: 700, Loss: 0.3336687982082367
setp: 800, Loss: 0.34070849418640137
setp: 900, Loss: 0.3196527659893036
setp: 1000, Loss: 0.38430336117744446
setp: 1100, Loss: 0.3194572329521179
setp: 1200, Loss: 0.31883925199508667
setp: 1300, Loss: 0.31868594884872437
setp: 1400, Loss: 0.31757652759552
setp: 1500, Loss: 0.3233136236667633
setp: 1600, Loss: 0.3185550272464752
setp: 1700, Loss: 0.31766167283058167
setp: 1800, Loss: 0.3181089162826538
setp: 1900, Loss: 0.3155200779438019
setp: 2000, Loss: 0.5006294250488281
setp: 2100, Loss: 0.31922316551208496
setp: 2200, Loss: 0.31621187925338745
setp: 2300, Loss: 0.3162805736064911
setp: 2400, Loss: 0.3180604875087738
setp: 2500, Loss: 0.31784188747406006
setp: 2600, Loss: 0.3175380825996399
setp: 2700, Loss: 0.3177964687347412
setp: 2800, Loss: 0.3170587122440338
setp: 2900, Loss: 0.316496342420578
setp: 3000, Loss: 0.31708627939224243
setp: 3100, Loss: 0.7126596570014954
setp: 3200, Loss: 0.5365970730781555
setp: 3300, Loss: 0.39841318130493164
setp: 3400, Loss: 0.3578858971595764
setp: 3500, Loss: 0.3777696490287781
setp: 3600, Loss: 0.3362223207950592
setp: 3700, Loss: 0.3526488244533539
setp: 3800, Loss: 0.36854639649391174
setp: 3900, Loss: 0.3254862129688263
setp: 4000, Loss: 0.3252154290676117
setp: 4100, Loss: 0.32673749327659607
setp: 4200, Loss: 0.3206769526004791
setp: 4300, Loss: 0.34398534893989563
setp: 4400, Loss: 0.3185366094112396
setp: 4500, Loss: 0.3206680417060852
setp: 4600, Loss: 0.3236856460571289
setp: 4700, Loss: 0.3195458650588989
setp: 4800, Loss: 0.31949925422668457
setp: 4900, Loss: 0.31842851638793945
training successfully ended.
validating...
acc: 0.9742268041237113
precision: 0.9509803921568627
recall: 1.0
F_score: 0.9748743718592965
validating...
acc: 0.9342105263157895
precision: 0.7435897435897436
recall: 1.0
F_score: 0.8529411764705882
******fold 1******
[115, 493]
training...
setp: 0, Loss: 0.7066149711608887
setp: 100, Loss: 0.6268830895423889
setp: 200, Loss: 0.46339794993400574
setp: 300, Loss: 0.3932591378688812
setp: 400, Loss: 0.39311859011650085
setp: 500, Loss: 0.36355170607566833
setp: 600, Loss: 0.3409464955329895
setp: 700, Loss: 0.3828293979167938
setp: 800, Loss: 0.3279886245727539
setp: 900, Loss: 0.3185040354728699
setp: 1000, Loss: 0.3225138485431671
setp: 1100, Loss: 0.31705722212791443
setp: 1200, Loss: 0.31609633564949036
setp: 1300, Loss: 0.31645911931991577
setp: 1400, Loss: 0.31968486309051514
setp: 1500, Loss: 0.3191033601760864
setp: 1600, Loss: 0.31842783093452454
setp: 1700, Loss: 0.3197052776813507
setp: 1800, Loss: 0.3165574073791504
setp: 1900, Loss: 0.31918463110923767
setp: 2000, Loss: 0.31690773367881775
setp: 2100, Loss: 0.32347947359085083
setp: 2200, Loss: 0.31648698449134827
setp: 2300, Loss: 0.31685253977775574
setp: 2400, Loss: 0.31820589303970337
setp: 2500, Loss: 0.315987765789032
setp: 2600, Loss: 0.32007431983947754
setp: 2700, Loss: 0.3171721398830414
setp: 2800, Loss: 0.31765422224998474
setp: 2900, Loss: 0.31659212708473206
setp: 3000, Loss: 0.31691989302635193
setp: 3100, Loss: 0.31599101424217224
setp: 3200, Loss: 0.3181511461734772
setp: 3300, Loss: 0.42925113439559937
setp: 3400, Loss: 0.31795212626457214
setp: 3500, Loss: 0.31770971417427063
setp: 3600, Loss: 0.31642863154411316
setp: 3700, Loss: 0.31783920526504517
setp: 3800, Loss: 0.3166683614253998
setp: 3900, Loss: 0.31601861119270325
setp: 4000, Loss: 0.3164716362953186
setp: 4100, Loss: 0.3193974196910858
setp: 4200, Loss: 0.316142737865448
setp: 4300, Loss: 0.31565210223197937
setp: 4400, Loss: 0.31579890847206116
setp: 4500, Loss: 0.31915029883384705
setp: 4600, Loss: 0.3179427981376648
setp: 4700, Loss: 0.3169388473033905
setp: 4800, Loss: 0.3181089460849762
setp: 4900, Loss: 0.31614983081817627
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9736842105263158
precision: 1.0
recall: 0.8918918918918919
F_score: 0.9428571428571428
******fold 2******
[124, 484]
training...
setp: 0, Loss: 0.7371947169303894
setp: 100, Loss: 0.578328549861908
setp: 200, Loss: 0.5388333201408386
setp: 300, Loss: 0.46436959505081177
setp: 400, Loss: 0.4844053089618683
setp: 500, Loss: 0.4827396273612976
setp: 600, Loss: 0.3550027012825012
setp: 700, Loss: 0.34054455161094666
setp: 800, Loss: 0.32719317078590393
setp: 900, Loss: 0.35463082790374756
setp: 1000, Loss: 0.3440777659416199
setp: 1100, Loss: 0.34991857409477234
setp: 1200, Loss: 0.31840085983276367
setp: 1300, Loss: 0.31854090094566345
setp: 1400, Loss: 0.326217383146286
setp: 1500, Loss: 0.3177982568740845
setp: 1600, Loss: 0.317440927028656
setp: 1700, Loss: 0.31882163882255554
setp: 1800, Loss: 0.3163388669490814
setp: 1900, Loss: 0.31658849120140076
setp: 2000, Loss: 0.31757810711860657
setp: 2100, Loss: 0.3160548508167267
setp: 2200, Loss: 0.3152012228965759
setp: 2300, Loss: 0.318313330411911
setp: 2400, Loss: 0.31731685996055603
setp: 2500, Loss: 0.3245372176170349
setp: 2600, Loss: 0.3223439157009125
setp: 2700, Loss: 0.31660032272338867
setp: 2800, Loss: 0.3202141523361206
setp: 2900, Loss: 0.3171800971031189
setp: 3000, Loss: 0.3167048394680023
setp: 3100, Loss: 0.31600630283355713
setp: 3200, Loss: 0.31685376167297363
setp: 3300, Loss: 0.3167649507522583
setp: 3400, Loss: 0.3174121379852295
setp: 3500, Loss: 0.31741219758987427
setp: 3600, Loss: 0.31800350546836853
setp: 3700, Loss: 0.31864529848098755
setp: 3800, Loss: 0.3160206377506256
setp: 3900, Loss: 0.316515177488327
setp: 4000, Loss: 0.31638821959495544
setp: 4100, Loss: 0.4464383125305176
setp: 4200, Loss: 0.40099993348121643
setp: 4300, Loss: 0.337535560131073
setp: 4400, Loss: 0.3239430785179138
setp: 4500, Loss: 0.3190329074859619
setp: 4600, Loss: 0.32005831599235535
setp: 4700, Loss: 0.3206315040588379
setp: 4800, Loss: 0.31693506240844727
setp: 4900, Loss: 0.31637415289878845
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.8214285714285714
recall: 0.8214285714285714
F_score: 0.8214285714285714
******fold 3******
[120, 488]
training...
setp: 0, Loss: 0.7476300597190857
setp: 100, Loss: 0.6507591605186462
setp: 200, Loss: 0.5635329484939575
setp: 300, Loss: 0.5352509021759033
setp: 400, Loss: 0.49165230989456177
setp: 500, Loss: 0.44965025782585144
setp: 600, Loss: 0.3609231412410736
setp: 700, Loss: 0.3454058766365051
setp: 800, Loss: 0.3357039988040924
setp: 900, Loss: 0.36682358384132385
setp: 1000, Loss: 0.3534891903400421
setp: 1100, Loss: 0.3178294003009796
setp: 1200, Loss: 0.3176685869693756
setp: 1300, Loss: 0.32107338309288025
setp: 1400, Loss: 0.32164496183395386
setp: 1500, Loss: 0.3181122839450836
setp: 1600, Loss: 0.31709131598472595
setp: 1700, Loss: 0.3171212673187256
setp: 1800, Loss: 0.3173062205314636
setp: 1900, Loss: 0.3990710377693176
setp: 2000, Loss: 0.3527827262878418
setp: 2100, Loss: 0.3520820736885071
setp: 2200, Loss: 0.31607306003570557
setp: 2300, Loss: 0.3166666328907013
setp: 2400, Loss: 0.32881173491477966
setp: 2500, Loss: 0.3171491026878357
setp: 2600, Loss: 0.3471829891204834
setp: 2700, Loss: 0.31630080938339233
setp: 2800, Loss: 0.3167126178741455
setp: 2900, Loss: 0.31586575508117676
setp: 3000, Loss: 0.31563523411750793
setp: 3100, Loss: 0.31619295477867126
setp: 3200, Loss: 0.3197418451309204
setp: 3300, Loss: 0.3168124258518219
setp: 3400, Loss: 0.3184973895549774
setp: 3500, Loss: 0.3171887993812561
setp: 3600, Loss: 0.3165108263492584
setp: 3700, Loss: 0.3157648742198944
setp: 3800, Loss: 0.3160969018936157
setp: 3900, Loss: 0.3158642053604126
setp: 4000, Loss: 0.31839728355407715
setp: 4100, Loss: 0.3179725706577301
setp: 4200, Loss: 0.3157743811607361
setp: 4300, Loss: 0.3155701756477356
setp: 4400, Loss: 0.31766578555107117
setp: 4500, Loss: 0.4349828064441681
setp: 4600, Loss: 0.31892749667167664
setp: 4700, Loss: 0.3154277205467224
setp: 4800, Loss: 0.3170410096645355
setp: 4900, Loss: 0.31588178873062134
training successfully ended.
validating...
acc: 0.9989754098360656
precision: 0.9979550102249489
recall: 1.0
F_score: 0.9989764585465711
validating...
acc: 0.9539473684210527
precision: 0.9310344827586207
recall: 0.84375
F_score: 0.8852459016393444
******fold 4******
[126, 482]
training...
setp: 0, Loss: 0.703074038028717
setp: 100, Loss: 0.6736380457878113
setp: 200, Loss: 0.5531715750694275
setp: 300, Loss: 0.5951705574989319
setp: 400, Loss: 0.5352293848991394
setp: 500, Loss: 0.4316956102848053
setp: 600, Loss: 0.37933921813964844
setp: 700, Loss: 0.371627539396286
setp: 800, Loss: 0.3238704800605774
setp: 900, Loss: 0.3500451147556305
setp: 1000, Loss: 0.3497045636177063
setp: 1100, Loss: 0.32338207960128784
setp: 1200, Loss: 0.32509657740592957
setp: 1300, Loss: 0.3597452938556671
setp: 1400, Loss: 0.3356563150882721
setp: 1500, Loss: 0.31803831458091736
setp: 1600, Loss: 0.353481650352478
setp: 1700, Loss: 0.31924954056739807
setp: 1800, Loss: 0.3182308077812195
setp: 1900, Loss: 0.3160247206687927
setp: 2000, Loss: 0.3567888140678406
setp: 2100, Loss: 0.3158916234970093
setp: 2200, Loss: 0.315215528011322
setp: 2300, Loss: 0.31531596183776855
setp: 2400, Loss: 0.3168957531452179
setp: 2500, Loss: 0.319845587015152
setp: 2600, Loss: 0.3197155296802521
setp: 2700, Loss: 0.31716468930244446
setp: 2800, Loss: 0.31805548071861267
setp: 2900, Loss: 0.31592023372650146
setp: 3000, Loss: 0.346061646938324
setp: 3100, Loss: 0.41176196932792664
setp: 3200, Loss: 0.31951695680618286
setp: 3300, Loss: 0.31575608253479004
setp: 3400, Loss: 0.3166145086288452
setp: 3500, Loss: 0.3168427050113678
setp: 3600, Loss: 0.31642088294029236
setp: 3700, Loss: 0.31606051325798035
setp: 3800, Loss: 0.3163261115550995
setp: 3900, Loss: 0.3157021403312683
setp: 4000, Loss: 0.3466070890426636
setp: 4100, Loss: 0.3466412425041199
setp: 4200, Loss: 0.3180147707462311
setp: 4300, Loss: 0.31588658690452576
setp: 4400, Loss: 0.39290764927864075
setp: 4500, Loss: 0.34138044714927673
setp: 4600, Loss: 0.31697654724121094
setp: 4700, Loss: 0.31587931513786316
setp: 4800, Loss: 0.31692519783973694
setp: 4900, Loss: 0.3160955607891083
training successfully ended.
validating...
acc: 0.995850622406639
precision: 1.0
recall: 0.991701244813278
F_score: 0.9958333333333333
validating...
acc: 0.9342105263157895
precision: 0.7666666666666667
recall: 0.8846153846153846
F_score: 0.8214285714285715
model saved.
avg_acc: 0.9460526315789475, avg_f_score: 0.8647802727648436
-------------subject: 22-------------
==========valence==========
******fold 0******
[294, 314]
training...
setp: 0, Loss: 0.7021497488021851
setp: 100, Loss: 0.677915632724762
setp: 200, Loss: 0.5687611699104309
setp: 300, Loss: 0.5566149950027466
setp: 400, Loss: 0.4617575705051422
setp: 500, Loss: 0.4004240930080414
setp: 600, Loss: 0.42208534479141235
setp: 700, Loss: 0.36471834778785706
setp: 800, Loss: 0.3618711531162262
setp: 900, Loss: 0.3250066339969635
setp: 1000, Loss: 0.3288578987121582
setp: 1100, Loss: 0.3341120779514313
setp: 1200, Loss: 0.32673874497413635
setp: 1300, Loss: 0.3229285180568695
setp: 1400, Loss: 0.3542550802230835
setp: 1500, Loss: 0.3240586519241333
setp: 1600, Loss: 0.33334973454475403
setp: 1700, Loss: 0.32195499539375305
setp: 1800, Loss: 0.32096946239471436
setp: 1900, Loss: 0.3227957487106323
setp: 2000, Loss: 0.31989744305610657
setp: 2100, Loss: 0.319609135389328
setp: 2200, Loss: 0.3516034185886383
setp: 2300, Loss: 0.32129719853401184
setp: 2400, Loss: 0.3210870921611786
setp: 2500, Loss: 0.3502667248249054
setp: 2600, Loss: 0.35437464714050293
setp: 2700, Loss: 0.3219011723995209
setp: 2800, Loss: 0.31805354356765747
setp: 2900, Loss: 0.31906479597091675
setp: 3000, Loss: 0.31961140036582947
setp: 3100, Loss: 0.31942927837371826
setp: 3200, Loss: 0.31998318433761597
setp: 3300, Loss: 0.35143810510635376
setp: 3400, Loss: 0.31917285919189453
setp: 3500, Loss: 0.3184622824192047
setp: 3600, Loss: 0.3708447217941284
setp: 3700, Loss: 0.3558180630207062
setp: 3800, Loss: 0.3197980523109436
setp: 3900, Loss: 0.31765732169151306
setp: 4000, Loss: 0.31865194439888
setp: 4100, Loss: 0.3501991629600525
setp: 4200, Loss: 0.3199380040168762
setp: 4300, Loss: 0.319240003824234
setp: 4400, Loss: 0.3507443368434906
setp: 4500, Loss: 0.3492383062839508
setp: 4600, Loss: 0.3201596438884735
setp: 4700, Loss: 0.31894680857658386
setp: 4800, Loss: 0.318989634513855
setp: 4900, Loss: 0.31973838806152344
training successfully ended.
validating...
acc: 0.9555921052631579
precision: 0.9619377162629758
recall: 0.9455782312925171
F_score: 0.95368782161235
validating...
acc: 0.9276315789473685
precision: 0.9518072289156626
recall: 0.9186046511627907
F_score: 0.9349112426035502
******fold 1******
[310, 298]
training...
setp: 0, Loss: 0.7606784105300903
setp: 100, Loss: 0.6673664450645447
setp: 200, Loss: 0.5680838823318481
setp: 300, Loss: 0.45933976769447327
setp: 400, Loss: 0.49770280718803406
setp: 500, Loss: 0.40251773595809937
setp: 600, Loss: 0.4679947793483734
setp: 700, Loss: 0.3952092230319977
setp: 800, Loss: 0.37331366539001465
setp: 900, Loss: 0.34234747290611267
setp: 1000, Loss: 0.37406906485557556
setp: 1100, Loss: 0.33825141191482544
setp: 1200, Loss: 0.3422350287437439
setp: 1300, Loss: 0.34184718132019043
setp: 1400, Loss: 0.32698652148246765
setp: 1500, Loss: 0.3522748053073883
setp: 1600, Loss: 0.3479919135570526
setp: 1700, Loss: 0.33617672324180603
setp: 1800, Loss: 0.32171258330345154
setp: 1900, Loss: 0.3550402522087097
setp: 2000, Loss: 0.32073086500167847
setp: 2100, Loss: 0.31965500116348267
setp: 2200, Loss: 0.31832438707351685
setp: 2300, Loss: 0.31853988766670227
setp: 2400, Loss: 0.3199845850467682
setp: 2500, Loss: 0.3228709399700165
setp: 2600, Loss: 0.31874269247055054
setp: 2700, Loss: 0.4263983964920044
setp: 2800, Loss: 0.3311343193054199
setp: 2900, Loss: 0.32627612352371216
setp: 3000, Loss: 0.31991225481033325
setp: 3100, Loss: 0.3190789818763733
setp: 3200, Loss: 0.3175649046897888
setp: 3300, Loss: 0.3188920319080353
setp: 3400, Loss: 0.3200165033340454
setp: 3500, Loss: 0.3213155269622803
setp: 3600, Loss: 0.3185342848300934
setp: 3700, Loss: 0.3189491629600525
setp: 3800, Loss: 0.351998895406723
setp: 3900, Loss: 0.39737972617149353
setp: 4000, Loss: 0.32250306010246277
setp: 4100, Loss: 0.3182227909564972
setp: 4200, Loss: 0.31709516048431396
setp: 4300, Loss: 0.31847530603408813
setp: 4400, Loss: 0.32137078046798706
setp: 4500, Loss: 0.31810206174850464
setp: 4600, Loss: 0.3194577991962433
setp: 4700, Loss: 0.31828945875167847
setp: 4800, Loss: 0.45998212695121765
setp: 4900, Loss: 0.3345251679420471
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9967741935483871
F_score: 0.9983844911147012
validating...
acc: 0.9605263157894737
precision: 0.9571428571428572
recall: 0.9571428571428572
F_score: 0.9571428571428572
******fold 2******
[301, 307]
training...
setp: 0, Loss: 0.6917063593864441
setp: 100, Loss: 0.6903724670410156
setp: 200, Loss: 0.4947119653224945
setp: 300, Loss: 0.4151434600353241
setp: 400, Loss: 0.4080085754394531
setp: 500, Loss: 0.3466845750808716
setp: 600, Loss: 0.3620219826698303
setp: 700, Loss: 0.34158065915107727
setp: 800, Loss: 0.3276049494743347
setp: 900, Loss: 0.3245801031589508
setp: 1000, Loss: 0.3217124044895172
setp: 1100, Loss: 0.3214881420135498
setp: 1200, Loss: 0.32194289565086365
setp: 1300, Loss: 0.3192981481552124
setp: 1400, Loss: 0.3248971104621887
setp: 1500, Loss: 0.3254471719264984
setp: 1600, Loss: 0.3283299207687378
setp: 1700, Loss: 0.31931427121162415
setp: 1800, Loss: 0.3198167681694031
setp: 1900, Loss: 0.3217596113681793
setp: 2000, Loss: 0.3202386796474457
setp: 2100, Loss: 0.3200984299182892
setp: 2200, Loss: 0.3196215331554413
setp: 2300, Loss: 0.3505026698112488
setp: 2400, Loss: 0.3195617198944092
setp: 2500, Loss: 0.32204118371009827
setp: 2600, Loss: 0.44277873635292053
setp: 2700, Loss: 0.37272918224334717
setp: 2800, Loss: 0.3266691267490387
setp: 2900, Loss: 0.32279250025749207
setp: 3000, Loss: 0.3200977146625519
setp: 3100, Loss: 0.3198506832122803
setp: 3200, Loss: 0.3188944160938263
setp: 3300, Loss: 0.322261244058609
setp: 3400, Loss: 0.31884467601776123
setp: 3500, Loss: 0.32030150294303894
setp: 3600, Loss: 0.319855660200119
setp: 3700, Loss: 0.32084277272224426
setp: 3800, Loss: 0.32194456458091736
setp: 3900, Loss: 0.3217450976371765
setp: 4000, Loss: 0.38096192479133606
setp: 4100, Loss: 0.3488251566886902
setp: 4200, Loss: 0.3659980297088623
setp: 4300, Loss: 0.32525476813316345
setp: 4400, Loss: 0.32145681977272034
setp: 4500, Loss: 0.3199019432067871
setp: 4600, Loss: 0.3198455572128296
setp: 4700, Loss: 0.32005301117897034
setp: 4800, Loss: 0.32076773047447205
setp: 4900, Loss: 0.32021355628967285
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9966777408637874
F_score: 0.9983361064891847
validating...
acc: 0.9210526315789473
precision: 0.935064935064935
recall: 0.9113924050632911
F_score: 0.923076923076923
******fold 3******
[302, 306]
training...
setp: 0, Loss: 0.6998064517974854
setp: 100, Loss: 0.6787574887275696
setp: 200, Loss: 0.6542200446128845
setp: 300, Loss: 0.618865430355072
setp: 400, Loss: 0.5145847201347351
setp: 500, Loss: 0.48472362756729126
setp: 600, Loss: 0.43945610523223877
setp: 700, Loss: 0.4238986372947693
setp: 800, Loss: 0.3733932077884674
setp: 900, Loss: 0.36701226234436035
setp: 1000, Loss: 0.4120160937309265
setp: 1100, Loss: 0.3920193910598755
setp: 1200, Loss: 0.3946446180343628
setp: 1300, Loss: 0.31744569540023804
setp: 1400, Loss: 0.41101959347724915
setp: 1500, Loss: 0.38435977697372437
setp: 1600, Loss: 0.35836657881736755
setp: 1700, Loss: 0.35317039489746094
setp: 1800, Loss: 0.35017144680023193
setp: 1900, Loss: 0.35432684421539307
setp: 2000, Loss: 0.36846911907196045
setp: 2100, Loss: 0.35266590118408203
setp: 2200, Loss: 0.3674568831920624
setp: 2300, Loss: 0.35465091466903687
setp: 2400, Loss: 0.34798499941825867
setp: 2500, Loss: 0.3956892788410187
setp: 2600, Loss: 0.3495720624923706
setp: 2700, Loss: 0.3500736355781555
setp: 2800, Loss: 0.3499695956707001
setp: 2900, Loss: 0.38028085231781006
setp: 3000, Loss: 0.37434446811676025
setp: 3100, Loss: 0.3482348918914795
setp: 3200, Loss: 0.31638699769973755
setp: 3300, Loss: 0.3816668391227722
setp: 3400, Loss: 0.32047104835510254
setp: 3500, Loss: 0.3494596481323242
setp: 3600, Loss: 0.3509589731693268
setp: 3700, Loss: 0.348184198141098
setp: 3800, Loss: 0.33441290259361267
setp: 3900, Loss: 0.3535326421260834
setp: 4000, Loss: 0.3546479642391205
setp: 4100, Loss: 0.34789371490478516
setp: 4200, Loss: 0.370780348777771
setp: 4300, Loss: 0.35004639625549316
setp: 4400, Loss: 0.3503750264644623
setp: 4500, Loss: 0.3480879068374634
setp: 4600, Loss: 0.3205721080303192
setp: 4700, Loss: 0.3476967215538025
setp: 4800, Loss: 0.3798180818557739
setp: 4900, Loss: 0.35074546933174133
training successfully ended.
validating...
acc: 0.96875
precision: 0.9964912280701754
recall: 0.9403973509933775
F_score: 0.9676320272572403
validating...
acc: 0.9276315789473685
precision: 0.971830985915493
recall: 0.8846153846153846
F_score: 0.9261744966442953
******fold 4******
[313, 295]
training...
setp: 0, Loss: 0.7062540650367737
setp: 100, Loss: 0.6925843954086304
setp: 200, Loss: 0.581696629524231
setp: 300, Loss: 0.4739550054073334
setp: 400, Loss: 0.4514763355255127
setp: 500, Loss: 0.38170894980430603
setp: 600, Loss: 0.3632770776748657
setp: 700, Loss: 0.331848680973053
setp: 800, Loss: 0.32966747879981995
setp: 900, Loss: 0.32617881894111633
setp: 1000, Loss: 0.3296208381652832
setp: 1100, Loss: 0.3276274800300598
setp: 1200, Loss: 0.32262808084487915
setp: 1300, Loss: 0.32417595386505127
setp: 1400, Loss: 0.32487985491752625
setp: 1500, Loss: 0.32308170199394226
setp: 1600, Loss: 0.322724848985672
setp: 1700, Loss: 0.400797963142395
setp: 1800, Loss: 0.34523600339889526
setp: 1900, Loss: 0.3482312262058258
setp: 2000, Loss: 0.32056570053100586
setp: 2100, Loss: 0.32150712609291077
setp: 2200, Loss: 0.32193508744239807
setp: 2300, Loss: 0.3220716416835785
setp: 2400, Loss: 0.32166847586631775
setp: 2500, Loss: 0.32131528854370117
setp: 2600, Loss: 0.32437920570373535
setp: 2700, Loss: 0.3210395574569702
setp: 2800, Loss: 0.41076910495758057
setp: 2900, Loss: 0.3352387547492981
setp: 3000, Loss: 0.3268716633319855
setp: 3100, Loss: 0.31992149353027344
setp: 3200, Loss: 0.31960704922676086
setp: 3300, Loss: 0.3213651776313782
setp: 3400, Loss: 0.31970587372779846
setp: 3500, Loss: 0.3198181092739105
setp: 3600, Loss: 0.31975042819976807
setp: 3700, Loss: 0.3227947950363159
setp: 3800, Loss: 0.3202027976512909
setp: 3900, Loss: 0.3200787305831909
setp: 4000, Loss: 0.3204571306705475
setp: 4100, Loss: 0.3203166723251343
setp: 4200, Loss: 0.3217085301876068
setp: 4300, Loss: 0.32058051228523254
setp: 4400, Loss: 0.3224596083164215
setp: 4500, Loss: 0.3401847183704376
setp: 4600, Loss: 0.32363420724868774
setp: 4700, Loss: 0.3180372416973114
setp: 4800, Loss: 0.3198949992656708
setp: 4900, Loss: 0.3189372420310974
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9027777777777778
recall: 0.9701492537313433
F_score: 0.9352517985611511
model saved.
avg_acc: 0.9355263157894737, avg_f_score: 0.9353114636057555
==========arousal==========
******fold 0******
[234, 374]
training...
setp: 0, Loss: 0.781631588935852
setp: 100, Loss: 0.6619255542755127
setp: 200, Loss: 0.6468749046325684
setp: 300, Loss: 0.5202471017837524
setp: 400, Loss: 0.5650442838668823
setp: 500, Loss: 0.46236974000930786
setp: 600, Loss: 0.5519568920135498
setp: 700, Loss: 0.4364571273326874
setp: 800, Loss: 0.4669487476348877
setp: 900, Loss: 0.4135439991950989
setp: 1000, Loss: 0.3818041980266571
setp: 1100, Loss: 0.46249648928642273
setp: 1200, Loss: 0.4589947462081909
setp: 1300, Loss: 0.3669993281364441
setp: 1400, Loss: 0.4184008240699768
setp: 1500, Loss: 0.413147896528244
setp: 1600, Loss: 0.3804275393486023
setp: 1700, Loss: 0.38019317388534546
setp: 1800, Loss: 0.4179237484931946
setp: 1900, Loss: 0.3800947070121765
setp: 2000, Loss: 0.397325724363327
setp: 2100, Loss: 0.41272372007369995
setp: 2200, Loss: 0.4124419689178467
setp: 2300, Loss: 0.4104565382003784
setp: 2400, Loss: 0.4117772877216339
setp: 2500, Loss: 0.381105899810791
setp: 2600, Loss: 0.38074538111686707
setp: 2700, Loss: 0.4131718575954437
setp: 2800, Loss: 0.41107380390167236
setp: 2900, Loss: 0.3495762050151825
setp: 3000, Loss: 0.4116075932979584
setp: 3100, Loss: 0.41375380754470825
setp: 3200, Loss: 0.3519928753376007
setp: 3300, Loss: 0.41848552227020264
setp: 3400, Loss: 0.41098451614379883
setp: 3500, Loss: 0.3796839714050293
setp: 3600, Loss: 0.37983450293540955
setp: 3700, Loss: 0.4138036370277405
setp: 3800, Loss: 0.38069355487823486
setp: 3900, Loss: 0.3837879002094269
setp: 4000, Loss: 0.4117804765701294
setp: 4100, Loss: 0.4103909134864807
setp: 4200, Loss: 0.4106975197792053
setp: 4300, Loss: 0.4122631549835205
setp: 4400, Loss: 0.37898722290992737
setp: 4500, Loss: 0.38061296939849854
setp: 4600, Loss: 0.4125223755836487
setp: 4700, Loss: 0.41080737113952637
setp: 4800, Loss: 0.35356175899505615
setp: 4900, Loss: 0.42109522223472595
training successfully ended.
validating...
acc: 0.875
precision: 0.9114583333333334
recall: 0.7478632478632479
F_score: 0.8215962441314554
validating...
acc: 0.8223684210526315
precision: 0.8
recall: 0.6274509803921569
F_score: 0.7032967032967032
******fold 1******
[219, 389]
training...
setp: 0, Loss: 0.6938766241073608
setp: 100, Loss: 0.6436552405357361
setp: 200, Loss: 0.6264135241508484
setp: 300, Loss: 0.5422282218933105
setp: 400, Loss: 0.5393075346946716
setp: 500, Loss: 0.4452851414680481
setp: 600, Loss: 0.4160434305667877
setp: 700, Loss: 0.3683708906173706
setp: 800, Loss: 0.3766331672668457
setp: 900, Loss: 0.35141721367836
setp: 1000, Loss: 0.344678670167923
setp: 1100, Loss: 0.33848461508750916
setp: 1200, Loss: 0.34114691615104675
setp: 1300, Loss: 0.35739418864250183
setp: 1400, Loss: 0.3531433939933777
setp: 1500, Loss: 0.32127976417541504
setp: 1600, Loss: 0.3309798836708069
setp: 1700, Loss: 0.3248051106929779
setp: 1800, Loss: 0.32898616790771484
setp: 1900, Loss: 0.4013199508190155
setp: 2000, Loss: 0.33784669637680054
setp: 2100, Loss: 0.3198665976524353
setp: 2200, Loss: 0.32115915417671204
setp: 2300, Loss: 0.32209211587905884
setp: 2400, Loss: 0.357158362865448
setp: 2500, Loss: 0.3244970738887787
setp: 2600, Loss: 0.3214327394962311
setp: 2700, Loss: 0.35947903990745544
setp: 2800, Loss: 0.3385414779186249
setp: 2900, Loss: 0.33099454641342163
setp: 3000, Loss: 0.32236242294311523
setp: 3100, Loss: 0.3296631872653961
setp: 3200, Loss: 0.3312971591949463
setp: 3300, Loss: 0.32291078567504883
setp: 3400, Loss: 0.31960180401802063
setp: 3500, Loss: 0.3254062831401825
setp: 3600, Loss: 0.32001593708992004
setp: 3700, Loss: 0.3344932794570923
setp: 3800, Loss: 0.3660273849964142
setp: 3900, Loss: 0.32029327750205994
setp: 4000, Loss: 0.3199214041233063
setp: 4100, Loss: 0.327042818069458
setp: 4200, Loss: 0.35169193148612976
setp: 4300, Loss: 0.3208889365196228
setp: 4400, Loss: 0.3210753798484802
setp: 4500, Loss: 0.3252413868904114
setp: 4600, Loss: 0.3480665385723114
setp: 4700, Loss: 0.3239017426967621
setp: 4800, Loss: 0.32073524594306946
setp: 4900, Loss: 0.4135485887527466
training successfully ended.
validating...
acc: 0.9868421052631579
precision: 1.0
recall: 0.9634703196347032
F_score: 0.9813953488372094
validating...
acc: 0.9276315789473685
precision: 1.0
recall: 0.8333333333333334
F_score: 0.9090909090909091
******fold 2******
[228, 380]
training...
setp: 0, Loss: 0.7311453223228455
setp: 100, Loss: 0.6439201235771179
setp: 200, Loss: 0.6370229125022888
setp: 300, Loss: 0.5163295269012451
setp: 400, Loss: 0.45725780725479126
setp: 500, Loss: 0.4909915626049042
setp: 600, Loss: 0.4336046278476715
setp: 700, Loss: 0.39707010984420776
setp: 800, Loss: 0.45875996351242065
setp: 900, Loss: 0.42790132761001587
setp: 1000, Loss: 0.3449121117591858
setp: 1100, Loss: 0.3804781436920166
setp: 1200, Loss: 0.3249285817146301
setp: 1300, Loss: 0.37550362944602966
setp: 1400, Loss: 0.35142436623573303
setp: 1500, Loss: 0.3226293921470642
setp: 1600, Loss: 0.36423009634017944
setp: 1700, Loss: 0.3230384588241577
setp: 1800, Loss: 0.3230375349521637
setp: 1900, Loss: 0.4193148910999298
setp: 2000, Loss: 0.3267187178134918
setp: 2100, Loss: 0.33765482902526855
setp: 2200, Loss: 0.3266015648841858
setp: 2300, Loss: 0.32925328612327576
setp: 2400, Loss: 0.35595041513442993
setp: 2500, Loss: 0.32238420844078064
setp: 2600, Loss: 0.3196159899234772
setp: 2700, Loss: 0.361960232257843
setp: 2800, Loss: 0.321256548166275
setp: 2900, Loss: 0.3206638991832733
setp: 3000, Loss: 0.32584148645401
setp: 3100, Loss: 0.37295323610305786
setp: 3200, Loss: 0.32360175251960754
setp: 3300, Loss: 0.3204449415206909
setp: 3400, Loss: 0.3294885754585266
setp: 3500, Loss: 0.3428070843219757
setp: 3600, Loss: 0.3205544054508209
setp: 3700, Loss: 0.31963852047920227
setp: 3800, Loss: 0.352672278881073
setp: 3900, Loss: 0.3193399906158447
setp: 4000, Loss: 0.32307466864585876
setp: 4100, Loss: 0.32027870416641235
setp: 4200, Loss: 0.32872751355171204
setp: 4300, Loss: 0.3546546399593353
setp: 4400, Loss: 0.34110671281814575
setp: 4500, Loss: 0.31869813799858093
setp: 4600, Loss: 0.32249921560287476
setp: 4700, Loss: 0.32096731662750244
setp: 4800, Loss: 0.32120031118392944
setp: 4900, Loss: 0.320818156003952
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9956140350877193
F_score: 0.9978021978021978
validating...
acc: 0.9802631578947368
precision: 0.9821428571428571
recall: 0.9649122807017544
F_score: 0.9734513274336283
******fold 3******
[230, 378]
training...
setp: 0, Loss: 0.6842147707939148
setp: 100, Loss: 0.6615871787071228
setp: 200, Loss: 0.6742172241210938
setp: 300, Loss: 0.5026015639305115
setp: 400, Loss: 0.5392172932624817
setp: 500, Loss: 0.5126341581344604
setp: 600, Loss: 0.4774339199066162
setp: 700, Loss: 0.41206425428390503
setp: 800, Loss: 0.38255053758621216
setp: 900, Loss: 0.3747289478778839
setp: 1000, Loss: 0.36859941482543945
setp: 1100, Loss: 0.4053318202495575
setp: 1200, Loss: 0.38021761178970337
setp: 1300, Loss: 0.33175596594810486
setp: 1400, Loss: 0.3215036690235138
setp: 1500, Loss: 0.3256855010986328
setp: 1600, Loss: 0.3560545742511749
setp: 1700, Loss: 0.33041274547576904
setp: 1800, Loss: 0.3199465572834015
setp: 1900, Loss: 0.3556017577648163
setp: 2000, Loss: 0.3220795691013336
setp: 2100, Loss: 0.32622092962265015
setp: 2200, Loss: 0.3268338143825531
setp: 2300, Loss: 0.31926992535591125
setp: 2400, Loss: 0.31926387548446655
setp: 2500, Loss: 0.3450596034526825
setp: 2600, Loss: 0.3366842567920685
setp: 2700, Loss: 0.3251228630542755
setp: 2800, Loss: 0.3208458721637726
setp: 2900, Loss: 0.33948764204978943
setp: 3000, Loss: 0.32019248604774475
setp: 3100, Loss: 0.34909674525260925
setp: 3200, Loss: 0.3239988684654236
setp: 3300, Loss: 0.3230005204677582
setp: 3400, Loss: 0.31759119033813477
setp: 3500, Loss: 0.3502408266067505
setp: 3600, Loss: 0.32014191150665283
setp: 3700, Loss: 0.4588778018951416
setp: 3800, Loss: 0.3624880611896515
setp: 3900, Loss: 0.31726324558258057
setp: 4000, Loss: 0.31722670793533325
setp: 4100, Loss: 0.3185975253582001
setp: 4200, Loss: 0.3179970979690552
setp: 4300, Loss: 0.31923532485961914
setp: 4400, Loss: 0.3299466669559479
setp: 4500, Loss: 0.3210009038448334
setp: 4600, Loss: 0.41501089930534363
setp: 4700, Loss: 0.3241976499557495
setp: 4800, Loss: 0.31869494915008545
setp: 4900, Loss: 0.31964531540870667
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 1.0
recall: 0.9869565217391304
F_score: 0.9934354485776805
validating...
acc: 0.9473684210526315
precision: 0.9607843137254902
recall: 0.8909090909090909
F_score: 0.9245283018867925
******fold 4******
[229, 379]
training...
setp: 0, Loss: 0.6622788310050964
setp: 100, Loss: 0.672799289226532
setp: 200, Loss: 0.5104352235794067
setp: 300, Loss: 0.5132415890693665
setp: 400, Loss: 0.40317025780677795
setp: 500, Loss: 0.4252353310585022
setp: 600, Loss: 0.3329007029533386
setp: 700, Loss: 0.32464516162872314
setp: 800, Loss: 0.3763909935951233
setp: 900, Loss: 0.3282534182071686
setp: 1000, Loss: 0.32085463404655457
setp: 1100, Loss: 0.34372732043266296
setp: 1200, Loss: 0.40935850143432617
setp: 1300, Loss: 0.3202456831932068
setp: 1400, Loss: 0.4324904680252075
setp: 1500, Loss: 0.3218390941619873
setp: 1600, Loss: 0.3487679958343506
setp: 1700, Loss: 0.3172953426837921
setp: 1800, Loss: 0.31927281618118286
setp: 1900, Loss: 0.3194189965724945
setp: 2000, Loss: 0.6584880352020264
setp: 2100, Loss: 0.3294583261013031
setp: 2200, Loss: 0.3933764696121216
setp: 2300, Loss: 0.3199089467525482
setp: 2400, Loss: 0.35926806926727295
setp: 2500, Loss: 0.318781316280365
setp: 2600, Loss: 0.31701353192329407
setp: 2700, Loss: 0.31890371441841125
setp: 2800, Loss: 0.32200467586517334
setp: 2900, Loss: 0.3190874755382538
setp: 3000, Loss: 0.3166470229625702
setp: 3100, Loss: 0.350737601518631
setp: 3200, Loss: 0.31691649556159973
setp: 3300, Loss: 0.34846463799476624
setp: 3400, Loss: 0.31825628876686096
setp: 3500, Loss: 0.3186986744403839
setp: 3600, Loss: 0.3174864947795868
setp: 3700, Loss: 0.33508944511413574
setp: 3800, Loss: 0.3186606168746948
setp: 3900, Loss: 0.3337024748325348
setp: 4000, Loss: 0.3173506259918213
setp: 4100, Loss: 0.31843888759613037
setp: 4200, Loss: 0.31747308373451233
setp: 4300, Loss: 0.3184591233730316
setp: 4400, Loss: 0.31797003746032715
setp: 4500, Loss: 0.3203732371330261
setp: 4600, Loss: 0.32935941219329834
setp: 4700, Loss: 0.31892964243888855
setp: 4800, Loss: 0.31768834590911865
setp: 4900, Loss: 0.3171643018722534
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9956331877729258
F_score: 0.9978118161925601
validating...
acc: 0.9539473684210527
precision: 0.9454545454545454
recall: 0.9285714285714286
F_score: 0.9369369369369368
model saved.
avg_acc: 0.9263157894736842, avg_f_score: 0.8894608357289939
-------------subject: 23-------------
==========valence==========
******fold 0******
[195, 413]
training...
setp: 0, Loss: 0.6123309135437012
setp: 100, Loss: 0.6376011371612549
setp: 200, Loss: 0.49753260612487793
setp: 300, Loss: 0.5553556680679321
setp: 400, Loss: 0.4691760540008545
setp: 500, Loss: 0.5130882859230042
setp: 600, Loss: 0.5034806132316589
setp: 700, Loss: 0.48461976647377014
setp: 800, Loss: 0.44339892268180847
setp: 900, Loss: 0.4261211156845093
setp: 1000, Loss: 0.45910879969596863
setp: 1100, Loss: 0.44352713227272034
setp: 1200, Loss: 0.45414409041404724
setp: 1300, Loss: 0.41028648614883423
setp: 1400, Loss: 0.3481624722480774
setp: 1500, Loss: 0.4116829037666321
setp: 1600, Loss: 0.4074100852012634
setp: 1700, Loss: 0.3857382535934448
setp: 1800, Loss: 0.3587581515312195
setp: 1900, Loss: 0.3246608376502991
setp: 2000, Loss: 0.34664323925971985
setp: 2100, Loss: 0.34844186902046204
setp: 2200, Loss: 0.3509831726551056
setp: 2300, Loss: 0.3503779470920563
setp: 2400, Loss: 0.35568904876708984
setp: 2500, Loss: 0.31697362661361694
setp: 2600, Loss: 0.32213735580444336
setp: 2700, Loss: 0.3195219337940216
setp: 2800, Loss: 0.3215644657611847
setp: 2900, Loss: 0.3185805678367615
setp: 3000, Loss: 0.3483010232448578
setp: 3100, Loss: 0.3176441192626953
setp: 3200, Loss: 0.31853073835372925
setp: 3300, Loss: 0.3158288896083832
setp: 3400, Loss: 0.3176286518573761
setp: 3500, Loss: 0.3178562819957733
setp: 3600, Loss: 0.4886242747306824
setp: 3700, Loss: 0.5749561786651611
setp: 3800, Loss: 0.3821863830089569
setp: 3900, Loss: 0.42895179986953735
setp: 4000, Loss: 0.3961487412452698
setp: 4100, Loss: 0.35202935338020325
setp: 4200, Loss: 0.3668070435523987
setp: 4300, Loss: 0.3312201201915741
setp: 4400, Loss: 0.3203286826610565
setp: 4500, Loss: 0.32007819414138794
setp: 4600, Loss: 0.3252239227294922
setp: 4700, Loss: 0.3189550042152405
setp: 4800, Loss: 0.31811195611953735
setp: 4900, Loss: 0.31798601150512695
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.9897435897435898
F_score: 0.9948453608247423
validating...
acc: 0.9868421052631579
precision: 0.9807692307692307
recall: 0.9807692307692307
F_score: 0.9807692307692307
******fold 1******
[202, 406]
training...
setp: 0, Loss: 0.6728485822677612
setp: 100, Loss: 0.6174890995025635
setp: 200, Loss: 0.5819224715232849
setp: 300, Loss: 0.4984743595123291
setp: 400, Loss: 0.5233058333396912
setp: 500, Loss: 0.4621813893318176
setp: 600, Loss: 0.4753420650959015
setp: 700, Loss: 0.5129549503326416
setp: 800, Loss: 0.42051079869270325
setp: 900, Loss: 0.4384101629257202
setp: 1000, Loss: 0.405764102935791
setp: 1100, Loss: 0.47242093086242676
setp: 1200, Loss: 0.4069822430610657
setp: 1300, Loss: 0.417918860912323
setp: 1400, Loss: 0.4123106896877289
setp: 1500, Loss: 0.41113606095314026
setp: 1600, Loss: 0.38322845101356506
setp: 1700, Loss: 0.38587677478790283
setp: 1800, Loss: 0.48196110129356384
setp: 1900, Loss: 0.42690780758857727
setp: 2000, Loss: 0.3779968321323395
setp: 2100, Loss: 0.44017744064331055
setp: 2200, Loss: 0.4096517860889435
setp: 2300, Loss: 0.40976616740226746
setp: 2400, Loss: 0.37989529967308044
setp: 2500, Loss: 0.40935850143432617
setp: 2600, Loss: 0.4407649636268616
setp: 2700, Loss: 0.41033005714416504
setp: 2800, Loss: 0.4153885543346405
setp: 2900, Loss: 0.42638471722602844
setp: 3000, Loss: 0.43367892503738403
setp: 3100, Loss: 0.3813527226448059
setp: 3200, Loss: 0.41167157888412476
setp: 3300, Loss: 0.3790062665939331
setp: 3400, Loss: 0.40018296241760254
setp: 3500, Loss: 0.3827446699142456
setp: 3600, Loss: 0.3460884690284729
setp: 3700, Loss: 0.40984803438186646
setp: 3800, Loss: 0.38366198539733887
setp: 3900, Loss: 0.363831490278244
setp: 4000, Loss: 0.3820212185382843
setp: 4100, Loss: 0.3785661458969116
setp: 4200, Loss: 0.3486489951610565
setp: 4300, Loss: 0.40519189834594727
setp: 4400, Loss: 0.35001450777053833
setp: 4500, Loss: 0.3168312907218933
setp: 4600, Loss: 0.3495956063270569
setp: 4700, Loss: 0.3203210234642029
setp: 4800, Loss: 0.3184453547000885
setp: 4900, Loss: 0.31893500685691833
training successfully ended.
validating...
acc: 0.9572368421052632
precision: 0.9150943396226415
recall: 0.9603960396039604
F_score: 0.9371980676328503
validating...
acc: 0.8618421052631579
precision: 0.7
recall: 0.9333333333333333
F_score: 0.8
******fold 2******
[198, 410]
training...
setp: 0, Loss: 0.6437428593635559
setp: 100, Loss: 0.5948911905288696
setp: 200, Loss: 0.5417320132255554
setp: 300, Loss: 0.5531020164489746
setp: 400, Loss: 0.49172553420066833
setp: 500, Loss: 0.5139736533164978
setp: 600, Loss: 0.5199318528175354
setp: 700, Loss: 0.4719858169555664
setp: 800, Loss: 0.4186096489429474
setp: 900, Loss: 0.4509894549846649
setp: 1000, Loss: 0.41037577390670776
setp: 1100, Loss: 0.5163169503211975
setp: 1200, Loss: 0.4039006233215332
setp: 1300, Loss: 0.3803633153438568
setp: 1400, Loss: 0.44624367356300354
setp: 1500, Loss: 0.35678955912590027
setp: 1600, Loss: 0.4438389837741852
setp: 1700, Loss: 0.40916046500205994
setp: 1800, Loss: 0.37920868396759033
setp: 1900, Loss: 0.37796443700790405
setp: 2000, Loss: 0.3874529004096985
setp: 2100, Loss: 0.4212803542613983
setp: 2200, Loss: 0.4100295305252075
setp: 2300, Loss: 0.37928998470306396
setp: 2400, Loss: 0.4093969166278839
setp: 2500, Loss: 0.4116267263889313
setp: 2600, Loss: 0.38970473408699036
setp: 2700, Loss: 0.40980038046836853
setp: 2800, Loss: 0.40937289595603943
setp: 2900, Loss: 0.37773066759109497
setp: 3000, Loss: 0.408683180809021
setp: 3100, Loss: 0.37852367758750916
setp: 3200, Loss: 0.3867441713809967
setp: 3300, Loss: 0.40876665711402893
setp: 3400, Loss: 0.34958311915397644
setp: 3500, Loss: 0.441755473613739
setp: 3600, Loss: 0.40974780917167664
setp: 3700, Loss: 0.3781031668186188
setp: 3800, Loss: 0.37805071473121643
setp: 3900, Loss: 0.37821608781814575
setp: 4000, Loss: 0.3785243630409241
setp: 4100, Loss: 0.41098228096961975
setp: 4200, Loss: 0.3786541521549225
setp: 4300, Loss: 0.41152843832969666
setp: 4400, Loss: 0.45735377073287964
setp: 4500, Loss: 0.4952774941921234
setp: 4600, Loss: 0.3795403242111206
setp: 4700, Loss: 0.4134955406188965
setp: 4800, Loss: 0.3773517608642578
setp: 4900, Loss: 0.3795678913593292
training successfully ended.
validating...
acc: 0.9194078947368421
precision: 1.0
recall: 0.7525252525252525
F_score: 0.8587896253602305
validating...
acc: 0.8618421052631579
precision: 1.0
recall: 0.5714285714285714
F_score: 0.7272727272727273
******fold 3******
[188, 420]
training...
setp: 0, Loss: 0.7913939356803894
setp: 100, Loss: 0.5962668657302856
setp: 200, Loss: 0.6203801035881042
setp: 300, Loss: 0.5844168663024902
setp: 400, Loss: 0.4811943769454956
setp: 500, Loss: 0.481855183839798
setp: 600, Loss: 0.4626646041870117
setp: 700, Loss: 0.4871583878993988
setp: 800, Loss: 0.46529385447502136
setp: 900, Loss: 0.4696469008922577
setp: 1000, Loss: 0.4956579804420471
setp: 1100, Loss: 0.4372403025627136
setp: 1200, Loss: 0.45350193977355957
setp: 1300, Loss: 0.4094255566596985
setp: 1400, Loss: 0.4414338171482086
setp: 1500, Loss: 0.474532812833786
setp: 1600, Loss: 0.45193028450012207
setp: 1700, Loss: 0.43363717198371887
setp: 1800, Loss: 0.410705029964447
setp: 1900, Loss: 0.352322518825531
setp: 2000, Loss: 0.3522901237010956
setp: 2100, Loss: 0.33218175172805786
setp: 2200, Loss: 0.31807929277420044
setp: 2300, Loss: 0.3477623164653778
setp: 2400, Loss: 0.34902793169021606
setp: 2500, Loss: 0.35052990913391113
setp: 2600, Loss: 0.3485819399356842
setp: 2700, Loss: 0.32034873962402344
setp: 2800, Loss: 0.3498651087284088
setp: 2900, Loss: 0.34804853796958923
setp: 3000, Loss: 0.31630054116249084
setp: 3100, Loss: 0.34777575731277466
setp: 3200, Loss: 0.34704896807670593
setp: 3300, Loss: 0.3489091992378235
setp: 3400, Loss: 0.3484992980957031
setp: 3500, Loss: 0.34813982248306274
setp: 3600, Loss: 0.34764614701271057
setp: 3700, Loss: 0.3783144950866699
setp: 3800, Loss: 0.5465437769889832
setp: 3900, Loss: 0.41395920515060425
setp: 4000, Loss: 0.3224109709262848
setp: 4100, Loss: 0.3253345787525177
setp: 4200, Loss: 0.38791623711586
setp: 4300, Loss: 0.3500174582004547
setp: 4400, Loss: 0.34896254539489746
setp: 4500, Loss: 0.34702593088150024
setp: 4600, Loss: 0.3183307945728302
setp: 4700, Loss: 0.34835293889045715
setp: 4800, Loss: 0.346718966960907
setp: 4900, Loss: 0.31719571352005005
training successfully ended.
validating...
acc: 0.9736842105263158
precision: 0.9257425742574258
recall: 0.9946808510638298
F_score: 0.958974358974359
validating...
acc: 0.8947368421052632
precision: 0.864406779661017
recall: 0.864406779661017
F_score: 0.864406779661017
******fold 4******
[205, 403]
training...
setp: 0, Loss: 0.718517541885376
setp: 100, Loss: 0.6235983967781067
setp: 200, Loss: 0.5287495255470276
setp: 300, Loss: 0.5195925831794739
setp: 400, Loss: 0.5336778163909912
setp: 500, Loss: 0.48624134063720703
setp: 600, Loss: 0.48549601435661316
setp: 700, Loss: 0.42833369970321655
setp: 800, Loss: 0.43749451637268066
setp: 900, Loss: 0.4012085497379303
setp: 1000, Loss: 0.45652395486831665
setp: 1100, Loss: 0.41616198420524597
setp: 1200, Loss: 0.3375523090362549
setp: 1300, Loss: 0.32163673639297485
setp: 1400, Loss: 0.40346360206604004
setp: 1500, Loss: 0.31785786151885986
setp: 1600, Loss: 0.3163728713989258
setp: 1700, Loss: 0.3175975978374481
setp: 1800, Loss: 0.3199211657047272
setp: 1900, Loss: 0.3198164999485016
setp: 2000, Loss: 0.3216250538825989
setp: 2100, Loss: 0.3174394965171814
setp: 2200, Loss: 0.31726643443107605
setp: 2300, Loss: 0.3481156826019287
setp: 2400, Loss: 0.31696221232414246
setp: 2500, Loss: 0.3171057105064392
setp: 2600, Loss: 0.3167561888694763
setp: 2700, Loss: 0.3173510730266571
setp: 2800, Loss: 0.3169213831424713
setp: 2900, Loss: 0.3201810121536255
setp: 3000, Loss: 0.31793931126594543
setp: 3100, Loss: 0.31691962480545044
setp: 3200, Loss: 0.3161380887031555
setp: 3300, Loss: 0.9974654316902161
setp: 3400, Loss: 0.7818393707275391
setp: 3500, Loss: 0.5958845019340515
setp: 3600, Loss: 0.5251709222793579
setp: 3700, Loss: 0.535964846611023
setp: 3800, Loss: 0.499380499124527
setp: 3900, Loss: 0.4790438413619995
setp: 4000, Loss: 0.43896111845970154
setp: 4100, Loss: 0.4229024648666382
setp: 4200, Loss: 0.4501293897628784
setp: 4300, Loss: 0.4187215268611908
setp: 4400, Loss: 0.41554585099220276
setp: 4500, Loss: 0.353274405002594
setp: 4600, Loss: 0.42617952823638916
setp: 4700, Loss: 0.36089831590652466
setp: 4800, Loss: 0.3917231261730194
setp: 4900, Loss: 0.3503327965736389
training successfully ended.
validating...
acc: 0.9638157894736842
precision: 0.9103139013452914
recall: 0.9902439024390244
F_score: 0.9485981308411214
validating...
acc: 0.9144736842105263
precision: 0.7959183673469388
recall: 0.9285714285714286
F_score: 0.8571428571428572
model saved.
avg_acc: 0.9039473684210526, avg_f_score: 0.8459183189691665
==========arousal==========
******fold 0******
[403, 205]
training...
setp: 0, Loss: 0.6477560997009277
setp: 100, Loss: 0.6227635145187378
setp: 200, Loss: 0.6574423313140869
setp: 300, Loss: 0.4024595618247986
setp: 400, Loss: 0.3663855195045471
setp: 500, Loss: 0.32973629236221313
setp: 600, Loss: 0.32523927092552185
setp: 700, Loss: 0.38278740644454956
setp: 800, Loss: 0.32445406913757324
setp: 900, Loss: 0.3193211555480957
setp: 1000, Loss: 0.32293689250946045
setp: 1100, Loss: 0.32759490609169006
setp: 1200, Loss: 0.31805166602134705
setp: 1300, Loss: 0.318493515253067
setp: 1400, Loss: 0.3193819224834442
setp: 1500, Loss: 0.3187350034713745
setp: 1600, Loss: 0.31920719146728516
setp: 1700, Loss: 0.3184323310852051
setp: 1800, Loss: 0.31946125626564026
setp: 1900, Loss: 0.31877127289772034
setp: 2000, Loss: 0.31961753964424133
setp: 2100, Loss: 0.32073140144348145
setp: 2200, Loss: 0.3186262249946594
setp: 2300, Loss: 0.31877195835113525
setp: 2400, Loss: 0.34931403398513794
setp: 2500, Loss: 0.3290277123451233
setp: 2600, Loss: 0.3215329945087433
setp: 2700, Loss: 0.3189622163772583
setp: 2800, Loss: 0.32440271973609924
setp: 2900, Loss: 0.3184972107410431
setp: 3000, Loss: 0.31833919882774353
setp: 3100, Loss: 0.31794318556785583
setp: 3200, Loss: 0.31803086400032043
setp: 3300, Loss: 0.31915003061294556
setp: 3400, Loss: 0.3185173273086548
setp: 3500, Loss: 0.3191376328468323
setp: 3600, Loss: 0.3185614347457886
setp: 3700, Loss: 0.31919777393341064
setp: 3800, Loss: 0.3188650906085968
setp: 3900, Loss: 0.31973549723625183
setp: 4000, Loss: 0.32053154706954956
setp: 4100, Loss: 0.31884753704071045
setp: 4200, Loss: 0.3189968764781952
setp: 4300, Loss: 0.3192848563194275
setp: 4400, Loss: 0.31820422410964966
setp: 4500, Loss: 0.712277889251709
setp: 4600, Loss: 0.5129827260971069
setp: 4700, Loss: 0.3445174992084503
setp: 4800, Loss: 0.3368496000766754
setp: 4900, Loss: 0.32780376076698303
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.8881578947368421
precision: 0.911504424778761
recall: 0.9363636363636364
F_score: 0.9237668161434978
******fold 1******
[423, 185]
training...
setp: 0, Loss: 0.6690891981124878
setp: 100, Loss: 0.5948432087898254
setp: 200, Loss: 0.5949655771255493
setp: 300, Loss: 0.6721361875534058
setp: 400, Loss: 0.5695146322250366
setp: 500, Loss: 0.5948421359062195
setp: 600, Loss: 0.6213797330856323
setp: 700, Loss: 0.5708484053611755
setp: 800, Loss: 0.6478086113929749
setp: 900, Loss: 0.6484958529472351
setp: 1000, Loss: 0.6213459372520447
setp: 1100, Loss: 0.5713159441947937
setp: 1200, Loss: 0.6750545501708984
setp: 1300, Loss: 0.6485021710395813
setp: 1400, Loss: 0.6212931275367737
setp: 1500, Loss: 0.546190619468689
setp: 1600, Loss: 0.6216073036193848
setp: 1700, Loss: 0.6215566992759705
setp: 1800, Loss: 0.621235191822052
setp: 1900, Loss: 0.6211664080619812
setp: 2000, Loss: 0.5947732329368591
setp: 2100, Loss: 0.5949567556381226
setp: 2200, Loss: 0.6723150610923767
setp: 2300, Loss: 0.569398820400238
setp: 2400, Loss: 0.5947470664978027
setp: 2500, Loss: 0.6214127540588379
setp: 2600, Loss: 0.5711020231246948
setp: 2700, Loss: 0.6480724811553955
setp: 2800, Loss: 0.6487153172492981
setp: 2900, Loss: 0.6213215589523315
setp: 3000, Loss: 0.571324348449707
setp: 3100, Loss: 0.6753686666488647
setp: 3200, Loss: 0.6485394835472107
setp: 3300, Loss: 0.621292769908905
setp: 3400, Loss: 0.5463137030601501
setp: 3500, Loss: 0.6216411590576172
setp: 3600, Loss: 0.6215559244155884
setp: 3700, Loss: 0.621216893196106
setp: 3800, Loss: 0.6211766004562378
setp: 3900, Loss: 0.5947381854057312
setp: 4000, Loss: 0.5950337648391724
setp: 4100, Loss: 0.6724643707275391
setp: 4200, Loss: 0.5693516731262207
setp: 4300, Loss: 0.5947456359863281
setp: 4400, Loss: 0.6213957667350769
setp: 4500, Loss: 0.570999801158905
setp: 4600, Loss: 0.6480004191398621
setp: 4700, Loss: 0.6486237049102783
setp: 4800, Loss: 0.6213387846946716
setp: 4900, Loss: 0.5714337825775146
training successfully ended.
validating...
acc: 0.6957236842105263
precision: 0.6957236842105263
recall: 1.0
F_score: 0.8205625606207566
validating...
acc: 0.5921052631578947
precision: 0.5921052631578947
recall: 1.0
F_score: 0.743801652892562
******fold 2******
[401, 207]
training...
setp: 0, Loss: 0.7466813325881958
setp: 100, Loss: 0.707548201084137
setp: 200, Loss: 0.6386153101921082
setp: 300, Loss: 0.5585796236991882
setp: 400, Loss: 0.4019443094730377
setp: 500, Loss: 0.4080374538898468
setp: 600, Loss: 0.36874711513519287
setp: 700, Loss: 0.360406756401062
setp: 800, Loss: 0.3210199475288391
setp: 900, Loss: 0.3199025094509125
setp: 1000, Loss: 0.3179667890071869
setp: 1100, Loss: 0.3199141025543213
setp: 1200, Loss: 0.31632646918296814
setp: 1300, Loss: 0.31735679507255554
setp: 1400, Loss: 0.32964667677879333
setp: 1500, Loss: 0.3710004389286041
setp: 1600, Loss: 0.31693166494369507
setp: 1700, Loss: 0.31694960594177246
setp: 1800, Loss: 0.31753748655319214
setp: 1900, Loss: 0.3174573481082916
setp: 2000, Loss: 0.31879064440727234
setp: 2100, Loss: 0.3171727955341339
setp: 2200, Loss: 0.31817564368247986
setp: 2300, Loss: 0.3169913589954376
setp: 2400, Loss: 0.3165513277053833
setp: 2500, Loss: 0.31838148832321167
setp: 2600, Loss: 0.3188393712043762
setp: 2700, Loss: 0.3203374147415161
setp: 2800, Loss: 0.31838375329971313
setp: 2900, Loss: 0.316195547580719
setp: 3000, Loss: 0.3167791962623596
setp: 3100, Loss: 0.3157324492931366
setp: 3200, Loss: 0.31719306111335754
setp: 3300, Loss: 0.3179927170276642
setp: 3400, Loss: 0.31671789288520813
setp: 3500, Loss: 0.31672659516334534
setp: 3600, Loss: 0.37132805585861206
setp: 3700, Loss: 0.3322235345840454
setp: 3800, Loss: 0.31885865330696106
setp: 3900, Loss: 0.34053903818130493
setp: 4000, Loss: 0.3171493411064148
setp: 4100, Loss: 0.31719446182250977
setp: 4200, Loss: 0.3167397975921631
setp: 4300, Loss: 0.3165857791900635
setp: 4400, Loss: 0.5684226155281067
setp: 4500, Loss: 0.3434508442878723
setp: 4600, Loss: 0.32612523436546326
setp: 4700, Loss: 0.3243331015110016
setp: 4800, Loss: 0.3234959542751312
setp: 4900, Loss: 0.3242473304271698
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9925742574257426
recall: 1.0
F_score: 0.9962732919254659
validating...
acc: 0.9407894736842105
precision: 0.9478260869565217
recall: 0.9732142857142857
F_score: 0.960352422907489
******fold 3******
[412, 196]
training...
setp: 0, Loss: 0.674558162689209
setp: 100, Loss: 0.5962169170379639
setp: 200, Loss: 0.6010070443153381
setp: 300, Loss: 0.4508889317512512
setp: 400, Loss: 0.40398210287094116
setp: 500, Loss: 0.3801225423812866
setp: 600, Loss: 0.3327740728855133
setp: 700, Loss: 0.358455628156662
setp: 800, Loss: 0.3206768333911896
setp: 900, Loss: 0.3195861876010895
setp: 1000, Loss: 0.32096385955810547
setp: 1100, Loss: 0.31948599219322205
setp: 1200, Loss: 0.31841912865638733
setp: 1300, Loss: 0.3231702744960785
setp: 1400, Loss: 0.32025471329689026
setp: 1500, Loss: 0.31800130009651184
setp: 1600, Loss: 0.3180060088634491
setp: 1700, Loss: 0.35991615056991577
setp: 1800, Loss: 0.37169796228408813
setp: 1900, Loss: 0.31840038299560547
setp: 2000, Loss: 0.3172956109046936
setp: 2100, Loss: 0.317732572555542
setp: 2200, Loss: 0.3172963559627533
setp: 2300, Loss: 0.3178994953632355
setp: 2400, Loss: 0.3179793655872345
setp: 2500, Loss: 0.3210001587867737
setp: 2600, Loss: 0.3179882764816284
setp: 2700, Loss: 0.3159918189048767
setp: 2800, Loss: 0.31669166684150696
setp: 2900, Loss: 0.3173997104167938
setp: 3000, Loss: 0.3172779679298401
setp: 3100, Loss: 0.3166695833206177
setp: 3200, Loss: 0.3209909498691559
setp: 3300, Loss: 0.32054775953292847
setp: 3400, Loss: 0.31801459193229675
setp: 3500, Loss: 0.31817305088043213
setp: 3600, Loss: 0.3195323944091797
setp: 3700, Loss: 0.3493853211402893
setp: 3800, Loss: 0.31849557161331177
setp: 3900, Loss: 0.3174288272857666
setp: 4000, Loss: 0.31801310181617737
setp: 4100, Loss: 0.31731632351875305
setp: 4200, Loss: 0.3178512156009674
setp: 4300, Loss: 0.31809496879577637
setp: 4400, Loss: 0.3620206415653229
setp: 4500, Loss: 0.35252755880355835
setp: 4600, Loss: 0.3255653977394104
setp: 4700, Loss: 0.3214546740055084
setp: 4800, Loss: 0.3217013478279114
setp: 4900, Loss: 0.32148462533950806
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9975786924939467
recall: 1.0
F_score: 0.9987878787878788
validating...
acc: 0.9144736842105263
precision: 0.9
recall: 0.9801980198019802
F_score: 0.9383886255924171
******fold 4******
[413, 195]
training...
setp: 0, Loss: 0.6541927456855774
setp: 100, Loss: 0.6430901288986206
setp: 200, Loss: 0.6078155636787415
setp: 300, Loss: 0.5612970590591431
setp: 400, Loss: 0.40617433190345764
setp: 500, Loss: 0.35034793615341187
setp: 600, Loss: 0.3261125087738037
setp: 700, Loss: 0.3262786567211151
setp: 800, Loss: 0.32279670238494873
setp: 900, Loss: 0.3282541036605835
setp: 1000, Loss: 0.3473372459411621
setp: 1100, Loss: 0.323039710521698
setp: 1200, Loss: 0.319608211517334
setp: 1300, Loss: 0.32069170475006104
setp: 1400, Loss: 0.3199697434902191
setp: 1500, Loss: 0.3200920820236206
setp: 1600, Loss: 0.3205634653568268
setp: 1700, Loss: 0.3201233148574829
setp: 1800, Loss: 0.32048362493515015
setp: 1900, Loss: 0.3204953074455261
setp: 2000, Loss: 0.32048818469047546
setp: 2100, Loss: 0.31992748379707336
setp: 2200, Loss: 0.3906639814376831
setp: 2300, Loss: 0.32137084007263184
setp: 2400, Loss: 0.31853607296943665
setp: 2500, Loss: 0.3179868757724762
setp: 2600, Loss: 0.34896358847618103
setp: 2700, Loss: 0.319035142660141
setp: 2800, Loss: 0.32009783387184143
setp: 2900, Loss: 0.3224669396877289
setp: 3000, Loss: 0.3192691504955292
setp: 3100, Loss: 0.3188755214214325
setp: 3200, Loss: 0.320266991853714
setp: 3300, Loss: 0.31957584619522095
setp: 3400, Loss: 0.3192604184150696
setp: 3500, Loss: 0.31934890151023865
setp: 3600, Loss: 0.31869569420814514
setp: 3700, Loss: 0.32061100006103516
setp: 3800, Loss: 0.3198319375514984
setp: 3900, Loss: 0.6458678841590881
setp: 4000, Loss: 0.39490604400634766
setp: 4100, Loss: 0.3559677004814148
setp: 4200, Loss: 0.3243734538555145
setp: 4300, Loss: 0.323167085647583
setp: 4400, Loss: 0.3220883011817932
setp: 4500, Loss: 0.3232349455356598
setp: 4600, Loss: 0.3221742808818817
setp: 4700, Loss: 0.32285362482070923
setp: 4800, Loss: 0.3225465714931488
setp: 4900, Loss: 0.3217448592185974
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9975786924939467
F_score: 0.9987878787878788
validating...
acc: 0.8552631578947368
precision: 0.90625
recall: 0.87
F_score: 0.8877551020408163
model saved.
avg_acc: 0.8381578947368421, avg_f_score: 0.8908129239153565
-------------subject: 24-------------
==========valence==========
******fold 0******
[314, 294]
training...
setp: 0, Loss: 0.7047660946846008
setp: 100, Loss: 0.6642585396766663
setp: 200, Loss: 0.5691707134246826
setp: 300, Loss: 0.5763413906097412
setp: 400, Loss: 0.41711217164993286
setp: 500, Loss: 0.4113047420978546
setp: 600, Loss: 0.43358907103538513
setp: 700, Loss: 0.3659742772579193
setp: 800, Loss: 0.39268559217453003
setp: 900, Loss: 0.3522275984287262
setp: 1000, Loss: 0.35120004415512085
setp: 1100, Loss: 0.37321141362190247
setp: 1200, Loss: 0.3508133888244629
setp: 1300, Loss: 0.3805701434612274
setp: 1400, Loss: 0.34543174505233765
setp: 1500, Loss: 0.33649736642837524
setp: 1600, Loss: 0.33271104097366333
setp: 1700, Loss: 0.319002628326416
setp: 1800, Loss: 0.3243386447429657
setp: 1900, Loss: 0.3178461194038391
setp: 2000, Loss: 0.3192756474018097
setp: 2100, Loss: 0.32091566920280457
setp: 2200, Loss: 0.32003381848335266
setp: 2300, Loss: 0.31922590732574463
setp: 2400, Loss: 0.3194239139556885
setp: 2500, Loss: 0.31985217332839966
setp: 2600, Loss: 0.3189346194267273
setp: 2700, Loss: 0.3207607865333557
setp: 2800, Loss: 0.32004740834236145
setp: 2900, Loss: 0.5064250826835632
setp: 3000, Loss: 0.3427996039390564
setp: 3100, Loss: 0.3484547436237335
setp: 3200, Loss: 0.3686094880104065
setp: 3300, Loss: 0.34067192673683167
setp: 3400, Loss: 0.3192318379878998
setp: 3500, Loss: 0.3180694878101349
setp: 3600, Loss: 0.3192829489707947
setp: 3700, Loss: 0.31898730993270874
setp: 3800, Loss: 0.31835290789604187
setp: 3900, Loss: 0.3191355764865875
setp: 4000, Loss: 0.3204714059829712
setp: 4100, Loss: 0.3201753497123718
setp: 4200, Loss: 0.31940606236457825
setp: 4300, Loss: 0.3198821544647217
setp: 4400, Loss: 0.32001328468322754
setp: 4500, Loss: 0.3335067331790924
setp: 4600, Loss: 0.32176804542541504
setp: 4700, Loss: 0.31775757670402527
setp: 4800, Loss: 0.3180554509162903
setp: 4900, Loss: 0.31864380836486816
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9968152866242038
recall: 0.9968152866242038
F_score: 0.9968152866242038
validating...
acc: 0.868421052631579
precision: 0.891566265060241
recall: 0.8705882352941177
F_score: 0.880952380952381
******fold 1******
[326, 282]
training...
setp: 0, Loss: 0.6877039670944214
setp: 100, Loss: 0.6697849035263062
setp: 200, Loss: 0.6251725554466248
setp: 300, Loss: 0.5000813007354736
setp: 400, Loss: 0.5267170071601868
setp: 500, Loss: 0.4610261619091034
setp: 600, Loss: 0.42587339878082275
setp: 700, Loss: 0.39003026485443115
setp: 800, Loss: 0.4349696636199951
setp: 900, Loss: 0.36290010809898376
setp: 1000, Loss: 0.32714736461639404
setp: 1100, Loss: 0.3767237365245819
setp: 1200, Loss: 0.32605594396591187
setp: 1300, Loss: 0.3200720548629761
setp: 1400, Loss: 0.3248416781425476
setp: 1500, Loss: 0.3202546238899231
setp: 1600, Loss: 0.31776201725006104
setp: 1700, Loss: 0.32244381308555603
setp: 1800, Loss: 0.3193804621696472
setp: 1900, Loss: 0.31730881333351135
setp: 2000, Loss: 0.32883501052856445
setp: 2100, Loss: 0.35178515315055847
setp: 2200, Loss: 0.32989251613616943
setp: 2300, Loss: 0.31848496198654175
setp: 2400, Loss: 0.34791436791419983
setp: 2500, Loss: 0.31763455271720886
setp: 2600, Loss: 0.31687799096107483
setp: 2700, Loss: 0.31829893589019775
setp: 2800, Loss: 0.31800317764282227
setp: 2900, Loss: 0.38259828090667725
setp: 3000, Loss: 0.3213047683238983
setp: 3100, Loss: 0.3585112690925598
setp: 3200, Loss: 0.3169000744819641
setp: 3300, Loss: 0.31806331872940063
setp: 3400, Loss: 0.31822454929351807
setp: 3500, Loss: 0.3164486885070801
setp: 3600, Loss: 0.3198217451572418
setp: 3700, Loss: 0.3184020221233368
setp: 3800, Loss: 0.31755006313323975
setp: 3900, Loss: 0.3163611888885498
setp: 4000, Loss: 0.38679832220077515
setp: 4100, Loss: 0.3233768045902252
setp: 4200, Loss: 0.3379986584186554
setp: 4300, Loss: 0.31723496317863464
setp: 4400, Loss: 0.3168998956680298
setp: 4500, Loss: 0.3165743052959442
setp: 4600, Loss: 0.3173315227031708
setp: 4700, Loss: 0.31740641593933105
setp: 4800, Loss: 0.316288560628891
setp: 4900, Loss: 0.31851208209991455
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.9295774647887324
recall: 0.9041095890410958
F_score: 0.9166666666666666
******fold 2******
[317, 291]
training...
setp: 0, Loss: 0.7077909111976624
setp: 100, Loss: 0.634468138217926
setp: 200, Loss: 0.5968604683876038
setp: 300, Loss: 0.5331170558929443
setp: 400, Loss: 0.4380732774734497
setp: 500, Loss: 0.42516225576400757
setp: 600, Loss: 0.3410227596759796
setp: 700, Loss: 0.36271458864212036
setp: 800, Loss: 0.32652267813682556
setp: 900, Loss: 0.3219492733478546
setp: 1000, Loss: 0.32302048802375793
setp: 1100, Loss: 0.3217635452747345
setp: 1200, Loss: 0.3184489607810974
setp: 1300, Loss: 0.3282669186592102
setp: 1400, Loss: 0.3338976204395294
setp: 1500, Loss: 0.31669971346855164
setp: 1600, Loss: 0.3188709318637848
setp: 1700, Loss: 0.3207043409347534
setp: 1800, Loss: 0.319000244140625
setp: 1900, Loss: 0.35126104950904846
setp: 2000, Loss: 0.32790398597717285
setp: 2100, Loss: 0.3961917757987976
setp: 2200, Loss: 0.3196704089641571
setp: 2300, Loss: 0.3164861798286438
setp: 2400, Loss: 0.31801265478134155
setp: 2500, Loss: 0.3196389377117157
setp: 2600, Loss: 0.31818634271621704
setp: 2700, Loss: 0.31997233629226685
setp: 2800, Loss: 0.446792334318161
setp: 2900, Loss: 0.3185100853443146
setp: 3000, Loss: 0.3166605234146118
setp: 3100, Loss: 0.3175782561302185
setp: 3200, Loss: 0.3175387680530548
setp: 3300, Loss: 0.31807902455329895
setp: 3400, Loss: 0.3172069191932678
setp: 3500, Loss: 0.3188992440700531
setp: 3600, Loss: 0.3192574381828308
setp: 3700, Loss: 0.37421703338623047
setp: 3800, Loss: 0.32252010703086853
setp: 3900, Loss: 0.3300764560699463
setp: 4000, Loss: 0.3202488124370575
setp: 4100, Loss: 0.3183104395866394
setp: 4200, Loss: 0.31715628504753113
setp: 4300, Loss: 0.31883591413497925
setp: 4400, Loss: 0.31948935985565186
setp: 4500, Loss: 0.3172139525413513
setp: 4600, Loss: 0.31798604130744934
setp: 4700, Loss: 0.3172316253185272
setp: 4800, Loss: 0.31755468249320984
setp: 4900, Loss: 0.3161882758140564
training successfully ended.
validating...
acc: 0.9720394736842105
precision: 1.0
recall: 0.9463722397476341
F_score: 0.9724473257698542
validating...
acc: 0.875
precision: 0.9701492537313433
recall: 0.7926829268292683
F_score: 0.8724832214765101
******fold 3******
[314, 294]
training...
setp: 0, Loss: 0.6929069757461548
setp: 100, Loss: 0.676112949848175
setp: 200, Loss: 0.6026451587677002
setp: 300, Loss: 0.5263277292251587
setp: 400, Loss: 0.4222909212112427
setp: 500, Loss: 0.45772358775138855
setp: 600, Loss: 0.3606054186820984
setp: 700, Loss: 0.3330821096897125
setp: 800, Loss: 0.32423022389411926
setp: 900, Loss: 0.32695314288139343
setp: 1000, Loss: 0.3276798725128174
setp: 1100, Loss: 0.33901625871658325
setp: 1200, Loss: 0.3458176851272583
setp: 1300, Loss: 0.35100290179252625
setp: 1400, Loss: 0.3284674286842346
setp: 1500, Loss: 0.31820932030677795
setp: 1600, Loss: 0.3215092718601227
setp: 1700, Loss: 0.32114899158477783
setp: 1800, Loss: 0.3239395320415497
setp: 1900, Loss: 0.319154292345047
setp: 2000, Loss: 0.3193657398223877
setp: 2100, Loss: 0.32060787081718445
setp: 2200, Loss: 0.3206983208656311
setp: 2300, Loss: 0.3207958936691284
setp: 2400, Loss: 0.321045845746994
setp: 2500, Loss: 0.3207397162914276
setp: 2600, Loss: 0.35336923599243164
setp: 2700, Loss: 0.3224101960659027
setp: 2800, Loss: 0.33190810680389404
setp: 2900, Loss: 0.33571702241897583
setp: 3000, Loss: 0.32297825813293457
setp: 3100, Loss: 0.31983575224876404
setp: 3200, Loss: 0.3271586000919342
setp: 3300, Loss: 0.31885984539985657
setp: 3400, Loss: 0.31759825348854065
setp: 3500, Loss: 0.3188253939151764
setp: 3600, Loss: 0.32140523195266724
setp: 3700, Loss: 0.3194926679134369
setp: 3800, Loss: 0.3173682689666748
setp: 3900, Loss: 0.31746381521224976
setp: 4000, Loss: 0.38353627920150757
setp: 4100, Loss: 0.3326190710067749
setp: 4200, Loss: 0.3273829519748688
setp: 4300, Loss: 0.33687081933021545
setp: 4400, Loss: 0.35298365354537964
setp: 4500, Loss: 0.32519376277923584
setp: 4600, Loss: 0.3208121657371521
setp: 4700, Loss: 0.3253243863582611
setp: 4800, Loss: 0.32601070404052734
setp: 4900, Loss: 0.32084447145462036
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.9518072289156626
recall: 0.9294117647058824
F_score: 0.9404761904761904
******fold 4******
[325, 283]
training...
setp: 0, Loss: 0.701594352722168
setp: 100, Loss: 0.6488625407218933
setp: 200, Loss: 0.5923219323158264
setp: 300, Loss: 0.5046789050102234
setp: 400, Loss: 0.4884955883026123
setp: 500, Loss: 0.4144234359264374
setp: 600, Loss: 0.3423018753528595
setp: 700, Loss: 0.36863452196121216
setp: 800, Loss: 0.3463670313358307
setp: 900, Loss: 0.3241327404975891
setp: 1000, Loss: 0.3266451060771942
setp: 1100, Loss: 0.3242111802101135
setp: 1200, Loss: 0.3256133496761322
setp: 1300, Loss: 0.3313843905925751
setp: 1400, Loss: 0.321059912443161
setp: 1500, Loss: 0.31924664974212646
setp: 1600, Loss: 0.31900644302368164
setp: 1700, Loss: 0.32128578424453735
setp: 1800, Loss: 0.31972557306289673
setp: 1900, Loss: 0.3198898136615753
setp: 2000, Loss: 0.317982017993927
setp: 2100, Loss: 0.33051082491874695
setp: 2200, Loss: 0.32617494463920593
setp: 2300, Loss: 0.3191535472869873
setp: 2400, Loss: 0.34894099831581116
setp: 2500, Loss: 0.31812793016433716
setp: 2600, Loss: 0.31877464056015015
setp: 2700, Loss: 0.31955912709236145
setp: 2800, Loss: 0.3174511790275574
setp: 2900, Loss: 0.32103630900382996
setp: 3000, Loss: 0.31917643547058105
setp: 3100, Loss: 0.33868956565856934
setp: 3200, Loss: 0.393375039100647
setp: 3300, Loss: 0.3180987238883972
setp: 3400, Loss: 0.3173646032810211
setp: 3500, Loss: 0.31755051016807556
setp: 3600, Loss: 0.31988683342933655
setp: 3700, Loss: 0.3187539875507355
setp: 3800, Loss: 0.3181600570678711
setp: 3900, Loss: 0.31697022914886475
setp: 4000, Loss: 0.33440008759498596
setp: 4100, Loss: 0.3172976076602936
setp: 4200, Loss: 0.3183523714542389
setp: 4300, Loss: 0.3483550250530243
setp: 4400, Loss: 0.3183649778366089
setp: 4500, Loss: 0.3401564657688141
setp: 4600, Loss: 0.3183995187282562
setp: 4700, Loss: 0.3174073398113251
setp: 4800, Loss: 0.3190107047557831
setp: 4900, Loss: 0.31800493597984314
training successfully ended.
validating...
acc: 0.930921052631579
precision: 0.885558583106267
recall: 1.0
F_score: 0.9393063583815029
validating...
acc: 0.7763157894736842
precision: 0.6851851851851852
recall: 1.0
F_score: 0.8131868131868133
model saved.
avg_acc: 0.875, avg_f_score: 0.8847530545517122
==========arousal==========
******fold 0******
[112, 496]
training...
setp: 0, Loss: 0.705517590045929
setp: 100, Loss: 0.496460884809494
setp: 200, Loss: 0.5011469721794128
setp: 300, Loss: 0.4389539062976837
setp: 400, Loss: 0.4363354742527008
setp: 500, Loss: 0.3561476171016693
setp: 600, Loss: 0.3426158130168915
setp: 700, Loss: 0.32712775468826294
setp: 800, Loss: 0.31902796030044556
setp: 900, Loss: 0.33065977692604065
setp: 1000, Loss: 0.34979093074798584
setp: 1100, Loss: 0.31673353910446167
setp: 1200, Loss: 0.31833356618881226
setp: 1300, Loss: 0.3189792335033417
setp: 1400, Loss: 0.31758782267570496
setp: 1500, Loss: 0.3161431550979614
setp: 1600, Loss: 0.31654781103134155
setp: 1700, Loss: 0.3161005973815918
setp: 1800, Loss: 0.3162984549999237
setp: 1900, Loss: 0.31754055619239807
setp: 2000, Loss: 0.33059054613113403
setp: 2100, Loss: 0.32035622000694275
setp: 2200, Loss: 0.3188696503639221
setp: 2300, Loss: 0.3172016143798828
setp: 2400, Loss: 0.3161219656467438
setp: 2500, Loss: 0.32037246227264404
setp: 2600, Loss: 0.31852948665618896
setp: 2700, Loss: 0.3167545199394226
setp: 2800, Loss: 0.3164416253566742
setp: 2900, Loss: 0.31665119528770447
setp: 3000, Loss: 0.3173789978027344
setp: 3100, Loss: 0.3486391603946686
setp: 3200, Loss: 0.35017111897468567
setp: 3300, Loss: 0.3813798725605011
setp: 3400, Loss: 0.34988269209861755
setp: 3500, Loss: 0.36737892031669617
setp: 3600, Loss: 0.3199264407157898
setp: 3700, Loss: 0.3176751136779785
setp: 3800, Loss: 0.3166434168815613
setp: 3900, Loss: 0.316884309053421
setp: 4000, Loss: 0.31721755862236023
setp: 4100, Loss: 0.31788718700408936
setp: 4200, Loss: 0.31596437096595764
setp: 4300, Loss: 0.31811633706092834
setp: 4400, Loss: 0.3228732943534851
setp: 4500, Loss: 0.34355053305625916
setp: 4600, Loss: 0.3153790533542633
setp: 4700, Loss: 0.31588414311408997
setp: 4800, Loss: 0.3159598410129547
setp: 4900, Loss: 0.31589698791503906
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9671052631578947
precision: 0.9
recall: 0.8571428571428571
F_score: 0.8780487804878048
******fold 1******
[104, 504]
training...
setp: 0, Loss: 0.7318724393844604
setp: 100, Loss: 0.5372652411460876
setp: 200, Loss: 0.4088937044143677
setp: 300, Loss: 0.3380524516105652
setp: 400, Loss: 0.4758443832397461
setp: 500, Loss: 0.3430660367012024
setp: 600, Loss: 0.31964489817619324
setp: 700, Loss: 0.319134920835495
setp: 800, Loss: 0.3314206898212433
setp: 900, Loss: 0.3186202347278595
setp: 1000, Loss: 0.3171056807041168
setp: 1100, Loss: 0.31751641631126404
setp: 1200, Loss: 0.3197909891605377
setp: 1300, Loss: 0.3163367807865143
setp: 1400, Loss: 0.3165547251701355
setp: 1500, Loss: 0.3168522119522095
setp: 1600, Loss: 0.3229382038116455
setp: 1700, Loss: 0.3263450264930725
setp: 1800, Loss: 0.31666427850723267
setp: 1900, Loss: 0.31685858964920044
setp: 2000, Loss: 0.31984710693359375
setp: 2100, Loss: 0.31617283821105957
setp: 2200, Loss: 0.3160223662853241
setp: 2300, Loss: 0.3161575198173523
setp: 2400, Loss: 0.31627359986305237
setp: 2500, Loss: 0.3167470693588257
setp: 2600, Loss: 0.3161974251270294
setp: 2700, Loss: 0.31684109568595886
setp: 2800, Loss: 0.31995689868927
setp: 2900, Loss: 0.3179858326911926
setp: 3000, Loss: 0.3173752725124359
setp: 3100, Loss: 0.3169887959957123
setp: 3200, Loss: 0.3162791430950165
setp: 3300, Loss: 0.3168357312679291
setp: 3400, Loss: 0.3163529336452484
setp: 3500, Loss: 0.7910752892494202
setp: 3600, Loss: 0.456246018409729
setp: 3700, Loss: 0.3977130949497223
setp: 3800, Loss: 0.35873720049858093
setp: 3900, Loss: 0.3303830921649933
setp: 4000, Loss: 0.33187368512153625
setp: 4100, Loss: 0.3370489776134491
setp: 4200, Loss: 0.33331912755966187
setp: 4300, Loss: 0.3218982517719269
setp: 4400, Loss: 0.3424578309059143
setp: 4500, Loss: 0.34705621004104614
setp: 4600, Loss: 0.38841453194618225
setp: 4700, Loss: 0.3186180293560028
setp: 4800, Loss: 0.3280421495437622
setp: 4900, Loss: 0.32082122564315796
training successfully ended.
validating...
acc: 0.9990079365079365
precision: 0.998019801980198
recall: 1.0
F_score: 0.9990089197224975
validating...
acc: 0.9078947368421053
precision: 0.8
recall: 0.6896551724137931
F_score: 0.7407407407407408
******fold 2******
[103, 505]
training...
setp: 0, Loss: 0.7067430019378662
setp: 100, Loss: 0.5223662257194519
setp: 200, Loss: 0.5035175085067749
setp: 300, Loss: 0.532306432723999
setp: 400, Loss: 0.3789706528186798
setp: 500, Loss: 0.3836820125579834
setp: 600, Loss: 0.4163052439689636
setp: 700, Loss: 0.3204997479915619
setp: 800, Loss: 0.41682571172714233
setp: 900, Loss: 0.33160871267318726
setp: 1000, Loss: 0.3310844600200653
setp: 1100, Loss: 0.3186739981174469
setp: 1200, Loss: 0.3238438665866852
setp: 1300, Loss: 0.3165190815925598
setp: 1400, Loss: 0.3185805678367615
setp: 1500, Loss: 0.3195207118988037
setp: 1600, Loss: 0.31775814294815063
setp: 1700, Loss: 0.31688475608825684
setp: 1800, Loss: 0.32040882110595703
setp: 1900, Loss: 0.31889262795448303
setp: 2000, Loss: 0.3186918795108795
setp: 2100, Loss: 0.3156849145889282
setp: 2200, Loss: 0.3181656002998352
setp: 2300, Loss: 0.3162470757961273
setp: 2400, Loss: 0.3167673349380493
setp: 2500, Loss: 0.33353567123413086
setp: 2600, Loss: 0.3171713054180145
setp: 2700, Loss: 0.3168334662914276
setp: 2800, Loss: 0.3162093460559845
setp: 2900, Loss: 0.31547775864601135
setp: 3000, Loss: 0.3177426755428314
setp: 3100, Loss: 0.31608593463897705
setp: 3200, Loss: 0.31661438941955566
setp: 3300, Loss: 0.3164391815662384
setp: 3400, Loss: 0.33804845809936523
setp: 3500, Loss: 0.31768766045570374
setp: 3600, Loss: 0.3160316050052643
setp: 3700, Loss: 0.3152991831302643
setp: 3800, Loss: 0.3171548843383789
setp: 3900, Loss: 0.31566452980041504
setp: 4000, Loss: 0.3173709511756897
setp: 4100, Loss: 0.3163313865661621
setp: 4200, Loss: 0.3173384964466095
setp: 4300, Loss: 0.31727704405784607
setp: 4400, Loss: 0.32047414779663086
setp: 4500, Loss: 0.31569212675094604
setp: 4600, Loss: 0.3172154128551483
setp: 4700, Loss: 0.3159773349761963
setp: 4800, Loss: 0.32127103209495544
setp: 4900, Loss: 0.3242441415786743
training successfully ended.
validating...
acc: 0.994059405940594
precision: 0.9882583170254403
recall: 1.0
F_score: 0.9940944881889764
validating...
acc: 0.9605263157894737
precision: 0.875
recall: 0.9333333333333333
F_score: 0.9032258064516129
******fold 3******
[110, 498]
training...
setp: 0, Loss: 0.6933419108390808
setp: 100, Loss: 0.6206076741218567
setp: 200, Loss: 0.4802513122558594
setp: 300, Loss: 0.3797868490219116
setp: 400, Loss: 0.4103374183177948
setp: 500, Loss: 0.47758206725120544
setp: 600, Loss: 0.36065343022346497
setp: 700, Loss: 0.3893232047557831
setp: 800, Loss: 0.4270307719707489
setp: 900, Loss: 0.3364894390106201
setp: 1000, Loss: 0.3243441581726074
setp: 1100, Loss: 0.31659114360809326
setp: 1200, Loss: 0.3301555812358856
setp: 1300, Loss: 0.3189287483692169
setp: 1400, Loss: 0.31699684262275696
setp: 1500, Loss: 0.3275572657585144
setp: 1600, Loss: 0.4181627035140991
setp: 1700, Loss: 0.3442656695842743
setp: 1800, Loss: 0.3526608347892761
setp: 1900, Loss: 0.31750914454460144
setp: 2000, Loss: 0.3175307810306549
setp: 2100, Loss: 0.31635960936546326
setp: 2200, Loss: 0.31592294573783875
setp: 2300, Loss: 0.31567904353141785
setp: 2400, Loss: 0.32090887427330017
setp: 2500, Loss: 0.3922128677368164
setp: 2600, Loss: 0.3175072968006134
setp: 2700, Loss: 0.31611910462379456
setp: 2800, Loss: 0.31742677092552185
setp: 2900, Loss: 0.3169986605644226
setp: 3000, Loss: 0.3157598078250885
setp: 3100, Loss: 0.31577014923095703
setp: 3200, Loss: 0.3194141983985901
setp: 3300, Loss: 0.3212639391422272
setp: 3400, Loss: 0.3380063474178314
setp: 3500, Loss: 0.31749776005744934
setp: 3600, Loss: 0.31572192907333374
setp: 3700, Loss: 0.3162553012371063
setp: 3800, Loss: 0.3157477378845215
setp: 3900, Loss: 0.3158619701862335
setp: 4000, Loss: 0.3185338079929352
setp: 4100, Loss: 0.31752756237983704
setp: 4200, Loss: 0.3189491629600525
setp: 4300, Loss: 0.31654471158981323
setp: 4400, Loss: 0.31572940945625305
setp: 4500, Loss: 0.3488123118877411
setp: 4600, Loss: 0.3169423043727875
setp: 4700, Loss: 0.31573212146759033
setp: 4800, Loss: 0.31774431467056274
setp: 4900, Loss: 0.31707045435905457
training successfully ended.
validating...
acc: 0.998995983935743
precision: 0.9979959919839679
recall: 1.0
F_score: 0.9989969909729188
validating...
acc: 0.9671052631578947
precision: 0.8214285714285714
recall: 1.0
F_score: 0.9019607843137255
******fold 4******
[103, 505]
training...
setp: 0, Loss: 0.6992548704147339
setp: 100, Loss: 0.6207091808319092
setp: 200, Loss: 0.5370387434959412
setp: 300, Loss: 0.48937615752220154
setp: 400, Loss: 0.4940349757671356
setp: 500, Loss: 0.5116370916366577
setp: 600, Loss: 0.387543648481369
setp: 700, Loss: 0.3368643820285797
setp: 800, Loss: 0.4278120696544647
setp: 900, Loss: 0.3239869773387909
setp: 1000, Loss: 0.38560357689857483
setp: 1100, Loss: 0.3816007375717163
setp: 1200, Loss: 0.3534289300441742
setp: 1300, Loss: 0.35239896178245544
setp: 1400, Loss: 0.32613056898117065
setp: 1500, Loss: 0.3180416226387024
setp: 1600, Loss: 0.3217061161994934
setp: 1700, Loss: 0.32168087363243103
setp: 1800, Loss: 0.323766827583313
setp: 1900, Loss: 0.33641815185546875
setp: 2000, Loss: 0.3227151334285736
setp: 2100, Loss: 0.3184505105018616
setp: 2200, Loss: 0.3160497844219208
setp: 2300, Loss: 0.3162190914154053
setp: 2400, Loss: 0.31770503520965576
setp: 2500, Loss: 0.3184402883052826
setp: 2600, Loss: 0.4060037136077881
setp: 2700, Loss: 0.3932509124279022
setp: 2800, Loss: 0.31691160798072815
setp: 2900, Loss: 0.3168402314186096
setp: 3000, Loss: 0.3166733384132385
setp: 3100, Loss: 0.31618618965148926
setp: 3200, Loss: 0.3173096776008606
setp: 3300, Loss: 0.31820017099380493
setp: 3400, Loss: 0.31741419434547424
setp: 3500, Loss: 0.31834903359413147
setp: 3600, Loss: 0.3186820149421692
setp: 3700, Loss: 0.3167089521884918
setp: 3800, Loss: 0.3605211675167084
setp: 3900, Loss: 0.3379511535167694
setp: 4000, Loss: 0.31704187393188477
setp: 4100, Loss: 0.31638267636299133
setp: 4200, Loss: 0.3166261315345764
setp: 4300, Loss: 0.3171001970767975
setp: 4400, Loss: 0.3179609179496765
setp: 4500, Loss: 0.31708380579948425
setp: 4600, Loss: 0.31632715463638306
setp: 4700, Loss: 0.3162136375904083
setp: 4800, Loss: 0.31705519556999207
setp: 4900, Loss: 0.3177228569984436
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9539473684210527
precision: 0.896551724137931
recall: 0.8666666666666667
F_score: 0.8813559322033899
model saved.
avg_acc: 0.9513157894736842, avg_f_score: 0.8610664088394548
-------------subject: 25-------------
==========valence==========
******fold 0******
[276, 332]
training...
setp: 0, Loss: 0.687877357006073
setp: 100, Loss: 0.6712406873703003
setp: 200, Loss: 0.5644222497940063
setp: 300, Loss: 0.4357817471027374
setp: 400, Loss: 0.4052709639072418
setp: 500, Loss: 0.3900180160999298
setp: 600, Loss: 0.3652538061141968
setp: 700, Loss: 0.34710896015167236
setp: 800, Loss: 0.34310364723205566
setp: 900, Loss: 0.3635905086994171
setp: 1000, Loss: 0.3394882380962372
setp: 1100, Loss: 0.3285422921180725
setp: 1200, Loss: 0.3248674273490906
setp: 1300, Loss: 0.3245599865913391
setp: 1400, Loss: 0.3201988935470581
setp: 1500, Loss: 0.32138094305992126
setp: 1600, Loss: 0.31980693340301514
setp: 1700, Loss: 0.3225055932998657
setp: 1800, Loss: 0.31971684098243713
setp: 1900, Loss: 0.3222849369049072
setp: 2000, Loss: 0.3214603364467621
setp: 2100, Loss: 0.34564125537872314
setp: 2200, Loss: 0.3281513452529907
setp: 2300, Loss: 0.3209264576435089
setp: 2400, Loss: 0.320100873708725
setp: 2500, Loss: 0.3182603418827057
setp: 2600, Loss: 0.32104620337486267
setp: 2700, Loss: 0.3201958239078522
setp: 2800, Loss: 0.32015177607536316
setp: 2900, Loss: 0.32597318291664124
setp: 3000, Loss: 0.3228340446949005
setp: 3100, Loss: 0.32063472270965576
setp: 3200, Loss: 0.3213696777820587
setp: 3300, Loss: 0.31876352429389954
setp: 3400, Loss: 0.319192498922348
setp: 3500, Loss: 0.3190487325191498
setp: 3600, Loss: 0.3590269386768341
setp: 3700, Loss: 0.3465806245803833
setp: 3800, Loss: 0.3188398778438568
setp: 3900, Loss: 0.31967389583587646
setp: 4000, Loss: 0.31915876269340515
setp: 4100, Loss: 0.31851673126220703
setp: 4200, Loss: 0.3201526403427124
setp: 4300, Loss: 0.3197236955165863
setp: 4400, Loss: 0.31883367896080017
setp: 4500, Loss: 0.32050904631614685
setp: 4600, Loss: 0.32025259733200073
setp: 4700, Loss: 0.40682709217071533
setp: 4800, Loss: 0.33069902658462524
setp: 4900, Loss: 0.33258935809135437
training successfully ended.
validating...
acc: 0.9835526315789473
precision: 1.0
recall: 0.9637681159420289
F_score: 0.981549815498155
validating...
acc: 0.8618421052631579
precision: 0.9848484848484849
recall: 0.7647058823529411
F_score: 0.8609271523178808
******fold 1******
[295, 313]
training...
setp: 0, Loss: 0.6955209374427795
setp: 100, Loss: 0.6908590793609619
setp: 200, Loss: 0.6895281672477722
setp: 300, Loss: 0.6146457195281982
setp: 400, Loss: 0.500349223613739
setp: 500, Loss: 0.4628509283065796
setp: 600, Loss: 0.49333587288856506
setp: 700, Loss: 0.4124715030193329
setp: 800, Loss: 0.3661249279975891
setp: 900, Loss: 0.3559207618236542
setp: 1000, Loss: 0.33472493290901184
setp: 1100, Loss: 0.3350323736667633
setp: 1200, Loss: 0.32160675525665283
setp: 1300, Loss: 0.324704110622406
setp: 1400, Loss: 0.32308530807495117
setp: 1500, Loss: 0.3298891484737396
setp: 1600, Loss: 0.31881389021873474
setp: 1700, Loss: 0.32294636964797974
setp: 1800, Loss: 0.3197025954723358
setp: 1900, Loss: 0.32075753808021545
setp: 2000, Loss: 0.3207037150859833
setp: 2100, Loss: 0.373176634311676
setp: 2200, Loss: 0.3347059488296509
setp: 2300, Loss: 0.3253746032714844
setp: 2400, Loss: 0.3220461308956146
setp: 2500, Loss: 0.32626569271087646
setp: 2600, Loss: 0.3188569247722626
setp: 2700, Loss: 0.3197392225265503
setp: 2800, Loss: 0.3196060359477997
setp: 2900, Loss: 0.31928551197052
setp: 3000, Loss: 0.31963083148002625
setp: 3100, Loss: 0.3202081322669983
setp: 3200, Loss: 0.3214159607887268
setp: 3300, Loss: 0.3187529742717743
setp: 3400, Loss: 0.31953132152557373
setp: 3500, Loss: 0.47557100653648376
setp: 3600, Loss: 0.37755057215690613
setp: 3700, Loss: 0.32652387022972107
setp: 3800, Loss: 0.32276833057403564
setp: 3900, Loss: 0.32106706500053406
setp: 4000, Loss: 0.3192201852798462
setp: 4100, Loss: 0.3202623724937439
setp: 4200, Loss: 0.31968799233436584
setp: 4300, Loss: 0.3207714259624481
setp: 4400, Loss: 0.3192403018474579
setp: 4500, Loss: 0.3203139305114746
setp: 4600, Loss: 0.3202335834503174
setp: 4700, Loss: 0.32107746601104736
setp: 4800, Loss: 0.3223916292190552
setp: 4900, Loss: 0.3581882119178772
training successfully ended.
validating...
acc: 0.9851973684210527
precision: 0.9965277777777778
recall: 0.9728813559322034
F_score: 0.9845626072041167
validating...
acc: 0.8881578947368421
precision: 0.9152542372881356
recall: 0.8181818181818182
F_score: 0.8640000000000001
******fold 2******
[284, 324]
training...
setp: 0, Loss: 0.6929898262023926
setp: 100, Loss: 0.6804492473602295
setp: 200, Loss: 0.6194887757301331
setp: 300, Loss: 0.6329046487808228
setp: 400, Loss: 0.5007434487342834
setp: 500, Loss: 0.4500461220741272
setp: 600, Loss: 0.4321284890174866
setp: 700, Loss: 0.35577768087387085
setp: 800, Loss: 0.34095829725265503
setp: 900, Loss: 0.33094093203544617
setp: 1000, Loss: 0.3410719037055969
setp: 1100, Loss: 0.32829931378364563
setp: 1200, Loss: 0.32702046632766724
setp: 1300, Loss: 0.32396644353866577
setp: 1400, Loss: 0.32077261805534363
setp: 1500, Loss: 0.31972578167915344
setp: 1600, Loss: 0.32018816471099854
setp: 1700, Loss: 0.3223806321620941
setp: 1800, Loss: 0.37453681230545044
setp: 1900, Loss: 0.3195558786392212
setp: 2000, Loss: 0.31943386793136597
setp: 2100, Loss: 0.31788963079452515
setp: 2200, Loss: 0.31940165162086487
setp: 2300, Loss: 0.319059818983078
setp: 2400, Loss: 0.3188246488571167
setp: 2500, Loss: 0.31943434476852417
setp: 2600, Loss: 0.3208887279033661
setp: 2700, Loss: 0.3193521201610565
setp: 2800, Loss: 0.3191473186016083
setp: 2900, Loss: 0.3180319666862488
setp: 3000, Loss: 0.32209262251853943
setp: 3100, Loss: 0.338766872882843
setp: 3200, Loss: 0.3230171203613281
setp: 3300, Loss: 0.3178693950176239
setp: 3400, Loss: 0.31802353262901306
setp: 3500, Loss: 0.3181554973125458
setp: 3600, Loss: 0.3210420310497284
setp: 3700, Loss: 0.3197686970233917
setp: 3800, Loss: 0.3189011812210083
setp: 3900, Loss: 0.31912311911582947
setp: 4000, Loss: 0.3186134397983551
setp: 4100, Loss: 0.31936001777648926
setp: 4200, Loss: 0.36668285727500916
setp: 4300, Loss: 0.31986695528030396
setp: 4400, Loss: 0.3173406422138214
setp: 4500, Loss: 0.3186953067779541
setp: 4600, Loss: 0.31851160526275635
setp: 4700, Loss: 0.31831368803977966
setp: 4800, Loss: 0.31834036111831665
setp: 4900, Loss: 0.3195963501930237
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9473684210526315
precision: 0.9726027397260274
recall: 0.922077922077922
F_score: 0.9466666666666667
******fold 3******
[288, 320]
training...
setp: 0, Loss: 0.6967441439628601
setp: 100, Loss: 0.6296390891075134
setp: 200, Loss: 0.6554875373840332
setp: 300, Loss: 0.5730549693107605
setp: 400, Loss: 0.5135100483894348
setp: 500, Loss: 0.5135036110877991
setp: 600, Loss: 0.43062493205070496
setp: 700, Loss: 0.5481928586959839
setp: 800, Loss: 0.4201502203941345
setp: 900, Loss: 0.3864724636077881
setp: 1000, Loss: 0.3233530819416046
setp: 1100, Loss: 0.36660531163215637
setp: 1200, Loss: 0.35105347633361816
setp: 1300, Loss: 0.3362637460231781
setp: 1400, Loss: 0.32545679807662964
setp: 1500, Loss: 0.3371262848377228
setp: 1600, Loss: 0.32575827836990356
setp: 1700, Loss: 0.33256882429122925
setp: 1800, Loss: 0.3181729316711426
setp: 1900, Loss: 0.331074595451355
setp: 2000, Loss: 0.3309285044670105
setp: 2100, Loss: 0.32064908742904663
setp: 2200, Loss: 0.31751927733421326
setp: 2300, Loss: 0.32059112191200256
setp: 2400, Loss: 0.31842002272605896
setp: 2500, Loss: 0.3182482123374939
setp: 2600, Loss: 0.3189501464366913
setp: 2700, Loss: 0.3228512406349182
setp: 2800, Loss: 0.3173646926879883
setp: 2900, Loss: 0.3191199004650116
setp: 3000, Loss: 0.31928008794784546
setp: 3100, Loss: 0.31687647104263306
setp: 3200, Loss: 0.3211316466331482
setp: 3300, Loss: 0.31718870997428894
setp: 3400, Loss: 0.3234542906284332
setp: 3500, Loss: 0.32044604420661926
setp: 3600, Loss: 0.3255687355995178
setp: 3700, Loss: 0.3178396224975586
setp: 3800, Loss: 0.3178705871105194
setp: 3900, Loss: 0.3172083795070648
setp: 4000, Loss: 0.3195270895957947
setp: 4100, Loss: 0.3177069127559662
setp: 4200, Loss: 0.31974729895591736
setp: 4300, Loss: 0.31936532258987427
setp: 4400, Loss: 0.33048591017723083
setp: 4500, Loss: 0.3910042643547058
setp: 4600, Loss: 0.31690606474876404
setp: 4700, Loss: 0.3167177140712738
setp: 4800, Loss: 0.3169631063938141
setp: 4900, Loss: 0.31741708517074585
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.9178082191780822
recall: 0.9178082191780822
F_score: 0.9178082191780822
******fold 4******
[301, 307]
training...
setp: 0, Loss: 0.7233732342720032
setp: 100, Loss: 0.6976780295372009
setp: 200, Loss: 0.6183188557624817
setp: 300, Loss: 0.4631378650665283
setp: 400, Loss: 0.3919863700866699
setp: 500, Loss: 0.35913699865341187
setp: 600, Loss: 0.36004942655563354
setp: 700, Loss: 0.33227381110191345
setp: 800, Loss: 0.3326197564601898
setp: 900, Loss: 0.34296682476997375
setp: 1000, Loss: 0.34323152899742126
setp: 1100, Loss: 0.3205236792564392
setp: 1200, Loss: 0.3222641050815582
setp: 1300, Loss: 0.3220665156841278
setp: 1400, Loss: 0.32036834955215454
setp: 1500, Loss: 0.3193310499191284
setp: 1600, Loss: 0.36179542541503906
setp: 1700, Loss: 0.34712210297584534
setp: 1800, Loss: 0.3200327157974243
setp: 1900, Loss: 0.32078200578689575
setp: 2000, Loss: 0.31971749663352966
setp: 2100, Loss: 0.3206965923309326
setp: 2200, Loss: 0.31973797082901
setp: 2300, Loss: 0.3223455548286438
setp: 2400, Loss: 0.329853892326355
setp: 2500, Loss: 0.3288753926753998
setp: 2600, Loss: 0.3202701807022095
setp: 2700, Loss: 0.3208548128604889
setp: 2800, Loss: 0.3188004493713379
setp: 2900, Loss: 0.32010671496391296
setp: 3000, Loss: 0.32484152913093567
setp: 3100, Loss: 0.3203005790710449
setp: 3200, Loss: 0.31912508606910706
setp: 3300, Loss: 0.3182010054588318
setp: 3400, Loss: 0.31862887740135193
setp: 3500, Loss: 0.3201170265674591
setp: 3600, Loss: 0.32076016068458557
setp: 3700, Loss: 0.32081353664398193
setp: 3800, Loss: 0.34265345335006714
setp: 3900, Loss: 0.3509979546070099
setp: 4000, Loss: 0.31946250796318054
setp: 4100, Loss: 0.35764217376708984
setp: 4200, Loss: 0.3192591667175293
setp: 4300, Loss: 0.318771630525589
setp: 4400, Loss: 0.31897464394569397
setp: 4500, Loss: 0.31955477595329285
setp: 4600, Loss: 0.3205566108226776
setp: 4700, Loss: 0.3199256360530853
setp: 4800, Loss: 0.31913822889328003
setp: 4900, Loss: 0.3195861577987671
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9013157894736842
precision: 0.835820895522388
recall: 0.9333333333333333
F_score: 0.8818897637795275
model saved.
avg_acc: 0.9039473684210526, avg_f_score: 0.8942583603884314
==========arousal==========
******fold 0******
[146, 462]
training...
setp: 0, Loss: 0.6941621899604797
setp: 100, Loss: 0.640947699546814
setp: 200, Loss: 0.6177799105644226
setp: 300, Loss: 0.5271375775337219
setp: 400, Loss: 0.44146037101745605
setp: 500, Loss: 0.4861147999763489
setp: 600, Loss: 0.497919499874115
setp: 700, Loss: 0.4608500599861145
setp: 800, Loss: 0.4762192368507385
setp: 900, Loss: 0.4327297508716583
setp: 1000, Loss: 0.41500386595726013
setp: 1100, Loss: 0.39140403270721436
setp: 1200, Loss: 0.3318733870983124
setp: 1300, Loss: 0.4500904381275177
setp: 1400, Loss: 0.3623445928096771
setp: 1500, Loss: 0.32838112115859985
setp: 1600, Loss: 0.37696516513824463
setp: 1700, Loss: 0.32780420780181885
setp: 1800, Loss: 0.3284551799297333
setp: 1900, Loss: 0.38498228788375854
setp: 2000, Loss: 0.32883092761039734
setp: 2100, Loss: 0.3204447329044342
setp: 2200, Loss: 0.3536509573459625
setp: 2300, Loss: 0.31967398524284363
setp: 2400, Loss: 0.3234826624393463
setp: 2500, Loss: 0.35411933064460754
setp: 2600, Loss: 0.31998592615127563
setp: 2700, Loss: 0.32177942991256714
setp: 2800, Loss: 0.3506867587566376
setp: 2900, Loss: 0.318559467792511
setp: 3000, Loss: 0.31811341643333435
setp: 3100, Loss: 0.47606948018074036
setp: 3200, Loss: 0.319581538438797
setp: 3300, Loss: 0.31777501106262207
setp: 3400, Loss: 0.3833447992801666
setp: 3500, Loss: 0.3173759877681732
setp: 3600, Loss: 0.3223645091056824
setp: 3700, Loss: 0.31802982091903687
setp: 3800, Loss: 0.3175968527793884
setp: 3900, Loss: 0.32134294509887695
setp: 4000, Loss: 0.3164122998714447
setp: 4100, Loss: 0.34414035081863403
setp: 4200, Loss: 0.3182530999183655
setp: 4300, Loss: 0.31726333498954773
setp: 4400, Loss: 0.31742438673973083
setp: 4500, Loss: 0.3189936578273773
setp: 4600, Loss: 0.3187956213951111
setp: 4700, Loss: 0.34127843379974365
setp: 4800, Loss: 0.3179264962673187
setp: 4900, Loss: 0.32049912214279175
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9473684210526315
precision: 0.9736842105263158
recall: 0.8409090909090909
F_score: 0.9024390243902439
******fold 1******
[149, 459]
training...
setp: 0, Loss: 0.7019643783569336
setp: 100, Loss: 0.6498238444328308
setp: 200, Loss: 0.6165907382965088
setp: 300, Loss: 0.4379218816757202
setp: 400, Loss: 0.40661412477493286
setp: 500, Loss: 0.4317118227481842
setp: 600, Loss: 0.38196197152137756
setp: 700, Loss: 0.35881561040878296
setp: 800, Loss: 0.3721747398376465
setp: 900, Loss: 0.36031368374824524
setp: 1000, Loss: 0.33526289463043213
setp: 1100, Loss: 0.32983049750328064
setp: 1200, Loss: 0.32458215951919556
setp: 1300, Loss: 0.32713595032691956
setp: 1400, Loss: 0.32130205631256104
setp: 1500, Loss: 0.32373934984207153
setp: 1600, Loss: 0.334672749042511
setp: 1700, Loss: 0.3170766532421112
setp: 1800, Loss: 0.3214855492115021
setp: 1900, Loss: 0.33132559061050415
setp: 2000, Loss: 0.3229861855506897
setp: 2100, Loss: 0.31744149327278137
setp: 2200, Loss: 0.35011497139930725
setp: 2300, Loss: 0.33309558033943176
setp: 2400, Loss: 0.31817325949668884
setp: 2500, Loss: 0.3177306652069092
setp: 2600, Loss: 0.3166956305503845
setp: 2700, Loss: 0.35102972388267517
setp: 2800, Loss: 0.3493887484073639
setp: 2900, Loss: 0.32337328791618347
setp: 3000, Loss: 0.3408913016319275
setp: 3100, Loss: 0.3491728901863098
setp: 3200, Loss: 0.316919207572937
setp: 3300, Loss: 0.35038551688194275
setp: 3400, Loss: 0.34828728437423706
setp: 3500, Loss: 0.32588279247283936
setp: 3600, Loss: 0.3203248679637909
setp: 3700, Loss: 0.3169388473033905
setp: 3800, Loss: 0.3199880123138428
setp: 3900, Loss: 0.32576149702072144
setp: 4000, Loss: 0.3169410824775696
setp: 4100, Loss: 0.3170907497406006
setp: 4200, Loss: 0.3179362416267395
setp: 4300, Loss: 0.31769129633903503
setp: 4400, Loss: 0.3327960968017578
setp: 4500, Loss: 0.3167932331562042
setp: 4600, Loss: 0.31623026728630066
setp: 4700, Loss: 0.31682199239730835
setp: 4800, Loss: 0.3160335421562195
setp: 4900, Loss: 0.3226746618747711
training successfully ended.
validating...
acc: 0.9836601307189542
precision: 0.9743589743589743
recall: 0.9934640522875817
F_score: 0.9838187702265372
validating...
acc: 0.9539473684210527
precision: 0.8695652173913043
recall: 0.975609756097561
F_score: 0.9195402298850575
******fold 2******
[158, 450]
training...
setp: 0, Loss: 0.703835666179657
setp: 100, Loss: 0.6302180290222168
setp: 200, Loss: 0.6017407178878784
setp: 300, Loss: 0.446308434009552
setp: 400, Loss: 0.41169920563697815
setp: 500, Loss: 0.45612895488739014
setp: 600, Loss: 0.37114694714546204
setp: 700, Loss: 0.3637981414794922
setp: 800, Loss: 0.3597572445869446
setp: 900, Loss: 0.3534616529941559
setp: 1000, Loss: 0.32195839285850525
setp: 1100, Loss: 0.3589625954627991
setp: 1200, Loss: 0.32573777437210083
setp: 1300, Loss: 0.32460856437683105
setp: 1400, Loss: 0.38861459493637085
setp: 1500, Loss: 0.31720083951950073
setp: 1600, Loss: 0.3177303969860077
setp: 1700, Loss: 0.31863129138946533
setp: 1800, Loss: 0.3230113685131073
setp: 1900, Loss: 0.3209426999092102
setp: 2000, Loss: 0.31563490629196167
setp: 2100, Loss: 0.31604668498039246
setp: 2200, Loss: 0.3228425979614258
setp: 2300, Loss: 0.31697919964790344
setp: 2400, Loss: 0.3273622393608093
setp: 2500, Loss: 0.3595025837421417
setp: 2600, Loss: 0.32495811581611633
setp: 2700, Loss: 0.35051631927490234
setp: 2800, Loss: 0.3155484199523926
setp: 2900, Loss: 0.31658047437667847
setp: 3000, Loss: 0.32438862323760986
setp: 3100, Loss: 0.3478716015815735
setp: 3200, Loss: 0.31672927737236023
setp: 3300, Loss: 0.31857460737228394
setp: 3400, Loss: 0.37994951009750366
setp: 3500, Loss: 0.32843005657196045
setp: 3600, Loss: 0.3318380117416382
setp: 3700, Loss: 0.31704434752464294
setp: 3800, Loss: 0.31710776686668396
setp: 3900, Loss: 0.31572434306144714
setp: 4000, Loss: 0.31948691606521606
setp: 4100, Loss: 0.3226839601993561
setp: 4200, Loss: 0.31993183493614197
setp: 4300, Loss: 0.31725582480430603
setp: 4400, Loss: 0.3162264823913574
setp: 4500, Loss: 0.31852084398269653
setp: 4600, Loss: 0.31684011220932007
setp: 4700, Loss: 0.35662490129470825
setp: 4800, Loss: 0.3202483355998993
setp: 4900, Loss: 0.31559431552886963
training successfully ended.
validating...
acc: 0.9977777777777778
precision: 0.995575221238938
recall: 1.0
F_score: 0.9977827050997783
validating...
acc: 0.9605263157894737
precision: 0.8611111111111112
recall: 0.96875
F_score: 0.911764705882353
******fold 3******
[151, 457]
training...
setp: 0, Loss: 0.6990429759025574
setp: 100, Loss: 0.563815712928772
setp: 200, Loss: 0.492870032787323
setp: 300, Loss: 0.36450788378715515
setp: 400, Loss: 0.41899773478507996
setp: 500, Loss: 0.39203551411628723
setp: 600, Loss: 0.3384675085544586
setp: 700, Loss: 0.32955434918403625
setp: 800, Loss: 0.3206695020198822
setp: 900, Loss: 0.3196737468242645
setp: 1000, Loss: 0.3277078866958618
setp: 1100, Loss: 0.3839503228664398
setp: 1200, Loss: 0.3180542588233948
setp: 1300, Loss: 0.31859543919563293
setp: 1400, Loss: 0.31695666909217834
setp: 1500, Loss: 0.3176611065864563
setp: 1600, Loss: 0.31638866662979126
setp: 1700, Loss: 0.4235423803329468
setp: 1800, Loss: 0.316769003868103
setp: 1900, Loss: 0.31863459944725037
setp: 2000, Loss: 0.3170520067214966
setp: 2100, Loss: 0.3205024003982544
setp: 2200, Loss: 0.3176316022872925
setp: 2300, Loss: 0.31600886583328247
setp: 2400, Loss: 0.3191112279891968
setp: 2500, Loss: 0.34280654788017273
setp: 2600, Loss: 0.3237054646015167
setp: 2700, Loss: 0.3390578329563141
setp: 2800, Loss: 0.31811580061912537
setp: 2900, Loss: 0.3174706697463989
setp: 3000, Loss: 0.3174974322319031
setp: 3100, Loss: 0.3485773205757141
setp: 3200, Loss: 0.31881505250930786
setp: 3300, Loss: 0.3472259044647217
setp: 3400, Loss: 0.3168103098869324
setp: 3500, Loss: 0.31778305768966675
setp: 3600, Loss: 0.31739503145217896
setp: 3700, Loss: 0.3176698684692383
setp: 3800, Loss: 0.3190781772136688
setp: 3900, Loss: 0.3167080879211426
setp: 4000, Loss: 0.3161715269088745
setp: 4100, Loss: 0.3168039619922638
setp: 4200, Loss: 0.3161533772945404
setp: 4300, Loss: 0.3174635171890259
setp: 4400, Loss: 0.6053245067596436
setp: 4500, Loss: 0.3163527548313141
setp: 4600, Loss: 0.31691300868988037
setp: 4700, Loss: 0.3162009119987488
setp: 4800, Loss: 0.31596410274505615
setp: 4900, Loss: 0.31640881299972534
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9473684210526315
precision: 0.8974358974358975
recall: 0.8974358974358975
F_score: 0.8974358974358975
******fold 4******
[156, 452]
training...
setp: 0, Loss: 0.7048770189285278
setp: 100, Loss: 0.6936931014060974
setp: 200, Loss: 0.6862981915473938
setp: 300, Loss: 0.6138537526130676
setp: 400, Loss: 0.5637844800949097
setp: 500, Loss: 0.5276110768318176
setp: 600, Loss: 0.47102120518684387
setp: 700, Loss: 0.4990178644657135
setp: 800, Loss: 0.4026902914047241
setp: 900, Loss: 0.3619237542152405
setp: 1000, Loss: 0.36462724208831787
setp: 1100, Loss: 0.35802286863327026
setp: 1200, Loss: 0.32739800214767456
setp: 1300, Loss: 0.351694792509079
setp: 1400, Loss: 0.32081714272499084
setp: 1500, Loss: 0.32072678208351135
setp: 1600, Loss: 0.3228421211242676
setp: 1700, Loss: 0.3223172128200531
setp: 1800, Loss: 0.3226556181907654
setp: 1900, Loss: 0.3192768394947052
setp: 2000, Loss: 0.40536245703697205
setp: 2100, Loss: 0.3181535601615906
setp: 2200, Loss: 0.31873273849487305
setp: 2300, Loss: 0.3186708390712738
setp: 2400, Loss: 0.3197515606880188
setp: 2500, Loss: 0.31822821497917175
setp: 2600, Loss: 0.31912484765052795
setp: 2700, Loss: 0.3492882251739502
setp: 2800, Loss: 0.31945085525512695
setp: 2900, Loss: 0.32496896386146545
setp: 3000, Loss: 0.32467710971832275
setp: 3100, Loss: 0.3213087320327759
setp: 3200, Loss: 0.3199075162410736
setp: 3300, Loss: 0.32003292441368103
setp: 3400, Loss: 0.321765273809433
setp: 3500, Loss: 0.32044821977615356
setp: 3600, Loss: 0.32199153304100037
setp: 3700, Loss: 0.32025113701820374
setp: 3800, Loss: 0.3215028643608093
setp: 3900, Loss: 0.3726825714111328
setp: 4000, Loss: 0.3263581395149231
setp: 4100, Loss: 0.3180304765701294
setp: 4200, Loss: 0.31869661808013916
setp: 4300, Loss: 0.31828853487968445
setp: 4400, Loss: 0.3190496265888214
setp: 4500, Loss: 0.3195970058441162
setp: 4600, Loss: 0.3187714219093323
setp: 4700, Loss: 0.3192441165447235
setp: 4800, Loss: 0.31870391964912415
setp: 4900, Loss: 0.32058075070381165
training successfully ended.
validating...
acc: 0.9900442477876106
precision: 0.9933184855233853
recall: 0.9867256637168141
F_score: 0.9900110987791343
validating...
acc: 0.9210526315789473
precision: 0.8928571428571429
recall: 0.7352941176470589
F_score: 0.806451612903226
model saved.
avg_acc: 0.9460526315789473, avg_f_score: 0.8875262940993555
-------------subject: 26-------------
==========valence==========
******fold 0******
[205, 403]
training...
setp: 0, Loss: 0.6528752446174622
setp: 100, Loss: 0.6221027374267578
setp: 200, Loss: 0.5992802977561951
setp: 300, Loss: 0.6857813000679016
setp: 400, Loss: 0.6037772297859192
setp: 500, Loss: 0.6213750839233398
setp: 600, Loss: 0.6684196591377258
setp: 700, Loss: 0.5110774636268616
setp: 800, Loss: 0.3868926167488098
setp: 900, Loss: 0.39153093099594116
setp: 1000, Loss: 0.371362566947937
setp: 1100, Loss: 0.32640913128852844
setp: 1200, Loss: 0.36029762029647827
setp: 1300, Loss: 0.3322907090187073
setp: 1400, Loss: 0.33339419960975647
setp: 1500, Loss: 0.3285733461380005
setp: 1600, Loss: 0.32701781392097473
setp: 1700, Loss: 0.33008572459220886
setp: 1800, Loss: 0.3221798539161682
setp: 1900, Loss: 0.36927998065948486
setp: 2000, Loss: 0.31954896450042725
setp: 2100, Loss: 0.3528980314731598
setp: 2200, Loss: 0.3209352493286133
setp: 2300, Loss: 0.3512536287307739
setp: 2400, Loss: 0.3213137686252594
setp: 2500, Loss: 0.32233110070228577
setp: 2600, Loss: 0.32052868604660034
setp: 2700, Loss: 0.46011263132095337
setp: 2800, Loss: 0.3249852657318115
setp: 2900, Loss: 0.32412466406822205
setp: 3000, Loss: 0.3300463557243347
setp: 3100, Loss: 0.32196879386901855
setp: 3200, Loss: 0.32081323862075806
setp: 3300, Loss: 0.31998416781425476
setp: 3400, Loss: 0.32045096158981323
setp: 3500, Loss: 0.32011497020721436
setp: 3600, Loss: 0.32088497281074524
setp: 3700, Loss: 0.32128220796585083
setp: 3800, Loss: 0.32147103548049927
setp: 3900, Loss: 0.3206711411476135
setp: 4000, Loss: 0.3514254093170166
setp: 4100, Loss: 0.32075440883636475
setp: 4200, Loss: 0.32027095556259155
setp: 4300, Loss: 0.35236847400665283
setp: 4400, Loss: 0.354389488697052
setp: 4500, Loss: 0.3228038251399994
setp: 4600, Loss: 0.31929832696914673
setp: 4700, Loss: 0.31995269656181335
setp: 4800, Loss: 0.3203023374080658
setp: 4900, Loss: 0.3198281228542328
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.875
precision: 0.92
recall: 0.7540983606557377
F_score: 0.8288288288288288
******fold 1******
[215, 393]
training...
setp: 0, Loss: 0.7998256683349609
setp: 100, Loss: 0.6105828285217285
setp: 200, Loss: 0.6843252778053284
setp: 300, Loss: 0.6434570550918579
setp: 400, Loss: 0.679995059967041
setp: 500, Loss: 0.6266700029373169
setp: 600, Loss: 0.643491804599762
setp: 700, Loss: 0.6842777729034424
setp: 800, Loss: 0.6105971336364746
setp: 900, Loss: 0.6438549757003784
setp: 1000, Loss: 0.6434919834136963
setp: 1100, Loss: 0.6634131669998169
setp: 1200, Loss: 0.628252387046814
setp: 1300, Loss: 0.6627748012542725
setp: 1400, Loss: 0.6434899568557739
setp: 1500, Loss: 0.6823464035987854
setp: 1600, Loss: 0.6282709240913391
setp: 1700, Loss: 0.6632360816001892
setp: 1800, Loss: 0.6638842225074768
setp: 1900, Loss: 0.662455141544342
setp: 2000, Loss: 0.6109503507614136
setp: 2100, Loss: 0.6837180852890015
setp: 2200, Loss: 0.6434915065765381
setp: 2300, Loss: 0.6799734830856323
setp: 2400, Loss: 0.6267606019973755
setp: 2500, Loss: 0.6434913873672485
setp: 2600, Loss: 0.6838500499725342
setp: 2700, Loss: 0.6104597449302673
setp: 2800, Loss: 0.6439038515090942
setp: 2900, Loss: 0.6434918642044067
setp: 3000, Loss: 0.6633304357528687
setp: 3100, Loss: 0.6281671524047852
setp: 3200, Loss: 0.6627222299575806
setp: 3300, Loss: 0.6434906721115112
setp: 3400, Loss: 0.6822943687438965
setp: 3500, Loss: 0.6282373666763306
setp: 3600, Loss: 0.663178026676178
setp: 3700, Loss: 0.6638144254684448
setp: 3800, Loss: 0.662447452545166
setp: 3900, Loss: 0.6109303832054138
setp: 4000, Loss: 0.6835967302322388
setp: 4100, Loss: 0.6434914469718933
setp: 4200, Loss: 0.6799963712692261
setp: 4300, Loss: 0.6267783045768738
setp: 4400, Loss: 0.6434924602508545
setp: 4500, Loss: 0.6837449073791504
setp: 4600, Loss: 0.6103272438049316
setp: 4700, Loss: 0.6439035534858704
setp: 4800, Loss: 0.6434919238090515
setp: 4900, Loss: 0.6632931232452393
training successfully ended.
validating...
acc: 0.6463815789473685
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.6644736842105263
precision: 0
recall: 0.0
F_score: 0
******fold 2******
[213, 395]
training...
setp: 0, Loss: 0.6816768646240234
setp: 100, Loss: 0.6640502214431763
setp: 200, Loss: 0.643436074256897
setp: 300, Loss: 0.662082314491272
setp: 400, Loss: 0.5908932089805603
setp: 500, Loss: 0.5605475306510925
setp: 600, Loss: 0.3955375850200653
setp: 700, Loss: 0.3725157678127289
setp: 800, Loss: 0.4038577079772949
setp: 900, Loss: 0.3531447649002075
setp: 1000, Loss: 0.3524729311466217
setp: 1100, Loss: 0.32509326934814453
setp: 1200, Loss: 0.3273152709007263
setp: 1300, Loss: 0.3248423933982849
setp: 1400, Loss: 0.35551342368125916
setp: 1500, Loss: 0.31993529200553894
setp: 1600, Loss: 0.3201056122779846
setp: 1700, Loss: 0.31982895731925964
setp: 1800, Loss: 0.3198293447494507
setp: 1900, Loss: 0.31892213225364685
setp: 2000, Loss: 0.3188794255256653
setp: 2100, Loss: 0.35211533308029175
setp: 2200, Loss: 0.3199988305568695
setp: 2300, Loss: 0.31952187418937683
setp: 2400, Loss: 0.3220638334751129
setp: 2500, Loss: 0.3198073208332062
setp: 2600, Loss: 0.31893810629844666
setp: 2700, Loss: 0.3318406939506531
setp: 2800, Loss: 0.3373238444328308
setp: 2900, Loss: 0.32154324650764465
setp: 3000, Loss: 0.3210344612598419
setp: 3100, Loss: 0.3434356153011322
setp: 3200, Loss: 0.3240993022918701
setp: 3300, Loss: 0.35815027356147766
setp: 3400, Loss: 0.3463825583457947
setp: 3500, Loss: 0.3219055235385895
setp: 3600, Loss: 0.3195400536060333
setp: 3700, Loss: 0.3191748559474945
setp: 3800, Loss: 0.3192802369594574
setp: 3900, Loss: 0.31927162408828735
setp: 4000, Loss: 0.35049664974212646
setp: 4100, Loss: 0.3199780583381653
setp: 4200, Loss: 0.31979596614837646
setp: 4300, Loss: 0.3212411403656006
setp: 4400, Loss: 0.31973186135292053
setp: 4500, Loss: 0.3192274868488312
setp: 4600, Loss: 0.32026222348213196
setp: 4700, Loss: 0.35314279794692993
setp: 4800, Loss: 0.37376075983047485
setp: 4900, Loss: 0.32131338119506836
training successfully ended.
validating...
acc: 0.993421052631579
precision: 0.9906103286384976
recall: 0.9906103286384976
F_score: 0.9906103286384976
validating...
acc: 0.9013157894736842
precision: 0.8958333333333334
recall: 0.8113207547169812
F_score: 0.8514851485148515
******fold 3******
[208, 400]
training...
setp: 0, Loss: 0.6710538864135742
setp: 100, Loss: 0.6220649480819702
setp: 200, Loss: 0.661821186542511
setp: 300, Loss: 0.6188939809799194
setp: 400, Loss: 0.5041297674179077
setp: 500, Loss: 0.5309976935386658
setp: 600, Loss: 0.34079450368881226
setp: 700, Loss: 0.32750120759010315
setp: 800, Loss: 0.38411957025527954
setp: 900, Loss: 0.329447865486145
setp: 1000, Loss: 0.36160093545913696
setp: 1100, Loss: 0.32431676983833313
setp: 1200, Loss: 0.38337329030036926
setp: 1300, Loss: 0.328830748796463
setp: 1400, Loss: 0.36376529932022095
setp: 1500, Loss: 0.3212850093841553
setp: 1600, Loss: 0.31832730770111084
setp: 1700, Loss: 0.31954947113990784
setp: 1800, Loss: 0.31967777013778687
setp: 1900, Loss: 0.3509005308151245
setp: 2000, Loss: 0.3190419673919678
setp: 2100, Loss: 0.35067278146743774
setp: 2200, Loss: 0.31963425874710083
setp: 2300, Loss: 0.35154014825820923
setp: 2400, Loss: 0.32178929448127747
setp: 2500, Loss: 0.31940364837646484
setp: 2600, Loss: 0.3193679749965668
setp: 2700, Loss: 0.7776724696159363
setp: 2800, Loss: 0.33121293783187866
setp: 2900, Loss: 0.3214210569858551
setp: 3000, Loss: 0.33846086263656616
setp: 3100, Loss: 0.332912415266037
setp: 3200, Loss: 0.32635462284088135
setp: 3300, Loss: 0.32110267877578735
setp: 3400, Loss: 0.32047468423843384
setp: 3500, Loss: 0.31984055042266846
setp: 3600, Loss: 0.32086482644081116
setp: 3700, Loss: 0.32036444544792175
setp: 3800, Loss: 0.35145363211631775
setp: 3900, Loss: 0.32063809037208557
setp: 4000, Loss: 0.35097262263298035
setp: 4100, Loss: 0.3205500841140747
setp: 4200, Loss: 0.3513965606689453
setp: 4300, Loss: 0.4055144190788269
setp: 4400, Loss: 0.3330399990081787
setp: 4500, Loss: 0.32355228066444397
setp: 4600, Loss: 0.3642215132713318
setp: 4700, Loss: 0.3406357765197754
setp: 4800, Loss: 0.3247735798358917
setp: 4900, Loss: 0.3229929506778717
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 1.0
recall: 0.9855769230769231
F_score: 0.9927360774818402
validating...
acc: 0.8881578947368421
precision: 0.847457627118644
recall: 0.8620689655172413
F_score: 0.8547008547008546
******fold 4******
[223, 385]
training...
setp: 0, Loss: 0.717557966709137
setp: 100, Loss: 0.6613004803657532
setp: 200, Loss: 0.6583636999130249
setp: 300, Loss: 0.5754842758178711
setp: 400, Loss: 0.47257065773010254
setp: 500, Loss: 0.39512479305267334
setp: 600, Loss: 0.36485007405281067
setp: 700, Loss: 0.3428269922733307
setp: 800, Loss: 0.3263988494873047
setp: 900, Loss: 0.32416805624961853
setp: 1000, Loss: 0.32051384449005127
setp: 1100, Loss: 0.32287588715553284
setp: 1200, Loss: 0.32326802611351013
setp: 1300, Loss: 0.330407977104187
setp: 1400, Loss: 0.3335239291191101
setp: 1500, Loss: 0.34785932302474976
setp: 1600, Loss: 0.31907549500465393
setp: 1700, Loss: 0.32197272777557373
setp: 1800, Loss: 0.31914472579956055
setp: 1900, Loss: 0.31995677947998047
setp: 2000, Loss: 0.3180638253688812
setp: 2100, Loss: 0.3192031979560852
setp: 2200, Loss: 0.3198601007461548
setp: 2300, Loss: 0.3195662200450897
setp: 2400, Loss: 0.32015153765678406
setp: 2500, Loss: 0.32050323486328125
setp: 2600, Loss: 0.32007625699043274
setp: 2700, Loss: 0.3209458887577057
setp: 2800, Loss: 0.32903149724006653
setp: 2900, Loss: 0.31774747371673584
setp: 3000, Loss: 0.3183819353580475
setp: 3100, Loss: 0.3187050521373749
setp: 3200, Loss: 0.3181065022945404
setp: 3300, Loss: 0.32045674324035645
setp: 3400, Loss: 0.31848084926605225
setp: 3500, Loss: 0.31938013434410095
setp: 3600, Loss: 0.31825417280197144
setp: 3700, Loss: 0.3192818760871887
setp: 3800, Loss: 0.32063400745391846
setp: 3900, Loss: 0.3219834268093109
setp: 4000, Loss: 0.3212498426437378
setp: 4100, Loss: 0.3182266354560852
setp: 4200, Loss: 0.31913813948631287
setp: 4300, Loss: 0.3196953535079956
setp: 4400, Loss: 0.3200231194496155
setp: 4500, Loss: 0.3194194734096527
setp: 4600, Loss: 0.31969913840293884
setp: 4700, Loss: 0.32015517354011536
setp: 4800, Loss: 0.31839925050735474
setp: 4900, Loss: 0.3196418583393097
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9078947368421053
precision: 0.8536585365853658
recall: 0.813953488372093
F_score: 0.8333333333333333
model saved.
avg_acc: 0.8473684210526315, avg_f_score: 0.6736696330755736
==========arousal==========
******fold 0******
[339, 269]
training...
setp: 0, Loss: 0.6913703680038452
setp: 100, Loss: 0.6225365400314331
setp: 200, Loss: 0.6510918140411377
setp: 300, Loss: 0.6301499605178833
setp: 400, Loss: 0.6098326444625854
setp: 500, Loss: 0.5875511169433594
setp: 600, Loss: 0.5594615340232849
setp: 700, Loss: 0.5169332027435303
setp: 800, Loss: 0.4160633981227875
setp: 900, Loss: 0.38695403933525085
setp: 1000, Loss: 0.3755742907524109
setp: 1100, Loss: 0.32965803146362305
setp: 1200, Loss: 0.3281850516796112
setp: 1300, Loss: 0.36020421981811523
setp: 1400, Loss: 0.3262113928794861
setp: 1500, Loss: 0.32547295093536377
setp: 1600, Loss: 0.3335394859313965
setp: 1700, Loss: 0.32708314061164856
setp: 1800, Loss: 0.33357909321784973
setp: 1900, Loss: 0.325679749250412
setp: 2000, Loss: 0.33340689539909363
setp: 2100, Loss: 0.34224382042884827
setp: 2200, Loss: 0.3540284335613251
setp: 2300, Loss: 0.34945040941238403
setp: 2400, Loss: 0.32082295417785645
setp: 2500, Loss: 0.3205970227718353
setp: 2600, Loss: 0.32147887349128723
setp: 2700, Loss: 0.3192245066165924
setp: 2800, Loss: 0.32074105739593506
setp: 2900, Loss: 0.3212567865848541
setp: 3000, Loss: 0.31883499026298523
setp: 3100, Loss: 0.31950727105140686
setp: 3200, Loss: 0.3207196891307831
setp: 3300, Loss: 0.3449913263320923
setp: 3400, Loss: 0.3283824026584625
setp: 3500, Loss: 0.3219892680644989
setp: 3600, Loss: 0.3223608434200287
setp: 3700, Loss: 0.3228073716163635
setp: 3800, Loss: 0.3214890658855438
setp: 3900, Loss: 0.3211843967437744
setp: 4000, Loss: 0.321197509765625
setp: 4100, Loss: 0.35300496220588684
setp: 4200, Loss: 0.32005858421325684
setp: 4300, Loss: 0.3226202130317688
setp: 4400, Loss: 0.32144463062286377
setp: 4500, Loss: 0.3211629092693329
setp: 4600, Loss: 0.5751314163208008
setp: 4700, Loss: 0.3634800612926483
setp: 4800, Loss: 0.3290686309337616
setp: 4900, Loss: 0.3248678743839264
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 0.9970414201183432
recall: 0.9941002949852508
F_score: 0.9955686853766617
validating...
acc: 0.9078947368421053
precision: 0.9375
recall: 0.9183673469387755
F_score: 0.9278350515463918
******fold 1******
[350, 258]
training...
setp: 0, Loss: 0.6773589849472046
setp: 100, Loss: 0.6317083239555359
setp: 200, Loss: 0.6420706510543823
setp: 300, Loss: 0.6209095120429993
setp: 400, Loss: 0.5636681914329529
setp: 500, Loss: 0.5033456683158875
setp: 600, Loss: 0.4377381503582001
setp: 700, Loss: 0.4190715551376343
setp: 800, Loss: 0.345245897769928
setp: 900, Loss: 0.34405383467674255
setp: 1000, Loss: 0.33310988545417786
setp: 1100, Loss: 0.332437127828598
setp: 1200, Loss: 0.32701748609542847
setp: 1300, Loss: 0.3432489037513733
setp: 1400, Loss: 0.32914915680885315
setp: 1500, Loss: 0.3248636722564697
setp: 1600, Loss: 0.32460540533065796
setp: 1700, Loss: 0.32374629378318787
setp: 1800, Loss: 0.37028926610946655
setp: 1900, Loss: 0.3247351050376892
setp: 2000, Loss: 0.32854363322257996
setp: 2100, Loss: 0.3231517970561981
setp: 2200, Loss: 0.32616764307022095
setp: 2300, Loss: 0.3213343620300293
setp: 2400, Loss: 0.32303059101104736
setp: 2500, Loss: 0.3219141960144043
setp: 2600, Loss: 0.3244022727012634
setp: 2700, Loss: 0.3213285207748413
setp: 2800, Loss: 0.3235090672969818
setp: 2900, Loss: 0.3212858736515045
setp: 3000, Loss: 0.3217914402484894
setp: 3100, Loss: 0.32114797830581665
setp: 3200, Loss: 0.3221394717693329
setp: 3300, Loss: 0.3231988549232483
setp: 3400, Loss: 0.3221260607242584
setp: 3500, Loss: 0.32072001695632935
setp: 3600, Loss: 0.32037997245788574
setp: 3700, Loss: 0.5378284454345703
setp: 3800, Loss: 0.39944595098495483
setp: 3900, Loss: 0.3524226248264313
setp: 4000, Loss: 0.3343151807785034
setp: 4100, Loss: 0.32722732424736023
setp: 4200, Loss: 0.3655283749103546
setp: 4300, Loss: 0.32558152079582214
setp: 4400, Loss: 0.3231320381164551
setp: 4500, Loss: 0.32713353633880615
setp: 4600, Loss: 0.32235661149024963
setp: 4700, Loss: 0.34609293937683105
setp: 4800, Loss: 0.3323496878147125
setp: 4900, Loss: 0.32189393043518066
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.868421052631579
precision: 0.8681318681318682
recall: 0.9080459770114943
F_score: 0.8876404494382023
******fold 2******
[348, 260]
training...
setp: 0, Loss: 0.6990483403205872
setp: 100, Loss: 0.624169647693634
setp: 200, Loss: 0.5636334419250488
setp: 300, Loss: 0.5945038199424744
setp: 400, Loss: 0.5651729702949524
setp: 500, Loss: 0.561913251876831
setp: 600, Loss: 0.5357235074043274
setp: 700, Loss: 0.47283077239990234
setp: 800, Loss: 0.39694687724113464
setp: 900, Loss: 0.4136364459991455
setp: 1000, Loss: 0.35619184374809265
setp: 1100, Loss: 0.3299369513988495
setp: 1200, Loss: 0.3702537417411804
setp: 1300, Loss: 0.33043450117111206
setp: 1400, Loss: 0.3316553235054016
setp: 1500, Loss: 0.3257902264595032
setp: 1600, Loss: 0.3299828767776489
setp: 1700, Loss: 0.32752174139022827
setp: 1800, Loss: 0.3228558599948883
setp: 1900, Loss: 0.33633914589881897
setp: 2000, Loss: 0.3247714042663574
setp: 2100, Loss: 0.3231770396232605
setp: 2200, Loss: 0.3236149251461029
setp: 2300, Loss: 0.3202536404132843
setp: 2400, Loss: 0.32158952951431274
setp: 2500, Loss: 0.3240630030632019
setp: 2600, Loss: 0.3216477036476135
setp: 2700, Loss: 0.3215493857860565
setp: 2800, Loss: 0.3225129246711731
setp: 2900, Loss: 0.3222784996032715
setp: 3000, Loss: 0.31942689418792725
setp: 3100, Loss: 0.35076263546943665
setp: 3200, Loss: 0.32066306471824646
setp: 3300, Loss: 0.32194194197654724
setp: 3400, Loss: 0.3206484913825989
setp: 3500, Loss: 0.32933509349823
setp: 3600, Loss: 0.3362830579280853
setp: 3700, Loss: 0.31886231899261475
setp: 3800, Loss: 0.32036343216896057
setp: 3900, Loss: 0.31891006231307983
setp: 4000, Loss: 0.3194827139377594
setp: 4100, Loss: 0.3196544349193573
setp: 4200, Loss: 0.31954240798950195
setp: 4300, Loss: 0.32019370794296265
setp: 4400, Loss: 0.32191726565361023
setp: 4500, Loss: 0.31970885396003723
setp: 4600, Loss: 0.3199680745601654
setp: 4700, Loss: 0.32033881545066833
setp: 4800, Loss: 0.3203907608985901
setp: 4900, Loss: 0.3182781934738159
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.997134670487106
recall: 1.0
F_score: 0.9985652797704447
validating...
acc: 0.8486842105263158
precision: 0.8928571428571429
recall: 0.8426966292134831
F_score: 0.8670520231213873
******fold 3******
[346, 262]
training...
setp: 0, Loss: 0.6853576898574829
setp: 100, Loss: 0.6544631719589233
setp: 200, Loss: 0.5575776696205139
setp: 300, Loss: 0.6952945590019226
setp: 400, Loss: 0.5954986214637756
setp: 500, Loss: 0.5901584625244141
setp: 600, Loss: 0.5365435481071472
setp: 700, Loss: 0.4965260922908783
setp: 800, Loss: 0.4863925576210022
setp: 900, Loss: 0.42038002610206604
setp: 1000, Loss: 0.3994135856628418
setp: 1100, Loss: 0.34555622935295105
setp: 1200, Loss: 0.3417370021343231
setp: 1300, Loss: 0.346558541059494
setp: 1400, Loss: 0.326104998588562
setp: 1500, Loss: 0.32583579421043396
setp: 1600, Loss: 0.32702887058258057
setp: 1700, Loss: 0.3220435380935669
setp: 1800, Loss: 0.32202672958374023
setp: 1900, Loss: 0.32283511757850647
setp: 2000, Loss: 0.31978684663772583
setp: 2100, Loss: 0.32044175267219543
setp: 2200, Loss: 0.4031326472759247
setp: 2300, Loss: 0.340404748916626
setp: 2400, Loss: 0.32051753997802734
setp: 2500, Loss: 0.31950417160987854
setp: 2600, Loss: 0.31765010952949524
setp: 2700, Loss: 0.318335622549057
setp: 2800, Loss: 0.31817319989204407
setp: 2900, Loss: 0.3197646737098694
setp: 3000, Loss: 0.3184501826763153
setp: 3100, Loss: 0.3183804154396057
setp: 3200, Loss: 0.3183004558086395
setp: 3300, Loss: 0.3187403678894043
setp: 3400, Loss: 0.3190362751483917
setp: 3500, Loss: 0.3177171051502228
setp: 3600, Loss: 0.5555739402770996
setp: 3700, Loss: 0.3664831817150116
setp: 3800, Loss: 0.3474951684474945
setp: 3900, Loss: 0.3382955491542816
setp: 4000, Loss: 0.33424392342567444
setp: 4100, Loss: 0.33012521266937256
setp: 4200, Loss: 0.32296204566955566
setp: 4300, Loss: 0.33219611644744873
setp: 4400, Loss: 0.3248487114906311
setp: 4500, Loss: 0.37831684947013855
setp: 4600, Loss: 0.3198500871658325
setp: 4700, Loss: 0.3209172487258911
setp: 4800, Loss: 0.32525551319122314
setp: 4900, Loss: 0.32201769948005676
training successfully ended.
validating...
acc: 0.912828947368421
precision: 0.8671679197994987
recall: 1.0
F_score: 0.9288590604026846
validating...
acc: 0.8618421052631579
precision: 0.8181818181818182
recall: 0.989010989010989
F_score: 0.8955223880597015
******fold 4******
[365, 243]
training...
setp: 0, Loss: 0.6854435205459595
setp: 100, Loss: 0.6605674624443054
setp: 200, Loss: 0.6276507377624512
setp: 300, Loss: 0.6287093758583069
setp: 400, Loss: 0.5404448509216309
setp: 500, Loss: 0.4787555932998657
setp: 600, Loss: 0.40773314237594604
setp: 700, Loss: 0.3750406801700592
setp: 800, Loss: 0.3782689869403839
setp: 900, Loss: 0.33560433983802795
setp: 1000, Loss: 0.32629886269569397
setp: 1100, Loss: 0.35530152916908264
setp: 1200, Loss: 0.3275993764400482
setp: 1300, Loss: 0.3222520649433136
setp: 1400, Loss: 0.332564115524292
setp: 1500, Loss: 0.31999140977859497
setp: 1600, Loss: 0.3219309151172638
setp: 1700, Loss: 0.3207128643989563
setp: 1800, Loss: 0.35100802779197693
setp: 1900, Loss: 0.32187798619270325
setp: 2000, Loss: 0.3211175501346588
setp: 2100, Loss: 0.3216122090816498
setp: 2200, Loss: 0.32220444083213806
setp: 2300, Loss: 0.4222654104232788
setp: 2400, Loss: 0.3246455490589142
setp: 2500, Loss: 0.32050859928131104
setp: 2600, Loss: 0.32200998067855835
setp: 2700, Loss: 0.3191353976726532
setp: 2800, Loss: 0.32182884216308594
setp: 2900, Loss: 0.32088837027549744
setp: 3000, Loss: 0.3208429217338562
setp: 3100, Loss: 0.31996628642082214
setp: 3200, Loss: 0.31823307275772095
setp: 3300, Loss: 0.32012826204299927
setp: 3400, Loss: 0.33661919832229614
setp: 3500, Loss: 0.3246818482875824
setp: 3600, Loss: 0.31892484426498413
setp: 3700, Loss: 0.31991592049598694
setp: 3800, Loss: 0.3200942277908325
setp: 3900, Loss: 0.3202856779098511
setp: 4000, Loss: 0.3204057812690735
setp: 4100, Loss: 0.3204399347305298
setp: 4200, Loss: 0.3193736970424652
setp: 4300, Loss: 0.319208025932312
setp: 4400, Loss: 0.32115551829338074
setp: 4500, Loss: 0.31986579298973083
setp: 4600, Loss: 0.3185695707798004
setp: 4700, Loss: 0.32143867015838623
setp: 4800, Loss: 0.43543556332588196
setp: 4900, Loss: 0.32153066992759705
training successfully ended.
validating...
acc: 0.9950657894736842
precision: 1.0
recall: 0.9917808219178083
F_score: 0.9958734525447043
validating...
acc: 0.9078947368421053
precision: 0.9027777777777778
recall: 0.9027777777777778
F_score: 0.9027777777777778
model saved.
avg_acc: 0.8789473684210527, avg_f_score: 0.896165537988692
-------------subject: 27-------------
==========valence==========
******fold 0******
[154, 454]
training...
setp: 0, Loss: 0.6996002197265625
setp: 100, Loss: 0.6314548850059509
setp: 200, Loss: 0.5683706998825073
setp: 300, Loss: 0.45452219247817993
setp: 400, Loss: 0.46755120158195496
setp: 500, Loss: 0.36657512187957764
setp: 600, Loss: 0.3333475589752197
setp: 700, Loss: 0.358442485332489
setp: 800, Loss: 0.3514934480190277
setp: 900, Loss: 0.3192952871322632
setp: 1000, Loss: 0.3209669589996338
setp: 1100, Loss: 0.349501371383667
setp: 1200, Loss: 0.32077541947364807
setp: 1300, Loss: 0.31871235370635986
setp: 1400, Loss: 0.3160856366157532
setp: 1500, Loss: 0.3159679174423218
setp: 1600, Loss: 0.31709223985671997
setp: 1700, Loss: 0.31683462858200073
setp: 1800, Loss: 0.31588608026504517
setp: 1900, Loss: 0.323754221200943
setp: 2000, Loss: 0.316175639629364
setp: 2100, Loss: 0.3191222846508026
setp: 2200, Loss: 0.31702134013175964
setp: 2300, Loss: 0.3161027729511261
setp: 2400, Loss: 0.31776902079582214
setp: 2500, Loss: 0.31704485416412354
setp: 2600, Loss: 0.31727710366249084
setp: 2700, Loss: 0.31696048378944397
setp: 2800, Loss: 0.32037845253944397
setp: 2900, Loss: 0.31666791439056396
setp: 3000, Loss: 0.3173612058162689
setp: 3100, Loss: 0.3157232701778412
setp: 3200, Loss: 0.3165643513202667
setp: 3300, Loss: 0.31756263971328735
setp: 3400, Loss: 0.3479366600513458
setp: 3500, Loss: 0.3165939748287201
setp: 3600, Loss: 0.31693127751350403
setp: 3700, Loss: 0.3962859511375427
setp: 3800, Loss: 0.31616929173469543
setp: 3900, Loss: 0.31652891635894775
setp: 4000, Loss: 0.3468855619430542
setp: 4100, Loss: 0.31635990738868713
setp: 4200, Loss: 0.31682088971138
setp: 4300, Loss: 0.3157002925872803
setp: 4400, Loss: 0.3155432641506195
setp: 4500, Loss: 0.31704655289649963
setp: 4600, Loss: 0.31613072752952576
setp: 4700, Loss: 0.31560441851615906
setp: 4800, Loss: 0.3171238899230957
setp: 4900, Loss: 0.314689040184021
training successfully ended.
validating...
acc: 0.9966960352422908
precision: 1.0
recall: 0.9933920704845814
F_score: 0.9966850828729282
validating...
acc: 0.9605263157894737
precision: 0.96875
recall: 0.8611111111111112
F_score: 0.911764705882353
******fold 1******
[156, 452]
training...
setp: 0, Loss: 0.7002095580101013
setp: 100, Loss: 0.6409403681755066
setp: 200, Loss: 0.56747967004776
setp: 300, Loss: 0.49335968494415283
setp: 400, Loss: 0.6124194860458374
setp: 500, Loss: 0.45131006836891174
setp: 600, Loss: 0.3772014081478119
setp: 700, Loss: 0.3372243344783783
setp: 800, Loss: 0.37656188011169434
setp: 900, Loss: 0.33060550689697266
setp: 1000, Loss: 0.34552112221717834
setp: 1100, Loss: 0.34631893038749695
setp: 1200, Loss: 0.32198357582092285
setp: 1300, Loss: 0.32155123353004456
setp: 1400, Loss: 0.31694039702415466
setp: 1500, Loss: 0.317688912153244
setp: 1600, Loss: 0.31629040837287903
setp: 1700, Loss: 0.31818079948425293
setp: 1800, Loss: 0.34871768951416016
setp: 1900, Loss: 0.3191527724266052
setp: 2000, Loss: 0.316251277923584
setp: 2100, Loss: 0.3475051820278168
setp: 2200, Loss: 0.31634947657585144
setp: 2300, Loss: 0.31634557247161865
setp: 2400, Loss: 0.3481258451938629
setp: 2500, Loss: 0.34760352969169617
setp: 2600, Loss: 0.31892454624176025
setp: 2700, Loss: 0.3228355348110199
setp: 2800, Loss: 0.34925416111946106
setp: 2900, Loss: 0.3164004683494568
setp: 3000, Loss: 0.3181602954864502
setp: 3100, Loss: 0.3483881950378418
setp: 3200, Loss: 0.3468411862850189
setp: 3300, Loss: 0.3161185681819916
setp: 3400, Loss: 0.31617897748947144
setp: 3500, Loss: 0.3165660500526428
setp: 3600, Loss: 0.31749287247657776
setp: 3700, Loss: 0.31829795241355896
setp: 3800, Loss: 0.31770089268684387
setp: 3900, Loss: 0.31815123558044434
setp: 4000, Loss: 0.31617608666419983
setp: 4100, Loss: 0.3167102336883545
setp: 4200, Loss: 0.5155795216560364
setp: 4300, Loss: 0.389811635017395
setp: 4400, Loss: 0.3662795424461365
setp: 4500, Loss: 0.3348572850227356
setp: 4600, Loss: 0.3656778633594513
setp: 4700, Loss: 0.3586605191230774
setp: 4800, Loss: 0.3579063415527344
setp: 4900, Loss: 0.31987324357032776
training successfully ended.
validating...
acc: 0.9933628318584071
precision: 0.9868995633187773
recall: 1.0
F_score: 0.9934065934065934
validating...
acc: 0.9539473684210527
precision: 0.9090909090909091
recall: 0.8823529411764706
F_score: 0.8955223880597014
******fold 2******
[151, 457]
training...
setp: 0, Loss: 0.6979972124099731
setp: 100, Loss: 0.6930243968963623
setp: 200, Loss: 0.5498391389846802
setp: 300, Loss: 0.3878645896911621
setp: 400, Loss: 0.4021960496902466
setp: 500, Loss: 0.3906186521053314
setp: 600, Loss: 0.3267837166786194
setp: 700, Loss: 0.32466456294059753
setp: 800, Loss: 0.3180159330368042
setp: 900, Loss: 0.3586593270301819
setp: 1000, Loss: 0.3743811845779419
setp: 1100, Loss: 0.364075243473053
setp: 1200, Loss: 0.33606386184692383
setp: 1300, Loss: 0.36384114623069763
setp: 1400, Loss: 0.31756478548049927
setp: 1500, Loss: 0.34632429480552673
setp: 1600, Loss: 0.3166963756084442
setp: 1700, Loss: 0.3477204144001007
setp: 1800, Loss: 0.31729334592819214
setp: 1900, Loss: 0.3186241686344147
setp: 2000, Loss: 0.37044912576675415
setp: 2100, Loss: 0.34842368960380554
setp: 2200, Loss: 0.3170514404773712
setp: 2300, Loss: 0.3477165699005127
setp: 2400, Loss: 0.3172794580459595
setp: 2500, Loss: 0.3256308436393738
setp: 2600, Loss: 0.36647501587867737
setp: 2700, Loss: 0.3209288418292999
setp: 2800, Loss: 0.3165378272533417
setp: 2900, Loss: 0.31780868768692017
setp: 3000, Loss: 0.3166307508945465
setp: 3100, Loss: 0.3470495939254761
setp: 3200, Loss: 0.3157847225666046
setp: 3300, Loss: 0.3167955279350281
setp: 3400, Loss: 0.3169921636581421
setp: 3500, Loss: 0.31730538606643677
setp: 3600, Loss: 0.3160094916820526
setp: 3700, Loss: 0.3154434263706207
setp: 3800, Loss: 0.33032432198524475
setp: 3900, Loss: 0.43570780754089355
setp: 4000, Loss: 0.31652525067329407
setp: 4100, Loss: 0.32925236225128174
setp: 4200, Loss: 0.3172423243522644
setp: 4300, Loss: 0.3154115676879883
setp: 4400, Loss: 0.31645727157592773
setp: 4500, Loss: 0.315619558095932
setp: 4600, Loss: 0.31667736172676086
setp: 4700, Loss: 0.3167637884616852
setp: 4800, Loss: 0.31836700439453125
setp: 4900, Loss: 0.3154374659061432
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9144736842105263
precision: 0.782608695652174
recall: 0.9230769230769231
F_score: 0.8470588235294118
******fold 3******
[148, 460]
training...
setp: 0, Loss: 0.7268477082252502
setp: 100, Loss: 0.60475754737854
setp: 200, Loss: 0.5584207773208618
setp: 300, Loss: 0.3785400390625
setp: 400, Loss: 0.34078794717788696
setp: 500, Loss: 0.33257439732551575
setp: 600, Loss: 0.3737073242664337
setp: 700, Loss: 0.3481122553348541
setp: 800, Loss: 0.3185017704963684
setp: 900, Loss: 0.32268020510673523
setp: 1000, Loss: 0.32121559977531433
setp: 1100, Loss: 0.3207184672355652
setp: 1200, Loss: 0.32029128074645996
setp: 1300, Loss: 0.32152652740478516
setp: 1400, Loss: 0.31749817728996277
setp: 1500, Loss: 0.3468843102455139
setp: 1600, Loss: 0.3182970881462097
setp: 1700, Loss: 0.31855204701423645
setp: 1800, Loss: 0.36088502407073975
setp: 1900, Loss: 0.3176191747188568
setp: 2000, Loss: 0.31683051586151123
setp: 2100, Loss: 0.3193047046661377
setp: 2200, Loss: 0.31705352663993835
setp: 2300, Loss: 0.3176485598087311
setp: 2400, Loss: 0.31698277592658997
setp: 2500, Loss: 0.3165043294429779
setp: 2600, Loss: 0.3166142404079437
setp: 2700, Loss: 0.3164444863796234
setp: 2800, Loss: 0.3195294737815857
setp: 2900, Loss: 0.3344917595386505
setp: 3000, Loss: 0.3614535927772522
setp: 3100, Loss: 0.3201870620250702
setp: 3200, Loss: 0.318037211894989
setp: 3300, Loss: 0.32015687227249146
setp: 3400, Loss: 0.31807786226272583
setp: 3500, Loss: 0.3171287178993225
setp: 3600, Loss: 0.3213655948638916
setp: 3700, Loss: 0.31659793853759766
setp: 3800, Loss: 0.31788355112075806
setp: 3900, Loss: 0.3195318877696991
setp: 4000, Loss: 0.31644725799560547
setp: 4100, Loss: 0.3174721896648407
setp: 4200, Loss: 0.3165237605571747
setp: 4300, Loss: 0.3212631940841675
setp: 4400, Loss: 0.3186483383178711
setp: 4500, Loss: 0.31668606400489807
setp: 4600, Loss: 0.3164808750152588
setp: 4700, Loss: 0.3179279565811157
setp: 4800, Loss: 0.3178558349609375
setp: 4900, Loss: 0.31701692938804626
training successfully ended.
validating...
acc: 0.8782608695652174
precision: 0.8041958041958042
recall: 1.0
F_score: 0.8914728682170543
validating...
acc: 0.7105263157894737
precision: 0.4883720930232558
recall: 1.0
F_score: 0.65625
******fold 4******
[151, 457]
training...
setp: 0, Loss: 0.6959034204483032
setp: 100, Loss: 0.6851398944854736
setp: 200, Loss: 0.5872788429260254
setp: 300, Loss: 0.5263209939002991
setp: 400, Loss: 0.47567853331565857
setp: 500, Loss: 0.46340250968933105
setp: 600, Loss: 0.41897889971733093
setp: 700, Loss: 0.3998001217842102
setp: 800, Loss: 0.3887850046157837
setp: 900, Loss: 0.38486555218696594
setp: 1000, Loss: 0.387751966714859
setp: 1100, Loss: 0.36246728897094727
setp: 1200, Loss: 0.37060362100601196
setp: 1300, Loss: 0.36998236179351807
setp: 1400, Loss: 0.34347718954086304
setp: 1500, Loss: 0.3492613732814789
setp: 1600, Loss: 0.31721970438957214
setp: 1700, Loss: 0.33551785349845886
setp: 1800, Loss: 0.3444119393825531
setp: 1900, Loss: 0.3420826494693756
setp: 2000, Loss: 0.315002977848053
setp: 2100, Loss: 0.31619447469711304
setp: 2200, Loss: 0.3169909715652466
setp: 2300, Loss: 0.3334397077560425
setp: 2400, Loss: 0.3161699175834656
setp: 2500, Loss: 0.3157931864261627
setp: 2600, Loss: 0.31635528802871704
setp: 2700, Loss: 0.3172188997268677
setp: 2800, Loss: 0.3170354962348938
setp: 2900, Loss: 0.3218853175640106
setp: 3000, Loss: 0.31760865449905396
setp: 3100, Loss: 0.3159129321575165
setp: 3200, Loss: 0.3171416223049164
setp: 3300, Loss: 0.31636130809783936
setp: 3400, Loss: 0.3158714771270752
setp: 3500, Loss: 0.46428102254867554
setp: 3600, Loss: 0.3182869851589203
setp: 3700, Loss: 0.3223053216934204
setp: 3800, Loss: 0.31607961654663086
setp: 3900, Loss: 0.31784915924072266
setp: 4000, Loss: 0.31639590859413147
setp: 4100, Loss: 0.3172684907913208
setp: 4200, Loss: 0.3161769211292267
setp: 4300, Loss: 0.3159499168395996
setp: 4400, Loss: 0.3164472281932831
setp: 4500, Loss: 0.3169218897819519
setp: 4600, Loss: 0.31738221645355225
setp: 4700, Loss: 0.6940193176269531
setp: 4800, Loss: 0.5907846689224243
setp: 4900, Loss: 0.3216302990913391
training successfully ended.
validating...
acc: 0.9934354485776805
precision: 0.9870410367170627
recall: 1.0
F_score: 0.9934782608695651
validating...
acc: 0.9539473684210527
precision: 0.9705882352941176
recall: 0.8461538461538461
F_score: 0.9041095890410958
model saved.
avg_acc: 0.8986842105263158, avg_f_score: 0.8429411013025124
==========arousal==========
******fold 0******
[188, 420]
training...
setp: 0, Loss: 0.8154054284095764
setp: 100, Loss: 0.5733779668807983
setp: 200, Loss: 0.6561091542243958
setp: 300, Loss: 0.587174654006958
setp: 400, Loss: 0.49952268600463867
setp: 500, Loss: 0.5597828030586243
setp: 600, Loss: 0.48398357629776
setp: 700, Loss: 0.44097432494163513
setp: 800, Loss: 0.47400963306427
setp: 900, Loss: 0.570052444934845
setp: 1000, Loss: 0.4133050739765167
setp: 1100, Loss: 0.47943341732025146
setp: 1200, Loss: 0.5036433935165405
setp: 1300, Loss: 0.4140682816505432
setp: 1400, Loss: 0.45714935660362244
setp: 1500, Loss: 0.5049023628234863
setp: 1600, Loss: 0.38085514307022095
setp: 1700, Loss: 0.4313479959964752
setp: 1800, Loss: 0.4457905888557434
setp: 1900, Loss: 0.5028525590896606
setp: 2000, Loss: 0.44446223974227905
setp: 2100, Loss: 0.4771367013454437
setp: 2200, Loss: 0.47688785195350647
setp: 2300, Loss: 0.3638855218887329
setp: 2400, Loss: 0.43505769968032837
setp: 2500, Loss: 0.4393244683742523
setp: 2600, Loss: 0.3795381784439087
setp: 2700, Loss: 0.3784511387348175
setp: 2800, Loss: 0.3825262188911438
setp: 2900, Loss: 0.378932386636734
setp: 3000, Loss: 0.3838839828968048
setp: 3100, Loss: 0.44139158725738525
setp: 3200, Loss: 0.4024710953235626
setp: 3300, Loss: 0.4255537986755371
setp: 3400, Loss: 0.44027674198150635
setp: 3500, Loss: 0.3477413058280945
setp: 3600, Loss: 0.3488296568393707
setp: 3700, Loss: 0.37958499789237976
setp: 3800, Loss: 0.48079097270965576
setp: 3900, Loss: 0.3483392298221588
setp: 4000, Loss: 0.4110775291919708
setp: 4100, Loss: 0.41177669167518616
setp: 4200, Loss: 0.40762704610824585
setp: 4300, Loss: 0.3790263533592224
setp: 4400, Loss: 0.4090585708618164
setp: 4500, Loss: 0.3468433916568756
setp: 4600, Loss: 0.34875577688217163
setp: 4700, Loss: 0.38758227229118347
setp: 4800, Loss: 0.3789716958999634
setp: 4900, Loss: 0.34869301319122314
training successfully ended.
validating...
acc: 0.9407894736842105
precision: 0.9935064935064936
recall: 0.8138297872340425
F_score: 0.8947368421052633
validating...
acc: 0.8289473684210527
precision: 1.0
recall: 0.559322033898305
F_score: 0.717391304347826
******fold 1******
[198, 410]
training...
setp: 0, Loss: 0.6806784868240356
setp: 100, Loss: 0.6004553437232971
setp: 200, Loss: 0.569256067276001
setp: 300, Loss: 0.502257764339447
setp: 400, Loss: 0.4952666461467743
setp: 500, Loss: 0.5080451369285583
setp: 600, Loss: 0.519718587398529
setp: 700, Loss: 0.4756680727005005
setp: 800, Loss: 0.5564305186271667
setp: 900, Loss: 0.49106913805007935
setp: 1000, Loss: 0.40689271688461304
setp: 1100, Loss: 0.38048434257507324
setp: 1200, Loss: 0.4276965856552124
setp: 1300, Loss: 0.4180435538291931
setp: 1400, Loss: 0.41444316506385803
setp: 1500, Loss: 0.40452128648757935
setp: 1600, Loss: 0.381331205368042
setp: 1700, Loss: 0.35403385758399963
setp: 1800, Loss: 0.37915322184562683
setp: 1900, Loss: 0.38603657484054565
setp: 2000, Loss: 0.38638049364089966
setp: 2100, Loss: 0.3816259801387787
setp: 2200, Loss: 0.3479432463645935
setp: 2300, Loss: 0.3784688413143158
setp: 2400, Loss: 0.40964579582214355
setp: 2500, Loss: 0.38638103008270264
setp: 2600, Loss: 0.38532671332359314
setp: 2700, Loss: 0.3533021807670593
setp: 2800, Loss: 0.3498837947845459
setp: 2900, Loss: 0.3484595715999603
setp: 3000, Loss: 0.35915276408195496
setp: 3100, Loss: 0.42014428973197937
setp: 3200, Loss: 0.38710981607437134
setp: 3300, Loss: 0.4507019817829132
setp: 3400, Loss: 0.3820159435272217
setp: 3500, Loss: 0.41015389561653137
setp: 3600, Loss: 0.36421796679496765
setp: 3700, Loss: 0.4843513071537018
setp: 3800, Loss: 0.3814162015914917
setp: 3900, Loss: 0.38036784529685974
setp: 4000, Loss: 0.37794265151023865
setp: 4100, Loss: 0.34785157442092896
setp: 4200, Loss: 0.3788955807685852
setp: 4300, Loss: 0.37869951128959656
setp: 4400, Loss: 0.381326287984848
setp: 4500, Loss: 0.3787095248699188
setp: 4600, Loss: 0.347732275724411
setp: 4700, Loss: 0.3507688641548157
setp: 4800, Loss: 0.34731197357177734
setp: 4900, Loss: 0.3476491868495941
training successfully ended.
validating...
acc: 0.9490131578947368
precision: 1.0
recall: 0.8434343434343434
F_score: 0.915068493150685
validating...
acc: 0.8881578947368421
precision: 0.8809523809523809
recall: 0.7551020408163265
F_score: 0.8131868131868131
******fold 2******
[204, 404]
training...
setp: 0, Loss: 0.636000394821167
setp: 100, Loss: 0.6115989089012146
setp: 200, Loss: 0.5897403359413147
setp: 300, Loss: 0.568737268447876
setp: 400, Loss: 0.5420815944671631
setp: 500, Loss: 0.4729115962982178
setp: 600, Loss: 0.4210969805717468
setp: 700, Loss: 0.39492955803871155
setp: 800, Loss: 0.36538487672805786
setp: 900, Loss: 0.33428800106048584
setp: 1000, Loss: 0.32348281145095825
setp: 1100, Loss: 0.32396289706230164
setp: 1200, Loss: 0.32301563024520874
setp: 1300, Loss: 0.32227256894111633
setp: 1400, Loss: 0.3190987706184387
setp: 1500, Loss: 0.33121752738952637
setp: 1600, Loss: 0.3191642463207245
setp: 1700, Loss: 0.3183012306690216
setp: 1800, Loss: 0.31851813197135925
setp: 1900, Loss: 0.3199254870414734
setp: 2000, Loss: 0.31817856431007385
setp: 2100, Loss: 0.319312185049057
setp: 2200, Loss: 0.3189300298690796
setp: 2300, Loss: 0.322636216878891
setp: 2400, Loss: 0.3177531361579895
setp: 2500, Loss: 0.3189481794834137
setp: 2600, Loss: 0.33634814620018005
setp: 2700, Loss: 0.3510396182537079
setp: 2800, Loss: 0.32083576917648315
setp: 2900, Loss: 0.318092942237854
setp: 3000, Loss: 0.32004499435424805
setp: 3100, Loss: 0.3218529522418976
setp: 3200, Loss: 0.3327944278717041
setp: 3300, Loss: 0.3173300325870514
setp: 3400, Loss: 0.317136287689209
setp: 3500, Loss: 0.317672461271286
setp: 3600, Loss: 0.3169327974319458
setp: 3700, Loss: 0.3745978772640228
setp: 3800, Loss: 0.31813254952430725
setp: 3900, Loss: 0.3172086775302887
setp: 4000, Loss: 0.32017213106155396
setp: 4100, Loss: 0.3185602128505707
setp: 4200, Loss: 0.3177614212036133
setp: 4300, Loss: 0.3169812560081482
setp: 4400, Loss: 0.31784331798553467
setp: 4500, Loss: 0.31776079535484314
setp: 4600, Loss: 0.31904542446136475
setp: 4700, Loss: 0.33106258511543274
setp: 4800, Loss: 0.31655946373939514
setp: 4900, Loss: 0.3175714313983917
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9671052631578947
precision: 1.0
recall: 0.8837209302325582
F_score: 0.9382716049382717
******fold 3******
[198, 410]
training...
setp: 0, Loss: 0.7040042877197266
setp: 100, Loss: 0.620154619216919
setp: 200, Loss: 0.5745835900306702
setp: 300, Loss: 0.6032349467277527
setp: 400, Loss: 0.5689769983291626
setp: 500, Loss: 0.5074306726455688
setp: 600, Loss: 0.5072298645973206
setp: 700, Loss: 0.4952106177806854
setp: 800, Loss: 0.46654945611953735
setp: 900, Loss: 0.38826805353164673
setp: 1000, Loss: 0.34932106733322144
setp: 1100, Loss: 0.41980910301208496
setp: 1200, Loss: 0.47563135623931885
setp: 1300, Loss: 0.3308892846107483
setp: 1400, Loss: 0.3820496201515198
setp: 1500, Loss: 0.3796105682849884
setp: 1600, Loss: 0.38112470507621765
setp: 1700, Loss: 0.4109555780887604
setp: 1800, Loss: 0.3793319761753082
setp: 1900, Loss: 0.42056751251220703
setp: 2000, Loss: 0.34904128313064575
setp: 2100, Loss: 0.35053715109825134
setp: 2200, Loss: 0.47401943802833557
setp: 2300, Loss: 0.3803338408470154
setp: 2400, Loss: 0.3672270178794861
setp: 2500, Loss: 0.3851791322231293
setp: 2600, Loss: 0.41999155282974243
setp: 2700, Loss: 0.40894556045532227
setp: 2800, Loss: 0.37764015793800354
setp: 2900, Loss: 0.34652963280677795
setp: 3000, Loss: 0.3810823857784271
setp: 3100, Loss: 0.3805401623249054
setp: 3200, Loss: 0.317993700504303
setp: 3300, Loss: 0.3784467577934265
setp: 3400, Loss: 0.3776693046092987
setp: 3500, Loss: 0.37954163551330566
setp: 3600, Loss: 0.37953776121139526
setp: 3700, Loss: 0.38130706548690796
setp: 3800, Loss: 0.3486759066581726
setp: 3900, Loss: 0.34811830520629883
setp: 4000, Loss: 0.34815114736557007
setp: 4100, Loss: 0.3493158519268036
setp: 4200, Loss: 0.3782009184360504
setp: 4300, Loss: 0.3563677668571472
setp: 4400, Loss: 0.397592157125473
setp: 4500, Loss: 0.39874717593193054
setp: 4600, Loss: 0.41165080666542053
setp: 4700, Loss: 0.40611734986305237
setp: 4800, Loss: 0.3466796278953552
setp: 4900, Loss: 0.40901654958724976
training successfully ended.
validating...
acc: 0.9490131578947368
precision: 1.0
recall: 0.8434343434343434
F_score: 0.915068493150685
validating...
acc: 0.9078947368421053
precision: 0.926829268292683
recall: 0.7755102040816326
F_score: 0.8444444444444446
******fold 4******
[200, 408]
training...
setp: 0, Loss: 0.7720966935157776
setp: 100, Loss: 0.6208885908126831
setp: 200, Loss: 0.5876845717430115
setp: 300, Loss: 0.5600782632827759
setp: 400, Loss: 0.5260120630264282
setp: 500, Loss: 0.46306997537612915
setp: 600, Loss: 0.43602341413497925
setp: 700, Loss: 0.4812973737716675
setp: 800, Loss: 0.4992992877960205
setp: 900, Loss: 0.4533481001853943
setp: 1000, Loss: 0.4570196866989136
setp: 1100, Loss: 0.45666229724884033
setp: 1200, Loss: 0.4473215341567993
setp: 1300, Loss: 0.41763532161712646
setp: 1400, Loss: 0.3920274078845978
setp: 1500, Loss: 0.41496285796165466
setp: 1600, Loss: 0.3271804451942444
setp: 1700, Loss: 0.37972530722618103
setp: 1800, Loss: 0.38332316279411316
setp: 1900, Loss: 0.41443371772766113
setp: 2000, Loss: 0.383628249168396
setp: 2100, Loss: 0.3852861821651459
setp: 2200, Loss: 0.37930551171302795
setp: 2300, Loss: 0.3799346685409546
setp: 2400, Loss: 0.34728655219078064
setp: 2500, Loss: 0.38561391830444336
setp: 2600, Loss: 0.3801899254322052
setp: 2700, Loss: 0.48557427525520325
setp: 2800, Loss: 0.41189247369766235
setp: 2900, Loss: 0.3178965449333191
setp: 3000, Loss: 0.3833242654800415
setp: 3100, Loss: 0.4017159938812256
setp: 3200, Loss: 0.3866170048713684
setp: 3300, Loss: 0.3917136490345001
setp: 3400, Loss: 0.4121236205101013
setp: 3500, Loss: 0.3458157181739807
setp: 3600, Loss: 0.37751615047454834
setp: 3700, Loss: 0.378017395734787
setp: 3800, Loss: 0.4114879071712494
setp: 3900, Loss: 0.3791658580303192
setp: 4000, Loss: 0.3790571093559265
setp: 4100, Loss: 0.3787941336631775
setp: 4200, Loss: 0.378470778465271
setp: 4300, Loss: 0.3480948209762573
setp: 4400, Loss: 0.3784858286380768
setp: 4500, Loss: 0.37888282537460327
setp: 4600, Loss: 0.3802877962589264
setp: 4700, Loss: 0.40998974442481995
setp: 4800, Loss: 0.3174528181552887
setp: 4900, Loss: 0.37961146235466003
training successfully ended.
validating...
acc: 0.9407894736842105
precision: 1.0
recall: 0.82
F_score: 0.9010989010989011
validating...
acc: 0.9144736842105263
precision: 0.9473684210526315
recall: 0.7659574468085106
F_score: 0.8470588235294116
model saved.
avg_acc: 0.9013157894736843, avg_f_score: 0.8320705980893534
-------------subject: 28-------------
==========valence==========
******fold 0******
[216, 392]
training...
setp: 0, Loss: 0.6816281080245972
setp: 100, Loss: 0.6396813988685608
setp: 200, Loss: 0.5398588180541992
setp: 300, Loss: 0.585207998752594
setp: 400, Loss: 0.45836135745048523
setp: 500, Loss: 0.3800566494464874
setp: 600, Loss: 0.3364713788032532
setp: 700, Loss: 0.3382909595966339
setp: 800, Loss: 0.3291628062725067
setp: 900, Loss: 0.3557068407535553
setp: 1000, Loss: 0.319082647562027
setp: 1100, Loss: 0.3202115595340729
setp: 1200, Loss: 0.3219865560531616
setp: 1300, Loss: 0.3198462426662445
setp: 1400, Loss: 0.32099026441574097
setp: 1500, Loss: 0.3181942105293274
setp: 1600, Loss: 0.31849348545074463
setp: 1700, Loss: 0.3177294135093689
setp: 1800, Loss: 0.32913973927497864
setp: 1900, Loss: 0.33203986287117004
setp: 2000, Loss: 0.3181588351726532
setp: 2100, Loss: 0.3170892894268036
setp: 2200, Loss: 0.31830641627311707
setp: 2300, Loss: 0.3173430562019348
setp: 2400, Loss: 0.3206023871898651
setp: 2500, Loss: 0.31829071044921875
setp: 2600, Loss: 0.4458918869495392
setp: 2700, Loss: 0.32566919922828674
setp: 2800, Loss: 0.31830403208732605
setp: 2900, Loss: 0.3168748915195465
setp: 3000, Loss: 0.3181418776512146
setp: 3100, Loss: 0.32029658555984497
setp: 3200, Loss: 0.31809401512145996
setp: 3300, Loss: 0.31873542070388794
setp: 3400, Loss: 0.3176867961883545
setp: 3500, Loss: 0.3184778392314911
setp: 3600, Loss: 0.324913889169693
setp: 3700, Loss: 0.31974563002586365
setp: 3800, Loss: 0.31908008456230164
setp: 3900, Loss: 0.31930214166641235
setp: 4000, Loss: 0.3171318471431732
setp: 4100, Loss: 0.3197939693927765
setp: 4200, Loss: 0.32249078154563904
setp: 4300, Loss: 0.3195788264274597
setp: 4400, Loss: 0.3173820376396179
setp: 4500, Loss: 0.31824955344200134
setp: 4600, Loss: 0.31872060894966125
setp: 4700, Loss: 0.31778016686439514
setp: 4800, Loss: 0.31794309616088867
setp: 4900, Loss: 0.31847721338272095
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9953703703703703
F_score: 0.9976798143851509
validating...
acc: 0.9539473684210527
precision: 1.0
recall: 0.8985507246376812
F_score: 0.9465648854961832
******fold 1******
[231, 377]
training...
setp: 0, Loss: 0.822339653968811
setp: 100, Loss: 0.6231852173805237
setp: 200, Loss: 0.5414208173751831
setp: 300, Loss: 0.5687283873558044
setp: 400, Loss: 0.524294376373291
setp: 500, Loss: 0.3916214108467102
setp: 600, Loss: 0.3706660270690918
setp: 700, Loss: 0.3450419306755066
setp: 800, Loss: 0.3351233899593353
setp: 900, Loss: 0.32438382506370544
setp: 1000, Loss: 0.3395414650440216
setp: 1100, Loss: 0.32376453280448914
setp: 1200, Loss: 0.321450799703598
setp: 1300, Loss: 0.322733610868454
setp: 1400, Loss: 0.3469429612159729
setp: 1500, Loss: 0.36784473061561584
setp: 1600, Loss: 0.32332369685173035
setp: 1700, Loss: 0.3181072473526001
setp: 1800, Loss: 0.31928780674934387
setp: 1900, Loss: 0.3209593594074249
setp: 2000, Loss: 0.31940165162086487
setp: 2100, Loss: 0.317892849445343
setp: 2200, Loss: 0.32015740871429443
setp: 2300, Loss: 0.32004716992378235
setp: 2400, Loss: 0.4061271846294403
setp: 2500, Loss: 0.3213222920894623
setp: 2600, Loss: 0.37970542907714844
setp: 2700, Loss: 0.3220272958278656
setp: 2800, Loss: 0.31784531474113464
setp: 2900, Loss: 0.31890901923179626
setp: 3000, Loss: 0.3177672326564789
setp: 3100, Loss: 0.31909847259521484
setp: 3200, Loss: 0.31983619928359985
setp: 3300, Loss: 0.3207579255104065
setp: 3400, Loss: 0.31768348813056946
setp: 3500, Loss: 0.3177601993083954
setp: 3600, Loss: 0.31740427017211914
setp: 3700, Loss: 0.3189659118652344
setp: 3800, Loss: 0.4273485243320465
setp: 3900, Loss: 0.3457178473472595
setp: 4000, Loss: 0.327997624874115
setp: 4100, Loss: 0.32705333828926086
setp: 4200, Loss: 0.328186959028244
setp: 4300, Loss: 0.3268335163593292
setp: 4400, Loss: 0.32806044816970825
setp: 4500, Loss: 0.323434978723526
setp: 4600, Loss: 0.32560354471206665
setp: 4700, Loss: 0.32219305634498596
setp: 4800, Loss: 0.32401859760284424
setp: 4900, Loss: 0.32409825921058655
training successfully ended.
validating...
acc: 0.9835526315789473
precision: 0.9585062240663901
recall: 1.0
F_score: 0.9788135593220338
validating...
acc: 0.881578947368421
precision: 0.7647058823529411
recall: 0.9629629629629629
F_score: 0.8524590163934426
******fold 2******
[226, 382]
training...
setp: 0, Loss: 0.821626603603363
setp: 100, Loss: 0.660685122013092
setp: 200, Loss: 0.5628464221954346
setp: 300, Loss: 0.5970632433891296
setp: 400, Loss: 0.4653772711753845
setp: 500, Loss: 0.3968636393547058
setp: 600, Loss: 0.35878199338912964
setp: 700, Loss: 0.3411279320716858
setp: 800, Loss: 0.4049447178840637
setp: 900, Loss: 0.3222070336341858
setp: 1000, Loss: 0.3520025312900543
setp: 1100, Loss: 0.3225436806678772
setp: 1200, Loss: 0.323288232088089
setp: 1300, Loss: 0.31863224506378174
setp: 1400, Loss: 0.3202875256538391
setp: 1500, Loss: 0.3467535078525543
setp: 1600, Loss: 0.3288618326187134
setp: 1700, Loss: 0.3186158835887909
setp: 1800, Loss: 0.3192083239555359
setp: 1900, Loss: 0.32082435488700867
setp: 2000, Loss: 0.31873422861099243
setp: 2100, Loss: 0.3172260522842407
setp: 2200, Loss: 0.32696446776390076
setp: 2300, Loss: 0.3198094367980957
setp: 2400, Loss: 0.3199736475944519
setp: 2500, Loss: 0.31800177693367004
setp: 2600, Loss: 0.3184235692024231
setp: 2700, Loss: 0.3189648389816284
setp: 2800, Loss: 0.31777918338775635
setp: 2900, Loss: 0.3185318410396576
setp: 3000, Loss: 0.31958460807800293
setp: 3100, Loss: 0.31957903504371643
setp: 3200, Loss: 0.31704071164131165
setp: 3300, Loss: 0.3196339011192322
setp: 3400, Loss: 0.5734171867370605
setp: 3500, Loss: 0.42159217596054077
setp: 3600, Loss: 0.34838035702705383
setp: 3700, Loss: 0.36701011657714844
setp: 3800, Loss: 0.3718280494213104
setp: 3900, Loss: 0.3258213698863983
setp: 4000, Loss: 0.3243633806705475
setp: 4100, Loss: 0.3274098336696625
setp: 4200, Loss: 0.32416996359825134
setp: 4300, Loss: 0.3254922032356262
setp: 4400, Loss: 0.347819060087204
setp: 4500, Loss: 0.3205016255378723
setp: 4600, Loss: 0.32242393493652344
setp: 4700, Loss: 0.3208845257759094
setp: 4800, Loss: 0.32115477323532104
setp: 4900, Loss: 0.3228159546852112
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9807692307692307
recall: 0.864406779661017
F_score: 0.918918918918919
******fold 3******
[224, 384]
training...
setp: 0, Loss: 0.6642327904701233
setp: 100, Loss: 0.6591355800628662
setp: 200, Loss: 0.5414785146713257
setp: 300, Loss: 0.49954429268836975
setp: 400, Loss: 0.4174717664718628
setp: 500, Loss: 0.3548407256603241
setp: 600, Loss: 0.3364611566066742
setp: 700, Loss: 0.3258001506328583
setp: 800, Loss: 0.3282620906829834
setp: 900, Loss: 0.3240564465522766
setp: 1000, Loss: 0.3216511309146881
setp: 1100, Loss: 0.3212257921695709
setp: 1200, Loss: 0.3208867609500885
setp: 1300, Loss: 0.3201442062854767
setp: 1400, Loss: 0.3772551119327545
setp: 1500, Loss: 0.3215683102607727
setp: 1600, Loss: 0.36780285835266113
setp: 1700, Loss: 0.3175961375236511
setp: 1800, Loss: 0.3381884694099426
setp: 1900, Loss: 0.3180455267429352
setp: 2000, Loss: 0.32489174604415894
setp: 2100, Loss: 0.31785449385643005
setp: 2200, Loss: 0.3192891776561737
setp: 2300, Loss: 0.3179245591163635
setp: 2400, Loss: 0.3207656443119049
setp: 2500, Loss: 0.31821075081825256
setp: 2600, Loss: 0.31786492466926575
setp: 2700, Loss: 0.31895485520362854
setp: 2800, Loss: 0.3196527361869812
setp: 2900, Loss: 0.3184210956096649
setp: 3000, Loss: 0.3185306191444397
setp: 3100, Loss: 0.31906309723854065
setp: 3200, Loss: 0.3189716041088104
setp: 3300, Loss: 0.3191295862197876
setp: 3400, Loss: 0.6616157293319702
setp: 3500, Loss: 0.32868874073028564
setp: 3600, Loss: 0.32291874289512634
setp: 3700, Loss: 0.3367544114589691
setp: 3800, Loss: 0.31755897402763367
setp: 3900, Loss: 0.31749579310417175
setp: 4000, Loss: 0.31691598892211914
setp: 4100, Loss: 0.31800538301467896
setp: 4200, Loss: 0.31819021701812744
setp: 4300, Loss: 0.3191654682159424
setp: 4400, Loss: 0.3184414505958557
setp: 4500, Loss: 0.31759628653526306
setp: 4600, Loss: 0.3175886869430542
setp: 4700, Loss: 0.3177362382411957
setp: 4800, Loss: 0.3181198537349701
setp: 4900, Loss: 0.3325901925563812
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9955555555555555
recall: 1.0
F_score: 0.9977728285077951
validating...
acc: 0.9802631578947368
precision: 0.953125
recall: 1.0
F_score: 0.976
******fold 4******
[243, 365]
training...
setp: 0, Loss: 0.6763795018196106
setp: 100, Loss: 0.5893585085868835
setp: 200, Loss: 0.49876242876052856
setp: 300, Loss: 0.49554935097694397
setp: 400, Loss: 0.43273892998695374
setp: 500, Loss: 0.4164584279060364
setp: 600, Loss: 0.3541819751262665
setp: 700, Loss: 0.33189770579338074
setp: 800, Loss: 0.3316268026828766
setp: 900, Loss: 0.32138484716415405
setp: 1000, Loss: 0.3541136384010315
setp: 1100, Loss: 0.32177767157554626
setp: 1200, Loss: 0.32299211621284485
setp: 1300, Loss: 0.3191300332546234
setp: 1400, Loss: 0.35655564069747925
setp: 1500, Loss: 0.32293790578842163
setp: 1600, Loss: 0.3197663724422455
setp: 1700, Loss: 0.32094380259513855
setp: 1800, Loss: 0.3657516539096832
setp: 1900, Loss: 0.3468720018863678
setp: 2000, Loss: 0.31910619139671326
setp: 2100, Loss: 0.31784260272979736
setp: 2200, Loss: 0.31833285093307495
setp: 2300, Loss: 0.31934377551078796
setp: 2400, Loss: 0.31845858693122864
setp: 2500, Loss: 0.3180910348892212
setp: 2600, Loss: 0.31861841678619385
setp: 2700, Loss: 0.32087406516075134
setp: 2800, Loss: 0.38146886229515076
setp: 2900, Loss: 0.3471088707447052
setp: 3000, Loss: 0.3319678008556366
setp: 3100, Loss: 0.3294961154460907
setp: 3200, Loss: 0.32666516304016113
setp: 3300, Loss: 0.32816648483276367
setp: 3400, Loss: 0.3253245949745178
setp: 3500, Loss: 0.3237748146057129
setp: 3600, Loss: 0.32364803552627563
setp: 3700, Loss: 0.32566890120506287
setp: 3800, Loss: 0.32433265447616577
setp: 3900, Loss: 0.3619925379753113
setp: 4000, Loss: 0.32220396399497986
setp: 4100, Loss: 0.32202088832855225
setp: 4200, Loss: 0.32447636127471924
setp: 4300, Loss: 0.3231165111064911
setp: 4400, Loss: 0.32350146770477295
setp: 4500, Loss: 0.3245558440685272
setp: 4600, Loss: 0.3238491714000702
setp: 4700, Loss: 0.3219979703426361
setp: 4800, Loss: 0.3233965337276459
setp: 4900, Loss: 0.3229592442512512
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.8
recall: 0.9523809523809523
F_score: 0.8695652173913043
model saved.
avg_acc: 0.9355263157894737, avg_f_score: 0.9127016076399699
==========arousal==========
******fold 0******
[300, 308]
training...
setp: 0, Loss: 0.706217348575592
setp: 100, Loss: 0.6194354295730591
setp: 200, Loss: 0.5797029137611389
setp: 300, Loss: 0.5447611212730408
setp: 400, Loss: 0.4982846677303314
setp: 500, Loss: 0.43978798389434814
setp: 600, Loss: 0.3715452551841736
setp: 700, Loss: 0.3558875024318695
setp: 800, Loss: 0.4072228670120239
setp: 900, Loss: 0.325349897146225
setp: 1000, Loss: 0.321887731552124
setp: 1100, Loss: 0.32207050919532776
setp: 1200, Loss: 0.3532857894897461
setp: 1300, Loss: 0.3191269040107727
setp: 1400, Loss: 0.32080334424972534
setp: 1500, Loss: 0.3184165358543396
setp: 1600, Loss: 0.31610557436943054
setp: 1700, Loss: 0.3194754123687744
setp: 1800, Loss: 0.34931302070617676
setp: 1900, Loss: 0.36556392908096313
setp: 2000, Loss: 0.31625425815582275
setp: 2100, Loss: 0.32101044058799744
setp: 2200, Loss: 0.34435319900512695
setp: 2300, Loss: 0.319942444562912
setp: 2400, Loss: 0.317971408367157
setp: 2500, Loss: 0.31815245747566223
setp: 2600, Loss: 0.3185921907424927
setp: 2700, Loss: 0.3194819390773773
setp: 2800, Loss: 0.31820371747016907
setp: 2900, Loss: 0.3206480145454407
setp: 3000, Loss: 0.31806111335754395
setp: 3100, Loss: 0.35006505250930786
setp: 3200, Loss: 0.34850114583969116
setp: 3300, Loss: 0.3446331322193146
setp: 3400, Loss: 0.33079198002815247
setp: 3500, Loss: 0.31570568680763245
setp: 3600, Loss: 0.31727221608161926
setp: 3700, Loss: 0.31813400983810425
setp: 3800, Loss: 0.31719180941581726
setp: 3900, Loss: 0.3167867362499237
setp: 4000, Loss: 0.317150741815567
setp: 4100, Loss: 0.31912729144096375
setp: 4200, Loss: 0.3175540864467621
setp: 4300, Loss: 0.3191155195236206
setp: 4400, Loss: 0.31949958205223083
setp: 4500, Loss: 0.5790388584136963
setp: 4600, Loss: 0.3304542899131775
setp: 4700, Loss: 0.3330787122249603
setp: 4800, Loss: 0.3243357241153717
setp: 4900, Loss: 0.31901803612709045
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9473684210526315
precision: 0.95
recall: 0.95
F_score: 0.9500000000000001
******fold 1******
[304, 304]
training...
setp: 0, Loss: 0.7060316801071167
setp: 100, Loss: 0.6965196132659912
setp: 200, Loss: 0.6928918957710266
setp: 300, Loss: 0.6339575052261353
setp: 400, Loss: 0.5614805221557617
setp: 500, Loss: 0.5766303539276123
setp: 600, Loss: 0.4783651530742645
setp: 700, Loss: 0.526732325553894
setp: 800, Loss: 0.4723597466945648
setp: 900, Loss: 0.4450395703315735
setp: 1000, Loss: 0.44513410329818726
setp: 1100, Loss: 0.36099350452423096
setp: 1200, Loss: 0.3737427592277527
setp: 1300, Loss: 0.35533225536346436
setp: 1400, Loss: 0.3285532593727112
setp: 1500, Loss: 0.35097408294677734
setp: 1600, Loss: 0.31758779287338257
setp: 1700, Loss: 0.32193779945373535
setp: 1800, Loss: 0.33031120896339417
setp: 1900, Loss: 0.33071306347846985
setp: 2000, Loss: 0.3180123567581177
setp: 2100, Loss: 0.3192901909351349
setp: 2200, Loss: 0.3189212381839752
setp: 2300, Loss: 0.38004904985427856
setp: 2400, Loss: 0.31846317648887634
setp: 2500, Loss: 0.31930533051490784
setp: 2600, Loss: 0.317729115486145
setp: 2700, Loss: 0.3648587465286255
setp: 2800, Loss: 0.31710079312324524
setp: 2900, Loss: 0.37440651655197144
setp: 3000, Loss: 0.3572681248188019
setp: 3100, Loss: 0.3170468509197235
setp: 3200, Loss: 0.31786492466926575
setp: 3300, Loss: 0.31987932324409485
setp: 3400, Loss: 0.3182574510574341
setp: 3500, Loss: 0.31635582447052
setp: 3600, Loss: 0.31775733828544617
setp: 3700, Loss: 0.3182525932788849
setp: 3800, Loss: 0.3173001706600189
setp: 3900, Loss: 0.3185151517391205
setp: 4000, Loss: 0.3177067041397095
setp: 4100, Loss: 0.31795862317085266
setp: 4200, Loss: 0.3493199050426483
setp: 4300, Loss: 0.31833750009536743
setp: 4400, Loss: 0.31770071387290955
setp: 4500, Loss: 0.3784305453300476
setp: 4600, Loss: 0.3194136321544647
setp: 4700, Loss: 0.339328795671463
setp: 4800, Loss: 0.31948116421699524
setp: 4900, Loss: 0.31738248467445374
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9967213114754099
recall: 1.0
F_score: 0.9983579638752053
validating...
acc: 0.9210526315789473
precision: 0.9102564102564102
recall: 0.9342105263157895
F_score: 0.922077922077922
******fold 2******
[300, 308]
training...
setp: 0, Loss: 0.7109819650650024
setp: 100, Loss: 0.715421199798584
setp: 200, Loss: 0.5530619025230408
setp: 300, Loss: 0.5090445280075073
setp: 400, Loss: 0.5161898136138916
setp: 500, Loss: 0.47566914558410645
setp: 600, Loss: 0.4831902086734772
setp: 700, Loss: 0.4224138855934143
setp: 800, Loss: 0.43237122893333435
setp: 900, Loss: 0.4042114317417145
setp: 1000, Loss: 0.4054447114467621
setp: 1100, Loss: 0.38383257389068604
setp: 1200, Loss: 0.38324838876724243
setp: 1300, Loss: 0.38101911544799805
setp: 1400, Loss: 0.3519088327884674
setp: 1500, Loss: 0.38271257281303406
setp: 1600, Loss: 0.362612783908844
setp: 1700, Loss: 0.37135785818099976
setp: 1800, Loss: 0.3201051950454712
setp: 1900, Loss: 0.3476197123527527
setp: 2000, Loss: 0.35176223516464233
setp: 2100, Loss: 0.33907610177993774
setp: 2200, Loss: 0.328429251909256
setp: 2300, Loss: 0.31859445571899414
setp: 2400, Loss: 0.3497292697429657
setp: 2500, Loss: 0.3186434209346771
setp: 2600, Loss: 0.31894657015800476
setp: 2700, Loss: 0.3170161247253418
setp: 2800, Loss: 0.3611457347869873
setp: 2900, Loss: 0.32170581817626953
setp: 3000, Loss: 0.317816823720932
setp: 3100, Loss: 0.3186940848827362
setp: 3200, Loss: 0.3185274302959442
setp: 3300, Loss: 0.31848058104515076
setp: 3400, Loss: 0.31716492772102356
setp: 3500, Loss: 0.3175632953643799
setp: 3600, Loss: 0.31962549686431885
setp: 3700, Loss: 0.31793373823165894
setp: 3800, Loss: 0.3184199035167694
setp: 3900, Loss: 0.3182522654533386
setp: 4000, Loss: 0.31830450892448425
setp: 4100, Loss: 0.3179817795753479
setp: 4200, Loss: 0.31755825877189636
setp: 4300, Loss: 0.31830844283103943
setp: 4400, Loss: 0.31956374645233154
setp: 4500, Loss: 0.3251136541366577
setp: 4600, Loss: 0.31911274790763855
setp: 4700, Loss: 0.34858769178390503
setp: 4800, Loss: 0.31829720735549927
setp: 4900, Loss: 0.31813308596611023
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9966777408637874
recall: 1.0
F_score: 0.9983361064891847
validating...
acc: 0.9276315789473685
precision: 0.96
recall: 0.9
F_score: 0.9290322580645162
******fold 3******
[302, 306]
training...
setp: 0, Loss: 0.6914460062980652
setp: 100, Loss: 0.6899496912956238
setp: 200, Loss: 0.6293845176696777
setp: 300, Loss: 0.5850200057029724
setp: 400, Loss: 0.5145351886749268
setp: 500, Loss: 0.46316277980804443
setp: 600, Loss: 0.5515977144241333
setp: 700, Loss: 0.4876401722431183
setp: 800, Loss: 0.4116578996181488
setp: 900, Loss: 0.40222427248954773
setp: 1000, Loss: 0.44721052050590515
setp: 1100, Loss: 0.3383142948150635
setp: 1200, Loss: 0.355174720287323
setp: 1300, Loss: 0.34468874335289
setp: 1400, Loss: 0.32907289266586304
setp: 1500, Loss: 0.3181702792644501
setp: 1600, Loss: 0.3213236629962921
setp: 1700, Loss: 0.317689448595047
setp: 1800, Loss: 0.3295348286628723
setp: 1900, Loss: 0.31893646717071533
setp: 2000, Loss: 0.32008418440818787
setp: 2100, Loss: 0.31765133142471313
setp: 2200, Loss: 0.31791478395462036
setp: 2300, Loss: 0.3487805724143982
setp: 2400, Loss: 0.3193683922290802
setp: 2500, Loss: 0.3191157579421997
setp: 2600, Loss: 0.32236987352371216
setp: 2700, Loss: 0.32509297132492065
setp: 2800, Loss: 0.3172628879547119
setp: 2900, Loss: 0.3498651683330536
setp: 3000, Loss: 0.31820154190063477
setp: 3100, Loss: 0.3190145492553711
setp: 3200, Loss: 0.3189827799797058
setp: 3300, Loss: 0.3183903694152832
setp: 3400, Loss: 0.31781744956970215
setp: 3500, Loss: 0.31645074486732483
setp: 3600, Loss: 0.3183799088001251
setp: 3700, Loss: 0.3195309042930603
setp: 3800, Loss: 0.31838536262512207
setp: 3900, Loss: 0.31848782300949097
setp: 4000, Loss: 0.3179228901863098
setp: 4100, Loss: 0.6387096047401428
setp: 4200, Loss: 0.514146625995636
setp: 4300, Loss: 0.43657198548316956
setp: 4400, Loss: 0.4369592070579529
setp: 4500, Loss: 0.4781705439090729
setp: 4600, Loss: 0.49048566818237305
setp: 4700, Loss: 0.38750267028808594
setp: 4800, Loss: 0.38129350543022156
setp: 4900, Loss: 0.3917255997657776
training successfully ended.
validating...
acc: 0.9736842105263158
precision: 0.9673202614379085
recall: 0.9801324503311258
F_score: 0.9736842105263158
validating...
acc: 0.8947368421052632
precision: 0.918918918918919
recall: 0.8717948717948718
F_score: 0.8947368421052632
******fold 4******
[314, 294]
training...
setp: 0, Loss: 0.6758991479873657
setp: 100, Loss: 0.6667668223381042
setp: 200, Loss: 0.5696399807929993
setp: 300, Loss: 0.48561418056488037
setp: 400, Loss: 0.41740018129348755
setp: 500, Loss: 0.39494508504867554
setp: 600, Loss: 0.40850719809532166
setp: 700, Loss: 0.36532533168792725
setp: 800, Loss: 0.3999952971935272
setp: 900, Loss: 0.36252889037132263
setp: 1000, Loss: 0.32507583498954773
setp: 1100, Loss: 0.34631216526031494
setp: 1200, Loss: 0.3232429027557373
setp: 1300, Loss: 0.31918099522590637
setp: 1400, Loss: 0.31852537393569946
setp: 1500, Loss: 0.3186304271221161
setp: 1600, Loss: 0.3184364438056946
setp: 1700, Loss: 0.3880442678928375
setp: 1800, Loss: 0.351919025182724
setp: 1900, Loss: 0.3268521726131439
setp: 2000, Loss: 0.32127508521080017
setp: 2100, Loss: 0.3186497390270233
setp: 2200, Loss: 0.3195260167121887
setp: 2300, Loss: 0.31746435165405273
setp: 2400, Loss: 0.31725525856018066
setp: 2500, Loss: 0.3779808282852173
setp: 2600, Loss: 0.3350962698459625
setp: 2700, Loss: 0.3165079355239868
setp: 2800, Loss: 0.31806907057762146
setp: 2900, Loss: 0.3169582486152649
setp: 3000, Loss: 0.31814154982566833
setp: 3100, Loss: 0.3192572295665741
setp: 3200, Loss: 0.31746602058410645
setp: 3300, Loss: 0.3174901306629181
setp: 3400, Loss: 0.3171762228012085
setp: 3500, Loss: 0.3170592188835144
setp: 3600, Loss: 0.3508681654930115
setp: 3700, Loss: 0.513433039188385
setp: 3800, Loss: 0.3191308081150055
setp: 3900, Loss: 0.3189847469329834
setp: 4000, Loss: 0.3161894679069519
setp: 4100, Loss: 0.3174531161785126
setp: 4200, Loss: 0.37101155519485474
setp: 4300, Loss: 0.328327476978302
setp: 4400, Loss: 0.3474815785884857
setp: 4500, Loss: 0.3168722987174988
setp: 4600, Loss: 0.331132709980011
setp: 4700, Loss: 0.3170076310634613
setp: 4800, Loss: 0.3168165683746338
setp: 4900, Loss: 0.3181885778903961
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9968152866242038
recall: 0.9968152866242038
F_score: 0.9968152866242038
validating...
acc: 0.9210526315789473
precision: 0.9090909090909091
recall: 0.9090909090909091
F_score: 0.9090909090909091
model saved.
avg_acc: 0.9223684210526315, avg_f_score: 0.920987586267722
-------------subject: 29-------------
==========valence==========
******fold 0******
[246, 362]
training...
setp: 0, Loss: 0.7725462317466736
setp: 100, Loss: 0.6996498107910156
setp: 200, Loss: 0.6175629496574402
setp: 300, Loss: 0.5902679562568665
setp: 400, Loss: 0.4625857174396515
setp: 500, Loss: 0.37034210562705994
setp: 600, Loss: 0.36429834365844727
setp: 700, Loss: 0.3214647173881531
setp: 800, Loss: 0.326682984828949
setp: 900, Loss: 0.3207687735557556
setp: 1000, Loss: 0.3190135657787323
setp: 1100, Loss: 0.31806105375289917
setp: 1200, Loss: 0.3175656795501709
setp: 1300, Loss: 0.33169281482696533
setp: 1400, Loss: 0.383060485124588
setp: 1500, Loss: 0.3164694011211395
setp: 1600, Loss: 0.32287973165512085
setp: 1700, Loss: 0.31665492057800293
setp: 1800, Loss: 0.3499210774898529
setp: 1900, Loss: 0.31895941495895386
setp: 2000, Loss: 0.31918245553970337
setp: 2100, Loss: 0.3171117305755615
setp: 2200, Loss: 0.32018595933914185
setp: 2300, Loss: 0.3174501657485962
setp: 2400, Loss: 0.31697729229927063
setp: 2500, Loss: 0.40078267455101013
setp: 2600, Loss: 0.3176795542240143
setp: 2700, Loss: 0.33676230907440186
setp: 2800, Loss: 0.3171130418777466
setp: 2900, Loss: 0.31546738743782043
setp: 3000, Loss: 0.31952759623527527
setp: 3100, Loss: 0.33324864506721497
setp: 3200, Loss: 0.3157561719417572
setp: 3300, Loss: 0.31728866696357727
setp: 3400, Loss: 0.31530606746673584
setp: 3500, Loss: 0.31899529695510864
setp: 3600, Loss: 0.31603097915649414
setp: 3700, Loss: 0.317953884601593
setp: 3800, Loss: 0.31647154688835144
setp: 3900, Loss: 0.318020761013031
setp: 4000, Loss: 0.3176032602787018
setp: 4100, Loss: 0.32173165678977966
setp: 4200, Loss: 0.3164818584918976
setp: 4300, Loss: 0.31787577271461487
setp: 4400, Loss: 0.3159240484237671
setp: 4500, Loss: 0.3176547586917877
setp: 4600, Loss: 0.31754451990127563
setp: 4700, Loss: 0.3182225227355957
setp: 4800, Loss: 0.31556957960128784
setp: 4900, Loss: 0.31603506207466125
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9959514170040485
recall: 1.0
F_score: 0.9979716024340771
validating...
acc: 0.9342105263157895
precision: 0.8941176470588236
recall: 0.987012987012987
F_score: 0.9382716049382716
******fold 1******
[266, 342]
training...
setp: 0, Loss: 0.6925250887870789
setp: 100, Loss: 0.6641576290130615
setp: 200, Loss: 0.5271316766738892
setp: 300, Loss: 0.5229343175888062
setp: 400, Loss: 0.4681853950023651
setp: 500, Loss: 0.4034948945045471
setp: 600, Loss: 0.3974144756793976
setp: 700, Loss: 0.325422465801239
setp: 800, Loss: 0.360416442155838
setp: 900, Loss: 0.3215022683143616
setp: 1000, Loss: 0.3220871388912201
setp: 1100, Loss: 0.32133322954177856
setp: 1200, Loss: 0.3481002449989319
setp: 1300, Loss: 0.3178307116031647
setp: 1400, Loss: 0.32277801632881165
setp: 1500, Loss: 0.31537294387817383
setp: 1600, Loss: 0.3204099237918854
setp: 1700, Loss: 0.31944727897644043
setp: 1800, Loss: 0.32503023743629456
setp: 1900, Loss: 0.3205301761627197
setp: 2000, Loss: 0.3312152326107025
setp: 2100, Loss: 0.3183378577232361
setp: 2200, Loss: 0.32931509613990784
setp: 2300, Loss: 0.3485340476036072
setp: 2400, Loss: 0.32668834924697876
setp: 2500, Loss: 0.36709392070770264
setp: 2600, Loss: 0.3184627592563629
setp: 2700, Loss: 0.32050231099128723
setp: 2800, Loss: 0.3190450370311737
setp: 2900, Loss: 0.31723684072494507
setp: 3000, Loss: 0.31697168946266174
setp: 3100, Loss: 0.3167208433151245
setp: 3200, Loss: 0.3168632388114929
setp: 3300, Loss: 0.31905844807624817
setp: 3400, Loss: 0.31537362933158875
setp: 3500, Loss: 0.3177472949028015
setp: 3600, Loss: 0.3157528042793274
setp: 3700, Loss: 0.31628838181495667
setp: 3800, Loss: 0.3179659843444824
setp: 3900, Loss: 0.3192354738712311
setp: 4000, Loss: 0.31703242659568787
setp: 4100, Loss: 0.3167046308517456
setp: 4200, Loss: 0.31572383642196655
setp: 4300, Loss: 0.3162568509578705
setp: 4400, Loss: 0.32472237944602966
setp: 4500, Loss: 0.31958577036857605
setp: 4600, Loss: 0.3152553141117096
setp: 4700, Loss: 0.3156701922416687
setp: 4800, Loss: 0.31628990173339844
setp: 4900, Loss: 0.3221345543861389
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9671052631578947
precision: 0.9642857142857143
recall: 0.9473684210526315
F_score: 0.9557522123893805
******fold 2******
[255, 353]
training...
setp: 0, Loss: 0.6466885805130005
setp: 100, Loss: 0.638992965221405
setp: 200, Loss: 0.5614784955978394
setp: 300, Loss: 0.45694443583488464
setp: 400, Loss: 0.40958935022354126
setp: 500, Loss: 0.38766974210739136
setp: 600, Loss: 0.3534972071647644
setp: 700, Loss: 0.3189767003059387
setp: 800, Loss: 0.33025890588760376
setp: 900, Loss: 0.3286624550819397
setp: 1000, Loss: 0.31940728425979614
setp: 1100, Loss: 0.31721198558807373
setp: 1200, Loss: 0.31663820147514343
setp: 1300, Loss: 0.3164847493171692
setp: 1400, Loss: 0.3462494909763336
setp: 1500, Loss: 0.3182436227798462
setp: 1600, Loss: 0.3169788718223572
setp: 1700, Loss: 0.3163829445838928
setp: 1800, Loss: 0.3182232081890106
setp: 1900, Loss: 0.31746751070022583
setp: 2000, Loss: 0.3174425959587097
setp: 2100, Loss: 0.3171067237854004
setp: 2200, Loss: 0.31762412190437317
setp: 2300, Loss: 0.31678375601768494
setp: 2400, Loss: 0.3179263770580292
setp: 2500, Loss: 0.3174714148044586
setp: 2600, Loss: 0.31667622923851013
setp: 2700, Loss: 0.31863829493522644
setp: 2800, Loss: 0.31622782349586487
setp: 2900, Loss: 0.31660303473472595
setp: 3000, Loss: 0.3158533275127411
setp: 3100, Loss: 0.3168722093105316
setp: 3200, Loss: 0.31571871042251587
setp: 3300, Loss: 0.31591662764549255
setp: 3400, Loss: 0.349599689245224
setp: 3500, Loss: 0.3471519351005554
setp: 3600, Loss: 0.3155635595321655
setp: 3700, Loss: 0.31693628430366516
setp: 3800, Loss: 0.31673380732536316
setp: 3900, Loss: 0.3166332244873047
setp: 4000, Loss: 0.3158966600894928
setp: 4100, Loss: 0.31779810786247253
setp: 4200, Loss: 0.3164362609386444
setp: 4300, Loss: 0.3180430829524994
setp: 4400, Loss: 0.3549336791038513
setp: 4500, Loss: 0.4065798223018646
setp: 4600, Loss: 0.3213089406490326
setp: 4700, Loss: 0.31564462184906006
setp: 4800, Loss: 0.31627175211906433
setp: 4900, Loss: 0.31579291820526123
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9539473684210527
precision: 0.9692307692307692
recall: 0.9264705882352942
F_score: 0.9473684210526316
******fold 3******
[257, 351]
training...
setp: 0, Loss: 0.6995276808738708
setp: 100, Loss: 0.6784608960151672
setp: 200, Loss: 0.669465959072113
setp: 300, Loss: 0.5466961860656738
setp: 400, Loss: 0.4695112407207489
setp: 500, Loss: 0.40634921193122864
setp: 600, Loss: 0.423730731010437
setp: 700, Loss: 0.32898634672164917
setp: 800, Loss: 0.3433777391910553
setp: 900, Loss: 0.32511386275291443
setp: 1000, Loss: 0.351529598236084
setp: 1100, Loss: 0.31931403279304504
setp: 1200, Loss: 0.3577733337879181
setp: 1300, Loss: 0.32176539301872253
setp: 1400, Loss: 0.36860981583595276
setp: 1500, Loss: 0.34053221344947815
setp: 1600, Loss: 0.3398435711860657
setp: 1700, Loss: 0.3443599045276642
setp: 1800, Loss: 0.3646250069141388
setp: 1900, Loss: 0.4483495354652405
setp: 2000, Loss: 0.3178160786628723
setp: 2100, Loss: 0.3198448121547699
setp: 2200, Loss: 0.31661128997802734
setp: 2300, Loss: 0.3177332878112793
setp: 2400, Loss: 0.3479251265525818
setp: 2500, Loss: 0.34901830554008484
setp: 2600, Loss: 0.3195571005344391
setp: 2700, Loss: 0.31561723351478577
setp: 2800, Loss: 0.3182515501976013
setp: 2900, Loss: 0.3184700012207031
setp: 3000, Loss: 0.3177655041217804
setp: 3100, Loss: 0.31731101870536804
setp: 3200, Loss: 0.38528960943222046
setp: 3300, Loss: 0.3185734450817108
setp: 3400, Loss: 0.3156171143054962
setp: 3500, Loss: 0.3166411221027374
setp: 3600, Loss: 0.3157532811164856
setp: 3700, Loss: 0.3169361650943756
setp: 3800, Loss: 0.31789541244506836
setp: 3900, Loss: 0.3207434117794037
setp: 4000, Loss: 0.31561633944511414
setp: 4100, Loss: 0.31707170605659485
setp: 4200, Loss: 0.31902366876602173
setp: 4300, Loss: 0.3155892491340637
setp: 4400, Loss: 0.3201550841331482
setp: 4500, Loss: 0.31789475679397583
setp: 4600, Loss: 0.316019743680954
setp: 4700, Loss: 0.3158503770828247
setp: 4800, Loss: 0.3166598081588745
setp: 4900, Loss: 0.3163596987724304
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 0.9961240310077519
recall: 1.0
F_score: 0.9980582524271845
validating...
acc: 0.9539473684210527
precision: 0.9154929577464789
recall: 0.9848484848484849
F_score: 0.948905109489051
******fold 4******
[268, 340]
training...
setp: 0, Loss: 0.7048330307006836
setp: 100, Loss: 0.6898021697998047
setp: 200, Loss: 0.5637582540512085
setp: 300, Loss: 0.5269020199775696
setp: 400, Loss: 0.4679320454597473
setp: 500, Loss: 0.4608539640903473
setp: 600, Loss: 0.4373222589492798
setp: 700, Loss: 0.36621400713920593
setp: 800, Loss: 0.333309531211853
setp: 900, Loss: 0.3223782777786255
setp: 1000, Loss: 0.31955960392951965
setp: 1100, Loss: 0.3177248239517212
setp: 1200, Loss: 0.3164525330066681
setp: 1300, Loss: 0.3162738084793091
setp: 1400, Loss: 0.3207892179489136
setp: 1500, Loss: 0.31563782691955566
setp: 1600, Loss: 0.3165876269340515
setp: 1700, Loss: 0.31579962372779846
setp: 1800, Loss: 0.3158242106437683
setp: 1900, Loss: 0.31727808713912964
setp: 2000, Loss: 0.3425047695636749
setp: 2100, Loss: 0.3202177584171295
setp: 2200, Loss: 0.3163803815841675
setp: 2300, Loss: 0.3178599774837494
setp: 2400, Loss: 0.3170453906059265
setp: 2500, Loss: 0.3479713797569275
setp: 2600, Loss: 0.3197156488895416
setp: 2700, Loss: 0.31669989228248596
setp: 2800, Loss: 0.6705940365791321
setp: 2900, Loss: 0.32190150022506714
setp: 3000, Loss: 0.34536993503570557
setp: 3100, Loss: 0.316062331199646
setp: 3200, Loss: 0.31601783633232117
setp: 3300, Loss: 0.31569892168045044
setp: 3400, Loss: 0.3153941333293915
setp: 3500, Loss: 0.316506564617157
setp: 3600, Loss: 0.31546059250831604
setp: 3700, Loss: 0.3157638609409332
setp: 3800, Loss: 0.3173482120037079
setp: 3900, Loss: 0.31635522842407227
setp: 4000, Loss: 0.3160786032676697
setp: 4100, Loss: 0.32069674134254456
setp: 4200, Loss: 0.3186674118041992
setp: 4300, Loss: 0.3165244460105896
setp: 4400, Loss: 0.31712210178375244
setp: 4500, Loss: 0.3174915015697479
setp: 4600, Loss: 0.3168182075023651
setp: 4700, Loss: 0.31636282801628113
setp: 4800, Loss: 0.5062517523765564
setp: 4900, Loss: 0.3971174359321594
training successfully ended.
validating...
acc: 0.9736842105263158
precision: 0.9436619718309859
recall: 1.0
F_score: 0.9710144927536231
validating...
acc: 0.9473684210526315
precision: 0.873015873015873
recall: 1.0
F_score: 0.9322033898305084
model saved.
avg_acc: 0.9513157894736842, avg_f_score: 0.9445001475399686
==========arousal==========
******fold 0******
[188, 420]
training...
setp: 0, Loss: 0.6496830582618713
setp: 100, Loss: 0.5864641070365906
setp: 200, Loss: 0.5478412508964539
setp: 300, Loss: 0.49205759167671204
setp: 400, Loss: 0.4689945876598358
setp: 500, Loss: 0.4167421758174896
setp: 600, Loss: 0.36828112602233887
setp: 700, Loss: 0.35358279943466187
setp: 800, Loss: 0.3906802535057068
setp: 900, Loss: 0.4454348683357239
setp: 1000, Loss: 0.38065069913864136
setp: 1100, Loss: 0.36340340971946716
setp: 1200, Loss: 0.32678866386413574
setp: 1300, Loss: 0.32405391335487366
setp: 1400, Loss: 0.3226078152656555
setp: 1500, Loss: 0.3460089862346649
setp: 1600, Loss: 0.31717637181282043
setp: 1700, Loss: 0.3168796896934509
setp: 1800, Loss: 0.37821492552757263
setp: 1900, Loss: 0.32351452112197876
setp: 2000, Loss: 0.357259064912796
setp: 2100, Loss: 0.32675662636756897
setp: 2200, Loss: 0.3216536045074463
setp: 2300, Loss: 0.31656932830810547
setp: 2400, Loss: 0.32329800724983215
setp: 2500, Loss: 0.31815871596336365
setp: 2600, Loss: 0.3211618661880493
setp: 2700, Loss: 0.3159828186035156
setp: 2800, Loss: 0.34746086597442627
setp: 2900, Loss: 0.4085802733898163
setp: 3000, Loss: 0.32018232345581055
setp: 3100, Loss: 0.31712907552719116
setp: 3200, Loss: 0.3164513409137726
setp: 3300, Loss: 0.3171406090259552
setp: 3400, Loss: 0.33506426215171814
setp: 3500, Loss: 0.31504809856414795
setp: 3600, Loss: 0.31606805324554443
setp: 3700, Loss: 0.38013526797294617
setp: 3800, Loss: 0.32120469212532043
setp: 3900, Loss: 0.31536880135536194
setp: 4000, Loss: 0.31937989592552185
setp: 4100, Loss: 0.31823962926864624
setp: 4200, Loss: 0.3198505938053131
setp: 4300, Loss: 0.31620272994041443
setp: 4400, Loss: 0.31870749592781067
setp: 4500, Loss: 0.3183886706829071
setp: 4600, Loss: 0.3177785873413086
setp: 4700, Loss: 0.31713542342185974
setp: 4800, Loss: 0.3164295256137848
setp: 4900, Loss: 0.3222372531890869
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.9893617021276596
F_score: 0.9946524064171123
validating...
acc: 0.9671052631578947
precision: 0.95
recall: 0.9661016949152542
F_score: 0.957983193277311
******fold 1******
[195, 413]
training...
setp: 0, Loss: 0.6596848964691162
setp: 100, Loss: 0.6128740310668945
setp: 200, Loss: 0.5587220788002014
setp: 300, Loss: 0.4669901430606842
setp: 400, Loss: 0.4706890285015106
setp: 500, Loss: 0.412191778421402
setp: 600, Loss: 0.4190033972263336
setp: 700, Loss: 0.48153558373451233
setp: 800, Loss: 0.3253733515739441
setp: 900, Loss: 0.34487733244895935
setp: 1000, Loss: 0.41750916838645935
setp: 1100, Loss: 0.3951617479324341
setp: 1200, Loss: 0.31620457768440247
setp: 1300, Loss: 0.31775638461112976
setp: 1400, Loss: 0.31917697191238403
setp: 1500, Loss: 0.3170667886734009
setp: 1600, Loss: 0.3176485002040863
setp: 1700, Loss: 0.32863035798072815
setp: 1800, Loss: 0.37748801708221436
setp: 1900, Loss: 0.3517318665981293
setp: 2000, Loss: 0.31577274203300476
setp: 2100, Loss: 0.31788313388824463
setp: 2200, Loss: 0.3165222704410553
setp: 2300, Loss: 0.34819698333740234
setp: 2400, Loss: 0.31742173433303833
setp: 2500, Loss: 0.3467978835105896
setp: 2600, Loss: 0.34711953997612
setp: 2700, Loss: 0.3168222904205322
setp: 2800, Loss: 0.36257404088974
setp: 2900, Loss: 0.35054099559783936
setp: 3000, Loss: 0.31987011432647705
setp: 3100, Loss: 0.31538522243499756
setp: 3200, Loss: 0.31716156005859375
setp: 3300, Loss: 0.3174811005592346
setp: 3400, Loss: 0.3159431219100952
setp: 3500, Loss: 0.3163536489009857
setp: 3600, Loss: 0.3406018018722534
setp: 3700, Loss: 0.34792467951774597
setp: 3800, Loss: 0.33672165870666504
setp: 3900, Loss: 0.31613829731941223
setp: 4000, Loss: 0.3153785765171051
setp: 4100, Loss: 0.3155289888381958
setp: 4200, Loss: 0.34679824113845825
setp: 4300, Loss: 0.3159114420413971
setp: 4400, Loss: 0.3465211093425751
setp: 4500, Loss: 0.31728002429008484
setp: 4600, Loss: 0.3162856698036194
setp: 4700, Loss: 0.3172433078289032
setp: 4800, Loss: 0.3177432119846344
setp: 4900, Loss: 0.31736820936203003
training successfully ended.
validating...
acc: 0.9868421052631579
precision: 1.0
recall: 0.958974358974359
F_score: 0.9790575916230366
validating...
acc: 0.9210526315789473
precision: 0.9347826086956522
recall: 0.8269230769230769
F_score: 0.8775510204081632
******fold 2******
[199, 409]
training...
setp: 0, Loss: 0.7181670665740967
setp: 100, Loss: 0.5786018371582031
setp: 200, Loss: 0.6615146398544312
setp: 300, Loss: 0.5174108743667603
setp: 400, Loss: 0.4453328847885132
setp: 500, Loss: 0.41831907629966736
setp: 600, Loss: 0.4572393000125885
setp: 700, Loss: 0.42426252365112305
setp: 800, Loss: 0.3377082049846649
setp: 900, Loss: 0.41454803943634033
setp: 1000, Loss: 0.3338643014431
setp: 1100, Loss: 0.35709503293037415
setp: 1200, Loss: 0.36804795265197754
setp: 1300, Loss: 0.31967803835868835
setp: 1400, Loss: 0.3197631239891052
setp: 1500, Loss: 0.31632328033447266
setp: 1600, Loss: 0.34680822491645813
setp: 1700, Loss: 0.31856009364128113
setp: 1800, Loss: 0.3781556189060211
setp: 1900, Loss: 0.32288143038749695
setp: 2000, Loss: 0.34195002913475037
setp: 2100, Loss: 0.3223324716091156
setp: 2200, Loss: 0.315741628408432
setp: 2300, Loss: 0.3488631844520569
setp: 2400, Loss: 0.3191847801208496
setp: 2500, Loss: 0.3160705864429474
setp: 2600, Loss: 0.31683292984962463
setp: 2700, Loss: 0.31639474630355835
setp: 2800, Loss: 0.41570815443992615
setp: 2900, Loss: 0.3208096921443939
setp: 3000, Loss: 0.32844358682632446
setp: 3100, Loss: 0.31572040915489197
setp: 3200, Loss: 0.3176313638687134
setp: 3300, Loss: 0.31718647480010986
setp: 3400, Loss: 0.3157307505607605
setp: 3500, Loss: 0.34572547674179077
setp: 3600, Loss: 0.31698140501976013
setp: 3700, Loss: 0.346703439950943
setp: 3800, Loss: 0.315767377614975
setp: 3900, Loss: 0.3158567547798157
setp: 4000, Loss: 0.3169613480567932
setp: 4100, Loss: 0.31710943579673767
setp: 4200, Loss: 0.31615856289863586
setp: 4300, Loss: 0.31633853912353516
setp: 4400, Loss: 0.3160824179649353
setp: 4500, Loss: 0.31716737151145935
setp: 4600, Loss: 0.3164460361003876
setp: 4700, Loss: 0.31785255670547485
setp: 4800, Loss: 0.39855343103408813
setp: 4900, Loss: 0.40873974561691284
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9949748743718593
F_score: 0.9974811083123425
validating...
acc: 0.9407894736842105
precision: 0.8679245283018868
recall: 0.9583333333333334
F_score: 0.910891089108911
******fold 3******
[202, 406]
training...
setp: 0, Loss: 0.7193220853805542
setp: 100, Loss: 0.642489492893219
setp: 200, Loss: 0.5962873697280884
setp: 300, Loss: 0.4858805537223816
setp: 400, Loss: 0.4939379096031189
setp: 500, Loss: 0.5029614567756653
setp: 600, Loss: 0.4847855865955353
setp: 700, Loss: 0.41227325797080994
setp: 800, Loss: 0.36163824796676636
setp: 900, Loss: 0.34281450510025024
setp: 1000, Loss: 0.3623124361038208
setp: 1100, Loss: 0.385476678609848
setp: 1200, Loss: 0.3331564664840698
setp: 1300, Loss: 0.3215828835964203
setp: 1400, Loss: 0.3730342388153076
setp: 1500, Loss: 0.3526689410209656
setp: 1600, Loss: 0.3528958559036255
setp: 1700, Loss: 0.35727182030677795
setp: 1800, Loss: 0.383259117603302
setp: 1900, Loss: 0.3963588774204254
setp: 2000, Loss: 0.32154691219329834
setp: 2100, Loss: 0.34205037355422974
setp: 2200, Loss: 0.31708845496177673
setp: 2300, Loss: 0.33473432064056396
setp: 2400, Loss: 0.38089266419410706
setp: 2500, Loss: 0.324616014957428
setp: 2600, Loss: 0.32213157415390015
setp: 2700, Loss: 0.31725022196769714
setp: 2800, Loss: 0.33005133271217346
setp: 2900, Loss: 0.33781030774116516
setp: 3000, Loss: 0.3281972110271454
setp: 3100, Loss: 0.3189404308795929
setp: 3200, Loss: 0.3160286247730255
setp: 3300, Loss: 0.317719429731369
setp: 3400, Loss: 0.316018670797348
setp: 3500, Loss: 0.3164925277233124
setp: 3600, Loss: 0.3172696828842163
setp: 3700, Loss: 0.3782370388507843
setp: 3800, Loss: 0.31667381525039673
setp: 3900, Loss: 0.3164527118206024
setp: 4000, Loss: 0.3168086111545563
setp: 4100, Loss: 0.3268057405948639
setp: 4200, Loss: 0.34570571780204773
setp: 4300, Loss: 0.31605958938598633
setp: 4400, Loss: 0.3172357380390167
setp: 4500, Loss: 0.3164728581905365
setp: 4600, Loss: 0.316494882106781
setp: 4700, Loss: 0.31563398241996765
setp: 4800, Loss: 0.31782111525535583
setp: 4900, Loss: 0.34591755270957947
training successfully ended.
validating...
acc: 0.8305921052631579
precision: 0.6644518272425249
recall: 0.9900990099009901
F_score: 0.7952286282306164
validating...
acc: 0.7697368421052632
precision: 0.5625
recall: 1.0
F_score: 0.72
******fold 4******
[204, 404]
training...
setp: 0, Loss: 0.6895861029624939
setp: 100, Loss: 0.6273289918899536
setp: 200, Loss: 0.560063362121582
setp: 300, Loss: 0.6290827393531799
setp: 400, Loss: 0.4224109351634979
setp: 500, Loss: 0.4455287456512451
setp: 600, Loss: 0.40026015043258667
setp: 700, Loss: 0.42744457721710205
setp: 800, Loss: 0.40034013986587524
setp: 900, Loss: 0.3537200689315796
setp: 1000, Loss: 0.3684294521808624
setp: 1100, Loss: 0.3716304898262024
setp: 1200, Loss: 0.3626477122306824
setp: 1300, Loss: 0.36310628056526184
setp: 1400, Loss: 0.3333909511566162
setp: 1500, Loss: 0.33269181847572327
setp: 1600, Loss: 0.3217039108276367
setp: 1700, Loss: 0.31797343492507935
setp: 1800, Loss: 0.33312028646469116
setp: 1900, Loss: 0.33042070269584656
setp: 2000, Loss: 0.315971702337265
setp: 2100, Loss: 0.3162040114402771
setp: 2200, Loss: 0.31648194789886475
setp: 2300, Loss: 0.31535014510154724
setp: 2400, Loss: 0.3216911852359772
setp: 2500, Loss: 0.31767094135284424
setp: 2600, Loss: 0.31704410910606384
setp: 2700, Loss: 0.31608298420906067
setp: 2800, Loss: 0.3162309229373932
setp: 2900, Loss: 0.3166581094264984
setp: 3000, Loss: 0.3222251534461975
setp: 3100, Loss: 0.3190966248512268
setp: 3200, Loss: 0.3188628554344177
setp: 3300, Loss: 0.3235476315021515
setp: 3400, Loss: 0.3734835386276245
setp: 3500, Loss: 0.31797870993614197
setp: 3600, Loss: 0.34656843543052673
setp: 3700, Loss: 0.3230532705783844
setp: 3800, Loss: 0.3169913589954376
setp: 3900, Loss: 0.31537672877311707
setp: 4000, Loss: 0.31623995304107666
setp: 4100, Loss: 0.31573790311813354
setp: 4200, Loss: 0.3153086006641388
setp: 4300, Loss: 0.3162052631378174
setp: 4400, Loss: 0.31662651896476746
setp: 4500, Loss: 0.31694450974464417
setp: 4600, Loss: 0.31749528646469116
setp: 4700, Loss: 0.31662485003471375
setp: 4800, Loss: 0.3193649649620056
setp: 4900, Loss: 0.3168269693851471
training successfully ended.
validating...
acc: 0.9835526315789473
precision: 0.9532710280373832
recall: 1.0
F_score: 0.9760765550239235
validating...
acc: 0.9144736842105263
precision: 0.8260869565217391
recall: 0.8837209302325582
F_score: 0.853932584269663
model saved.
avg_acc: 0.9026315789473685, avg_f_score: 0.8640715774128097
-------------subject: 30-------------
==========valence==========
******fold 0******
[198, 410]
training...
setp: 0, Loss: 0.733748197555542
setp: 100, Loss: 0.642693281173706
setp: 200, Loss: 0.5838309526443481
setp: 300, Loss: 0.5824720859527588
setp: 400, Loss: 0.4956122636795044
setp: 500, Loss: 0.5152276754379272
setp: 600, Loss: 0.5330201983451843
setp: 700, Loss: 0.4025225341320038
setp: 800, Loss: 0.432556688785553
setp: 900, Loss: 0.3298538327217102
setp: 1000, Loss: 0.31958043575286865
setp: 1100, Loss: 0.3280206620693207
setp: 1200, Loss: 0.35168737173080444
setp: 1300, Loss: 0.31839776039123535
setp: 1400, Loss: 0.32183173298835754
setp: 1500, Loss: 0.45029833912849426
setp: 1600, Loss: 0.3479640781879425
setp: 1700, Loss: 0.32170766592025757
setp: 1800, Loss: 0.31697389483451843
setp: 1900, Loss: 0.32043662667274475
setp: 2000, Loss: 0.3522167205810547
setp: 2100, Loss: 0.3169174790382385
setp: 2200, Loss: 0.3211158215999603
setp: 2300, Loss: 0.3179399371147156
setp: 2400, Loss: 0.3162569999694824
setp: 2500, Loss: 0.31851616501808167
setp: 2600, Loss: 0.3169463276863098
setp: 2700, Loss: 0.3803712725639343
setp: 2800, Loss: 0.3765997588634491
setp: 2900, Loss: 0.3646402657032013
setp: 3000, Loss: 0.3161604404449463
setp: 3100, Loss: 0.347736120223999
setp: 3200, Loss: 0.3166123926639557
setp: 3300, Loss: 0.3164340555667877
setp: 3400, Loss: 0.31928563117980957
setp: 3500, Loss: 0.3181123435497284
setp: 3600, Loss: 0.3196413516998291
setp: 3700, Loss: 0.3163889944553375
setp: 3800, Loss: 0.31766772270202637
setp: 3900, Loss: 0.5379663109779358
setp: 4000, Loss: 0.3196876645088196
setp: 4100, Loss: 0.31896111369132996
setp: 4200, Loss: 0.3172551691532135
setp: 4300, Loss: 0.3164227604866028
setp: 4400, Loss: 0.3176564574241638
setp: 4500, Loss: 0.31632983684539795
setp: 4600, Loss: 0.3493236303329468
setp: 4700, Loss: 0.3171425461769104
setp: 4800, Loss: 0.31660744547843933
setp: 4900, Loss: 0.3172794282436371
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.99
recall: 1.0
F_score: 0.9949748743718593
validating...
acc: 0.868421052631579
precision: 0.7843137254901961
recall: 0.8163265306122449
F_score: 0.8
******fold 1******
[197, 411]
training...
setp: 0, Loss: 0.7288399338722229
setp: 100, Loss: 0.5988278388977051
setp: 200, Loss: 0.6703474521636963
setp: 300, Loss: 0.6212177276611328
setp: 400, Loss: 0.622032642364502
setp: 500, Loss: 0.6212469339370728
setp: 600, Loss: 0.6460147500038147
setp: 700, Loss: 0.6445484161376953
setp: 800, Loss: 0.5800389647483826
setp: 900, Loss: 0.6203789710998535
setp: 1000, Loss: 0.6405734419822693
setp: 1100, Loss: 0.6010379195213318
setp: 1200, Loss: 0.5430022478103638
setp: 1300, Loss: 0.585505485534668
setp: 1400, Loss: 0.4410400092601776
setp: 1500, Loss: 0.5166851878166199
setp: 1600, Loss: 0.5367680788040161
setp: 1700, Loss: 0.4555736780166626
setp: 1800, Loss: 0.4255320727825165
setp: 1900, Loss: 0.38007885217666626
setp: 2000, Loss: 0.35405081510543823
setp: 2100, Loss: 0.4421808421611786
setp: 2200, Loss: 0.36794033646583557
setp: 2300, Loss: 0.3827667236328125
setp: 2400, Loss: 0.3602932393550873
setp: 2500, Loss: 0.3607202172279358
setp: 2600, Loss: 0.3210633397102356
setp: 2700, Loss: 0.3241346776485443
setp: 2800, Loss: 0.31591373682022095
setp: 2900, Loss: 0.32077112793922424
setp: 3000, Loss: 0.3255420923233032
setp: 3100, Loss: 0.36103448271751404
setp: 3200, Loss: 0.33930400013923645
setp: 3300, Loss: 0.33246639370918274
setp: 3400, Loss: 0.31935012340545654
setp: 3500, Loss: 0.34415513277053833
setp: 3600, Loss: 0.32463133335113525
setp: 3700, Loss: 0.3567625880241394
setp: 3800, Loss: 0.3387208580970764
setp: 3900, Loss: 0.3173690438270569
setp: 4000, Loss: 0.3191947042942047
setp: 4100, Loss: 0.318989098072052
setp: 4200, Loss: 0.3173210024833679
setp: 4300, Loss: 0.3171287775039673
setp: 4400, Loss: 0.320115327835083
setp: 4500, Loss: 0.31567808985710144
setp: 4600, Loss: 0.32561007142066956
setp: 4700, Loss: 0.3298449218273163
setp: 4800, Loss: 0.3167516887187958
setp: 4900, Loss: 0.3179756700992584
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 0.9899497487437185
recall: 1.0
F_score: 0.9949494949494949
validating...
acc: 0.9144736842105263
precision: 0.8363636363636363
recall: 0.92
F_score: 0.8761904761904761
******fold 2******
[200, 408]
training...
setp: 0, Loss: 0.6657997369766235
setp: 100, Loss: 0.6449993252754211
setp: 200, Loss: 0.6217914819717407
setp: 300, Loss: 0.6634276509284973
setp: 400, Loss: 0.5517299771308899
setp: 500, Loss: 0.6253103017807007
setp: 600, Loss: 0.47880589962005615
setp: 700, Loss: 0.43223142623901367
setp: 800, Loss: 0.4557149112224579
setp: 900, Loss: 0.3412898778915405
setp: 1000, Loss: 0.32983696460723877
setp: 1100, Loss: 0.3273259103298187
setp: 1200, Loss: 0.33872610330581665
setp: 1300, Loss: 0.349606454372406
setp: 1400, Loss: 0.31811976432800293
setp: 1500, Loss: 0.3475211262702942
setp: 1600, Loss: 0.31885838508605957
setp: 1700, Loss: 0.31891244649887085
setp: 1800, Loss: 0.3208233118057251
setp: 1900, Loss: 0.3203086256980896
setp: 2000, Loss: 0.348457932472229
setp: 2100, Loss: 0.3175469636917114
setp: 2200, Loss: 0.349257230758667
setp: 2300, Loss: 0.3176560401916504
setp: 2400, Loss: 0.31677645444869995
setp: 2500, Loss: 0.31855690479278564
setp: 2600, Loss: 0.3158986568450928
setp: 2700, Loss: 0.4047752916812897
setp: 2800, Loss: 0.32212698459625244
setp: 2900, Loss: 0.3225807249546051
setp: 3000, Loss: 0.31906116008758545
setp: 3100, Loss: 0.31912824511528015
setp: 3200, Loss: 0.347866952419281
setp: 3300, Loss: 0.31775543093681335
setp: 3400, Loss: 0.34754669666290283
setp: 3500, Loss: 0.31591328978538513
setp: 3600, Loss: 0.3184660077095032
setp: 3700, Loss: 0.3163548707962036
setp: 3800, Loss: 0.3185015618801117
setp: 3900, Loss: 0.34824010729789734
setp: 4000, Loss: 0.31762370467185974
setp: 4100, Loss: 0.34881478548049927
setp: 4200, Loss: 0.3173622190952301
setp: 4300, Loss: 0.31832045316696167
setp: 4400, Loss: 0.32040005922317505
setp: 4500, Loss: 0.3159925937652588
setp: 4600, Loss: 0.32050082087516785
setp: 4700, Loss: 0.3253786563873291
setp: 4800, Loss: 0.31560221314430237
setp: 4900, Loss: 0.3245759904384613
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.99
F_score: 0.9949748743718593
validating...
acc: 0.9802631578947368
precision: 0.94
recall: 1.0
F_score: 0.9690721649484536
******fold 3******
[185, 423]
training...
setp: 0, Loss: 0.6479917764663696
setp: 100, Loss: 0.6208364963531494
setp: 200, Loss: 0.620999276638031
setp: 300, Loss: 0.594003438949585
setp: 400, Loss: 0.6455389857292175
setp: 500, Loss: 0.5121455788612366
setp: 600, Loss: 0.5763026475906372
setp: 700, Loss: 0.595784068107605
setp: 800, Loss: 0.5930712819099426
setp: 900, Loss: 0.5797662138938904
setp: 1000, Loss: 0.5837307572364807
setp: 1100, Loss: 0.553918182849884
setp: 1200, Loss: 0.48507946729660034
setp: 1300, Loss: 0.3808220624923706
setp: 1400, Loss: 0.3370020389556885
setp: 1500, Loss: 0.3260025084018707
setp: 1600, Loss: 0.33423957228660583
setp: 1700, Loss: 0.3183971047401428
setp: 1800, Loss: 0.3196035921573639
setp: 1900, Loss: 0.31950974464416504
setp: 2000, Loss: 0.32003253698349
setp: 2100, Loss: 0.3196113407611847
setp: 2200, Loss: 0.3181125819683075
setp: 2300, Loss: 0.31901922821998596
setp: 2400, Loss: 0.31731274724006653
setp: 2500, Loss: 0.35019972920417786
setp: 2600, Loss: 0.38701146841049194
setp: 2700, Loss: 0.32157665491104126
setp: 2800, Loss: 0.31701821088790894
setp: 2900, Loss: 0.3160823583602905
setp: 3000, Loss: 0.3169132173061371
setp: 3100, Loss: 0.31821349263191223
setp: 3200, Loss: 0.3174608647823334
setp: 3300, Loss: 0.3169962763786316
setp: 3400, Loss: 0.3178618848323822
setp: 3500, Loss: 0.31609833240509033
setp: 3600, Loss: 0.31713372468948364
setp: 3700, Loss: 0.3166596293449402
setp: 3800, Loss: 0.3175241947174072
setp: 3900, Loss: 0.3184894919395447
setp: 4000, Loss: 0.3297436535358429
setp: 4100, Loss: 0.356097012758255
setp: 4200, Loss: 0.32082900404930115
setp: 4300, Loss: 0.3161008656024933
setp: 4400, Loss: 0.3174762725830078
setp: 4500, Loss: 0.3168909549713135
setp: 4600, Loss: 0.3210859000682831
setp: 4700, Loss: 0.31745341420173645
setp: 4800, Loss: 0.3160848319530487
setp: 4900, Loss: 0.31675538420677185
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9210526315789473
precision: 0.9166666666666666
recall: 0.8870967741935484
F_score: 0.9016393442622951
******fold 4******
[208, 400]
training...
setp: 0, Loss: 0.8530349731445312
setp: 100, Loss: 0.6426251530647278
setp: 200, Loss: 0.5684801936149597
setp: 300, Loss: 0.5396065711975098
setp: 400, Loss: 0.50052809715271
setp: 500, Loss: 0.39476725459098816
setp: 600, Loss: 0.3811855912208557
setp: 700, Loss: 0.3535139858722687
setp: 800, Loss: 0.39624929428100586
setp: 900, Loss: 0.32885199785232544
setp: 1000, Loss: 0.320014089345932
setp: 1100, Loss: 0.31838923692703247
setp: 1200, Loss: 0.3562295734882355
setp: 1300, Loss: 0.3177662193775177
setp: 1400, Loss: 0.3167978525161743
setp: 1500, Loss: 0.32116737961769104
setp: 1600, Loss: 0.3200065493583679
setp: 1700, Loss: 0.32041868567466736
setp: 1800, Loss: 0.31712305545806885
setp: 1900, Loss: 0.31892868876457214
setp: 2000, Loss: 0.31923338770866394
setp: 2100, Loss: 0.35097163915634155
setp: 2200, Loss: 0.3486107885837555
setp: 2300, Loss: 0.33524656295776367
setp: 2400, Loss: 0.3221733868122101
setp: 2500, Loss: 0.3208383023738861
setp: 2600, Loss: 0.31847649812698364
setp: 2700, Loss: 0.3194873631000519
setp: 2800, Loss: 0.3167053163051605
setp: 2900, Loss: 0.3176957964897156
setp: 3000, Loss: 0.31667017936706543
setp: 3100, Loss: 0.3483702838420868
setp: 3200, Loss: 0.31713834404945374
setp: 3300, Loss: 0.3160126805305481
setp: 3400, Loss: 0.31771349906921387
setp: 3500, Loss: 0.3186882436275482
setp: 3600, Loss: 0.5080035328865051
setp: 3700, Loss: 0.3301789462566376
setp: 3800, Loss: 0.3245062232017517
setp: 3900, Loss: 0.3173721730709076
setp: 4000, Loss: 0.3170030117034912
setp: 4100, Loss: 0.34972789883613586
setp: 4200, Loss: 0.31770721077919006
setp: 4300, Loss: 0.31597787141799927
setp: 4400, Loss: 0.31716886162757874
setp: 4500, Loss: 0.31649842858314514
setp: 4600, Loss: 0.3194519281387329
setp: 4700, Loss: 0.3171508014202118
setp: 4800, Loss: 0.31740301847457886
setp: 4900, Loss: 0.31753674149513245
training successfully ended.
validating...
acc: 0.6578947368421053
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.743421052631579
precision: 0
recall: 0.0
F_score: 0
model saved.
avg_acc: 0.8855263157894736, avg_f_score: 0.7093803970802449
==========arousal==========
******fold 0******
[299, 309]
training...
setp: 0, Loss: 0.7329865097999573
setp: 100, Loss: 0.6929661631584167
setp: 200, Loss: 0.635387659072876
setp: 300, Loss: 0.5950095057487488
setp: 400, Loss: 0.4354521334171295
setp: 500, Loss: 0.3484795093536377
setp: 600, Loss: 0.35931867361068726
setp: 700, Loss: 0.3472440838813782
setp: 800, Loss: 0.3285938501358032
setp: 900, Loss: 0.3248055577278137
setp: 1000, Loss: 0.3221460282802582
setp: 1100, Loss: 0.32198500633239746
setp: 1200, Loss: 0.3705907166004181
setp: 1300, Loss: 0.32661882042884827
setp: 1400, Loss: 0.3343937397003174
setp: 1500, Loss: 0.3209454119205475
setp: 1600, Loss: 0.32009920477867126
setp: 1700, Loss: 0.31899234652519226
setp: 1800, Loss: 0.31854215264320374
setp: 1900, Loss: 0.31908679008483887
setp: 2000, Loss: 0.32016393542289734
setp: 2100, Loss: 0.3816714584827423
setp: 2200, Loss: 0.32073885202407837
setp: 2300, Loss: 0.32204145193099976
setp: 2400, Loss: 0.31766101717948914
setp: 2500, Loss: 0.31723031401634216
setp: 2600, Loss: 0.32003748416900635
setp: 2700, Loss: 0.3188910186290741
setp: 2800, Loss: 0.3189997375011444
setp: 2900, Loss: 0.31845319271087646
setp: 3000, Loss: 0.3198353350162506
setp: 3100, Loss: 0.31814149022102356
setp: 3200, Loss: 0.3172811269760132
setp: 3300, Loss: 0.3186471164226532
setp: 3400, Loss: 0.32034608721733093
setp: 3500, Loss: 0.3179488182067871
setp: 3600, Loss: 0.31830477714538574
setp: 3700, Loss: 0.3175479471683502
setp: 3800, Loss: 0.31785914301872253
setp: 3900, Loss: 0.31856316328048706
setp: 4000, Loss: 0.3185144066810608
setp: 4100, Loss: 0.32246434688568115
setp: 4200, Loss: 0.31835266947746277
setp: 4300, Loss: 0.3175242841243744
setp: 4400, Loss: 0.31724604964256287
setp: 4500, Loss: 0.3190656304359436
setp: 4600, Loss: 0.31920021772384644
setp: 4700, Loss: 0.3188953399658203
setp: 4800, Loss: 0.3181537985801697
setp: 4900, Loss: 0.3823480010032654
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9342105263157895
precision: 0.961038961038961
recall: 0.9135802469135802
F_score: 0.9367088607594937
******fold 1******
[303, 305]
training...
setp: 0, Loss: 0.6910642385482788
setp: 100, Loss: 0.6852940917015076
setp: 200, Loss: 0.5874872803688049
setp: 300, Loss: 0.4884529113769531
setp: 400, Loss: 0.4282987713813782
setp: 500, Loss: 0.351927250623703
setp: 600, Loss: 0.36131858825683594
setp: 700, Loss: 0.32956409454345703
setp: 800, Loss: 0.3286198377609253
setp: 900, Loss: 0.32243964076042175
setp: 1000, Loss: 0.32255467772483826
setp: 1100, Loss: 0.3204783797264099
setp: 1200, Loss: 0.3213343024253845
setp: 1300, Loss: 0.32911381125450134
setp: 1400, Loss: 0.3438865840435028
setp: 1500, Loss: 0.3225265145301819
setp: 1600, Loss: 0.3207789957523346
setp: 1700, Loss: 0.32248979806900024
setp: 1800, Loss: 0.31889504194259644
setp: 1900, Loss: 0.32186028361320496
setp: 2000, Loss: 0.31902047991752625
setp: 2100, Loss: 0.32052111625671387
setp: 2200, Loss: 0.4013351798057556
setp: 2300, Loss: 0.32765328884124756
setp: 2400, Loss: 0.3204653561115265
setp: 2500, Loss: 0.3183293640613556
setp: 2600, Loss: 0.31979793310165405
setp: 2700, Loss: 0.3204845190048218
setp: 2800, Loss: 0.31780752539634705
setp: 2900, Loss: 0.322802871465683
setp: 3000, Loss: 0.3517063558101654
setp: 3100, Loss: 0.31921279430389404
setp: 3200, Loss: 0.31994733214378357
setp: 3300, Loss: 0.3207516670227051
setp: 3400, Loss: 0.32114723324775696
setp: 3500, Loss: 0.31920862197875977
setp: 3600, Loss: 0.3193763494491577
setp: 3700, Loss: 0.3179585337638855
setp: 3800, Loss: 0.34334519505500793
setp: 3900, Loss: 0.3281814455986023
setp: 4000, Loss: 0.3260186016559601
setp: 4100, Loss: 0.3192462623119354
setp: 4200, Loss: 0.32009515166282654
setp: 4300, Loss: 0.3182157576084137
setp: 4400, Loss: 0.3176567852497101
setp: 4500, Loss: 0.31886026263237
setp: 4600, Loss: 0.3191874921321869
setp: 4700, Loss: 0.3173089027404785
setp: 4800, Loss: 0.3526048958301544
setp: 4900, Loss: 0.3203706443309784
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9358974358974359
recall: 0.948051948051948
F_score: 0.9419354838709677
******fold 2******
[300, 308]
training...
setp: 0, Loss: 0.692409336566925
setp: 100, Loss: 0.6709312796592712
setp: 200, Loss: 0.5552170872688293
setp: 300, Loss: 0.4587012529373169
setp: 400, Loss: 0.38283419609069824
setp: 500, Loss: 0.3692799210548401
setp: 600, Loss: 0.3322991728782654
setp: 700, Loss: 0.33419254422187805
setp: 800, Loss: 0.3220898509025574
setp: 900, Loss: 0.3204825818538666
setp: 1000, Loss: 0.32167762517929077
setp: 1100, Loss: 0.32245853543281555
setp: 1200, Loss: 0.3210337460041046
setp: 1300, Loss: 0.321410208940506
setp: 1400, Loss: 0.32476162910461426
setp: 1500, Loss: 0.3328061103820801
setp: 1600, Loss: 0.32222217321395874
setp: 1700, Loss: 0.3195943534374237
setp: 1800, Loss: 0.3194020986557007
setp: 1900, Loss: 0.3208331763744354
setp: 2000, Loss: 0.3198481798171997
setp: 2100, Loss: 0.32036617398262024
setp: 2200, Loss: 0.3197227716445923
setp: 2300, Loss: 0.31991639733314514
setp: 2400, Loss: 0.31989967823028564
setp: 2500, Loss: 0.32282769680023193
setp: 2600, Loss: 0.3422335684299469
setp: 2700, Loss: 0.326680451631546
setp: 2800, Loss: 0.3186584711074829
setp: 2900, Loss: 0.3200097978115082
setp: 3000, Loss: 0.318806916475296
setp: 3100, Loss: 0.3188015818595886
setp: 3200, Loss: 0.3191223442554474
setp: 3300, Loss: 0.31794771552085876
setp: 3400, Loss: 0.31988006830215454
setp: 3500, Loss: 0.31929922103881836
setp: 3600, Loss: 0.3204866051673889
setp: 3700, Loss: 0.3267410695552826
setp: 3800, Loss: 0.32182997465133667
setp: 3900, Loss: 0.32155168056488037
setp: 4000, Loss: 0.3176979124546051
setp: 4100, Loss: 0.3177697956562042
setp: 4200, Loss: 0.3190274238586426
setp: 4300, Loss: 0.31943613290786743
setp: 4400, Loss: 0.31832262873649597
setp: 4500, Loss: 0.3195194900035858
setp: 4600, Loss: 0.31751662492752075
setp: 4700, Loss: 0.3349238634109497
setp: 4800, Loss: 0.3204445242881775
setp: 4900, Loss: 0.31829094886779785
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.9176470588235294
recall: 0.975
F_score: 0.9454545454545454
******fold 3******
[305, 303]
training...
setp: 0, Loss: 0.7425287961959839
setp: 100, Loss: 0.6915848255157471
setp: 200, Loss: 0.582088053226471
setp: 300, Loss: 0.6176617741584778
setp: 400, Loss: 0.5671465396881104
setp: 500, Loss: 0.5174345970153809
setp: 600, Loss: 0.3577539622783661
setp: 700, Loss: 0.3748994469642639
setp: 800, Loss: 0.3582000732421875
setp: 900, Loss: 0.320711612701416
setp: 1000, Loss: 0.3202515244483948
setp: 1100, Loss: 0.3442543148994446
setp: 1200, Loss: 0.3173534572124481
setp: 1300, Loss: 0.31766271591186523
setp: 1400, Loss: 0.3224217891693115
setp: 1500, Loss: 0.3215157389640808
setp: 1600, Loss: 0.3182171881198883
setp: 1700, Loss: 0.3202988803386688
setp: 1800, Loss: 0.31702664494514465
setp: 1900, Loss: 0.318498820066452
setp: 2000, Loss: 0.372911274433136
setp: 2100, Loss: 0.3327091932296753
setp: 2200, Loss: 0.3291492760181427
setp: 2300, Loss: 0.3178568184375763
setp: 2400, Loss: 0.3160117268562317
setp: 2500, Loss: 0.3174554705619812
setp: 2600, Loss: 0.3169113099575043
setp: 2700, Loss: 0.31700095534324646
setp: 2800, Loss: 0.33026373386383057
setp: 2900, Loss: 0.31724992394447327
setp: 3000, Loss: 0.317518949508667
setp: 3100, Loss: 0.3223249614238739
setp: 3200, Loss: 0.32176339626312256
setp: 3300, Loss: 0.31996968388557434
setp: 3400, Loss: 0.32263123989105225
setp: 3500, Loss: 0.3183343708515167
setp: 3600, Loss: 0.3415147066116333
setp: 3700, Loss: 0.3168399930000305
setp: 3800, Loss: 0.31747716665267944
setp: 3900, Loss: 0.31758320331573486
setp: 4000, Loss: 0.31763771176338196
setp: 4100, Loss: 0.3185685873031616
setp: 4200, Loss: 0.31869858503341675
setp: 4300, Loss: 0.31687262654304504
setp: 4400, Loss: 0.3165538012981415
setp: 4500, Loss: 0.3188251554965973
setp: 4600, Loss: 0.31712672114372253
setp: 4700, Loss: 0.3171182870864868
setp: 4800, Loss: 0.3175278306007385
setp: 4900, Loss: 0.36985382437705994
training successfully ended.
validating...
acc: 0.993421052631579
precision: 0.990228013029316
recall: 0.9967213114754099
F_score: 0.9934640522875817
validating...
acc: 0.9539473684210527
precision: 0.9722222222222222
recall: 0.9333333333333333
F_score: 0.9523809523809524
******fold 4******
[313, 295]
training...
setp: 0, Loss: 0.6933230757713318
setp: 100, Loss: 0.6880310773849487
setp: 200, Loss: 0.5892955660820007
setp: 300, Loss: 0.5907324552536011
setp: 400, Loss: 0.5052462220191956
setp: 500, Loss: 0.3580961227416992
setp: 600, Loss: 0.3342343270778656
setp: 700, Loss: 0.36645686626434326
setp: 800, Loss: 0.3249416649341583
setp: 900, Loss: 0.3972664475440979
setp: 1000, Loss: 0.32000112533569336
setp: 1100, Loss: 0.32215163111686707
setp: 1200, Loss: 0.3214564621448517
setp: 1300, Loss: 0.3189934194087982
setp: 1400, Loss: 0.33552366495132446
setp: 1500, Loss: 0.3188101351261139
setp: 1600, Loss: 0.319884717464447
setp: 1700, Loss: 0.3181210160255432
setp: 1800, Loss: 0.3187883496284485
setp: 1900, Loss: 0.3196135461330414
setp: 2000, Loss: 0.3173867464065552
setp: 2100, Loss: 0.3191278874874115
setp: 2200, Loss: 0.3195590376853943
setp: 2300, Loss: 0.3189590275287628
setp: 2400, Loss: 0.4918229281902313
setp: 2500, Loss: 0.31916964054107666
setp: 2600, Loss: 0.32452601194381714
setp: 2700, Loss: 0.3174671530723572
setp: 2800, Loss: 0.316794216632843
setp: 2900, Loss: 0.3165738880634308
setp: 3000, Loss: 0.3173597753047943
setp: 3100, Loss: 0.31780633330345154
setp: 3200, Loss: 0.31829845905303955
setp: 3300, Loss: 0.31806230545043945
setp: 3400, Loss: 0.3196330666542053
setp: 3500, Loss: 0.3180648684501648
setp: 3600, Loss: 0.3174850046634674
setp: 3700, Loss: 0.3177337646484375
setp: 3800, Loss: 0.31798821687698364
setp: 3900, Loss: 0.3166126608848572
setp: 4000, Loss: 0.31775686144828796
setp: 4100, Loss: 0.6537784337997437
setp: 4200, Loss: 0.5426573753356934
setp: 4300, Loss: 0.4164883494377136
setp: 4400, Loss: 0.3416577875614166
setp: 4500, Loss: 0.3409683406352997
setp: 4600, Loss: 0.33263424038887024
setp: 4700, Loss: 0.3639369606971741
setp: 4800, Loss: 0.32605335116386414
setp: 4900, Loss: 0.342647522687912
training successfully ended.
validating...
acc: 0.9424342105263158
precision: 0.9964285714285714
recall: 0.8913738019169329
F_score: 0.9409780775716695
validating...
acc: 0.8618421052631579
precision: 1.0
recall: 0.6865671641791045
F_score: 0.8141592920353982
model saved.
avg_acc: 0.9263157894736842, avg_f_score: 0.9181278269002714
-------------subject: 31-------------
==========valence==========
******fold 0******
[204, 404]
training...
setp: 0, Loss: 0.6967030763626099
setp: 100, Loss: 0.6216425895690918
setp: 200, Loss: 0.5970099568367004
setp: 300, Loss: 0.5806048512458801
setp: 400, Loss: 0.43503791093826294
setp: 500, Loss: 0.34246695041656494
setp: 600, Loss: 0.32542845606803894
setp: 700, Loss: 0.32128825783729553
setp: 800, Loss: 0.3563494384288788
setp: 900, Loss: 0.3235599398612976
setp: 1000, Loss: 0.3195604681968689
setp: 1100, Loss: 0.31941717863082886
setp: 1200, Loss: 0.34891098737716675
setp: 1300, Loss: 0.3209737539291382
setp: 1400, Loss: 0.3187229037284851
setp: 1500, Loss: 0.31898269057273865
setp: 1600, Loss: 0.31810706853866577
setp: 1700, Loss: 0.36610549688339233
setp: 1800, Loss: 0.3189571797847748
setp: 1900, Loss: 0.3186156749725342
setp: 2000, Loss: 0.3189966678619385
setp: 2100, Loss: 0.3176838159561157
setp: 2200, Loss: 0.34638160467147827
setp: 2300, Loss: 0.33332863450050354
setp: 2400, Loss: 0.3171849846839905
setp: 2500, Loss: 0.31813618540763855
setp: 2600, Loss: 0.31813016533851624
setp: 2700, Loss: 0.3493061065673828
setp: 2800, Loss: 0.31810322403907776
setp: 2900, Loss: 0.31927064061164856
setp: 3000, Loss: 0.31843650341033936
setp: 3100, Loss: 0.31815212965011597
setp: 3200, Loss: 0.3196892738342285
setp: 3300, Loss: 0.317946195602417
setp: 3400, Loss: 0.3944139778614044
setp: 3500, Loss: 0.3447709083557129
setp: 3600, Loss: 0.3218836486339569
setp: 3700, Loss: 0.3212527632713318
setp: 3800, Loss: 0.32687681913375854
setp: 3900, Loss: 0.32292914390563965
setp: 4000, Loss: 0.3185131549835205
setp: 4100, Loss: 0.3208550214767456
setp: 4200, Loss: 0.32054516673088074
setp: 4300, Loss: 0.3187047839164734
setp: 4400, Loss: 0.32002609968185425
setp: 4500, Loss: 0.32004186511039734
setp: 4600, Loss: 0.35089313983917236
setp: 4700, Loss: 0.3199654221534729
setp: 4800, Loss: 0.32095739245414734
setp: 4900, Loss: 0.3200308084487915
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9950980392156863
F_score: 0.9975429975429976
validating...
acc: 0.875
precision: 0.8909090909090909
recall: 0.7903225806451613
F_score: 0.8376068376068375
******fold 1******
[211, 397]
training...
setp: 0, Loss: 0.6559189558029175
setp: 100, Loss: 0.6269384622573853
setp: 200, Loss: 0.6629281640052795
setp: 300, Loss: 0.6095952987670898
setp: 400, Loss: 0.5609584450721741
setp: 500, Loss: 0.3966875672340393
setp: 600, Loss: 0.33901113271713257
setp: 700, Loss: 0.3724327087402344
setp: 800, Loss: 0.40929558873176575
setp: 900, Loss: 0.3306804299354553
setp: 1000, Loss: 0.33294469118118286
setp: 1100, Loss: 0.32120224833488464
setp: 1200, Loss: 0.3189607560634613
setp: 1300, Loss: 0.31998559832572937
setp: 1400, Loss: 0.3181329667568207
setp: 1500, Loss: 0.31903278827667236
setp: 1600, Loss: 0.319604754447937
setp: 1700, Loss: 0.321506530046463
setp: 1800, Loss: 0.3199866712093353
setp: 1900, Loss: 0.3327449858188629
setp: 2000, Loss: 0.32940173149108887
setp: 2100, Loss: 0.32082340121269226
setp: 2200, Loss: 0.32039210200309753
setp: 2300, Loss: 0.35042035579681396
setp: 2400, Loss: 0.31995075941085815
setp: 2500, Loss: 0.319872111082077
setp: 2600, Loss: 0.3211435079574585
setp: 2700, Loss: 0.3716789782047272
setp: 2800, Loss: 0.3410685062408447
setp: 2900, Loss: 0.3318342864513397
setp: 3000, Loss: 0.3249163329601288
setp: 3100, Loss: 0.32322928309440613
setp: 3200, Loss: 0.32029229402542114
setp: 3300, Loss: 0.31855323910713196
setp: 3400, Loss: 0.31931567192077637
setp: 3500, Loss: 0.31981080770492554
setp: 3600, Loss: 0.3208141326904297
setp: 3700, Loss: 0.3194398880004883
setp: 3800, Loss: 0.32130712270736694
setp: 3900, Loss: 0.3199669122695923
setp: 4000, Loss: 0.3198933005332947
setp: 4100, Loss: 0.3199228346347809
setp: 4200, Loss: 0.32089659571647644
setp: 4300, Loss: 0.3199141025543213
setp: 4400, Loss: 0.3198070228099823
setp: 4500, Loss: 0.3207336366176605
setp: 4600, Loss: 0.3500361144542694
setp: 4700, Loss: 0.31930476427078247
setp: 4800, Loss: 0.6569777727127075
setp: 4900, Loss: 0.6881032586097717
training successfully ended.
validating...
acc: 0.6529605263157895
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.6381578947368421
precision: 0
recall: 0.0
F_score: 0
******fold 2******
[214, 394]
training...
setp: 0, Loss: 0.7156060338020325
setp: 100, Loss: 0.6425166130065918
setp: 200, Loss: 0.6203359365463257
setp: 300, Loss: 0.4633009433746338
setp: 400, Loss: 0.37337079644203186
setp: 500, Loss: 0.35090628266334534
setp: 600, Loss: 0.3916850686073303
setp: 700, Loss: 0.3291918933391571
setp: 800, Loss: 0.35212841629981995
setp: 900, Loss: 0.3509417772293091
setp: 1000, Loss: 0.3213960528373718
setp: 1100, Loss: 0.3181944191455841
setp: 1200, Loss: 0.3369375467300415
setp: 1300, Loss: 0.33008891344070435
setp: 1400, Loss: 0.3211660087108612
setp: 1500, Loss: 0.3199618458747864
setp: 1600, Loss: 0.3197375535964966
setp: 1700, Loss: 0.32074812054634094
setp: 1800, Loss: 0.3184821307659149
setp: 1900, Loss: 0.3212631344795227
setp: 2000, Loss: 0.319699227809906
setp: 2100, Loss: 0.31936928629875183
setp: 2200, Loss: 0.3198411166667938
setp: 2300, Loss: 0.318842351436615
setp: 2400, Loss: 0.31876036524772644
setp: 2500, Loss: 0.31957384943962097
setp: 2600, Loss: 0.3187306821346283
setp: 2700, Loss: 0.34871354699134827
setp: 2800, Loss: 0.3196871280670166
setp: 2900, Loss: 0.4666901230812073
setp: 3000, Loss: 0.35705772042274475
setp: 3100, Loss: 0.3577544689178467
setp: 3200, Loss: 0.3348402678966522
setp: 3300, Loss: 0.34224623441696167
setp: 3400, Loss: 0.3232928216457367
setp: 3500, Loss: 0.3639211654663086
setp: 3600, Loss: 0.3391391336917877
setp: 3700, Loss: 0.32421159744262695
setp: 3800, Loss: 0.3329121172428131
setp: 3900, Loss: 0.32299983501434326
setp: 4000, Loss: 0.3239502012729645
setp: 4100, Loss: 0.32235923409461975
setp: 4200, Loss: 0.3205595314502716
setp: 4300, Loss: 0.3200736939907074
setp: 4400, Loss: 0.3213983178138733
setp: 4500, Loss: 0.32097944617271423
setp: 4600, Loss: 0.3509698212146759
setp: 4700, Loss: 0.3212403357028961
setp: 4800, Loss: 0.32163703441619873
setp: 4900, Loss: 0.320178747177124
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9953271028037384
F_score: 0.9976580796252927
validating...
acc: 0.9013157894736842
precision: 0.8936170212765957
recall: 0.8076923076923077
F_score: 0.8484848484848485
******fold 3******
[213, 395]
training...
setp: 0, Loss: 0.8133751749992371
setp: 100, Loss: 0.6223355531692505
setp: 200, Loss: 0.6611915826797485
setp: 300, Loss: 0.6206073760986328
setp: 400, Loss: 0.48696431517601013
setp: 500, Loss: 0.354544073343277
setp: 600, Loss: 0.33824747800827026
setp: 700, Loss: 0.3353536128997803
setp: 800, Loss: 0.376630961894989
setp: 900, Loss: 0.3581297993659973
setp: 1000, Loss: 0.3240259289741516
setp: 1100, Loss: 0.319750040769577
setp: 1200, Loss: 0.32049560546875
setp: 1300, Loss: 0.3195721209049225
setp: 1400, Loss: 0.3196350932121277
setp: 1500, Loss: 0.31831857562065125
setp: 1600, Loss: 0.32039037346839905
setp: 1700, Loss: 0.319461464881897
setp: 1800, Loss: 0.4328054189682007
setp: 1900, Loss: 0.32988637685775757
setp: 2000, Loss: 0.3213076591491699
setp: 2100, Loss: 0.3566390872001648
setp: 2200, Loss: 0.33003222942352295
setp: 2300, Loss: 0.325137197971344
setp: 2400, Loss: 0.31893929839134216
setp: 2500, Loss: 0.3192952871322632
setp: 2600, Loss: 0.31962713599205017
setp: 2700, Loss: 0.3212525248527527
setp: 2800, Loss: 0.3200366795063019
setp: 2900, Loss: 0.3209323287010193
setp: 3000, Loss: 0.32028716802597046
setp: 3100, Loss: 0.32022160291671753
setp: 3200, Loss: 0.3202403783798218
setp: 3300, Loss: 0.3197844326496124
setp: 3400, Loss: 0.3194248378276825
setp: 3500, Loss: 0.32021620869636536
setp: 3600, Loss: 0.320383757352829
setp: 3700, Loss: 0.3188467025756836
setp: 3800, Loss: 0.31941038370132446
setp: 3900, Loss: 0.32731950283050537
setp: 4000, Loss: 0.3389870226383209
setp: 4100, Loss: 0.3359760046005249
setp: 4200, Loss: 0.3305540680885315
setp: 4300, Loss: 0.33512184023857117
setp: 4400, Loss: 0.3187529444694519
setp: 4500, Loss: 0.3190801739692688
setp: 4600, Loss: 0.32074522972106934
setp: 4700, Loss: 0.3203539252281189
setp: 4800, Loss: 0.3201136589050293
setp: 4900, Loss: 0.31974348425865173
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9953051643192489
F_score: 0.9976470588235293
validating...
acc: 0.7960526315789473
precision: 0.7115384615384616
recall: 0.6981132075471698
F_score: 0.7047619047619047
******fold 4******
[222, 386]
training...
setp: 0, Loss: 0.8148118257522583
setp: 100, Loss: 0.6609645485877991
setp: 200, Loss: 0.6585837006568909
setp: 300, Loss: 0.6330423951148987
setp: 400, Loss: 0.6412506699562073
setp: 500, Loss: 0.4075029194355011
setp: 600, Loss: 0.39622268080711365
setp: 700, Loss: 0.3468346893787384
setp: 800, Loss: 0.32821837067604065
setp: 900, Loss: 0.35492971539497375
setp: 1000, Loss: 0.3233073949813843
setp: 1100, Loss: 0.3248058259487152
setp: 1200, Loss: 0.3226985037326813
setp: 1300, Loss: 0.31995826959609985
setp: 1400, Loss: 0.3197709023952484
setp: 1500, Loss: 0.32076773047447205
setp: 1600, Loss: 0.31996864080429077
setp: 1700, Loss: 0.3225007951259613
setp: 1800, Loss: 0.3237857222557068
setp: 1900, Loss: 0.319430947303772
setp: 2000, Loss: 0.31892403960227966
setp: 2100, Loss: 0.3188668489456177
setp: 2200, Loss: 0.32031866908073425
setp: 2300, Loss: 0.3213787376880646
setp: 2400, Loss: 0.31858986616134644
setp: 2500, Loss: 0.31984439492225647
setp: 2600, Loss: 0.3206523358821869
setp: 2700, Loss: 0.3193098306655884
setp: 2800, Loss: 0.3581607937812805
setp: 2900, Loss: 0.3239130973815918
setp: 3000, Loss: 0.3193538784980774
setp: 3100, Loss: 0.32010596990585327
setp: 3200, Loss: 0.31848353147506714
setp: 3300, Loss: 0.3192604184150696
setp: 3400, Loss: 0.3199104964733124
setp: 3500, Loss: 0.3189825117588043
setp: 3600, Loss: 0.32090792059898376
setp: 3700, Loss: 0.31826052069664
setp: 3800, Loss: 0.3189084827899933
setp: 3900, Loss: 0.31931453943252563
setp: 4000, Loss: 0.3303692936897278
setp: 4100, Loss: 0.32439762353897095
setp: 4200, Loss: 0.32130253314971924
setp: 4300, Loss: 0.3181114196777344
setp: 4400, Loss: 0.3198550343513489
setp: 4500, Loss: 0.3202040493488312
setp: 4600, Loss: 0.31952714920043945
setp: 4700, Loss: 0.3498614728450775
setp: 4800, Loss: 0.3196795880794525
setp: 4900, Loss: 0.3199029862880707
training successfully ended.
validating...
acc: 0.9983552631578947
precision: 1.0
recall: 0.9954954954954955
F_score: 0.9977426636568849
validating...
acc: 0.9013157894736842
precision: 0.8085106382978723
recall: 0.8636363636363636
F_score: 0.8351648351648351
model saved.
avg_acc: 0.8223684210526315, avg_f_score: 0.6452036852036851
==========arousal==========
******fold 0******
[307, 301]
training...
setp: 0, Loss: 0.6754134297370911
setp: 100, Loss: 0.6926561594009399
setp: 200, Loss: 0.690078854560852
setp: 300, Loss: 0.6804189085960388
setp: 400, Loss: 0.6128453612327576
setp: 500, Loss: 0.5889667272567749
setp: 600, Loss: 0.5102130174636841
setp: 700, Loss: 0.5890347957611084
setp: 800, Loss: 0.4562445282936096
setp: 900, Loss: 0.40722477436065674
setp: 1000, Loss: 0.50372314453125
setp: 1100, Loss: 0.40590813755989075
setp: 1200, Loss: 0.4024796187877655
setp: 1300, Loss: 0.390180766582489
setp: 1400, Loss: 0.3980475962162018
setp: 1500, Loss: 0.43988946080207825
setp: 1600, Loss: 0.3292500376701355
setp: 1700, Loss: 0.32962995767593384
setp: 1800, Loss: 0.317175030708313
setp: 1900, Loss: 0.35048407316207886
setp: 2000, Loss: 0.3561870753765106
setp: 2100, Loss: 0.3247104287147522
setp: 2200, Loss: 0.37952321767807007
setp: 2300, Loss: 0.3183395564556122
setp: 2400, Loss: 0.3494053781032562
setp: 2500, Loss: 0.34834423661231995
setp: 2600, Loss: 0.32387223839759827
setp: 2700, Loss: 0.3864457905292511
setp: 2800, Loss: 0.31669479608535767
setp: 2900, Loss: 0.3310546875
setp: 3000, Loss: 0.34639617800712585
setp: 3100, Loss: 0.317438006401062
setp: 3200, Loss: 0.3168569505214691
setp: 3300, Loss: 0.3508875370025635
setp: 3400, Loss: 0.3788875639438629
setp: 3500, Loss: 0.3193531036376953
setp: 3600, Loss: 0.3179577887058258
setp: 3700, Loss: 0.316322922706604
setp: 3800, Loss: 0.34971216320991516
setp: 3900, Loss: 0.31873074173927307
setp: 4000, Loss: 0.3170291483402252
setp: 4100, Loss: 0.32635632157325745
setp: 4200, Loss: 0.3281092941761017
setp: 4300, Loss: 0.34704941511154175
setp: 4400, Loss: 0.34852269291877747
setp: 4500, Loss: 0.31901946663856506
setp: 4600, Loss: 0.3500159978866577
setp: 4700, Loss: 0.31595584750175476
setp: 4800, Loss: 0.31657981872558594
setp: 4900, Loss: 0.3467705547809601
training successfully ended.
validating...
acc: 0.9868421052631579
precision: 0.9746031746031746
recall: 1.0
F_score: 0.9871382636655948
validating...
acc: 0.881578947368421
precision: 0.8571428571428571
recall: 0.9041095890410958
F_score: 0.88
******fold 1******
[302, 306]
training...
setp: 0, Loss: 0.6778588891029358
setp: 100, Loss: 0.6952381134033203
setp: 200, Loss: 0.692704975605011
setp: 300, Loss: 0.6930016875267029
setp: 400, Loss: 0.6931614875793457
setp: 500, Loss: 0.6936748623847961
setp: 600, Loss: 0.6931506991386414
setp: 700, Loss: 0.693473756313324
setp: 800, Loss: 0.693998396396637
setp: 900, Loss: 0.6935796141624451
setp: 1000, Loss: 0.6932733654975891
setp: 1100, Loss: 0.6930193305015564
setp: 1200, Loss: 0.692730724811554
setp: 1300, Loss: 0.6931591033935547
setp: 1400, Loss: 0.6930246949195862
setp: 1500, Loss: 0.6929064989089966
setp: 1600, Loss: 0.6936676502227783
setp: 1700, Loss: 0.6931524276733398
setp: 1800, Loss: 0.6931037902832031
setp: 1900, Loss: 0.6926844120025635
setp: 2000, Loss: 0.6946988105773926
setp: 2100, Loss: 0.6929154396057129
setp: 2200, Loss: 0.6931008100509644
setp: 2300, Loss: 0.6931555867195129
setp: 2400, Loss: 0.6936025619506836
setp: 2500, Loss: 0.6931477785110474
setp: 2600, Loss: 0.6932820677757263
setp: 2700, Loss: 0.693852424621582
setp: 2800, Loss: 0.6934974193572998
setp: 2900, Loss: 0.6931893825531006
setp: 3000, Loss: 0.6930991411209106
setp: 3100, Loss: 0.6927854418754578
setp: 3200, Loss: 0.6931540369987488
setp: 3300, Loss: 0.693160891532898
setp: 3400, Loss: 0.6930474638938904
setp: 3500, Loss: 0.6936114430427551
setp: 3600, Loss: 0.6931494474411011
setp: 3700, Loss: 0.693162739276886
setp: 3800, Loss: 0.6928678154945374
setp: 3900, Loss: 0.6945575475692749
setp: 4000, Loss: 0.6930222511291504
setp: 4100, Loss: 0.6931533813476562
setp: 4200, Loss: 0.6931524276733398
setp: 4300, Loss: 0.6935504078865051
setp: 4400, Loss: 0.6931472420692444
setp: 4500, Loss: 0.6931877732276917
setp: 4600, Loss: 0.6937702298164368
setp: 4700, Loss: 0.6934488415718079
setp: 4800, Loss: 0.6931517124176025
setp: 4900, Loss: 0.6931393146514893
training successfully ended.
validating...
acc: 0.5032894736842105
precision: 0
recall: 0.0
F_score: 0
validating...
acc: 0.4868421052631579
precision: 0
recall: 0.0
F_score: 0
******fold 2******
[299, 309]
training...
setp: 0, Loss: 0.6827443838119507
setp: 100, Loss: 0.6922748684883118
setp: 200, Loss: 0.6671252250671387
setp: 300, Loss: 0.45042604207992554
setp: 400, Loss: 0.3882044553756714
setp: 500, Loss: 0.40469977259635925
setp: 600, Loss: 0.3471281826496124
setp: 700, Loss: 0.3468482196331024
setp: 800, Loss: 0.37605318427085876
setp: 900, Loss: 0.3255898952484131
setp: 1000, Loss: 0.32264837622642517
setp: 1100, Loss: 0.32836833596229553
setp: 1200, Loss: 0.31840381026268005
setp: 1300, Loss: 0.3187500238418579
setp: 1400, Loss: 0.31812694668769836
setp: 1500, Loss: 0.3194834291934967
setp: 1600, Loss: 0.319289892911911
setp: 1700, Loss: 0.3193475902080536
setp: 1800, Loss: 0.3181353211402893
setp: 1900, Loss: 0.32001158595085144
setp: 2000, Loss: 0.3195149898529053
setp: 2100, Loss: 0.31903213262557983
setp: 2200, Loss: 0.31795060634613037
setp: 2300, Loss: 0.3188403844833374
setp: 2400, Loss: 0.3190913796424866
setp: 2500, Loss: 0.3735368549823761
setp: 2600, Loss: 0.38309475779533386
setp: 2700, Loss: 0.3559248447418213
setp: 2800, Loss: 0.31901323795318604
setp: 2900, Loss: 0.3211347758769989
setp: 3000, Loss: 0.32005149126052856
setp: 3100, Loss: 0.3190958499908447
setp: 3200, Loss: 0.31975221633911133
setp: 3300, Loss: 0.31930360198020935
setp: 3400, Loss: 0.3205168545246124
setp: 3500, Loss: 0.32013365626335144
setp: 3600, Loss: 0.32012423872947693
setp: 3700, Loss: 0.31911423802375793
setp: 3800, Loss: 0.3206210732460022
setp: 3900, Loss: 0.3209022879600525
setp: 4000, Loss: 0.3199171721935272
setp: 4100, Loss: 0.3190144896507263
setp: 4200, Loss: 0.3199692964553833
setp: 4300, Loss: 0.31968799233436584
setp: 4400, Loss: 0.3465009927749634
setp: 4500, Loss: 0.37046512961387634
setp: 4600, Loss: 0.351340115070343
setp: 4700, Loss: 0.32547205686569214
setp: 4800, Loss: 0.3223536014556885
setp: 4900, Loss: 0.31923791766166687
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9078947368421053
precision: 0.8850574712643678
recall: 0.9506172839506173
F_score: 0.9166666666666666
******fold 3******
[313, 295]
training...
setp: 0, Loss: 0.6764122843742371
setp: 100, Loss: 0.6877076029777527
setp: 200, Loss: 0.6621777415275574
setp: 300, Loss: 0.6342043280601501
setp: 400, Loss: 0.4848810136318207
setp: 500, Loss: 0.4386935830116272
setp: 600, Loss: 0.46440815925598145
setp: 700, Loss: 0.3538808822631836
setp: 800, Loss: 0.4334762394428253
setp: 900, Loss: 0.40830063819885254
setp: 1000, Loss: 0.32185518741607666
setp: 1100, Loss: 0.3229425847530365
setp: 1200, Loss: 0.3212744891643524
setp: 1300, Loss: 0.3216097950935364
setp: 1400, Loss: 0.320114403963089
setp: 1500, Loss: 0.32010069489479065
setp: 1600, Loss: 0.31914466619491577
setp: 1700, Loss: 0.31856274604797363
setp: 1800, Loss: 0.318879634141922
setp: 1900, Loss: 0.31968140602111816
setp: 2000, Loss: 0.32054588198661804
setp: 2100, Loss: 0.3193100690841675
setp: 2200, Loss: 0.3194364011287689
setp: 2300, Loss: 0.3194112181663513
setp: 2400, Loss: 0.4579044282436371
setp: 2500, Loss: 0.359022855758667
setp: 2600, Loss: 0.3275519013404846
setp: 2700, Loss: 0.3481472432613373
setp: 2800, Loss: 0.35883235931396484
setp: 2900, Loss: 0.3181374669075012
setp: 3000, Loss: 0.31742095947265625
setp: 3100, Loss: 0.3183387219905853
setp: 3200, Loss: 0.31941384077072144
setp: 3300, Loss: 0.3185449540615082
setp: 3400, Loss: 0.31918859481811523
setp: 3500, Loss: 0.31890955567359924
setp: 3600, Loss: 0.31851229071617126
setp: 3700, Loss: 0.3188149333000183
setp: 3800, Loss: 0.31970933079719543
setp: 3900, Loss: 0.3201640844345093
setp: 4000, Loss: 0.31871941685676575
setp: 4100, Loss: 0.31948062777519226
setp: 4200, Loss: 0.3193471133708954
setp: 4300, Loss: 0.31975415349006653
setp: 4400, Loss: 0.3195197880268097
setp: 4500, Loss: 0.6726036667823792
setp: 4600, Loss: 0.4904213547706604
setp: 4700, Loss: 0.3351493775844574
setp: 4800, Loss: 0.35469579696655273
setp: 4900, Loss: 0.3399539589881897
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.9936102236421726
F_score: 0.9967948717948718
validating...
acc: 0.8947368421052632
precision: 0.9180327868852459
recall: 0.835820895522388
F_score: 0.875
******fold 4******
[299, 309]
training...
setp: 0, Loss: 0.7044603228569031
setp: 100, Loss: 0.684162974357605
setp: 200, Loss: 0.48130348324775696
setp: 300, Loss: 0.48985475301742554
setp: 400, Loss: 0.34843873977661133
setp: 500, Loss: 0.3965538740158081
setp: 600, Loss: 0.3266105651855469
setp: 700, Loss: 0.33238574862480164
setp: 800, Loss: 0.32494622468948364
setp: 900, Loss: 0.351478636264801
setp: 1000, Loss: 0.3199584484100342
setp: 1100, Loss: 0.3220773935317993
setp: 1200, Loss: 0.34542587399482727
setp: 1300, Loss: 0.3201209306716919
setp: 1400, Loss: 0.32230016589164734
setp: 1500, Loss: 0.3522540032863617
setp: 1600, Loss: 0.34266388416290283
setp: 1700, Loss: 0.3188124895095825
setp: 1800, Loss: 0.3183492124080658
setp: 1900, Loss: 0.3205004632472992
setp: 2000, Loss: 0.3199640214443207
setp: 2100, Loss: 0.31980884075164795
setp: 2200, Loss: 0.31895458698272705
setp: 2300, Loss: 0.31889599561691284
setp: 2400, Loss: 0.31966647505760193
setp: 2500, Loss: 0.3188408613204956
setp: 2600, Loss: 0.3204594850540161
setp: 2700, Loss: 0.3189122676849365
setp: 2800, Loss: 0.31715336441993713
setp: 2900, Loss: 0.31890955567359924
setp: 3000, Loss: 0.3184916079044342
setp: 3100, Loss: 0.3560086786746979
setp: 3200, Loss: 0.39534488320350647
setp: 3300, Loss: 0.32045602798461914
setp: 3400, Loss: 0.3209037482738495
setp: 3500, Loss: 0.32083141803741455
setp: 3600, Loss: 0.3204019367694855
setp: 3700, Loss: 0.319712370634079
setp: 3800, Loss: 0.3223732113838196
setp: 3900, Loss: 0.3212369382381439
setp: 4000, Loss: 0.3204042911529541
setp: 4100, Loss: 0.3201899826526642
setp: 4200, Loss: 0.3209805190563202
setp: 4300, Loss: 0.32102635502815247
setp: 4400, Loss: 0.320068895816803
setp: 4500, Loss: 0.3215940296649933
setp: 4600, Loss: 0.3201139271259308
setp: 4700, Loss: 0.31827494502067566
setp: 4800, Loss: 0.3205370306968689
setp: 4900, Loss: 0.5499307513237
training successfully ended.
validating...
acc: 0.9736842105263158
precision: 0.9896193771626297
recall: 0.9565217391304348
F_score: 0.9727891156462585
validating...
acc: 0.875
precision: 0.9305555555555556
recall: 0.8271604938271605
F_score: 0.8758169934640523
model saved.
avg_acc: 0.8092105263157894, avg_f_score: 0.7094967320261438
-------------subject: 32-------------
==========valence==========
******fold 0******
[297, 311]
training...
setp: 0, Loss: 0.7018668055534363
setp: 100, Loss: 0.6695301532745361
setp: 200, Loss: 0.5722489356994629
setp: 300, Loss: 0.6146866679191589
setp: 400, Loss: 0.6677143573760986
setp: 500, Loss: 0.6172116994857788
setp: 600, Loss: 0.550114095211029
setp: 700, Loss: 0.5539227724075317
setp: 800, Loss: 0.5810413360595703
setp: 900, Loss: 0.4515915513038635
setp: 1000, Loss: 0.4260784983634949
setp: 1100, Loss: 0.3902096450328827
setp: 1200, Loss: 0.4376789629459381
setp: 1300, Loss: 0.4252464771270752
setp: 1400, Loss: 0.5485327839851379
setp: 1500, Loss: 0.3722558319568634
setp: 1600, Loss: 0.4135088324546814
setp: 1700, Loss: 0.4645282030105591
setp: 1800, Loss: 0.3884131610393524
setp: 1900, Loss: 0.41644832491874695
setp: 2000, Loss: 0.3605523109436035
setp: 2100, Loss: 0.4262928366661072
setp: 2200, Loss: 0.43799373507499695
setp: 2300, Loss: 0.4144620895385742
setp: 2400, Loss: 0.49442732334136963
setp: 2500, Loss: 0.372640997171402
setp: 2600, Loss: 0.369015634059906
setp: 2700, Loss: 0.35233113169670105
setp: 2800, Loss: 0.32059165835380554
setp: 2900, Loss: 0.3286432921886444
setp: 3000, Loss: 0.31979721784591675
setp: 3100, Loss: 0.3187149167060852
setp: 3200, Loss: 0.37879669666290283
setp: 3300, Loss: 0.3574310839176178
setp: 3400, Loss: 0.3257818818092346
setp: 3500, Loss: 0.3226017653942108
setp: 3600, Loss: 0.3632788360118866
setp: 3700, Loss: 0.3296668231487274
setp: 3800, Loss: 0.31923115253448486
setp: 3900, Loss: 0.3174208700656891
setp: 4000, Loss: 0.3172042667865753
setp: 4100, Loss: 0.330214262008667
setp: 4200, Loss: 0.32106325030326843
setp: 4300, Loss: 0.35008370876312256
setp: 4400, Loss: 0.3551258444786072
setp: 4500, Loss: 0.321112722158432
setp: 4600, Loss: 0.3367330729961395
setp: 4700, Loss: 0.3172992467880249
setp: 4800, Loss: 0.31696322560310364
setp: 4900, Loss: 0.3473415672779083
training successfully ended.
validating...
acc: 0.9967105263157895
precision: 1.0
recall: 0.9932659932659933
F_score: 0.9966216216216216
validating...
acc: 0.8486842105263158
precision: 0.9285714285714286
recall: 0.7831325301204819
F_score: 0.8496732026143791
******fold 1******
[312, 296]
training...
setp: 0, Loss: 0.7507728338241577
setp: 100, Loss: 0.6296307444572449
setp: 200, Loss: 0.5853955149650574
setp: 300, Loss: 0.5744955539703369
setp: 400, Loss: 0.650691568851471
setp: 500, Loss: 0.5837084650993347
setp: 600, Loss: 0.5738139748573303
setp: 700, Loss: 0.6426123380661011
setp: 800, Loss: 0.5517910718917847
setp: 900, Loss: 0.567410945892334
setp: 1000, Loss: 0.5693175196647644
setp: 1100, Loss: 0.5647558569908142
setp: 1200, Loss: 0.576397180557251
setp: 1300, Loss: 0.4586663246154785
setp: 1400, Loss: 0.39023831486701965
setp: 1500, Loss: 0.42453277111053467
setp: 1600, Loss: 0.4307994842529297
setp: 1700, Loss: 0.4829987585544586
setp: 1800, Loss: 0.44881463050842285
setp: 1900, Loss: 0.47287797927856445
setp: 2000, Loss: 0.3960441052913666
setp: 2100, Loss: 0.4260903596878052
setp: 2200, Loss: 0.3609812557697296
setp: 2300, Loss: 0.41840237379074097
setp: 2400, Loss: 0.40390196442604065
setp: 2500, Loss: 0.3542451560497284
setp: 2600, Loss: 0.3815307915210724
setp: 2700, Loss: 0.3496432304382324
setp: 2800, Loss: 0.35005542635917664
setp: 2900, Loss: 0.3521288335323334
setp: 3000, Loss: 0.36208072304725647
setp: 3100, Loss: 0.37969696521759033
setp: 3200, Loss: 0.3815779983997345
setp: 3300, Loss: 0.3173668384552002
setp: 3400, Loss: 0.34927770495414734
setp: 3500, Loss: 0.3186143934726715
setp: 3600, Loss: 0.3233523964881897
setp: 3700, Loss: 0.38179320096969604
setp: 3800, Loss: 0.4181077182292938
setp: 3900, Loss: 0.34962594509124756
setp: 4000, Loss: 0.3491023778915405
setp: 4100, Loss: 0.3469122648239136
setp: 4200, Loss: 0.35052114725112915
setp: 4300, Loss: 0.3472055494785309
setp: 4400, Loss: 0.31911447644233704
setp: 4500, Loss: 0.3507015109062195
setp: 4600, Loss: 0.3567480444908142
setp: 4700, Loss: 0.4165065884590149
setp: 4800, Loss: 0.3500421643257141
setp: 4900, Loss: 0.3496544063091278
training successfully ended.
validating...
acc: 0.9786184210526315
precision: 0.96
recall: 1.0
F_score: 0.9795918367346939
validating...
acc: 0.8881578947368421
precision: 0.84
recall: 0.9264705882352942
F_score: 0.881118881118881
******fold 2******
[302, 306]
training...
setp: 0, Loss: 0.701877236366272
setp: 100, Loss: 0.5744234323501587
setp: 200, Loss: 0.6071328520774841
setp: 300, Loss: 0.6212693452835083
setp: 400, Loss: 0.709804356098175
setp: 500, Loss: 0.5904867053031921
setp: 600, Loss: 0.5020548701286316
setp: 700, Loss: 0.5729267597198486
setp: 800, Loss: 0.520490288734436
setp: 900, Loss: 0.542872965335846
setp: 1000, Loss: 0.5897186994552612
setp: 1100, Loss: 0.5313273668289185
setp: 1200, Loss: 0.5251587629318237
setp: 1300, Loss: 0.453319251537323
setp: 1400, Loss: 0.587160050868988
setp: 1500, Loss: 0.37513259053230286
setp: 1600, Loss: 0.4213119149208069
setp: 1700, Loss: 0.43111297488212585
setp: 1800, Loss: 0.39210450649261475
setp: 1900, Loss: 0.4031279981136322
setp: 2000, Loss: 0.36307787895202637
setp: 2100, Loss: 0.3524308204650879
setp: 2200, Loss: 0.4096040427684784
setp: 2300, Loss: 0.36274418234825134
setp: 2400, Loss: 0.37066134810447693
setp: 2500, Loss: 0.3479132056236267
setp: 2600, Loss: 0.3311823904514313
setp: 2700, Loss: 0.34760820865631104
setp: 2800, Loss: 0.35765498876571655
setp: 2900, Loss: 0.3547278940677643
setp: 3000, Loss: 0.36680036783218384
setp: 3100, Loss: 0.36871862411499023
setp: 3200, Loss: 0.3372925817966461
setp: 3300, Loss: 0.34486380219459534
setp: 3400, Loss: 0.354525625705719
setp: 3500, Loss: 0.3838503062725067
setp: 3600, Loss: 0.335089772939682
setp: 3700, Loss: 0.3207148015499115
setp: 3800, Loss: 0.33735141158103943
setp: 3900, Loss: 0.31746575236320496
setp: 4000, Loss: 0.3456825017929077
setp: 4100, Loss: 0.3192005157470703
setp: 4200, Loss: 0.32873964309692383
setp: 4300, Loss: 0.3178517520427704
setp: 4400, Loss: 0.3264578878879547
setp: 4500, Loss: 0.3177175521850586
setp: 4600, Loss: 0.3544449210166931
setp: 4700, Loss: 0.352974534034729
setp: 4800, Loss: 0.3189283609390259
setp: 4900, Loss: 0.31833499670028687
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.881578947368421
precision: 0.8846153846153846
recall: 0.8846153846153846
F_score: 0.8846153846153846
******fold 3******
[299, 309]
training...
setp: 0, Loss: 0.6856115460395813
setp: 100, Loss: 0.6396904587745667
setp: 200, Loss: 0.5566838383674622
setp: 300, Loss: 0.5742215514183044
setp: 400, Loss: 0.5940036773681641
setp: 500, Loss: 0.5638480186462402
setp: 600, Loss: 0.5280395150184631
setp: 700, Loss: 0.4999699592590332
setp: 800, Loss: 0.503240168094635
setp: 900, Loss: 0.46357840299606323
setp: 1000, Loss: 0.4022310674190521
setp: 1100, Loss: 0.38559865951538086
setp: 1200, Loss: 0.4025995433330536
setp: 1300, Loss: 0.40977638959884644
setp: 1400, Loss: 0.3266022801399231
setp: 1500, Loss: 0.354518324136734
setp: 1600, Loss: 0.3656977415084839
setp: 1700, Loss: 0.3636227548122406
setp: 1800, Loss: 0.32348665595054626
setp: 1900, Loss: 0.3722527325153351
setp: 2000, Loss: 0.32683372497558594
setp: 2100, Loss: 0.3765883445739746
setp: 2200, Loss: 0.33461278676986694
setp: 2300, Loss: 0.32104355096817017
setp: 2400, Loss: 0.3525451719760895
setp: 2500, Loss: 0.3169519901275635
setp: 2600, Loss: 0.3185795247554779
setp: 2700, Loss: 0.3173525333404541
setp: 2800, Loss: 0.3788681626319885
setp: 2900, Loss: 0.32059457898139954
setp: 3000, Loss: 0.32034963369369507
setp: 3100, Loss: 0.32301902770996094
setp: 3200, Loss: 0.31779977679252625
setp: 3300, Loss: 0.3261697590351105
setp: 3400, Loss: 0.31727135181427
setp: 3500, Loss: 0.35077840089797974
setp: 3600, Loss: 0.3233460485935211
setp: 3700, Loss: 0.3204006552696228
setp: 3800, Loss: 0.39479848742485046
setp: 3900, Loss: 0.3189815282821655
setp: 4000, Loss: 0.3220200836658478
setp: 4100, Loss: 0.31700775027275085
setp: 4200, Loss: 0.3202044367790222
setp: 4300, Loss: 0.32289087772369385
setp: 4400, Loss: 0.3161908686161041
setp: 4500, Loss: 0.3181709051132202
setp: 4600, Loss: 0.3175549805164337
setp: 4700, Loss: 0.4031037390232086
setp: 4800, Loss: 0.3176237940788269
setp: 4900, Loss: 0.31981465220451355
training successfully ended.
validating...
acc: 0.8717105263157895
precision: 0.7946666666666666
recall: 0.9966555183946488
F_score: 0.884272997032641
validating...
acc: 0.868421052631579
precision: 0.8144329896907216
recall: 0.9753086419753086
F_score: 0.8876404494382022
******fold 4******
[310, 298]
training...
setp: 0, Loss: 0.7138701677322388
setp: 100, Loss: 0.6612871289253235
setp: 200, Loss: 0.581556499004364
setp: 300, Loss: 0.6255512833595276
setp: 400, Loss: 0.6524585485458374
setp: 500, Loss: 0.5161559581756592
setp: 600, Loss: 0.5618540048599243
setp: 700, Loss: 0.6375001072883606
setp: 800, Loss: 0.5661535263061523
setp: 900, Loss: 0.5711280107498169
setp: 1000, Loss: 0.5282999277114868
setp: 1100, Loss: 0.4647012948989868
setp: 1200, Loss: 0.4294247329235077
setp: 1300, Loss: 0.41527748107910156
setp: 1400, Loss: 0.5513893365859985
setp: 1500, Loss: 0.4720584750175476
setp: 1600, Loss: 0.5899795293807983
setp: 1700, Loss: 0.48930981755256653
setp: 1800, Loss: 0.42667219042778015
setp: 1900, Loss: 0.514928936958313
setp: 2000, Loss: 0.47828879952430725
setp: 2100, Loss: 0.3974367380142212
setp: 2200, Loss: 0.4183311462402344
setp: 2300, Loss: 0.41439592838287354
setp: 2400, Loss: 0.3204629719257355
setp: 2500, Loss: 0.40685713291168213
setp: 2600, Loss: 0.38089466094970703
setp: 2700, Loss: 0.3502766191959381
setp: 2800, Loss: 0.406566321849823
setp: 2900, Loss: 0.3887792229652405
setp: 3000, Loss: 0.4134780764579773
setp: 3100, Loss: 0.3229627013206482
setp: 3200, Loss: 0.355611115694046
setp: 3300, Loss: 0.3815767765045166
setp: 3400, Loss: 0.3171572983264923
setp: 3500, Loss: 0.45126232504844666
setp: 3600, Loss: 0.3820778727531433
setp: 3700, Loss: 0.3792003095149994
setp: 3800, Loss: 0.35181793570518494
setp: 3900, Loss: 0.3493310213088989
setp: 4000, Loss: 0.3185611069202423
setp: 4100, Loss: 0.35075536370277405
setp: 4200, Loss: 0.45976370573043823
setp: 4300, Loss: 0.3450256884098053
setp: 4400, Loss: 0.348656564950943
setp: 4500, Loss: 0.3791781961917877
setp: 4600, Loss: 0.3472687005996704
setp: 4700, Loss: 0.3790246844291687
setp: 4800, Loss: 0.31773269176483154
setp: 4900, Loss: 0.3185689151287079
training successfully ended.
validating...
acc: 0.9736842105263158
precision: 0.9537037037037037
recall: 0.9967741935483871
F_score: 0.974763406940063
validating...
acc: 0.8157894736842105
precision: 0.7916666666666666
recall: 0.8142857142857143
F_score: 0.8028169014084506
model saved.
avg_acc: 0.8605263157894736, avg_f_score: 0.8611729638390596
==========arousal==========
******fold 0******
[191, 417]
training...
setp: 0, Loss: 0.8023285269737244
setp: 100, Loss: 0.5977827906608582
setp: 200, Loss: 0.6192903518676758
setp: 300, Loss: 0.5677797198295593
setp: 400, Loss: 0.5570501685142517
setp: 500, Loss: 0.3947298526763916
setp: 600, Loss: 0.3587587773799896
setp: 700, Loss: 0.3299117386341095
setp: 800, Loss: 0.3234935700893402
setp: 900, Loss: 0.32782182097435
setp: 1000, Loss: 0.3202528953552246
setp: 1100, Loss: 0.32309702038764954
setp: 1200, Loss: 0.32123130559921265
setp: 1300, Loss: 0.32015761733055115
setp: 1400, Loss: 0.3197140097618103
setp: 1500, Loss: 0.3199995756149292
setp: 1600, Loss: 0.3204204738140106
setp: 1700, Loss: 0.31816187500953674
setp: 1800, Loss: 0.3212927579879761
setp: 1900, Loss: 0.3188433051109314
setp: 2000, Loss: 0.32007187604904175
setp: 2100, Loss: 0.318867027759552
setp: 2200, Loss: 0.31856203079223633
setp: 2300, Loss: 0.3198763132095337
setp: 2400, Loss: 0.3198338449001312
setp: 2500, Loss: 0.317812442779541
setp: 2600, Loss: 0.3708071708679199
setp: 2700, Loss: 0.35143980383872986
setp: 2800, Loss: 0.32608699798583984
setp: 2900, Loss: 0.3220174014568329
setp: 3000, Loss: 0.3361426889896393
setp: 3100, Loss: 0.32558026909828186
setp: 3200, Loss: 0.3190712332725525
setp: 3300, Loss: 0.31967106461524963
setp: 3400, Loss: 0.32039159536361694
setp: 3500, Loss: 0.3198583722114563
setp: 3600, Loss: 0.3208088278770447
setp: 3700, Loss: 0.32176536321640015
setp: 3800, Loss: 0.3213609755039215
setp: 3900, Loss: 0.3213860094547272
setp: 4000, Loss: 0.32118403911590576
setp: 4100, Loss: 0.320320188999176
setp: 4200, Loss: 0.32081857323646545
setp: 4300, Loss: 0.3205263316631317
setp: 4400, Loss: 0.3194861114025116
setp: 4500, Loss: 0.5421348810195923
setp: 4600, Loss: 0.34647029638290405
setp: 4700, Loss: 0.34043464064598083
setp: 4800, Loss: 0.32693013548851013
setp: 4900, Loss: 0.32451915740966797
training successfully ended.
validating...
acc: 0.9736842105263158
precision: 0.926829268292683
recall: 0.9947643979057592
F_score: 0.9595959595959597
validating...
acc: 0.9078947368421053
precision: 0.7446808510638298
recall: 0.9459459459459459
F_score: 0.8333333333333334
******fold 1******
[171, 437]
training...
setp: 0, Loss: 0.7051947116851807
setp: 100, Loss: 0.6379306316375732
setp: 200, Loss: 0.5855792164802551
setp: 300, Loss: 0.4754340648651123
setp: 400, Loss: 0.41560301184654236
setp: 500, Loss: 0.3845606744289398
setp: 600, Loss: 0.47224634885787964
setp: 700, Loss: 0.3915124237537384
setp: 800, Loss: 0.32290369272232056
setp: 900, Loss: 0.38692042231559753
setp: 1000, Loss: 0.317912757396698
setp: 1100, Loss: 0.36160820722579956
setp: 1200, Loss: 0.43216046690940857
setp: 1300, Loss: 0.31820547580718994
setp: 1400, Loss: 0.3187374770641327
setp: 1500, Loss: 0.3179053068161011
setp: 1600, Loss: 0.37170320749282837
setp: 1700, Loss: 0.3160732686519623
setp: 1800, Loss: 0.32434406876564026
setp: 1900, Loss: 0.31911778450012207
setp: 2000, Loss: 0.3172048330307007
setp: 2100, Loss: 0.31848835945129395
setp: 2200, Loss: 0.3179537355899811
setp: 2300, Loss: 0.3204561769962311
setp: 2400, Loss: 0.31748661398887634
setp: 2500, Loss: 0.3182224631309509
setp: 2600, Loss: 0.3167631924152374
setp: 2700, Loss: 0.31789183616638184
setp: 2800, Loss: 0.45009851455688477
setp: 2900, Loss: 0.316969633102417
setp: 3000, Loss: 0.3183583617210388
setp: 3100, Loss: 0.3158714175224304
setp: 3200, Loss: 0.3186239004135132
setp: 3300, Loss: 0.31992393732070923
setp: 3400, Loss: 0.3168189525604248
setp: 3500, Loss: 0.31724098324775696
setp: 3600, Loss: 0.3845173418521881
setp: 3700, Loss: 0.42861902713775635
setp: 3800, Loss: 0.3159160017967224
setp: 3900, Loss: 0.3159601390361786
setp: 4000, Loss: 0.3197249472141266
setp: 4100, Loss: 0.31703412532806396
setp: 4200, Loss: 0.3171146512031555
setp: 4300, Loss: 0.31650659441947937
setp: 4400, Loss: 0.32790225744247437
setp: 4500, Loss: 0.3160419762134552
setp: 4600, Loss: 0.3165380358695984
setp: 4700, Loss: 0.31931835412979126
setp: 4800, Loss: 0.3167249858379364
setp: 4900, Loss: 0.3172842562198639
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9605263157894737
precision: 0.9636363636363636
recall: 0.9298245614035088
F_score: 0.9464285714285715
******fold 2******
[183, 425]
training...
setp: 0, Loss: 0.7259762287139893
setp: 100, Loss: 0.6213177442550659
setp: 200, Loss: 0.5938857793807983
setp: 300, Loss: 0.5515991449356079
setp: 400, Loss: 0.47909122705459595
setp: 500, Loss: 0.49269670248031616
setp: 600, Loss: 0.4584364891052246
setp: 700, Loss: 0.3881024122238159
setp: 800, Loss: 0.37603092193603516
setp: 900, Loss: 0.36350736021995544
setp: 1000, Loss: 0.32788941264152527
setp: 1100, Loss: 0.32316988706588745
setp: 1200, Loss: 0.32072803378105164
setp: 1300, Loss: 0.35060298442840576
setp: 1400, Loss: 0.32255205512046814
setp: 1500, Loss: 0.3212040066719055
setp: 1600, Loss: 0.32019445300102234
setp: 1700, Loss: 0.32224521040916443
setp: 1800, Loss: 0.3214268684387207
setp: 1900, Loss: 0.3203818202018738
setp: 2000, Loss: 0.32245129346847534
setp: 2100, Loss: 0.3207012414932251
setp: 2200, Loss: 0.3198150098323822
setp: 2300, Loss: 0.3196559250354767
setp: 2400, Loss: 0.3193225562572479
setp: 2500, Loss: 0.35338470339775085
setp: 2600, Loss: 0.32445448637008667
setp: 2700, Loss: 0.32663750648498535
setp: 2800, Loss: 0.3190150856971741
setp: 2900, Loss: 0.3181435167789459
setp: 3000, Loss: 0.31925642490386963
setp: 3100, Loss: 0.320538192987442
setp: 3200, Loss: 0.320075124502182
setp: 3300, Loss: 0.3194967210292816
setp: 3400, Loss: 0.3192458152770996
setp: 3500, Loss: 0.3186238706111908
setp: 3600, Loss: 0.31956562399864197
setp: 3700, Loss: 0.31907081604003906
setp: 3800, Loss: 0.3193717896938324
setp: 3900, Loss: 0.3210557997226715
setp: 4000, Loss: 0.31861233711242676
setp: 4100, Loss: 0.40719613432884216
setp: 4200, Loss: 0.3374691605567932
setp: 4300, Loss: 0.32138025760650635
setp: 4400, Loss: 0.31904229521751404
setp: 4500, Loss: 0.31815090775489807
setp: 4600, Loss: 0.3192507028579712
setp: 4700, Loss: 0.3190298080444336
setp: 4800, Loss: 0.31863439083099365
setp: 4900, Loss: 0.31917804479599
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9671052631578947
precision: 0.9347826086956522
recall: 0.9555555555555556
F_score: 0.945054945054945
******fold 3******
[192, 416]
training...
setp: 0, Loss: 0.7047367095947266
setp: 100, Loss: 0.595609188079834
setp: 200, Loss: 0.6383639574050903
setp: 300, Loss: 0.5623849630355835
setp: 400, Loss: 0.47814249992370605
setp: 500, Loss: 0.4075179398059845
setp: 600, Loss: 0.3789137601852417
setp: 700, Loss: 0.4409235119819641
setp: 800, Loss: 0.3366732597351074
setp: 900, Loss: 0.32595205307006836
setp: 1000, Loss: 0.3811883330345154
setp: 1100, Loss: 0.354299932718277
setp: 1200, Loss: 0.3518345355987549
setp: 1300, Loss: 0.35126346349716187
setp: 1400, Loss: 0.35210660099983215
setp: 1500, Loss: 0.3522694408893585
setp: 1600, Loss: 0.35044488310813904
setp: 1700, Loss: 0.3513737618923187
setp: 1800, Loss: 0.3815639317035675
setp: 1900, Loss: 0.36502236127853394
setp: 2000, Loss: 0.3629361093044281
setp: 2100, Loss: 0.328055202960968
setp: 2200, Loss: 0.3197815418243408
setp: 2300, Loss: 0.31876713037490845
setp: 2400, Loss: 0.31840839982032776
setp: 2500, Loss: 0.31919774413108826
setp: 2600, Loss: 0.3188426196575165
setp: 2700, Loss: 0.31891411542892456
setp: 2800, Loss: 0.3209177553653717
setp: 2900, Loss: 0.31928539276123047
setp: 3000, Loss: 0.31998899579048157
setp: 3100, Loss: 0.3204033374786377
setp: 3200, Loss: 0.31981128454208374
setp: 3300, Loss: 0.3193313181400299
setp: 3400, Loss: 0.3190958499908447
setp: 3500, Loss: 0.38810792565345764
setp: 3600, Loss: 0.3327369689941406
setp: 3700, Loss: 0.35399648547172546
setp: 3800, Loss: 0.32049429416656494
setp: 3900, Loss: 0.3189172148704529
setp: 4000, Loss: 0.3181666433811188
setp: 4100, Loss: 0.31884467601776123
setp: 4200, Loss: 0.3191789984703064
setp: 4300, Loss: 0.3194732964038849
setp: 4400, Loss: 0.3190624713897705
setp: 4500, Loss: 0.3192557990550995
setp: 4600, Loss: 0.3192529082298279
setp: 4700, Loss: 0.3204522132873535
setp: 4800, Loss: 0.31917548179626465
setp: 4900, Loss: 0.3196810483932495
training successfully ended.
validating...
acc: 1.0
precision: 1.0
recall: 1.0
F_score: 1.0
validating...
acc: 0.9407894736842105
precision: 0.8648648648648649
recall: 0.8888888888888888
F_score: 0.8767123287671232
******fold 4******
[175, 433]
training...
setp: 0, Loss: 0.6933673024177551
setp: 100, Loss: 0.5502830743789673
setp: 200, Loss: 0.4874497652053833
setp: 300, Loss: 0.3825025260448456
setp: 400, Loss: 0.4068554639816284
setp: 500, Loss: 0.3948817551136017
setp: 600, Loss: 0.4085284471511841
setp: 700, Loss: 0.41358086466789246
setp: 800, Loss: 0.34521180391311646
setp: 900, Loss: 0.3575538694858551
setp: 1000, Loss: 0.37466996908187866
setp: 1100, Loss: 0.3211548924446106
setp: 1200, Loss: 0.3649193346500397
setp: 1300, Loss: 0.32388344407081604
setp: 1400, Loss: 0.3530868887901306
setp: 1500, Loss: 0.32490482926368713
setp: 1600, Loss: 0.320568710565567
setp: 1700, Loss: 0.3551632761955261
setp: 1800, Loss: 0.3183647096157074
setp: 1900, Loss: 0.31706100702285767
setp: 2000, Loss: 0.31862226128578186
setp: 2100, Loss: 0.3221069276332855
setp: 2200, Loss: 0.3170676529407501
setp: 2300, Loss: 0.3182680904865265
setp: 2400, Loss: 0.31737542152404785
setp: 2500, Loss: 0.3192523717880249
setp: 2600, Loss: 0.31663867831230164
setp: 2700, Loss: 0.31713220477104187
setp: 2800, Loss: 0.44565343856811523
setp: 2900, Loss: 0.32190752029418945
setp: 3000, Loss: 0.3192552328109741
setp: 3100, Loss: 0.3163537383079529
setp: 3200, Loss: 0.3196130394935608
setp: 3300, Loss: 0.31642183661460876
setp: 3400, Loss: 0.31737038493156433
setp: 3500, Loss: 0.3240746855735779
setp: 3600, Loss: 0.3868334889411926
setp: 3700, Loss: 0.35169267654418945
setp: 3800, Loss: 0.3346947133541107
setp: 3900, Loss: 0.31736740469932556
setp: 4000, Loss: 0.31643903255462646
setp: 4100, Loss: 0.31698256731033325
setp: 4200, Loss: 0.3184501826763153
setp: 4300, Loss: 0.31700918078422546
setp: 4400, Loss: 0.31735315918922424
setp: 4500, Loss: 0.31620579957962036
setp: 4600, Loss: 0.31726348400115967
setp: 4700, Loss: 0.31656453013420105
setp: 4800, Loss: 0.31748101115226746
setp: 4900, Loss: 0.31884393095970154
training successfully ended.
validating...
acc: 0.8903002309468823
precision: 0.8200757575757576
recall: 1.0
F_score: 0.9011446409989594
validating...
acc: 0.9078947368421053
precision: 0.7910447761194029
recall: 1.0
F_score: 0.8833333333333333
model saved.
avg_acc: 0.9368421052631579, avg_f_score: 0.8969725023834613
D:\学习\毕设\project\train.py:233: MatplotlibDeprecationWarning: Support for FigureCanvases without a required_interactive_framework attribute was deprecated in Matplotlib 3.6 and will be removed two minor releases later.
  fig1 = plt.figure(figsize=(10, 5))
D:\学习\毕设\project\train.py:246: MatplotlibDeprecationWarning: Support for FigureCanvases without a required_interactive_framework attribute was deprecated in Matplotlib 3.6 and will be removed two minor releases later.
  fig2 = plt.figure(figsize=(10, 5))
---------RESULT---------
valence
  acc: [0.84868421 0.89210526 0.94736842 0.86842105 0.90526316 0.95526316
 0.88552632 0.91315789 0.86842105 0.93157895 0.83421053 0.90789474
 0.88815789 0.91578947 0.91315789 0.92236842 0.72105263 0.85526316
 0.91184211 0.91447368 0.81315789 0.93552632 0.90394737 0.875
 0.90394737 0.84736842 0.89868421 0.93552632 0.95131579 0.88552632
 0.82236842 0.86052632]
  average acc: 0.8885279605263158
  f-score: [0.88399749 0.84798371 0.93557556 0.88917741 0.86665592 0.90883302
 0.77691855 0.90400716 0.85084346 0.93132285 0.77715051 0.90224516
 0.9013443  0.91393649 0.9092079  0.93924921 0.3653716  0.66135539
 0.89549444 0.89712402 0.69840136 0.93531146 0.84591832 0.88475305
 0.89425836 0.67366963 0.8429411  0.91270161 0.94450015 0.7093804
 0.64520369 0.86117296]
  average f-score: 0.8376876950406511
arousal
  acc: [0.92894737 0.87763158 0.90921053 0.89210526 0.87105263 0.88684211
 0.90394737 0.93026316 0.88684211 0.92631579 0.88289474 0.92631579
 0.93026316 0.84473684 0.85263158 0.92105263 0.79342105 0.92631579
 0.94605263 0.94210526 0.94605263 0.92631579 0.83815789 0.95131579
 0.94605263 0.87894737 0.90131579 0.92236842 0.90263158 0.92631579
 0.80921053 0.93684211]
  average acc: 0.902014802631579
  f-score: [0.90997274 0.83679766 0.94194903 0.90815803 0.87312231 0.89794373
 0.87730401 0.91097803 0.84438376 0.91764413 0.90631266 0.7820423
 0.76389347 0.64969631 0.7369481  0.92365912 0.59455226 0.89676139
 0.9110309  0.86525984 0.86478027 0.88946084 0.89081292 0.86106641
 0.88752629 0.89616554 0.8320706  0.92098759 0.86407158 0.91812783
 0.70949673 0.8969725 ]
  average f-score: 0.8556234024059234

Process finished with exit code 0
